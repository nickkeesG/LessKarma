{"results": [{"createdAt": null, "postedAt": "2009-09-19T21:05:07.256Z", "modifiedAt": "2020-04-13T04:38:13.297Z", "url": null, "title": "Reason as memetic immune disorder", "slug": "reason-as-memetic-immune-disorder", "viewCount": null, "lastCommentedAt": "2022-05-07T02:56:36.420Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "PhilGoetz", "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder", "pageUrlRelative": "/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder", "linkUrl": "https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder", "postedAtFormatted": "Saturday, September 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reason%20as%20memetic%20immune%20disorder&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReason%20as%20memetic%20immune%20disorder%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHaqgTNnFzD7NGLMx%2Freason-as-memetic-immune-disorder%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reason%20as%20memetic%20immune%20disorder%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHaqgTNnFzD7NGLMx%2Freason-as-memetic-immune-disorder", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaHaqgTNnFzD7NGLMx%2Freason-as-memetic-immune-disorder", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1394, "htmlBody": "<h4>A prophet is without dishonor in his hometown<br /></h4>\n<p>I'm reading the book \"<a href=\"http://www.ajjacobs.com/books/yolb.asp\">The Year of Living Biblically</a>,\" by A.J. Jacobs.&nbsp; He tried to follow all of the commandments in the Bible (Old and New Testaments) for one year.&nbsp; He quickly found that</p>\n<ul>\n<li>a lot of the rules in the Bible are impossible, illegal, or embarassing to follow nowadays; like wearing tassels, tying your money to yourself, stoning adulterers, not eating fruit from a tree less than 5 years old, and not touching anything that a menstruating woman has touched; and</li>\n<li>this didn't seem to bother more than a handful of the one-third to one-half of Americans who claim the Bible is the word of God.</li>\n</ul>\n<p>You may have noticed that people who convert to religion after the age of 20 or so are generally more zealous than people who grew up with the same religion.&nbsp; People who grow up with a religion learn how to cope with its more inconvenient parts by partitioning them off, rationalizing them away, or forgetting about them.&nbsp; Religious communities actually protect their members from religion in one sense - they develop an unspoken consensus on which parts of their religion members can legitimately ignore.&nbsp; New converts sometimes try to actually do what their religion tells them to do.</p>\n<p>I remember many times growing up when missionaries described the crazy things their new converts in remote areas did on reading the Bible for the first time - they refused to be taught by female missionaries; they insisted on following Old Testament commandments; they decided that everyone in the village had to confess all of their sins against everyone else in the village; they prayed to God and assumed He would do what they asked; they believed the Christian God would cure their diseases.&nbsp; We would always laugh a little at the naivete of these new converts; I could barely hear the tiny voice in my head saying <em>but they're just believing that the Bible means what it says...</em></p>\n<p>How do we explain the blindness of people to a religion they grew up with?<a id=\"more\"></a></p>\n<h4>Cultural immunity<br /></h4>\n<p>Europe has lived with Christianity for nearly 2000 years.&nbsp; European culture has co-evolved with Christianity.&nbsp; Culturally, memetically, it's developed a tolerance for Christianity.&nbsp; These new Christian converts, in Uganda, Papua New Guinea, and other remote parts of the world, were being exposed to Christian memes for the first time, and had no immunity to them.</p>\n<p>The history of religions sometimes resembles the history of viruses.&nbsp; Judaism and Islam were both highly virulent when they first broke out, driving the first generations of their people to conquer (Islam) or just slaughter (Judaism) everyone around them for the sin of not being them.&nbsp; They both grew more sedate over time.&nbsp; (Christianity was pacifist at the start, as it arose in a conquered people.&nbsp; When the Romans adopted it, it didn't make them any more militaristic than they already were.)</p>\n<p>The mechanism isn't the same as for diseases, which can't be too virulent or they kill their hosts.&nbsp; Religions don't generally kill their hosts.&nbsp; I suspect that, over time, individual selection favors those who are less zealous.&nbsp; The point is that a culture develops antibodies for the particular religions it co-exists with - attitudes and practices that make them less virulent.</p>\n<p>I have a theory that \"radical Islam\" is not native Islam, but Westernized Islam.&nbsp; Over half of 75 Muslim terrorists studied by <a href=\"http://www.nytimes.com/2005/06/14/opinion/14bergen.html\">Bergen &amp; Pandey 2005 in the New York Times</a> had gone to a Western college.&nbsp; (Only 9% had attended madrassas.)&nbsp; A very small percentage of all Muslims have received a Western college education. &nbsp; When someone lives all their life in a Muslim country, they're not likely to be hit with the urge to travel abroad and blow something up.&nbsp; But when someone from an Islamic nation goes to Europe for college, and comes back with Enlightenment ideas about reason and seeking logical closure over beliefs, and applies them to the Koran, <em>then</em> you have troubles.&nbsp; They have lost their cultural immunity.</p>\n<p>I'm also reminded of a talk I attended by one of the Dalai Lama's assistants.&nbsp; This was not slick, Westernized Buddhism; this was saffron-robed fresh-off-the-plane-from-Tibet Buddhism.&nbsp; He spoke about his beliefs, and then took questions.&nbsp; People began asking him about some of the implications of his belief that life, love, feelings, and the universe as a whole are inherently bad and undesirable.&nbsp; He had great difficulty comprehending the questions - not because of his English, I think; but because the notion of taking a belief expressed in one context, and applying it in another, seemed completely new to him.&nbsp; To him, knowledge came in units; each unit of knowledge was a story with a conclusion and a specific application.&nbsp; (No wonder they think understanding Buddhism takes decades.)&nbsp; He seemed not to have the idea that these units could interact; that you could take an idea from one setting, and explore its implications in completely different settings.&nbsp; This may have been an extreme form of cultural immunity.</p>\n<p>We think of Buddhism as a peaceful, caring religion.&nbsp; A religion that teaches that striving and status are useless is probably going to be more peaceful than one that teaches that the whole world must be brought under its dominion; and religions that lack the power of the state (e.g., the early Christians) are usually gentler than those with the power of life and death.&nbsp; But much of Buddhism's kind public face may be due to cultural norms that prevent Buddhists from connecting all of their dots.&nbsp; Today, we worry about Islamic terrorists.&nbsp; A hundred years from now, we'll worry about Buddhist physicists.</p>\n<h4>Reason as immune suppression</h4>\n<p>The reason I bring this up is that intelligent people sometimes do things more stupid than stupid people are capable of.&nbsp; There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes.&nbsp; The trouble is that these antibodies are not logical.&nbsp; On the contrary; these antibodies are often highly <em>illogical</em>.&nbsp; They are the blind spots that let us live with a dangerous meme without being impelled to action by it.&nbsp; The dangerous effects of these memes are most obvious with religion; but I think there is an element of this in many social norms.&nbsp; We have a powerful cultural norm in America that says that all people are equal (whatever that means); originally, this powerful and ambiguous belief was counterbalanced by a set of blind spots so large that this belief did not even impel us to free slaves or let women or non-property-owners vote.&nbsp; We have another cultural norm that says that hard work reliably and exclusively leads to success; and another set of blind spots that prevent this belief from turning us all into Objectivists.</p>\n<p>A little reason can be a dangerous thing.&nbsp; The landscape of rationality is not smooth; there is no guarantee that removing one false belief will improve your reasoning instead of degrading it.&nbsp; Sometimes, reason lets us see the dangerous aspects of our memes, but not the blind spots that protect us from them.&nbsp; Sometimes, it lets us see the blind spots, but not the dangerous memes.&nbsp; Either of these ways, reason can lead an individual to be unbalanced, no longer adapted to their memetic environment, and free to follow previously-dormant memes through to their logical conclusions.&nbsp; &nbsp; (To paraphrase Steve Weinberg, \"For a smart person to do something truly stupid, they need a theory.\"&nbsp; Actually, I could have quoted him directly - \"stupid\" is just a lighter shade of \"evil\".&nbsp; Communism and fascism both begin by exercising complete control over the memetic environment, in order to create a new man stripped of cultural immunity, who will do whatever they tell him to.)</p>\n<h4>The vaccines: Updating and emotions<br /></h4>\n<p>How can you tell when you have removed one set of blind spots from your reasoning without removing its counterbalances?&nbsp; One heuristic to counter this loss of immunity, is to be very careful when you find yourself deviating from everyone around you.&nbsp; I deviate from those around me all the time, so I admit I haven't found this heuristic to be very helpful.</p>\n<p>Another heuristic is to listen to your feelings.&nbsp; If your conclusions seem repulsive to you, you may have stripped yourself of cognitive immunity to something dangerous.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LNsEBXoFdAy8yzvbw": 13, "Ng8Gice9KNkncxqcj": 7, "9YFoDPFwMoWthzgkY": 6, "x5TtBDjRg9egvg9gm": 5, "5f5c37ee1b5cdee568cfb1f2": 7, "T4GgauaEfp6dHsR5P": 3, "KWFhr6A2dHEb6wmWJ": 5}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aHaqgTNnFzD7NGLMx", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 330, "baseScore": 385, "extendedScore": null, "score": 0.000607, "legacy": true, "legacyId": "1595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 385, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h4 id=\"A_prophet_is_without_dishonor_in_his_hometown\">A prophet is without dishonor in his hometown<br></h4>\n<p>I'm reading the book \"<a href=\"http://www.ajjacobs.com/books/yolb.asp\">The Year of Living Biblically</a>,\" by A.J. Jacobs.&nbsp; He tried to follow all of the commandments in the Bible (Old and New Testaments) for one year.&nbsp; He quickly found that</p>\n<ul>\n<li>a lot of the rules in the Bible are impossible, illegal, or embarassing to follow nowadays; like wearing tassels, tying your money to yourself, stoning adulterers, not eating fruit from a tree less than 5 years old, and not touching anything that a menstruating woman has touched; and</li>\n<li>this didn't seem to bother more than a handful of the one-third to one-half of Americans who claim the Bible is the word of God.</li>\n</ul>\n<p>You may have noticed that people who convert to religion after the age of 20 or so are generally more zealous than people who grew up with the same religion.&nbsp; People who grow up with a religion learn how to cope with its more inconvenient parts by partitioning them off, rationalizing them away, or forgetting about them.&nbsp; Religious communities actually protect their members from religion in one sense - they develop an unspoken consensus on which parts of their religion members can legitimately ignore.&nbsp; New converts sometimes try to actually do what their religion tells them to do.</p>\n<p>I remember many times growing up when missionaries described the crazy things their new converts in remote areas did on reading the Bible for the first time - they refused to be taught by female missionaries; they insisted on following Old Testament commandments; they decided that everyone in the village had to confess all of their sins against everyone else in the village; they prayed to God and assumed He would do what they asked; they believed the Christian God would cure their diseases.&nbsp; We would always laugh a little at the naivete of these new converts; I could barely hear the tiny voice in my head saying <em>but they're just believing that the Bible means what it says...</em></p>\n<p>How do we explain the blindness of people to a religion they grew up with?<a id=\"more\"></a></p>\n<h4 id=\"Cultural_immunity\">Cultural immunity<br></h4>\n<p>Europe has lived with Christianity for nearly 2000 years.&nbsp; European culture has co-evolved with Christianity.&nbsp; Culturally, memetically, it's developed a tolerance for Christianity.&nbsp; These new Christian converts, in Uganda, Papua New Guinea, and other remote parts of the world, were being exposed to Christian memes for the first time, and had no immunity to them.</p>\n<p>The history of religions sometimes resembles the history of viruses.&nbsp; Judaism and Islam were both highly virulent when they first broke out, driving the first generations of their people to conquer (Islam) or just slaughter (Judaism) everyone around them for the sin of not being them.&nbsp; They both grew more sedate over time.&nbsp; (Christianity was pacifist at the start, as it arose in a conquered people.&nbsp; When the Romans adopted it, it didn't make them any more militaristic than they already were.)</p>\n<p>The mechanism isn't the same as for diseases, which can't be too virulent or they kill their hosts.&nbsp; Religions don't generally kill their hosts.&nbsp; I suspect that, over time, individual selection favors those who are less zealous.&nbsp; The point is that a culture develops antibodies for the particular religions it co-exists with - attitudes and practices that make them less virulent.</p>\n<p>I have a theory that \"radical Islam\" is not native Islam, but Westernized Islam.&nbsp; Over half of 75 Muslim terrorists studied by <a href=\"http://www.nytimes.com/2005/06/14/opinion/14bergen.html\">Bergen &amp; Pandey 2005 in the New York Times</a> had gone to a Western college.&nbsp; (Only 9% had attended madrassas.)&nbsp; A very small percentage of all Muslims have received a Western college education. &nbsp; When someone lives all their life in a Muslim country, they're not likely to be hit with the urge to travel abroad and blow something up.&nbsp; But when someone from an Islamic nation goes to Europe for college, and comes back with Enlightenment ideas about reason and seeking logical closure over beliefs, and applies them to the Koran, <em>then</em> you have troubles.&nbsp; They have lost their cultural immunity.</p>\n<p>I'm also reminded of a talk I attended by one of the Dalai Lama's assistants.&nbsp; This was not slick, Westernized Buddhism; this was saffron-robed fresh-off-the-plane-from-Tibet Buddhism.&nbsp; He spoke about his beliefs, and then took questions.&nbsp; People began asking him about some of the implications of his belief that life, love, feelings, and the universe as a whole are inherently bad and undesirable.&nbsp; He had great difficulty comprehending the questions - not because of his English, I think; but because the notion of taking a belief expressed in one context, and applying it in another, seemed completely new to him.&nbsp; To him, knowledge came in units; each unit of knowledge was a story with a conclusion and a specific application.&nbsp; (No wonder they think understanding Buddhism takes decades.)&nbsp; He seemed not to have the idea that these units could interact; that you could take an idea from one setting, and explore its implications in completely different settings.&nbsp; This may have been an extreme form of cultural immunity.</p>\n<p>We think of Buddhism as a peaceful, caring religion.&nbsp; A religion that teaches that striving and status are useless is probably going to be more peaceful than one that teaches that the whole world must be brought under its dominion; and religions that lack the power of the state (e.g., the early Christians) are usually gentler than those with the power of life and death.&nbsp; But much of Buddhism's kind public face may be due to cultural norms that prevent Buddhists from connecting all of their dots.&nbsp; Today, we worry about Islamic terrorists.&nbsp; A hundred years from now, we'll worry about Buddhist physicists.</p>\n<h4 id=\"Reason_as_immune_suppression\">Reason as immune suppression</h4>\n<p>The reason I bring this up is that intelligent people sometimes do things more stupid than stupid people are capable of.&nbsp; There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes.&nbsp; The trouble is that these antibodies are not logical.&nbsp; On the contrary; these antibodies are often highly <em>illogical</em>.&nbsp; They are the blind spots that let us live with a dangerous meme without being impelled to action by it.&nbsp; The dangerous effects of these memes are most obvious with religion; but I think there is an element of this in many social norms.&nbsp; We have a powerful cultural norm in America that says that all people are equal (whatever that means); originally, this powerful and ambiguous belief was counterbalanced by a set of blind spots so large that this belief did not even impel us to free slaves or let women or non-property-owners vote.&nbsp; We have another cultural norm that says that hard work reliably and exclusively leads to success; and another set of blind spots that prevent this belief from turning us all into Objectivists.</p>\n<p>A little reason can be a dangerous thing.&nbsp; The landscape of rationality is not smooth; there is no guarantee that removing one false belief will improve your reasoning instead of degrading it.&nbsp; Sometimes, reason lets us see the dangerous aspects of our memes, but not the blind spots that protect us from them.&nbsp; Sometimes, it lets us see the blind spots, but not the dangerous memes.&nbsp; Either of these ways, reason can lead an individual to be unbalanced, no longer adapted to their memetic environment, and free to follow previously-dormant memes through to their logical conclusions.&nbsp; &nbsp; (To paraphrase Steve Weinberg, \"For a smart person to do something truly stupid, they need a theory.\"&nbsp; Actually, I could have quoted him directly - \"stupid\" is just a lighter shade of \"evil\".&nbsp; Communism and fascism both begin by exercising complete control over the memetic environment, in order to create a new man stripped of cultural immunity, who will do whatever they tell him to.)</p>\n<h4 id=\"The_vaccines__Updating_and_emotions\">The vaccines: Updating and emotions<br></h4>\n<p>How can you tell when you have removed one set of blind spots from your reasoning without removing its counterbalances?&nbsp; One heuristic to counter this loss of immunity, is to be very careful when you find yourself deviating from everyone around you.&nbsp; I deviate from those around me all the time, so I admit I haven't found this heuristic to be very helpful.</p>\n<p>Another heuristic is to listen to your feelings.&nbsp; If your conclusions seem repulsive to you, you may have stripped yourself of cognitive immunity to something dangerous.</p>", "sections": [{"title": "A prophet is without dishonor in his hometown", "anchor": "A_prophet_is_without_dishonor_in_his_hometown", "level": 1}, {"title": "Cultural immunity", "anchor": "Cultural_immunity", "level": 1}, {"title": "Reason as immune suppression", "anchor": "Reason_as_immune_suppression", "level": 1}, {"title": "The vaccines: Updating and emotions", "anchor": "The_vaccines__Updating_and_emotions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "180 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 180, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 22, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2009-09-19T21:05:07.256Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-20T12:08:10.702Z", "modifiedAt": null, "url": null, "title": "How to use SMILE to solve Bayes Nets", "slug": "how-to-use-smile-to-solve-bayes-nets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:29.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M4urAEehmHzMMpDCF/how-to-use-smile-to-solve-bayes-nets", "pageUrlRelative": "/posts/M4urAEehmHzMMpDCF/how-to-use-smile-to-solve-bayes-nets", "linkUrl": "https://www.lesswrong.com/posts/M4urAEehmHzMMpDCF/how-to-use-smile-to-solve-bayes-nets", "postedAtFormatted": "Sunday, September 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20use%20SMILE%20to%20solve%20Bayes%20Nets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20use%20SMILE%20to%20solve%20Bayes%20Nets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4urAEehmHzMMpDCF%2Fhow-to-use-smile-to-solve-bayes-nets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20use%20SMILE%20to%20solve%20Bayes%20Nets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4urAEehmHzMMpDCF%2Fhow-to-use-smile-to-solve-bayes-nets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM4urAEehmHzMMpDCF%2Fhow-to-use-smile-to-solve-bayes-nets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 990, "htmlBody": "<p>This is an account of downloading and using SMILE, a free-as-in-beer-but-not-open-source <a href=\"http://en.wikipedia.org/wiki/Bayesian_network\">bayes net</a> library. SMILE powers GENIE, a <a href=\"http://genie.sis.pitt.edu/screenshots.html\">graphical</a> bayes net tool. SMILE can do a lot of things, but I only used the simplest features - building a network and, given evidence, inferring probability distributions on the unobserved features.</p>\n<p><a id=\"more\"></a></p>\n<p>After <a href=\"http://genie.sis.pitt.edu/downloads.html\">registering</a>, there is a download page, containing binary distributions for various operating systems and architectures, including a .tar.gz intended for 32-bit Linux, which is what I picked. Possibly someone else can describe the experience of using it from Windows. The tarball turned out to contain about 50 .h files (C++ header files) and two .a files (statically linked libraries). I also downloaded the \"smilehelp.zip\", which turned out to contain a .chm (compiled hypertext help). I didn't have a viewer at the time, but <a href=\"http://gnochm.sourceforge.net/\">gnochm</a> worked out of the box. Most of the smilehelp text was only moderately helpful, but the appendices with tutorial code were very helpful.</p>\n<p>As I understand it, there are a lot of different things that one could be interested in doing (influence diagrams, learning parameters, learning structure) I did the very simplest thing, building a net and performing inference on it. This is basically \"tutorial 2\" from the reference manual. (Note: The tutorial 2 code is not correct, but the appendix tutorial 2 code is).</p>\n<p>The experience of using the library was not magically smooth, but then, the number of large-ish old-ish libraries which are magically smooth to use can probably be counted on the fingers of one foot. It might be obvious to some, but the friction impressed on me that even if some people are getting solid results that they trust from SMILE, I need to verify my <em>use</em> of it independently. Even straightforward uses of a trusted library could be buggy or (less likely) an unusual usage could reveal bugs internal to the library which existing users never encounter.</p>\n<p>The two annoyances that I encountered were <a href=\"http://www.ccs.neu.edu/research/demeter/demeter-method/LawOfDemeter/general-formulation.html\">Law of Demeter</a> ugliness and the flat conditional probability tables. The Law of Demeter is the principle that interfaces should not reveal their implementations, or alternatively, clambering through object graphs is a bad idea. It is a version of the general programmer injunction: \"Decouple!\". Of course, once an interface is published, it is very hard to break backwards compatibility and revise away from it - there are almost certainly best-option-at-the-time historical reasons for the SMILE interface to be the way it is.</p>\n<p>In order to understand the issue with the conditional probability tables, consider writing a data structure to support interactive editing of a Bayes Net. The graph-editing parts are relatively standard and straightforward, but each node has a table of probabilities associated with it, and the table's dimensionality (number of dimensions, as well as size of dimensions) depends on the node's parents, and they might change as the graph is edited. This sounds somewhat tricky.<br /><br />The data structure that SMILE seems to have picked is a flat sequence of probabilities. The flat sequence is interpreted as a grid through the standard <a href=\"http://en.wikipedia.org/wiki/Mixed_radix\">mixed-radix</a> arithmetic. The first parent is the most-significant dimension, then the next parent, and so on, and the node itself is the least-significant. So if a node X has two parents, Y and Z, in that order, and all three of them have two outcomes, true and false, in that order, then the probabilities go like this:</p>\n<pre>P(X|Y&amp;Z), P(~X|Y&amp;Z), P(X|Y&amp;~Z), P(~X|Y&amp;~Z), P(X|~Y&amp;Z), P(~X|~Y&amp;Z), P(X|~Y&amp;~Z), P(~X|~Y&amp;~Z)</pre>\n<p>But of course, that's the symbolic form. Inside someone's code (or inside the XML), you might see something like this:</p>\n<pre>0.8 0.2 0.9 0.1 0 1 1 0</pre>\n<p>There is an iterator-like class, sysCoords, which encapsulates the mixed-radix logic one uses to navigate the matrix, but I couldn't find a clean symbolic way to access or modify \"P(foo=bar|baz)\". Maybe most of the users learn to read these sequences, or maybe most users learn parameters or use noisy-or models.</p>\n<p><img src=\"http://www.johnicholas.com/absent_minded_driver.png\" alt=\"A graph with four nodes (Strategy, X, Y, Outcome), and directed arcs from Strategy to X and Y, from X to Y and Outcome, and from Y to Outcome.\" width=\"241\" height=\"312\" align=\"left\" />The net that I built is quite simple - an absent-minded driver picks one of three strategies at random - always take the exit, always continue, or flip a fair coin. After that, the driver tries to get home in the <a href=\"/lw/182/the_absentminded_driver/\">usual</a> way, following that strategy. To build a net with SMILE it is probably helpful to be very clear on the network structure, and in particular a standard linear order (<a href=\"http://en.wikipedia.org/wiki/Topological_sorting\">topological sort</a>) of the nodes in your net.</p>\n<p>Very probably, it is almost never appropriate to build a net like this inside a executable. SMILE makes parsing a net very easy, either from XDSL format, or any of several competing formats. Building a net as an XML document would be easier to write, easier to read, and more portable. Note: The smilehelp document claims that to use the XML format, you need a separate library, but this is no longer correct. Another small quirk, which took some debugging: strings that are names of things should not contain spaces.</p>\n<p>The best part about SMILE interface is the inference step - one method call on the network: \"UpdateBeliefs()\". There is a standard \"Lauritzen\" algorithm for computing beliefs. Setting evidence requires the usual (LoD violating) graph navigation (from the network to the node to the node's value), but it seemed relatively straightforward. Of course, I may have grown accustomed to the interface.</p>\n<p>Once you get comfortable with the flat conditional tables, it seems very feasible to use SMILE as a bayes-net calculator. I look forward to the day when people discussing probability on LessWrong routinely exchange bayes nets, either in XDSL some other standard format.</p>\n<p>If you're interested in reading my code, it's <a href=\"http://www.johnicholas.com/absent_minded_driver.tar.bz2\">here</a>. Since the SMILE authors want people to register before downloading the library (I presume so they can use the numbers to justify their research to grant-giving institutions), I didn't include the library in my tarball, so you'll have to download it separately.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M4urAEehmHzMMpDCF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 15, "extendedScore": null, "score": 5.236852369072903e-07, "legacy": true, "legacyId": "1598", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GfHdNfqxe3cSCfpHL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-20T17:21:08.994Z", "modifiedAt": null, "url": null, "title": "The Finale of the Ultimate Meta Mega Crossover", "slug": "the-finale-of-the-ultimate-meta-mega-crossover", "viewCount": null, "lastCommentedAt": "2021-12-02T04:00:45.577Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XSqYe5Rsqq4TR7ryL/the-finale-of-the-ultimate-meta-mega-crossover", "pageUrlRelative": "/posts/XSqYe5Rsqq4TR7ryL/the-finale-of-the-ultimate-meta-mega-crossover", "linkUrl": "https://www.lesswrong.com/posts/XSqYe5Rsqq4TR7ryL/the-finale-of-the-ultimate-meta-mega-crossover", "postedAtFormatted": "Sunday, September 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Finale%20of%20the%20Ultimate%20Meta%20Mega%20Crossover&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Finale%20of%20the%20Ultimate%20Meta%20Mega%20Crossover%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSqYe5Rsqq4TR7ryL%2Fthe-finale-of-the-ultimate-meta-mega-crossover%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Finale%20of%20the%20Ultimate%20Meta%20Mega%20Crossover%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSqYe5Rsqq4TR7ryL%2Fthe-finale-of-the-ultimate-meta-mega-crossover", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXSqYe5Rsqq4TR7ryL%2Fthe-finale-of-the-ultimate-meta-mega-crossover", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 114, "htmlBody": "<p>So I'd intended this story as a bit of utterly deranged fun, but it got out of control and ended up as a deep philosophical exploration, and now those of you who care will have to wade through the insanity.&nbsp; I'm sorry.&nbsp; I just can't seem to help myself.</p>\n<p>I know that writing crossover fanfiction is considered one of the lower levels to which an author can sink.&nbsp; Alas, I've always been a sucker for audacity, and I <em>am </em>the sort of person who couldn't resist trying to top the entire... but never mind, you can see for yourself.</p>\n<p>Click on to read my latest story and first fanfiction, a <a href=\"http://www.fanfiction.net/s/5389450/1/The_Finale_of_the_Ultimate_Meta_Mega_Crossover\">Vernor Vinge x Greg Egan crackfic</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XSqYe5Rsqq4TR7ryL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 49, "extendedScore": null, "score": 7.8e-05, "legacy": true, "legacyId": "1600", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 49, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-22T17:25:47.094Z", "modifiedAt": null, "url": null, "title": "Ethics as a black box function", "slug": "ethics-as-a-black-box-function", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:17.376Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LCnryDjnrM7Kcikx7/ethics-as-a-black-box-function", "pageUrlRelative": "/posts/LCnryDjnrM7Kcikx7/ethics-as-a-black-box-function", "linkUrl": "https://www.lesswrong.com/posts/LCnryDjnrM7Kcikx7/ethics-as-a-black-box-function", "postedAtFormatted": "Tuesday, September 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20as%20a%20black%20box%20function&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20as%20a%20black%20box%20function%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCnryDjnrM7Kcikx7%2Fethics-as-a-black-box-function%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20as%20a%20black%20box%20function%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCnryDjnrM7Kcikx7%2Fethics-as-a-black-box-function", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCnryDjnrM7Kcikx7%2Fethics-as-a-black-box-function", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p>(Edited to add: See also <a href=\"/lw/18l/ethics_as_a_black_box_function/14ha\">this addendum</a>.)</p>\r\n<p>I commented on Facebook that I think our ethics is three-tiered. There are the things we imagine we consider right, the things we consider right, and the things we actually do. I was then asked to elaborate between the difference of the first two.<br /><br />For the first one, I was primarily thinking about people following any idealized, formal ethical theories. People considering themselves act utilitarians, for instance. Yet when presented with real-life situations, they may often reply that the right course of action is different than what the purely act utilitarian framework would imply, taking into account things such as keeping promises and so on. Of course, a rule utilitarian would avoid that particular trap, but in general nobody is a pure follower of any formal ethical theory.<br /><br />Now, people who don't even try to follow any formal ethical systems probably have a closer match between their first and second categories. But I recently came to view as our moral intuitions as a function that takes the circumstances of the situation as an input and gives a moral judgement as an output. We do not have access to the inner workings of that function, though we can and do try to build models that attempt to capture its inner workings. Still, as our understanding of the function is incomplete, our models are bound to sometimes produce mistaken predictions.<a id=\"more\"></a></p>\r\n<p>Based on our model, we imagine (if not thinking about the situations too much) that in certain kinds of situations we would arrive at a specific judgement, but a closer examination of them reveals that the function outputs the opposite value. For instance, we might think that maximizing total welfare is always for the best, but then realize that we don't actually want to maximize total welfare if the people we consider our friends would be hurt. This might happen even if you weren't explicitly following any formal theory of ethics. And if *actually* faced with that situation, we might end up acting selfishly instead.<br /><br />This implies that people pick the moral frameworks which are best at justifying the ethical intuitions they already had. Of course, we knew that much already (even if we sometimes fail to apply it - I was previously puzzled over why so many smart people reject all forms of utilitarianism, as ultimately everyone has to perform <em>some</em> sort of expected utility calculations in order to make moral decisions at all, but then realized it had little to do with utilitarianism's merits as such). Some of us attempt to reprogram their moral intuitions, by taking those models and following them even when they fail to predict the correct response of the moral function. With enough practice, our intuitions may be shifted towards the consciously held stance, which may be a good or bad thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LCnryDjnrM7Kcikx7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 14, "extendedScore": null, "score": 5.242205348512205e-07, "legacy": true, "legacyId": "1605", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-23T14:54:32.288Z", "modifiedAt": null, "url": null, "title": "Avoiding doomsday: a \"proof\" of the self-indication assumption", "slug": "avoiding-doomsday-a-proof-of-the-self-indication-assumption", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:38.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5A9x74mgCwJwSg4sN/avoiding-doomsday-a-proof-of-the-self-indication-assumption", "pageUrlRelative": "/posts/5A9x74mgCwJwSg4sN/avoiding-doomsday-a-proof-of-the-self-indication-assumption", "linkUrl": "https://www.lesswrong.com/posts/5A9x74mgCwJwSg4sN/avoiding-doomsday-a-proof-of-the-self-indication-assumption", "postedAtFormatted": "Wednesday, September 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Avoiding%20doomsday%3A%20a%20%22proof%22%20of%20the%20self-indication%20assumption&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAvoiding%20doomsday%3A%20a%20%22proof%22%20of%20the%20self-indication%20assumption%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9x74mgCwJwSg4sN%2Favoiding-doomsday-a-proof-of-the-self-indication-assumption%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Avoiding%20doomsday%3A%20a%20%22proof%22%20of%20the%20self-indication%20assumption%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9x74mgCwJwSg4sN%2Favoiding-doomsday-a-proof-of-the-self-indication-assumption", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5A9x74mgCwJwSg4sN%2Favoiding-doomsday-a-proof-of-the-self-indication-assumption", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 754, "htmlBody": "<p><span class=\"head\"><span class=\"author\">EDIT: This post has been superceeded by <a href=\"/lw/4fl/dead_men_tell_tales_falling_out_of_love_with_sia/\">this one</a>.<br /></span></span></p>\n<p>The doomsday argument, in its simplest form, claims that since 2/3 of all humans will be in the final 2/3 of all humans, we should conclude it is more likely we are in the final two thirds of all humans who&rsquo;ve ever lived, than in the first third. In our current state of quasi-exponential population growth, this would mean that we are likely very close to the final end of humanity. The argument gets somewhat more sophisticated than that, but that's it in a nutshell.<br /><br />There are many immediate rebuttals that spring to mind - there is something about the doomsday argument that brings out the certainty in most people that it <strong>must </strong>be wrong. But nearly all those supposed rebuttals are erroneous (see Nick Bostrom's book <a href=\"http://www.anthropic-principle.com/book/\">Anthropic Bias: Observation Selection Effects in Science and Philosophy</a>). Essentially the only consistent low-level rebuttal to the doomsday argument is to use the self indication assumption (SIA).<br /><br />The non-intuitive form of SIA simply says that since you exist, it is more likely that your universe contains many observers, rather than few; the more intuitive formulation is that you should consider yourself as a random observer drawn from the space of possible observers (weighted according to the probability of that observer existing).<br /><br />Even in that form, it may seem counter-intuitive; but I came up with a series of small steps leading from a generally accepted result straight to the SIA. This clinched the argument for me. The starting point is:</p>\n<p><strong>A </strong>- A hundred people are created in a hundred rooms. Room 1 has a red door (on the outside), the outsides of all other doors are blue. You wake up in a room, fully aware of these facts; what probability should you put on being inside a room with a blue door?</p>\n<p>Here, the probability is certainly 99%. But now consider the situation:<br /><a id=\"more\"></a><br /><strong>B</strong> - same as before, but an hour after you wake up, it is announced that a coin will be flipped, and if it comes up heads, the guy behind the red door will be killed, and if it comes up tails, everyone behind a blue door will be killed. A few minutes later, it is announced that whoever was to be killed has been killed. What are your odds of being blue-doored now?<br /><br />There should be no difference from A; since your odds of dying are exactly fifty-fifty whether you are blue-doored or red-doored, your probability estimate should not change upon being updated. The further modifications are then:<br /><br /><strong>C</strong> - same as B, except the coin is flipped before you are created (the killing still happens later).<br /><br /><strong>D</strong> - same as C, except that you are only made aware of the rules of the set-up after the people to be killed have already been killed.<br /><br /><strong>E</strong> - same as C, except the people to be killed are killed before awakening.<br /><br /><strong>F</strong> - same as C, except the people to be killed are simply not created in the first place.<br /><br />I see no justification for changing your odds as you move from A to F; but 99% odd of being blue-doored at F is precisely the SIA: you are saying that a universe with 99 people in it is 99 times more probable than a universe with a single person in it.<br /><br />If you can't see any flaw in the chain either, then you can rest easy, knowing the human race is no more likely to vanish than objective factors indicate (ok, maybe you won't rest that easy, in fact...)<br /><br />(Apologies if this post is preaching to the choir of flogged dead horses along well beaten tracks: I was unable to keep up with Less Wrong these past few months, so may be going over points already dealt with!)</p>\n<p>&nbsp;</p>\n<p>EDIT: Corrected the language in the presentation of the SIA, after <span class=\"head\"><span class=\"author\"><a id=\"author_t1_14ig\" href=\"/user/SilasBarta\">SilasBarta</a>'s comments.</span></span></p>\n<p><span class=\"head\"><span class=\"author\">EDIT2: There are some objections to the transfer from D to C. Thus I suggest sliding in C' and C'' between them; C' is the same as D, execpt those due to die have the situation explained to them before being killed; C'' is the same as C' except those due to die are told \"you will be killed\" before having the situation explained to them (and then being killed).</span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5A9x74mgCwJwSg4sN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 26, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "1611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 238, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LnearFbA4thE646tR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-23T15:10:41.114Z", "modifiedAt": null, "url": null, "title": "Anthropic reasoning and correlated decision making", "slug": "anthropic-reasoning-and-correlated-decision-making", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:06.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ab6gFDx4LxxySY3E8/anthropic-reasoning-and-correlated-decision-making", "pageUrlRelative": "/posts/Ab6gFDx4LxxySY3E8/anthropic-reasoning-and-correlated-decision-making", "linkUrl": "https://www.lesswrong.com/posts/Ab6gFDx4LxxySY3E8/anthropic-reasoning-and-correlated-decision-making", "postedAtFormatted": "Wednesday, September 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20reasoning%20and%20correlated%20decision%20making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20reasoning%20and%20correlated%20decision%20making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAb6gFDx4LxxySY3E8%2Fanthropic-reasoning-and-correlated-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20reasoning%20and%20correlated%20decision%20making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAb6gFDx4LxxySY3E8%2Fanthropic-reasoning-and-correlated-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAb6gFDx4LxxySY3E8%2Fanthropic-reasoning-and-correlated-decision-making", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 551, "htmlBody": "<p>There seems to be some confusion on how to deal with correlated decision making - such as with absent-minded drivers and multiple copies of yourself; any situation in which many agents will allreach the same decision. Building on Nick Bostrom's division-of-responsibility principle mentioned in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">Outlawing Anthropics</a>, I propose the following correlated decision principle:<br /><br /><strong>CDP</strong>: If you are part of a group of N individuals whose decision is perfectly correlated, then you should reason as if you had a 1/N chance of being the dictator of the group (in which case your decision is applied to all) and a (N-1)/N chance of being a dictatee (in which case your decision is ignored).<br /><br />What justification could there be to this principle? A simple thought experiment: imagine if you were one of N individuals who had to make a decision in secret. One of the decisions is opened at random, the others are discarded, and each person has his mind modified to believe that what was decided was in fact what they decided. This process is called a \"dictator filter\".<br /><br />If you apply this dictator filter any situation S, then in \"S + dictator filter\", you should reason as in the CDP. If you apply it to perfectly correlated decision making, however, then the dictator filter changes nothing at all to anyone's decision - hence we should treat \"perfectly correlated\" as isomorphic to \"perfectly correlated + dictator filter\", which establishes the CDP.<br /><br />Used alongside the <a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">SIA</a>, this solves many puzzles on this blog, without needing advanced decision theory.<br /><a id=\"more\"></a><br />For instance, the situation in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">Outlawing Anthropics</a> is simple: the SIA implies the 90% view, giving you a 90% chance of being in a group of 18, and a 10% of being in a group of two. Then you were offered a deal in which $3 is stolen from the red rooms, and $1 given to the green rooms. The initial expected gain from accepting the deal was -$20; the problem came that when you woke up in a green room, you were far more likely to be in the group of 18, giving an expected gain of +$5.60. The CDP cancels out this effect, returning you to an expected individual gain of -$2, and a global expected gain of -$20.<br /><br />The <a href=\"/lw/182/the_absentminded_driver/\">Absent-Minded driver</a> problem is even more interesting, and requires a more subtle reasoning. The SIA implies that if your probability of continuing is p, then the chance that you are at the first intersection is 1/(1+p), while the chance that you are at the second is p/(1+p). Using these number, it appears that your expected gain is [p<sup>2</sup> + 4(1-p)p + p(p+4(1-p))]/(1+p) which is 2[p<sup>2</sup> + 4(1-p)p]/(1+p).<br /><br />If you were the dictator, deciding the behaviour at both intersections, your expected gain would be 1+p times this amount, since the driver at the first intersection exist with probability 1, while that at the second exists with probability p. Since there are N=2 individuals, the CDP thus cancels both the 2 and the (1+p) factors, returning the situation to the expected gain of p<sup>2</sup> -4(1-p)p, maximised at p = 2/3.<br /><br />The CDP also solves the issues in my old <a href=\"/lw/5k/sleeping_beauty_gets_counterfactually_mugged/\">Sleeping Beauty</a> problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ab6gFDx4LxxySY3E8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 5.244392608097673e-07, "legacy": true, "legacyId": "1612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTEkZNLrmycNuCNYq", "5A9x74mgCwJwSg4sN", "GfHdNfqxe3cSCfpHL", "CcjcCYYEB5KNHCpEZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-24T11:45:54.121Z", "modifiedAt": null, "url": null, "title": "Boredom vs. Scope Insensitivity", "slug": "boredom-vs-scope-insensitivity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.233Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6S4Lf2tCMWAfbGtdt/boredom-vs-scope-insensitivity", "pageUrlRelative": "/posts/6S4Lf2tCMWAfbGtdt/boredom-vs-scope-insensitivity", "linkUrl": "https://www.lesswrong.com/posts/6S4Lf2tCMWAfbGtdt/boredom-vs-scope-insensitivity", "postedAtFormatted": "Thursday, September 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boredom%20vs.%20Scope%20Insensitivity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoredom%20vs.%20Scope%20Insensitivity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S4Lf2tCMWAfbGtdt%2Fboredom-vs-scope-insensitivity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boredom%20vs.%20Scope%20Insensitivity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S4Lf2tCMWAfbGtdt%2Fboredom-vs-scope-insensitivity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6S4Lf2tCMWAfbGtdt%2Fboredom-vs-scope-insensitivity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 979, "htmlBody": "<p class=\"MsoNormal\">How much would you pay to see a typical movie? How much would you pay to see it 100 times?</p>\n<p class=\"MsoNormal\">How much would you pay to save a random stranger&rsquo;s life? How much would you pay to save 100 strangers?</p>\n<p class=\"MsoNormal\">If you are like a typical human being, your answers to both sets of questions probably exhibit failures to aggregate value linearly. In the first case, we call it boredom. In the second case, we call it scope insensitivity.</p>\n<p class=\"MsoNormal\">Eliezer has argued on <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">separate</a> <a href=\"/lw/xr/in_praise_of_boredom/\">occasions</a> that one should be regarded as an obvious error to be corrected, and the other is a gift bestowed by evolution, to be treasured and safeguarded. Here, I propose to consider them side by side, and see what we can learn by doing that.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">(Eliezer sometimes treats scope insensitivity as a simple arithmetical error that the brain commits, like in this quote: &ldquo;the brain can't successfully multiply by eight and get a larger quantity than it started with&rdquo;. Considering that the brain has little trouble multiplying by eight in other contexts and the fact that scope insensitivity starts with numbers as low as 2, it seems more likely that it&rsquo;s not an error but an adaptation, just like boredom.)</p>\n<p class=\"MsoNormal\">The nonlinearities in boredom and scope insensitivity both occur at two different levels. On the affective or hedonic level, our emotions fail to respond in a linear fashion to the relevant input. Watching a movie twice doesn&rsquo;t give us twice the pleasure of watching it once, nor does saving two lives feel twice as good as saving one life. And on the level of decision making and revealed preferences, we fail to act as if our utilities scale linearly with the number of times we watch a movie, or the number of lives we save.</p>\n<p class=\"MsoNormal\">Note that these two types of nonlinearities are logically distinct, and it seems quite possible to have one without the other. The refrain &ldquo;<a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">shut up and multiply</a>&rdquo; is an illustration of this. It exhorts (or reminds) us to value lives directly and linearly in our utility functions and decisions, instead of only valuing the sublinear emotions we get from saving lives.</p>\n<p class=\"MsoNormal\">We sometimes <a href=\"/lw/v5/inner_goodness/\">feel bad</a> that we aren&rsquo;t sufficiently empathetic. Similarly, we feel bad about some of our boredoms. For example, consider a music lover who regrets no longer being as deeply affected by his favorite piece of music as when he first heard it, or a wife who wishes she was still as deeply in love with her husband as she once was. If they had the opportunity, they may very well choose to edit those boredoms away.</p>\n<p class=\"MsoNormal\">Self-modification is dangerous, and the bad feelings we sometimes have about the way we feel were never meant to be used directly as a guide to change the wetware behind those feelings. If we choose to edit some of our boredoms away, while leaving others intact, we may find ourselves doing the one thing that we&rsquo;re not bored with, over and over again. Similarly, if we choose to edit our scope insensitivity away completely, we may find ourselves sacrificing all of our other values to help random strangers, who in turn care little about ourselves or our values. I bet that in the end, if we reach reflective equilibrium after careful consideration, we&rsquo;ll decide to reduce some of our boredoms, but not eliminate them completely, and become more empathetic, but not to the extent of full linearity.</p>\n<p class=\"MsoNormal\">But that&rsquo;s a problem for a later time. What should we do today, when we can&rsquo;t change the way our emotions work to a large extent? Well, first, nobody argues for &ldquo;shut up and multiply&rdquo; in the case of boredom. It&rsquo;s clearly absurd to watch a movie 100 times, as if you&rsquo;re not bored with it, when you actually are. We simply don&rsquo;t value the experience of watching a movie apart from whatever positive emotions it gives us.</p>\n<p class=\"MsoNormal\">Do we value saving lives independently of the good feelings we get from it? Some people seem to (or claim to), while others don&rsquo;t (or claim not to). For those who do, some value (or claim to value) the lives saved linearly, and others don&rsquo;t. So the analogy between boredom and scope insensitivity starts to break down here. But perhaps we can still make some final use out of it: whatever arguments we have to support the position that lives saved ought to be valued apart from our feelings, and linearly, we better make sure those arguments do not apply equally well to the case of boredom.</p>\n<p class=\"MsoNormal\">Here&rsquo;s an example of what I mean. Consider the question of why we should consider the lives of random strangers to be valuable. You may be tempted to answer that we know those lives are valuable because we feel good when we consider the possibility of saving a stranger&rsquo;s life. But we also feel good when we watch a well-made movie, and we don&rsquo;t consider the watching of a movie to be valuable apart from that good feeling. This suggests that the answer is not a very good one.</p>\n<h4 class=\"MsoNormal\"><a name=\"cooperation\"></a>Appendix: Altruism vs. Cooperation</h4>\n<p class=\"MsoNormal\">This may be a good time to point out/clarify that I consider cooperation, but not altruism, to be a core element of rationality. By &ldquo;cooperation&rdquo; I mean techniques that can be used by groups of individuals with disparate values to better approximate the ideals of group rationality (such as Pareto optimality). <a href=\"/lw/v5/inner_goodness/\">According to Eliezer</a>,</p>\n<blockquote>\n<p class=\"MsoNormal\">\"altruist\" is someone who chooses between actions according to the criterion of others' welfare</p>\n</blockquote>\n<p>In cooperation, we often takes others' welfare into account when choosing between actions, but this \"altruism\" is conditional on others reciprocating and taking our welfare into account in return. I expect that what Eliezer and others here mean by \"altruist\" must consider others&rsquo; welfare to be a terminal value, not just an instrumental one, and therefore cooperation and true altruism are non-overlapping concepts. (Please correct me if I'm wrong about this.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 1, "5f5c37ee1b5cdee568cfb154": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6S4Lf2tCMWAfbGtdt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 50, "extendedScore": null, "score": 8.2e-05, "legacy": true, "legacyId": "1626", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 50, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r5MSQ83gtbjWRBDWJ", "WMDy4GxbyYkNrbmrs", "EA39yRbhBbrccXnHi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-24T20:18:58.532Z", "modifiedAt": null, "url": null, "title": "Minneapolis Meetup, This Saturday (26th) at 3:00 PM, University of Minnesota ", "slug": "minneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MNEQn2qsxMGAgmQH2/minneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "pageUrlRelative": "/posts/MNEQn2qsxMGAgmQH2/minneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "linkUrl": "https://www.lesswrong.com/posts/MNEQn2qsxMGAgmQH2/minneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "postedAtFormatted": "Thursday, September 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Minneapolis%20Meetup%2C%20This%20Saturday%20(26th)%20at%203%3A00%20PM%2C%20University%20of%20Minnesota%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMinneapolis%20Meetup%2C%20This%20Saturday%20(26th)%20at%203%3A00%20PM%2C%20University%20of%20Minnesota%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNEQn2qsxMGAgmQH2%2Fminneapolis-meetup-this-saturday-26th-at-3-00-pm-university%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Minneapolis%20Meetup%2C%20This%20Saturday%20(26th)%20at%203%3A00%20PM%2C%20University%20of%20Minnesota%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNEQn2qsxMGAgmQH2%2Fminneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMNEQn2qsxMGAgmQH2%2Fminneapolis-meetup-this-saturday-26th-at-3-00-pm-university", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 103, "htmlBody": "<p>The <a href=\"/lw/18a/minneapolis_meetup_survey_of_interest/\" target=\"_self\">Minneapolis meetup</a> will take place this Saturday the 26th at 3:00 PM in Coffman Memorial Union at the University of Minnesota. Directions and address can be found <a href=\"http://www.coffman.umn.edu/about/directions.php\" target=\"_blank\">here</a>, and we'll be meeting inside near Starbucks,&nbsp;next to the northwest entrance to the bookstore, and&nbsp;underneath the northwest entrance to the Union. That entrance is a separate glass-sided structure right next to Washington Avenue, not the multistory brick building, but you can get to Starbucks from either entrance; it's on level 2.</p>\n<p>RSVP and we'll keep an extra eye out for you.&nbsp;You can contact me at f.adamek@yahoo.com or at (952) 217-0505, or Justin Shovelain at jshovelainsiai@gmail.com.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MNEQn2qsxMGAgmQH2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 5.247325605849748e-07, "legacy": true, "legacyId": "1628", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NbDFqQ887mgKQACS7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-24T21:00:41.266Z", "modifiedAt": null, "url": null, "title": "The utility curve of the human population", "slug": "the-utility-curve-of-the-human-population", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.471Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dclayh", "createdAt": "2009-03-07T01:16:38.966Z", "isAdmin": false, "displayName": "dclayh"}, "userId": "E7xnxwP5EPuGiP99X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9ige644KxQ2oiBuKy/the-utility-curve-of-the-human-population", "pageUrlRelative": "/posts/9ige644KxQ2oiBuKy/the-utility-curve-of-the-human-population", "linkUrl": "https://www.lesswrong.com/posts/9ige644KxQ2oiBuKy/the-utility-curve-of-the-human-population", "postedAtFormatted": "Thursday, September 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20utility%20curve%20of%20the%20human%20population&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20utility%20curve%20of%20the%20human%20population%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ige644KxQ2oiBuKy%2Fthe-utility-curve-of-the-human-population%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20utility%20curve%20of%20the%20human%20population%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ige644KxQ2oiBuKy%2Fthe-utility-curve-of-the-human-population", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9ige644KxQ2oiBuKy%2Fthe-utility-curve-of-the-human-population", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 592, "htmlBody": "<blockquote>\n<p>\"Whoever saves a single life, it is as if he had saved the whole world.\"</p>\n<p>&nbsp; &mdash;The Talmud, Sanhedrin 4:5</p>\n</blockquote>\n<p>That was the epigraph Eliezer used on a <a href=\"/lw/hx/one_life_against_the_world/\">perfectly nice post</a> reminding us to shut up and multiply when valuing human lives, rather than relying on the (roughly) logarithmic amount of warm fuzzies we'd receive.&nbsp; Implicit in the expected utility calculation is the idea that the value of human lives scales linearly: indeed, Eliezer explicitly says, \"I agree that one human life is of unimaginably high value. I also hold that two human lives are twice as unimaginably valuable.\"</p>\n<p>However, in a <a href=\"/lw/196/boredom_vs_scope_insensitivity/14i3\">comment</a> on Wei Dai's brilliant recent <a href=\"/lw/196/boredom_vs_scope_insensitivity/\">post</a> comparing boredom and altruism, Vladimir Nesov points out that \"you can value lives sublinearly\" and still make an expected utility calculation rather than relying on warm-fuzzy intuition.&nbsp; This got me thinking about just what the functional form of U(N<sub>living-persons</sub>) might be.&nbsp;</p>\n<p><a id=\"more\"></a>Attacking from the high end (the \"marginal\" calculation), it seems to me that the utility of human lives is actually superlinear to a modest degree<sup>1</sup>; that is, U(N+1)-U(N) &gt; U(N)-U(N-1).&nbsp; As an example, consider a parent and young child.&nbsp; If you allow one of them to die, not only do you end that life, but you make the other one <em>significantly worse off</em>.&nbsp; But this generalizes: the marginal person (on average) produces positive net value to society (though being an employee, friend, spouse, etc.) in addition to accruing their own utilons, and economies of scale dictate that adding another person allows a little more specialization and hence a little more efficiency.&nbsp; I.e., the larger the pool of potential co-workers/friends/spouses is, the pickier everyone can be, and the better matches they're likely to end up with.&nbsp; Steven Landsburg (in <a href=\"http://www.amazon.com/Fair-Play-Steven-Landsburg/dp/0684827557/ref=sr_1_4?ie=UTF8&amp;s=books&amp;qid=1253822564&amp;sr=8-4\"><em>Fair Play</em></a>) uses a version of this argument to conclude that children have positive externalities and therefore people on average have fewer children than would be optimal.&nbsp;</p>\n<p>In societies with readily available birth control, that is.&nbsp; And naturally, in societies which are insufficiently technological for each marginal person to be able to make a contribution, however indirect, to (e.g.) the food output, it's quite easy for the utility of lives to be sublinear, which is the classical Malthusian problem, and still very much with us in the poorer areas of the world. (In fact, I was recently <a href=\"http://www.cracked.com/article/131_5-great-things-you-didnt-know-came-from-horrific-tragedies/\">informed</a> by a humor website that the Black Death had some very positive effects for medieval Europe.)</p>\n<p>Now let's consider the other end of the problem (the \"inductive\" calculation).&nbsp; As an example let's assume that humanity has been mostly supplanted by AIs or some alien species.&nbsp; I would certainly prefer to have at least one human still alive: such a person could represent humanity (and by extension, me), carry on our culture, values and perspective on the universe, and generally push for our agenda.&nbsp; Adding a second human seems far less important&mdash;but still quite important, since social interactions (with other humans) are such a vital component of humanity. So adding a third person would be less important still, and so on. A sublinear utility function.</p>\n<p>So are the marginal calculation and the inductive calculation inconsistent?&nbsp; I don't think so: it's perfectly possible to have a utility function whose first derivative is complex and non-monotonic.&nbsp; The two calculations are simply presenting two different terms of the function, which are dominant in different regimes.&nbsp; Moreover the linear approximation is probably good enough for most ordinary circumstances; let's just remember that it is an approximation.</p>\n<p>&nbsp;</p>\n<p>1. Note that in these arguments I'm averaging over \"ability to create utility\" (not to mention \"capacity to experience utility\").</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9ige644KxQ2oiBuKy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 5.24739561920725e-07, "legacy": true, "legacyId": "1627", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xiHy3kFni8nsxfdcP", "6S4Lf2tCMWAfbGtdt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-25T17:21:50.038Z", "modifiedAt": null, "url": null, "title": "Solutions to Political Problems As Counterfactuals", "slug": "solutions-to-political-problems-as-counterfactuals", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2p8BWvcJvKkXGMsch/solutions-to-political-problems-as-counterfactuals", "pageUrlRelative": "/posts/2p8BWvcJvKkXGMsch/solutions-to-political-problems-as-counterfactuals", "linkUrl": "https://www.lesswrong.com/posts/2p8BWvcJvKkXGMsch/solutions-to-political-problems-as-counterfactuals", "postedAtFormatted": "Friday, September 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Solutions%20to%20Political%20Problems%20As%20Counterfactuals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASolutions%20to%20Political%20Problems%20As%20Counterfactuals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2p8BWvcJvKkXGMsch%2Fsolutions-to-political-problems-as-counterfactuals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Solutions%20to%20Political%20Problems%20As%20Counterfactuals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2p8BWvcJvKkXGMsch%2Fsolutions-to-political-problems-as-counterfactuals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2p8BWvcJvKkXGMsch%2Fsolutions-to-political-problems-as-counterfactuals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1769, "htmlBody": "<blockquote>\n<p><em>A mathematician wakes up to find his house on fire. He frantically looks around before seeing the fire extinguisher on the far wall of the room. \"Aha!\" he says, \"a solution exists!\" and goes back to sleep.</em></p>\n<p>&nbsp;&nbsp;&nbsp; -- Popular math students' joke</p>\n</blockquote>\n<p>There has been <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">much discussion</a> of <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">coulds, woulds, and shoulds</a> recently. Agents imagine different counterfactual states of their own minds or actions, then select the most desirable. Something similar seems to happen during political discussions, but the multiplicity of agents involved muddles it a little.<br /><br />I recently read a letter to the editor in my local paper. The city was launching a public education campaign against binge drinking, and this letter writer thought that all the billboards and lectures and what-not were a waste of time. She said that instead of a flashy and expensive public awareness campaign, the real solution was for binge drinkers to take responsibility for their own actions and learn that there were ways to have fun that didn't involve alcohol.<br /><br />This struck me as a misguided line of thinking. Consider this analogy: pretend that the city government was, instead, increasing the number of police to prevent terrorist attacks. And that the writer was arguing that no, we shouldn't get the police involved: the real solution was for terrorists to stop being so violent and attacking people. This would be a weird and completely useless response.<br /><br />Attempts to solve political problems are counterfactuals in the same way attempts to solve other problems are. In Newcomb's Problem, I modify the \"my decision\" node and watch what happens to the \"money in the box\" and \"money I get\" nodes. When I say \"Increasing local police would prevent terrorist attacks,\" I am modifying the \"local police\" node, and positing that this would have a certain inhibitory effect on the connected \"terrorist attacks\" node.<br /><br />The hypothetical second letter-writer's argument, then, is that if we counterfactually modified the \"terrorists' attitude\" node, then the \"terrorist attacks\" node would change. This is correct but useless.<a id=\"more\"></a></p>\n<p>But it's harder to see exactly why it's useless. Consider the original argument \"We should raise the number of police and train them in counter-terrorism techniques.\" In this case, I would be counterfactually modifying the attitudes of (for example) the chief of police. But I'm not the chief of police, any more than I'm Osama bin Laden. If I'm going to let myself modify the chief of police's attitude just because it would be convenient, I might as well let myself turn Osama into a pretty decent guy. Yet \"the chief of police should train more policemen\" sounds like a potential solution, whereas \"terrorists should be nicer\" doesn't.<br /><br />Here's one possible resolution to the problem: it's much more likely I could convince the chief of police to train more policemen than that I could convince terrorists to be nonviolent. Since the police chief shares my goal of stopping terrorists, all I need to do is tell them why training more police would accomplish this goal, and the problem is solved. So the reason why \"train more police\" counts as a solution is that, as soon as the statement and the evidence supporting the statement reaches the right person, the problem will be solved.<br /><br />In this case, changing the \"chief of police\" node is really a proxy for changing the \"my actions\" node. The solution could be rephrased as \"You or I could go to the chief of police and sugget they add more policemen, thus preventing further terrorist attacks.\" Counterfactually changing your own actions is entirely kosher. This also throws into greater relief the problems with \"You or I could find and approach all terrorists and convince them to be nicer,\" or \"You or I could go to every single binge drinker in the city and convince them to be more responsible.\"<sup>1</sup><br /><br />Actually, when phrased like that, the binge drinking example doesn't sound so bad. Add a comprehensive plan for doing it, enough funding to reach them all, and some idea of how you're going to phrase the \"be more responsible\" point, and it sounds like, well, a grassroots public awareness campaign. Which is kind of ironic, seeing as the letter started out as an argument against a public awareness campaign, and maybe a sign that I'm taking the Principle of Charity too far here.</p>\n<p><strong>...in more realistic situations</strong><br /><br />It's more complex when there are only small probabilities of your own actions having any effect, but the principle stays the same. For example, I recently heard a doctor say that a single-payer system would best solve the US' health care woes, but since that was politically infeasible he was backing Obama's plan. This one doctor's support will have minimal effect on the chances of Obama's plan passing, but it will have even less of an effect on the chances of single-payer passing. If the expected utilities multiply out in such a way as to make supporting Obama more likely to gain more utility than supporting single-payer, the doctor is justified in his strategy of support for Obama's plan.<br /><br />One more example from real history I learned recently. Suppose you are a Communist, and your fellow Reds are proposing ways to create a socialist paradise. One says that you must incite the workers to violent revolution. Another says you must petition the current government to support labor reform laws. A third says you must petition the current government to <em>oppose</em> labor reform laws.<br /><br />Before you expel the third communist from the Party, let them make their argument. They say that the Party doesn't have enough resources to incite violent revolution, and the workers don't want to revolt anyway. Counterfactually modifying the \"workers' actions\" node to a revolutionary state is a waste of time, because there's no link between any modification of your own actions and that node reaching the state you want. Likewise, modifying \"government policy\" is useless, because the Communists don't have any clout in the government, so even if you found a wonderful value for that node that would make all workers happy forever, you couldn't change it. <br /><br />Instead, she says, oppose labor reform laws. These are already unpopular, and even a small party like the Communists would probably have enough power to get them shot down. When there's no labor reform, workers will get angrier and angrier, until they gradually revolt and overthrow the system, getting you what you wanted in the first place.<br /><br />There were communists in the early 1900s who actually tried this third approach. It didn't work, but I admire their thought processes. They ignored solutions that would never happen, and found an action they thought they could enact, that they thought they <em>would</em> raise the chance of revolution significantly. Compare this kind of cunning to the vapidity of the letter-writer who says \"Binge drinkers should become more responsible.\"</p>\n<p><strong>...as an unrealized ideal</strong></p>\n<p>I like this way of viewing the problem, because it explains why a certain class of argument feels wrong: arguments that go \"The solution to binge drinking is more personal responsibility\" or \"The solution to poverty is for the poor to work harder,\" or so on. But do people actually think this way?<br /><br />The most glaring reason to believe they don't is that most people who \"solve\" societal problems have no interest in actually enacting their solution. The attitude is something like \"Hey, if the federal government passed a single-payer health plan, then all our health troubles would be over!\" and then don't bother to write a letter to their representative about it or even convince their next door neighbor.<br /><br />For reasons that have been discussed ad nauseum on Overcoming Bias, politics is very much a signalling game. In particular, it seems to be a game in which you counterfactually propose different states of the \"government policy\" node and explain why these would have the best effects, and whoever can give the best explanation gets rewarded with higher status. Sometimes you're also allowed to edit the policies of large private organizations, or of influential individuals. In this case, the problem with the original letter writer wasn't just that she had no plan to enact her solution, but that she was breaking the rule which said that you're only allowed to play with relatively unified, powerful organizations, and not things like \"the set of all binge drinkers\".<br /><br />In a way, this isn't so bad. When enough people play this game, their opinions get out to the voters, consumers, politicians, and business leaders, and eventually do change government and private policy.<br /><br />The point is that if your goal is to actually personally affect things in a direct, immediate way, you can't <em>just</em> apply the rules of this game without thinking beforehand. Communists, when discussing politics for \"fun\", would never say \"I think the government should oppose labor reforms,\" but that might be the winning move for them when they're actually trying to increase utility. Likewise, libertarians spend a lot of time discussing different ways the government could implement libertarian policy, but when they actually have to take action, the best choice might be seasteads or charter cities or something else that doesn't involve policy at all.<br /><br />And if you are content to just play the game, at least keep it interesting. No fair counterfactually editing things like \"terrorists' behavior\" or \"poor people's work ethic\" or \"how responsible binge drinkers are\".</p>\n<p><strong>Footnotes</strong></p>\n<p><strong>1: </strong>Or, here's an alternate interpretation of \"Binge drinkers should be more responsible\": it's not worth trying to prevent binge drinking, because binge drinkers could prevent it themselves if they were more responsible, so it's their own fault. This is not illogical, but applying the argument to a case like \"Drunk drivers should be more responsible\" would be. There, even if we have no sympathy for drunk drivers, we still need to prevent drunk driving because many of the victims are innocents. The other issue is that people process the two statements \"The solution to binge drinking is for binge drinkers to be more responsible\" and \"We don't need to solve binge drinking; it's the drinkers' own fault and we need not care\" differently; the first sounds wise and reasonable, the second callous. For both these reasons, I don't think this interpretation is entirely what the original&nbsp; letter writer, or other people who use this sort&nbsp; of argument, are thinking of.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2p8BWvcJvKkXGMsch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 44, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "1585", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p><em>A mathematician wakes up to find his house on fire. He frantically looks around before seeing the fire extinguisher on the far wall of the room. \"Aha!\" he says, \"a solution exists!\" and goes back to sleep.</em></p>\n<p>&nbsp;&nbsp;&nbsp; -- Popular math students' joke</p>\n</blockquote>\n<p>There has been <a href=\"/lw/174/decision_theory_why_we_need_to_reduce_could_would/\">much discussion</a> of <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">coulds, woulds, and shoulds</a> recently. Agents imagine different counterfactual states of their own minds or actions, then select the most desirable. Something similar seems to happen during political discussions, but the multiplicity of agents involved muddles it a little.<br><br>I recently read a letter to the editor in my local paper. The city was launching a public education campaign against binge drinking, and this letter writer thought that all the billboards and lectures and what-not were a waste of time. She said that instead of a flashy and expensive public awareness campaign, the real solution was for binge drinkers to take responsibility for their own actions and learn that there were ways to have fun that didn't involve alcohol.<br><br>This struck me as a misguided line of thinking. Consider this analogy: pretend that the city government was, instead, increasing the number of police to prevent terrorist attacks. And that the writer was arguing that no, we shouldn't get the police involved: the real solution was for terrorists to stop being so violent and attacking people. This would be a weird and completely useless response.<br><br>Attempts to solve political problems are counterfactuals in the same way attempts to solve other problems are. In Newcomb's Problem, I modify the \"my decision\" node and watch what happens to the \"money in the box\" and \"money I get\" nodes. When I say \"Increasing local police would prevent terrorist attacks,\" I am modifying the \"local police\" node, and positing that this would have a certain inhibitory effect on the connected \"terrorist attacks\" node.<br><br>The hypothetical second letter-writer's argument, then, is that if we counterfactually modified the \"terrorists' attitude\" node, then the \"terrorist attacks\" node would change. This is correct but useless.<a id=\"more\"></a></p>\n<p>But it's harder to see exactly why it's useless. Consider the original argument \"We should raise the number of police and train them in counter-terrorism techniques.\" In this case, I would be counterfactually modifying the attitudes of (for example) the chief of police. But I'm not the chief of police, any more than I'm Osama bin Laden. If I'm going to let myself modify the chief of police's attitude just because it would be convenient, I might as well let myself turn Osama into a pretty decent guy. Yet \"the chief of police should train more policemen\" sounds like a potential solution, whereas \"terrorists should be nicer\" doesn't.<br><br>Here's one possible resolution to the problem: it's much more likely I could convince the chief of police to train more policemen than that I could convince terrorists to be nonviolent. Since the police chief shares my goal of stopping terrorists, all I need to do is tell them why training more police would accomplish this goal, and the problem is solved. So the reason why \"train more police\" counts as a solution is that, as soon as the statement and the evidence supporting the statement reaches the right person, the problem will be solved.<br><br>In this case, changing the \"chief of police\" node is really a proxy for changing the \"my actions\" node. The solution could be rephrased as \"You or I could go to the chief of police and sugget they add more policemen, thus preventing further terrorist attacks.\" Counterfactually changing your own actions is entirely kosher. This also throws into greater relief the problems with \"You or I could find and approach all terrorists and convince them to be nicer,\" or \"You or I could go to every single binge drinker in the city and convince them to be more responsible.\"<sup>1</sup><br><br>Actually, when phrased like that, the binge drinking example doesn't sound so bad. Add a comprehensive plan for doing it, enough funding to reach them all, and some idea of how you're going to phrase the \"be more responsible\" point, and it sounds like, well, a grassroots public awareness campaign. Which is kind of ironic, seeing as the letter started out as an argument against a public awareness campaign, and maybe a sign that I'm taking the Principle of Charity too far here.</p>\n<p><strong>...in more realistic situations</strong><br><br>It's more complex when there are only small probabilities of your own actions having any effect, but the principle stays the same. For example, I recently heard a doctor say that a single-payer system would best solve the US' health care woes, but since that was politically infeasible he was backing Obama's plan. This one doctor's support will have minimal effect on the chances of Obama's plan passing, but it will have even less of an effect on the chances of single-payer passing. If the expected utilities multiply out in such a way as to make supporting Obama more likely to gain more utility than supporting single-payer, the doctor is justified in his strategy of support for Obama's plan.<br><br>One more example from real history I learned recently. Suppose you are a Communist, and your fellow Reds are proposing ways to create a socialist paradise. One says that you must incite the workers to violent revolution. Another says you must petition the current government to support labor reform laws. A third says you must petition the current government to <em>oppose</em> labor reform laws.<br><br>Before you expel the third communist from the Party, let them make their argument. They say that the Party doesn't have enough resources to incite violent revolution, and the workers don't want to revolt anyway. Counterfactually modifying the \"workers' actions\" node to a revolutionary state is a waste of time, because there's no link between any modification of your own actions and that node reaching the state you want. Likewise, modifying \"government policy\" is useless, because the Communists don't have any clout in the government, so even if you found a wonderful value for that node that would make all workers happy forever, you couldn't change it. <br><br>Instead, she says, oppose labor reform laws. These are already unpopular, and even a small party like the Communists would probably have enough power to get them shot down. When there's no labor reform, workers will get angrier and angrier, until they gradually revolt and overthrow the system, getting you what you wanted in the first place.<br><br>There were communists in the early 1900s who actually tried this third approach. It didn't work, but I admire their thought processes. They ignored solutions that would never happen, and found an action they thought they could enact, that they thought they <em>would</em> raise the chance of revolution significantly. Compare this kind of cunning to the vapidity of the letter-writer who says \"Binge drinkers should become more responsible.\"</p>\n<p><strong id=\"___as_an_unrealized_ideal\">...as an unrealized ideal</strong></p>\n<p>I like this way of viewing the problem, because it explains why a certain class of argument feels wrong: arguments that go \"The solution to binge drinking is more personal responsibility\" or \"The solution to poverty is for the poor to work harder,\" or so on. But do people actually think this way?<br><br>The most glaring reason to believe they don't is that most people who \"solve\" societal problems have no interest in actually enacting their solution. The attitude is something like \"Hey, if the federal government passed a single-payer health plan, then all our health troubles would be over!\" and then don't bother to write a letter to their representative about it or even convince their next door neighbor.<br><br>For reasons that have been discussed ad nauseum on Overcoming Bias, politics is very much a signalling game. In particular, it seems to be a game in which you counterfactually propose different states of the \"government policy\" node and explain why these would have the best effects, and whoever can give the best explanation gets rewarded with higher status. Sometimes you're also allowed to edit the policies of large private organizations, or of influential individuals. In this case, the problem with the original letter writer wasn't just that she had no plan to enact her solution, but that she was breaking the rule which said that you're only allowed to play with relatively unified, powerful organizations, and not things like \"the set of all binge drinkers\".<br><br>In a way, this isn't so bad. When enough people play this game, their opinions get out to the voters, consumers, politicians, and business leaders, and eventually do change government and private policy.<br><br>The point is that if your goal is to actually personally affect things in a direct, immediate way, you can't <em>just</em> apply the rules of this game without thinking beforehand. Communists, when discussing politics for \"fun\", would never say \"I think the government should oppose labor reforms,\" but that might be the winning move for them when they're actually trying to increase utility. Likewise, libertarians spend a lot of time discussing different ways the government could implement libertarian policy, but when they actually have to take action, the best choice might be seasteads or charter cities or something else that doesn't involve policy at all.<br><br>And if you are content to just play the game, at least keep it interesting. No fair counterfactually editing things like \"terrorists' behavior\" or \"poor people's work ethic\" or \"how responsible binge drinkers are\".</p>\n<p><strong id=\"Footnotes\">Footnotes</strong></p>\n<p><strong>1: </strong>Or, here's an alternate interpretation of \"Binge drinkers should be more responsible\": it's not worth trying to prevent binge drinking, because binge drinkers could prevent it themselves if they were more responsible, so it's their own fault. This is not illogical, but applying the argument to a case like \"Drunk drivers should be more responsible\" would be. There, even if we have no sympathy for drunk drivers, we still need to prevent drunk driving because many of the victims are innocents. The other issue is that people process the two statements \"The solution to binge drinking is for binge drinkers to be more responsible\" and \"We don't need to solve binge drinking; it's the drinkers' own fault and we need not care\" differently; the first sounds wise and reasonable, the second callous. For both these reasons, I don't think this interpretation is entirely what the original&nbsp; letter writer, or other people who use this sort&nbsp; of argument, are thinking of.</p>", "sections": [{"title": "...as an unrealized ideal", "anchor": "___as_an_unrealized_ideal", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gxxpK3eiSQ3XG3DW7", "miwf7qQTh2HXNnSuq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-26T02:44:53.571Z", "modifiedAt": null, "url": null, "title": "Non-Malthusian Scenarios", "slug": "non-malthusian-scenarios", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:19.652Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jpPLLyyb5ASeYivsL/non-malthusian-scenarios", "pageUrlRelative": "/posts/jpPLLyyb5ASeYivsL/non-malthusian-scenarios", "linkUrl": "https://www.lesswrong.com/posts/jpPLLyyb5ASeYivsL/non-malthusian-scenarios", "postedAtFormatted": "Saturday, September 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Non-Malthusian%20Scenarios&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANon-Malthusian%20Scenarios%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpPLLyyb5ASeYivsL%2Fnon-malthusian-scenarios%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Non-Malthusian%20Scenarios%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpPLLyyb5ASeYivsL%2Fnon-malthusian-scenarios", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjpPLLyyb5ASeYivsL%2Fnon-malthusian-scenarios", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 442, "htmlBody": "<p>This is an attempt to list all of the possible ways in which humanity <em>may </em>avoid scenarios where the average standard of living is close to subsistence, in response to Robin Hanson's recent series of posts on Overcoming Bias, where he argues that such an outcome is likely in the long run.</p>\n<p>I'll start with six, some suggested by myself, and others collected from comments on Overcoming Bias and Robin's own posts. If anyone provides additional ideas, I'll add them to the list.</p>\n<p>(I have a more general point here, BTW, which is that predicting the far future is very difficult. Before thinking that some outcome is inevitable or highly likely, it's a good idea to repeatedly ask oneself \"This is all the ways that I can think of why it may fail to come true. Am I sure that all of them have low probability and that I'm not missing anything?\" There may be some scenario with a non-negligible probability that your brain simply overlooked when you first asked it.)</p>\n<h4><a id=\"more\"></a>Singleton</h4>\n<p>A world government or superpower imposes a population control policy over the whole world.</p>\n<h4>Strong Security</h4>\n<p>Strong defensive technologies and doctrines (such as Mutually Assured Destruction) allow nations, communities, and maybe tribes and families to unilaterally limit their populations within their own borders, while holding off hordes of would-be invaders and immigrants.</p>\n<h4>Non-Human Capital</h4>\n<p>Maximizing the wealth and power of a nation requires an optimal mix of human and non-human capital. Nations that fail to adopt population controls find their relative wealth and power fade over time as their mixes deviate from the optimum (i.e., they find themselves spending too much resources on raising humans, and not enough on building machines), and either move to correct this or are taken over by stronger powers. (I believe that historically this was the reason China adopted its one-child policy.)</p>\n<h4>Unlimited Growth</h4>\n<p>We don't completely understand the laws of physics, nor the nature of value. There turns out to be some way for economic growth to continue without limit. (Robin himself once wrote \"I know of no law limiting economic value per atom\" but apparently changed his mind later.)</p>\n<h4>Selfish Memes</h4>\n<p>Memes that manage to divert people's resources away from biological reproduction and towards memetic reproduction will have an advantage over memes that don't. On the other hand, genes that manage to block such memes will have an advantage over genes that don't. Memes manage to keep the upper hand in this struggle (or periodically regain the upper hand).</p>\n<h4>Disease, Warfare, Natural Disasters, Aliens, Keeper of the Simulation<br /></h4>\n<p>One or more of these come along regularly to keep the human population in check and per capita incomes above subsistence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"o9aQASibdsECTfYF6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jpPLLyyb5ASeYivsL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 24, "extendedScore": null, "score": 3.4e-05, "legacy": true, "legacyId": "1629", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is an attempt to list all of the possible ways in which humanity <em>may </em>avoid scenarios where the average standard of living is close to subsistence, in response to Robin Hanson's recent series of posts on Overcoming Bias, where he argues that such an outcome is likely in the long run.</p>\n<p>I'll start with six, some suggested by myself, and others collected from comments on Overcoming Bias and Robin's own posts. If anyone provides additional ideas, I'll add them to the list.</p>\n<p>(I have a more general point here, BTW, which is that predicting the far future is very difficult. Before thinking that some outcome is inevitable or highly likely, it's a good idea to repeatedly ask oneself \"This is all the ways that I can think of why it may fail to come true. Am I sure that all of them have low probability and that I'm not missing anything?\" There may be some scenario with a non-negligible probability that your brain simply overlooked when you first asked it.)</p>\n<h4 id=\"Singleton\"><a id=\"more\"></a>Singleton</h4>\n<p>A world government or superpower imposes a population control policy over the whole world.</p>\n<h4 id=\"Strong_Security\">Strong Security</h4>\n<p>Strong defensive technologies and doctrines (such as Mutually Assured Destruction) allow nations, communities, and maybe tribes and families to unilaterally limit their populations within their own borders, while holding off hordes of would-be invaders and immigrants.</p>\n<h4 id=\"Non_Human_Capital\">Non-Human Capital</h4>\n<p>Maximizing the wealth and power of a nation requires an optimal mix of human and non-human capital. Nations that fail to adopt population controls find their relative wealth and power fade over time as their mixes deviate from the optimum (i.e., they find themselves spending too much resources on raising humans, and not enough on building machines), and either move to correct this or are taken over by stronger powers. (I believe that historically this was the reason China adopted its one-child policy.)</p>\n<h4 id=\"Unlimited_Growth\">Unlimited Growth</h4>\n<p>We don't completely understand the laws of physics, nor the nature of value. There turns out to be some way for economic growth to continue without limit. (Robin himself once wrote \"I know of no law limiting economic value per atom\" but apparently changed his mind later.)</p>\n<h4 id=\"Selfish_Memes\">Selfish Memes</h4>\n<p>Memes that manage to divert people's resources away from biological reproduction and towards memetic reproduction will have an advantage over memes that don't. On the other hand, genes that manage to block such memes will have an advantage over genes that don't. Memes manage to keep the upper hand in this struggle (or periodically regain the upper hand).</p>\n<h4 id=\"Disease__Warfare__Natural_Disasters__Aliens__Keeper_of_the_Simulation\">Disease, Warfare, Natural Disasters, Aliens, Keeper of the Simulation<br></h4>\n<p>One or more of these come along regularly to keep the human population in check and per capita incomes above subsistence.</p>", "sections": [{"title": "Singleton", "anchor": "Singleton", "level": 1}, {"title": "Strong Security", "anchor": "Strong_Security", "level": 1}, {"title": "Non-Human Capital", "anchor": "Non_Human_Capital", "level": 1}, {"title": "Unlimited Growth", "anchor": "Unlimited_Growth", "level": 1}, {"title": "Selfish Memes", "anchor": "Selfish_Memes", "level": 1}, {"title": "Disease, Warfare, Natural Disasters, Aliens, Keeper of the Simulation", "anchor": "Disease__Warfare__Natural_Disasters__Aliens__Keeper_of_the_Simulation", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "89 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-26T11:47:09.061Z", "modifiedAt": null, "url": null, "title": "Correlated decision making: a complete theory", "slug": "correlated-decision-making-a-complete-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mmuNCfLyArLodpGLt/correlated-decision-making-a-complete-theory", "pageUrlRelative": "/posts/mmuNCfLyArLodpGLt/correlated-decision-making-a-complete-theory", "linkUrl": "https://www.lesswrong.com/posts/mmuNCfLyArLodpGLt/correlated-decision-making-a-complete-theory", "postedAtFormatted": "Saturday, September 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Correlated%20decision%20making%3A%20a%20complete%20theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACorrelated%20decision%20making%3A%20a%20complete%20theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmuNCfLyArLodpGLt%2Fcorrelated-decision-making-a-complete-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Correlated%20decision%20making%3A%20a%20complete%20theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmuNCfLyArLodpGLt%2Fcorrelated-decision-making-a-complete-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmmuNCfLyArLodpGLt%2Fcorrelated-decision-making-a-complete-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 898, "htmlBody": "<p>The title of this post most probably deserves a cautious question mark at the end, but I'll go out on a limb and start sawing it behind me: I think I've got a framework that consistently solves correlated decision problems. That it is, those situation where different agents (a forgetful you at different times, your duplicates, or Omega&rsquo;s prediction of you) will come to the same decision.</p>\n<p class=\"MsoNormal\">After my <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/\">first post</a> on the subject, <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14g7\">Wei Dai</a> asked whether my ideas could be<span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> formalised</span><span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> </span>enough that it could applied mechanically. There were further challenges: <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14fq\">introducing further positional information</a>, and dealing with the difference between simulations and predictions. Since I <a href=\"/lw/18d/hypothetical_paradoxes/14ja\">claimed</a> this sort of approach could apply to the Newcomb&rsquo;s problem, it is also useful to see it work in cases were the two decisions are only partially correlated - where Omega is good, but he&rsquo;s not perfect.</p>\n<p class=\"MsoNormal\"><strong>The theory</strong></p>\n<p class=\"MsoNormal\">In standard decision making, it is easy to estimate your own contribution to your own utility; the contribution of others to your own utility is then estimated separately. In correlated decision-making, both steps are trickier; estimating your contribution is non-obvious, and the contribution from others is not independent. In fact, the question to ask is not \"if I decide this, how much return will I make\", but rather \"in a world in which I decide this, how much return will I make\".</p>\n<p class=\"MsoNormal\">You first estimate the contribution of each decision made to your own utility, using a simplified version of the <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/\">CDP</a>: if N correlated decisions are needed to gain some utility, then each decision maker is estimated to have contributed 1/N of the effort towards the gain of that utility.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Then the procedure under correlated decision making is:</p>\n<p class=\"MsoNormal\">1) Estimate the contribution of each correlated decision towards your utility, using CDP.</p>\n<p class=\"MsoNormal\">2) Estimate the probability that each decision actually happens (this is an implicit use of the <a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">SIA</a>).</p>\n<p class=\"MsoNormal\">3) Use 1) and 2) to estimate the total utility that emerges from the decision.</p>\n<p class=\"MsoNormal\">To illustrate, apply it to the<span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> generalised</span> <a href=\"/lw/182/the_absentminded_driver/\">absent minded driver problem</a>, where the return for turning off at the first and second intersection are x and y, <span lang=\"EN-GB\">respectively</span>, while driving straight through grants a return of z. The expected return for going straight with probability p is R = (1-p)x + p(1-p)y + p<sup>2</sup>z.</p>\n<p class=\"MsoNormal\">Then the expected return for the driver at the first intersection is (1-p)x + [p(1-p)y + p<sup>2</sup>z]/2, since the y and z returns require two decisions before being claimed. The expected return for the second driver is [(1-p)y + pz]/2. The first driver exists with probability one, while the second driver exists with probability p, giving the correct return of R.</p>\n<p class=\"MsoNormal\">In the example given in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">Outlawing Athropics</a>, there are twenty correlated decision makers, all existing with probability 1/2. Two of them contribute towards a decision which has utility -52, hence each generates a utility of -52/2. Eighteen of them contribute towards a decision which has utility 12, hence each one generates a utility of 12/18. Summing this up, the total utility generated is [2*(-52)/2 + 18*(12)/18]/2 = -20, which is correct.</p>\n<p class=\"MsoNormal\"><strong>Simulation versus prediction</strong></p>\n<p class=\"MsoNormal\">In the Newcomb problem, there are two correlated decisions: your choice of one- or two-boxing, and Omega's decision on whether to put the money. The return to you for one-boxing in either case is X/2; for two boxing, the return is 1000/2.</p>\n<p class=\"MsoNormal\">If Omega simulates you, you can be either decision maker, with probability 1/2; if he predicts without simulating, you are certainly the box-chooser. But it makes no difference - who you are is not an issue, you are simply looking at the probability of each decision maker existing, which is 1 in both cases. So adding up the two utilities gives you the correct estimate.</p>\n<p class=\"MsoNormal\">Consequently, predictions and simulations can be treated similarly in this setup.</p>\n<p class=\"MsoNormal\"><strong>Partial correlation</strong></p>\n<p class=\"MsoNormal\">If two decisions are partially correlated - say, Newcomb's problem where Omega has a probability p of correctly guessing your decision - then the way of modeling it is to split it into several perfectly correlated pieces.</p>\n<p class=\"MsoNormal\">For instance, the partially correlated Newcomb's problem can be split into one model which is perfectly correlated (with probability p), and one model which is perfectly anti-correlated (with probability (1-p)). The return from two-boxing in the first case is 1000, and X+1000 is the second case. One-boxing gives a return of X in the first case and 0 in the second case. Hence the expected return from one-boxing is p(X), and for two-boxing is 1000 + (1-p)X, which are the correct odds.</p>\n<p class=\"MsoNormal\"><strong>Adding positional information</strong></p>\n<p class=\"MsoNormal\"><a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14fq\">SilasBarta</a> asked whether my old model could deal with the absent-minded driver if there were some positional information. For instance, imagine if there were a light at each crossing that could be red or green, and it was green 1/2 the time at the first crossing and 2/3 of the time in the second crossing. Then if your probability of continuing on a green light was g, and if on a red light it was r, your initial expected return is R = (2-r-g)x/2 + (r+g)D/2, where D = ((1-r)y + rz)/3 + 2((1-g)y + gz)/3 ).</p>\n<p class=\"MsoNormal\">Then if you are at the first intersection, your expected return must be (2-r-g)x/2 + (r+g)D/4 (CDP on y and z, which require 2 decisions), while if you are at the second intersection, your expected return is D/4. The first driver exists with certainty, while the second one exists with probability r+g, giving us the correct return R.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mmuNCfLyArLodpGLt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 5.251303178912598e-07, "legacy": true, "legacyId": "1630", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The title of this post most probably deserves a cautious question mark at the end, but I'll go out on a limb and start sawing it behind me: I think I've got a framework that consistently solves correlated decision problems. That it is, those situation where different agents (a forgetful you at different times, your duplicates, or Omega\u2019s prediction of you) will come to the same decision.</p>\n<p class=\"MsoNormal\">After my <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/\">first post</a> on the subject, <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14g7\">Wei Dai</a> asked whether my ideas could be<span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> formalised</span><span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> </span>enough that it could applied mechanically. There were further challenges: <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14fq\">introducing further positional information</a>, and dealing with the difference between simulations and predictions. Since I <a href=\"/lw/18d/hypothetical_paradoxes/14ja\">claimed</a> this sort of approach could apply to the Newcomb\u2019s problem, it is also useful to see it work in cases were the two decisions are only partially correlated - where Omega is good, but he\u2019s not perfect.</p>\n<p class=\"MsoNormal\"><strong id=\"The_theory\">The theory</strong></p>\n<p class=\"MsoNormal\">In standard decision making, it is easy to estimate your own contribution to your own utility; the contribution of others to your own utility is then estimated separately. In correlated decision-making, both steps are trickier; estimating your contribution is non-obvious, and the contribution from others is not independent. In fact, the question to ask is not \"if I decide this, how much return will I make\", but rather \"in a world in which I decide this, how much return will I make\".</p>\n<p class=\"MsoNormal\">You first estimate the contribution of each decision made to your own utility, using a simplified version of the <a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/\">CDP</a>: if N correlated decisions are needed to gain some utility, then each decision maker is estimated to have contributed 1/N of the effort towards the gain of that utility.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Then the procedure under correlated decision making is:</p>\n<p class=\"MsoNormal\">1) Estimate the contribution of each correlated decision towards your utility, using CDP.</p>\n<p class=\"MsoNormal\">2) Estimate the probability that each decision actually happens (this is an implicit use of the <a href=\"/lw/18r/avoiding_doomsday_a_proof_of_the_selfindication/\">SIA</a>).</p>\n<p class=\"MsoNormal\">3) Use 1) and 2) to estimate the total utility that emerges from the decision.</p>\n<p class=\"MsoNormal\">To illustrate, apply it to the<span style=\"font-size: 10pt; font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\" lang=\"EN-GB\"> generalised</span> <a href=\"/lw/182/the_absentminded_driver/\">absent minded driver problem</a>, where the return for turning off at the first and second intersection are x and y, <span lang=\"EN-GB\">respectively</span>, while driving straight through grants a return of z. The expected return for going straight with probability p is R = (1-p)x + p(1-p)y + p<sup>2</sup>z.</p>\n<p class=\"MsoNormal\">Then the expected return for the driver at the first intersection is (1-p)x + [p(1-p)y + p<sup>2</sup>z]/2, since the y and z returns require two decisions before being claimed. The expected return for the second driver is [(1-p)y + pz]/2. The first driver exists with probability one, while the second driver exists with probability p, giving the correct return of R.</p>\n<p class=\"MsoNormal\">In the example given in <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/\">Outlawing Athropics</a>, there are twenty correlated decision makers, all existing with probability 1/2. Two of them contribute towards a decision which has utility -52, hence each generates a utility of -52/2. Eighteen of them contribute towards a decision which has utility 12, hence each one generates a utility of 12/18. Summing this up, the total utility generated is [2*(-52)/2 + 18*(12)/18]/2 = -20, which is correct.</p>\n<p class=\"MsoNormal\"><strong id=\"Simulation_versus_prediction\">Simulation versus prediction</strong></p>\n<p class=\"MsoNormal\">In the Newcomb problem, there are two correlated decisions: your choice of one- or two-boxing, and Omega's decision on whether to put the money. The return to you for one-boxing in either case is X/2; for two boxing, the return is 1000/2.</p>\n<p class=\"MsoNormal\">If Omega simulates you, you can be either decision maker, with probability 1/2; if he predicts without simulating, you are certainly the box-chooser. But it makes no difference - who you are is not an issue, you are simply looking at the probability of each decision maker existing, which is 1 in both cases. So adding up the two utilities gives you the correct estimate.</p>\n<p class=\"MsoNormal\">Consequently, predictions and simulations can be treated similarly in this setup.</p>\n<p class=\"MsoNormal\"><strong id=\"Partial_correlation\">Partial correlation</strong></p>\n<p class=\"MsoNormal\">If two decisions are partially correlated - say, Newcomb's problem where Omega has a probability p of correctly guessing your decision - then the way of modeling it is to split it into several perfectly correlated pieces.</p>\n<p class=\"MsoNormal\">For instance, the partially correlated Newcomb's problem can be split into one model which is perfectly correlated (with probability p), and one model which is perfectly anti-correlated (with probability (1-p)). The return from two-boxing in the first case is 1000, and X+1000 is the second case. One-boxing gives a return of X in the first case and 0 in the second case. Hence the expected return from one-boxing is p(X), and for two-boxing is 1000 + (1-p)X, which are the correct odds.</p>\n<p class=\"MsoNormal\"><strong id=\"Adding_positional_information\">Adding positional information</strong></p>\n<p class=\"MsoNormal\"><a href=\"/lw/18s/anthropic_reasoning_and_correlated_decision_making/14fq\">SilasBarta</a> asked whether my old model could deal with the absent-minded driver if there were some positional information. For instance, imagine if there were a light at each crossing that could be red or green, and it was green 1/2 the time at the first crossing and 2/3 of the time in the second crossing. Then if your probability of continuing on a green light was g, and if on a red light it was r, your initial expected return is R = (2-r-g)x/2 + (r+g)D/2, where D = ((1-r)y + rz)/3 + 2((1-g)y + gz)/3 ).</p>\n<p class=\"MsoNormal\">Then if you are at the first intersection, your expected return must be (2-r-g)x/2 + (r+g)D/4 (CDP on y and z, which require 2 decisions), while if you are at the second intersection, your expected return is D/4. The first driver exists with certainty, while the second one exists with probability r+g, giving us the correct return R.</p>", "sections": [{"title": "The theory", "anchor": "The_theory", "level": 1}, {"title": "Simulation versus prediction", "anchor": "Simulation_versus_prediction", "level": 1}, {"title": "Partial correlation", "anchor": "Partial_correlation", "level": 1}, {"title": "Adding positional information", "anchor": "Adding_positional_information", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ab6gFDx4LxxySY3E8", "5A9x74mgCwJwSg4sN", "GfHdNfqxe3cSCfpHL", "ZTEkZNLrmycNuCNYq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-26T16:01:20.938Z", "modifiedAt": null, "url": null, "title": "The Scylla of Error and the Charybdis of Paralysis", "slug": "the-scylla-of-error-and-the-charybdis-of-paralysis", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:33.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n585YgAhDFo5DkYvt/the-scylla-of-error-and-the-charybdis-of-paralysis", "pageUrlRelative": "/posts/n585YgAhDFo5DkYvt/the-scylla-of-error-and-the-charybdis-of-paralysis", "linkUrl": "https://www.lesswrong.com/posts/n585YgAhDFo5DkYvt/the-scylla-of-error-and-the-charybdis-of-paralysis", "postedAtFormatted": "Saturday, September 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Scylla%20of%20Error%20and%20the%20Charybdis%20of%20Paralysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Scylla%20of%20Error%20and%20the%20Charybdis%20of%20Paralysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn585YgAhDFo5DkYvt%2Fthe-scylla-of-error-and-the-charybdis-of-paralysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Scylla%20of%20Error%20and%20the%20Charybdis%20of%20Paralysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn585YgAhDFo5DkYvt%2Fthe-scylla-of-error-and-the-charybdis-of-paralysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn585YgAhDFo5DkYvt%2Fthe-scylla-of-error-and-the-charybdis-of-paralysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 542, "htmlBody": "<p>We're interested in improving human rationality. Many of our techniques for improving human rationality take time. In real-time situations, you can lose by making the wrong decision, or by making the \"right\" decision too slowly. Most of us do not have inflexible-schedule, high-stakes decisions to make, though. How often does real-time decision making really come up?</p>\n<p>Suppose you are making a fairly long-ranged decision. Call this decision 1. While analyzing decision 1, you come to a natural pause. At this pause you need to decide whether to analyze further, or to act on your best-so-far analysis. Call this decision 2. Note that decision 2 is made under tighter time pressure than decision 1.&nbsp;This scenario argues that decision-making is recursive, and so if there are any time bounds, then many decisions will need to be made at very tight time bounds.</p>\n<p>A second, \"covert\" goal of this post is to provide a definitely-not-paradoxical problem for people to practice their Bayseian reasoning on. Here is a concrete model of real-time decisionmaking, motivated by medical-drama television shows, where the team diagnoses and treats a patient over the course of each episode. Diagnosing and treating a patient who is dying of an unknown disease is a colorful example of real-time decisionmaking.</p>\n<p>To play this game, you need a coin, two six-sided dice, a deck of cards, and a helper to manipulate these objects. The manipulator sets up the game by flipping a coin. If heads (tails) the patient is suffering from an exotic fungus (allergy). Then the manipulator prepares a deck by removing all of the clubs (diamonds) so that the deck is a red-biased (black-biased) random-color generator. Finally, the manipulator determines the patients starting health by rolling the dice and summing them. All of this is done secretly.</p>\n<p>Play proceeds in turns. At the beginning of each turn, the manipulator flips a coin to determine whether test results are available. If test results are available, the manipulator draws a card from the deck and reports its color. A red (black) card gives you suggestive evidence that the patient is suffering from a fungus (allergy). You choose whether to treat a fungus, allergy, or wait. If you treat correctly, the manipulator leaves the patient's health where it is (they're improving, but on a longer timescale). If you wait, the manipulator reduces the patient's health by one. If you treat incorrectly, the manipulator reduces the patient's health by two.</p>\n<p>Play ends when you treat the patient for the same disease for six consecutive turns or when the patient reaches zero health.</p>\n<p>Here is some Python&nbsp;<a href=\"http://www.johnicholas.com/medical_drama_simulation.py\">code</a>&nbsp;simulating a simplistic strategy.&nbsp;What Bayesian strategy yields the best results? Is there a concise description of this strategy?&nbsp;</p>\n<p>The model can be made more complicated. The space of possible actions is small. There is no choice of what to investigate next. In the real world, there are likely to be diminishing returns to further tests or further analysis. There could be uncertainty about how much time pressure there is. There could be uncertainty about how much information future tests will reveal.&nbsp;Every complication will make the task of computing the best strategy more difficult.</p>\n<p>We need fast approximations to rationality (even quite bad approximations, if they're fast enough), as well as procedures that spend time in order to purchase a better result.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n585YgAhDFo5DkYvt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 16, "extendedScore": null, "score": 5.251730445180234e-07, "legacy": true, "legacyId": "1631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-27T01:47:54.920Z", "modifiedAt": null, "url": null, "title": "The Anthropic Trilemma", "slug": "the-anthropic-trilemma", "viewCount": null, "lastCommentedAt": "2021-12-12T19:53:03.628Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y7jZ9BLEeuNTzgAE5/the-anthropic-trilemma", "pageUrlRelative": "/posts/y7jZ9BLEeuNTzgAE5/the-anthropic-trilemma", "linkUrl": "https://www.lesswrong.com/posts/y7jZ9BLEeuNTzgAE5/the-anthropic-trilemma", "postedAtFormatted": "Sunday, September 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Anthropic%20Trilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Anthropic%20Trilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7jZ9BLEeuNTzgAE5%2Fthe-anthropic-trilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Anthropic%20Trilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7jZ9BLEeuNTzgAE5%2Fthe-anthropic-trilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy7jZ9BLEeuNTzgAE5%2Fthe-anthropic-trilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1757, "htmlBody": "<p>Speaking of problems I don't know how to solve, here's one that's been gnawing at me for years.</p>\n<p>The operation of splitting a subjective worldline seems obvious enough - the skeptical initiate can consider the <a href=\"/lw/ps/where_physics_meets_experience/\">Ebborians</a>, creatures whose brains come in flat sheets and who can symmetrically divide down their thickness.&nbsp; The more sophisticated need merely consider a sentient computer program: stop, copy, paste, start, and what was one person has now continued on in two places.&nbsp; If one of your future selves will see red, and one of your future selves will see green, then (it seems) you should <em>anticipate</em> seeing red or green when you wake up with 50% probability.&nbsp; That is, it's a known fact that different versions of you will see red, or alternatively green, and you should weight the two anticipated possibilities equally.&nbsp; (Consider what happens when you're flipping a quantum coin: half your measure will continue into either branch, and <a href=\"/lw/py/the_born_probabilities/\">subjective probability will follow quantum measure for unknown reasons</a>.)</p>\n<p>But if I make two copies of the same computer program, is there twice as much experience, or only the same experience?&nbsp; Does someone who runs redundantly on three processors, get three times as much weight as someone who runs on one processor?</p>\n<p>Let's suppose that three copies get three times as much experience.&nbsp; (If not, then, in a Big universe, large enough that at least one copy of anything exists <em>somewhere,</em> you run into the <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann Brain problem</a>.)</p>\n<p>Just as computer programs or brains can split, they ought to be able to merge.&nbsp; If we imagine a version of the Ebborian species that computes digitally, so that the brains remain synchronized so long as they go on getting the same sensory inputs, then we ought to be able to put two brains back together along the thickness, after dividing them.&nbsp; In the case of computer programs, we should be able to perform an operation where we compare each two bits in the program, and if they are the same, copy them, and if they are different, delete the whole program.&nbsp; (This seems to establish an equal causal dependency of the final program on the two original programs that went into it.&nbsp; E.g., if you test the causal dependency via counterfactuals, then disturbing any bit of the two originals, results in the final program being completely different (namely deleted).)</p>\n<p>So here's a simple algorithm for winning the lottery:<a id=\"more\"></a></p>\n<p>Buy a ticket.&nbsp; Suspend your computer program just before the lottery drawing - which should of course be a quantum lottery, so that every ticket wins somewhere.&nbsp; Program your computational environment to, if you win, make a trillion copies of yourself, and wake them up for ten seconds, long enough to experience winning the lottery.&nbsp; Then suspend the programs, merge them again, and start the result.&nbsp; If you don't win the lottery, then just wake up automatically.</p>\n<p>The odds of winning the lottery are ordinarily a billion to one.&nbsp; But now the branch in which you <em>win </em>has your \"measure\", your \"amount of experience\", <em>temporarily</em> multiplied by a trillion.&nbsp; So with the brief expenditure of a little extra computing power, you can subjectively win the lottery - be reasonably sure that when next you open your eyes, you will see a computer screen flashing \"You won!\"&nbsp; As for what happens ten seconds after that, you have no way of knowing how many processors you run on, so you shouldn't feel a thing.</p>\n<p>Now you could just bite this bullet.&nbsp; You could say, \"Sounds to me like it should work fine.\"&nbsp; You could say, \"There's no reason why you <em>shouldn't </em>be able to exert anthropic psychic powers.\"&nbsp; You could say, \"I have no problem with the idea that no one else could see you exerting your anthropic psychic powers, and I have no problem with the idea that different people can send different portions of their subjective futures into different realities.\"</p>\n<p>I find myself somewhat reluctant to bite that bullet, personally.</p>\n<p>Nick Bostrom, when I proposed this problem to him, offered that you should anticipate winning the lottery after five seconds, but anticipate losing the lottery after fifteen seconds.</p>\n<p>To bite this bullet, you have to throw away the idea that your joint subjective probabilities are the product of your conditional subjective probabilities.&nbsp; If you win the lottery, the subjective probability of having still won the lottery, ten seconds later, is ~1.&nbsp; And if you lose the lottery, the subjective probability of having lost the lottery, ten seconds later, is ~1.&nbsp; But we don't have p(\"experience win after 15s\") = p(\"experience win after 15s\"|\"experience win after 5s\")*p(\"experience win after 5s\") + p(\"experience win after 15s\"|\"experience not-win after 5s\")*p(\"experience not-win after 5s\").</p>\n<p>I'm reluctant to bite that bullet too.</p>\n<p>And the third horn of the trilemma is to reject the idea of the personal future - that there's any <em>meaningful </em>sense in which I can anticipate waking up as <em>myself</em> tomorrow, rather than Britney Spears.&nbsp; Or, for that matter, that there's any meaningful sense in which I can anticipate being <em>myself</em> in five seconds, rather than Britney Spears.&nbsp; In five seconds there will be an Eliezer Yudkowsky, and there will be a Britney Spears, but it is meaningless to speak of the <em>current</em> Eliezer \"continuing on\" as Eliezer+5 rather than Britney+5; these are simply three different people we are talking about.</p>\n<p>There are no threads connecting subjective experiences.&nbsp; There are simply different subjective experiences.&nbsp; Even if some subjective experiences are highly similar to, and <a href=\"/lw/qx/timeless_identity/\">causally computed from</a>, other subjective experiences, they are not <em>connected</em>.</p>\n<p>I still have trouble biting that bullet for some reason.&nbsp; Maybe I'm naive, I know, but there's a sense in which I just can't seem to let go of the question, \"What will I see happen next?\"&nbsp; I strive for altruism, but I'm not sure I can believe that subjective selfishness - caring about your own future experiences - is an <em>incoherent</em> utility function; that we are <em>forced</em> to be Buddhists who dare not cheat a neighbor, not because we are kind, but because we anticipate experiencing their consequences just as much as we anticipate experiencing our own.&nbsp; I don't think that, if I were <em>really</em> selfish, I could jump off a cliff knowing smugly that a different person would experience the consequence of hitting the ground.</p>\n<p>Bound to my naive intuitions that can be explained away by obvious evolutionary instincts, you say?&nbsp; It's plausible that I could be forced down this path, but I don't feel forced down it quite <em>yet.</em>&nbsp; It would feel like a <a href=\"/lw/op/fake_reductionism/\">fake reduction</a>.&nbsp; I have rather the sense that my confusion here is tied up with my confusion over what sort of physical configurations, or cascades of cause and effect, \"exist\" in any sense and \"experience\" anything in any sense, and flatly denying the existence of subjective continuity would not make me feel any less confused about that.</p>\n<p>The fourth horn of the trilemma (as 'twere) would be denying that two copies of the same computation had any more \"weight of experience\" than one; but in addition to the <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann Brain problem</a> in large universes, you might develop similar anthropic psychic powers if you could split a trillion times, have each computation view a slightly different scene in some small detail, forget that detail, and converge the computations so they could be reunified afterward - then you were temporarily a trillion different people who all happened to develop into the same future self.&nbsp; So it's not clear that the fourth horn actually changes anything, which is why I call it a trilemma.</p>\n<p>I should mention, in this connection, a truly remarkable observation: <em>quantum</em> measure seems to behave in a way that would avoid this trilemma completely, if you tried the analogue using quantum branching within a large coherent superposition (e.g. a quantum computer).&nbsp; If you quantum-split into a trillion copies, those trillion copies would have the same total quantum measure after being merged or converged.</p>\n<p>It's a remarkable fact that the one sort of branching we do have extensive actual experience with - though we don't know <em>why</em> it behaves the way it does - seems to behave in a very strange way that is exactly right to avoid anthropic superpowers <em>and</em> goes on obeying the standard axioms for conditional probability.</p>\n<p>In quantum copying and merging, every \"branch\" operation preserves the total measure of the original branch, and every \"merge\" operation (which you could theoretically do in large coherent superpositions) likewise preserves the total measure of the incoming branches.</p>\n<p>Great for QM.&nbsp; But it's not clear to me at all how to set up an analogous set of rules for making copies of sentient beings, in which the total number of processors can go up or down and you can transfer processors from one set of minds to another.</p>\n<p>To sum up:</p>\n<ul>\n<li>The first horn of the anthropic trilemma is to confess that there are simple algorithms whereby you can, indetectably to anyone but yourself, exert the subjective equivalent of psychic powers - use a temporary expenditure of computing power to permanently send your subjective future into particular branches of reality.</li>\n<li>The second horn of the anthropic trilemma is to deny that subjective joint probabilities behave like probabilities - you can coherently anticipate winning the lottery after five seconds, anticipate the experience of having lost the lottery after fifteen seconds, and anticipate that once you experience winning the lottery you will experience having still won it ten seconds later.</li>\n<li>The third horn of the anthropic trilemma is to deny that there is any meaningful sense whatsoever in which you can anticipate being <em>yourself</em> in five seconds, rather than Britney Spears; to deny that selfishness is coherently possible; to assert that you can hurl yourself off a cliff without fear, because whoever hits the ground will be another person not particularly connected to you by any such ridiculous thing as a \"thread of subjective experience\".</li>\n<li>The fourth horn of the anthropic trilemma is to deny that increasing the number of physical copies increases the weight of an experience, which leads into <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">Boltzmann brain</a> problems, and may not help much (because alternatively designed brains may be able to diverge and then converge as different experiences have their details forgotten).</li>\n<li>The fifth horn of the anthropic trilemma is to observe that the only form of splitting we have accumulated experience with, the <a href=\"/lw/py/the_born_probabilities/\">mysterious Born probabilities</a> of quantum mechanics, would seem to avoid the trilemma; but it's not clear how to have analogous rules could possibly govern information flows in computer processors.</li>\n</ul>\n<p>I will be <em>extremely</em> impressed if Less Wrong solves this one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 3, "x6evH6MyPK3nxsoff": 2, "5f5c37ee1b5cdee568cfb2fa": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y7jZ9BLEeuNTzgAE5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 51, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "1633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 233, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WajiC3YWeJutyAXTn", "3ZKvf9u2XEWddGZmS", "LubwxZHKKvCivYGzx", "924arDrTu3QRHFA5r", "mTf8MkpAigm3HP6x2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-27T17:01:10.306Z", "modifiedAt": null, "url": null, "title": "Your Most Valuable Skill", "slug": "your-most-valuable-skill", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:26.966Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JTFGokioPRJjw3Zas/your-most-valuable-skill", "pageUrlRelative": "/posts/JTFGokioPRJjw3Zas/your-most-valuable-skill", "linkUrl": "https://www.lesswrong.com/posts/JTFGokioPRJjw3Zas/your-most-valuable-skill", "postedAtFormatted": "Sunday, September 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Your%20Most%20Valuable%20Skill&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYour%20Most%20Valuable%20Skill%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTFGokioPRJjw3Zas%2Fyour-most-valuable-skill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Your%20Most%20Valuable%20Skill%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTFGokioPRJjw3Zas%2Fyour-most-valuable-skill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJTFGokioPRJjw3Zas%2Fyour-most-valuable-skill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p>Knowledge is great: I suspect we can agree there.&nbsp; Sadly, though, we can't guarantee ourselves infinite time in which to learn everything eventually, and in the meantime, there are plenty of situations where having irrelevant knowledge instead of more instrumentally useful knowledge can be decidedly suboptimal.&nbsp; Therefore, there's good reason to work out what facts we'll need to deploy and give special priority to learning those facts.&nbsp; There's nothing intrinsically more interesting or valuable about the knowledge that the capital of the United States is Washington, D.C. than there is about the knowledge that the capital of Bali is Denpasar, but unless you live or spend a lot of time in Indonesia, the latter knowledge will be less likely to come up.</p>\n<p>It seems the same is true of procedural knowledge (with the quirk that it's easier to deliberately put yourself in situations where you use whatever procedural knowledge you have than it is to arrange to need to know the capital of Bali.)&nbsp; If your procedural knowledge is useful, and also difficult to obtain or unpopular to practice or both, you might even turn it into a career (or save money that you would have spent hiring people who have).</p>\n<p>Rationality is sort of the ur-procedure, but after a certain point - the point where you're no longer buying into supernaturalist superstition, begging for a Darwin Award, or falling for cheap scams - its marginal practical value diminishes.&nbsp; Practicing rationality as an art is fun and there's some chance it'll yield a high return, but evolution (genetic and memetic) didn't do <em>that</em> bad of a job on us: we enter adulthood with an arsenal of heuristics that are mostly good enough.&nbsp; A little patching of the worst leaks, some bailing of bilge that got in early on, and you have a serviceable brain-yacht.&nbsp; (Sound of metaphor straining.)<a id=\"more\"></a></p>\n<p>So when you want to spend time on learning or honing a skill, it makes sense to choose skills with a high return on investment, be it in terms of fun, resources, the goodwill of others, insurance against emergency, or other valuable results.&nbsp; Note that if you learned a skill, used it to learn a non-customized fact, and do not anticipate using the skill again, it's not the skill that was useful; the skill was just a sine qua non for the useful fact, and others don't have to duplicate the research process to benefit.&nbsp; A skill that yielded one (or more) customized facts - i.e., facts about yourself, that you can't go on to share straight up with other people - might be a useful skill in this way, however.</p>\n<p>For practical daily purposes, what is your most valuable skill (or what most valuable skill are you trying to attain now)?&nbsp; Post it in the comments, along with what makes your skill valuable, tips for picking it up, and what made you first investigate it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fR7QfYx4JA3BnptT9": 2, "Ng8Gice9KNkncxqcj": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JTFGokioPRJjw3Zas", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 32, "extendedScore": null, "score": 5.254250968269267e-07, "legacy": true, "legacyId": "1634", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-29T00:40:12.159Z", "modifiedAt": null, "url": null, "title": "Privileging the Hypothesis", "slug": "privileging-the-hypothesis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:34.062Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Eliezer_Yudkowsky", "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X2AD2LgtKgkRNPj2a/privileging-the-hypothesis", "pageUrlRelative": "/posts/X2AD2LgtKgkRNPj2a/privileging-the-hypothesis", "linkUrl": "https://www.lesswrong.com/posts/X2AD2LgtKgkRNPj2a/privileging-the-hypothesis", "postedAtFormatted": "Tuesday, September 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Privileging%20the%20Hypothesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrivileging%20the%20Hypothesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2AD2LgtKgkRNPj2a%2Fprivileging-the-hypothesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Privileging%20the%20Hypothesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2AD2LgtKgkRNPj2a%2Fprivileging-the-hypothesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX2AD2LgtKgkRNPj2a%2Fprivileging-the-hypothesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1367, "htmlBody": "<p>Suppose that the police of Largeville, a town with a million inhabitants, are investigating a murder in which there are few or no clues\u2014the victim was stabbed to death in an alley, and there are no fingerprints and no witnesses.</p><p>Then, one of the detectives says, \u201cWell\u2026 we have no idea who did it\u2026 no particular evidence singling out any of the million people in this city\u2026 but let\u2019s <em>consider the hypothesis</em> that this murder was committed by Mortimer Q. Snodgrass, who lives at 128 Ordinary Ln. It <em>could</em> have been him, after all.\u201d</p><p>I\u2019ll label this <em>the fallacy of privileging the hypothesis</em>. (Do let me know if it already has an official name\u2014I can\u2019t recall seeing it described.)</p><p>Now the detective may perhaps have some form of <a href=\"https://wiki.lesswrong.com/wiki/Rational_evidence\">rational evidence</a> that is not legal evidence admissible in court\u2014hearsay from an informant, for example. But if the detective does not have <em>some</em> justification <em>already in hand</em> for promoting Mortimer to the police\u2019s special attention\u2014if the name is pulled entirely out of a hat\u2014then Mortimer\u2019s rights are being violated.</p><p>And this is true even if the detective is not claiming that Mortimer \u201cdid\u201d do it, but only asking the police to spend time pondering that Mortimer <em>might</em> have done it\u2014unjustifiably promoting that particular hypothesis to attention. It\u2019s human nature to <a href=\"https://www.lesswrong.com/posts/rmAbiEKQDpDnZzcRf/positive-bias-look-into-the-dark\">look for confirmation rather than disconfirmation</a>. Suppose that three detectives each suggest their hated enemies, as names to be considered; and Mortimer is brown-haired, Frederick is black-haired, and Helen is blonde. Then a witness is found who says that the person leaving the scene was brown-haired. \u201cAha!\u201d say the police. \u201cWe previously had no evidence to distinguish among the possibilities, but <em>now</em> we know that Mortimer did it!\u201d</p><p>This is related to the principle I\u2019ve started calling \u201c<a href=\"https://wiki.lesswrong.com/wiki/Locate_the_hypothesis\">locating the hypothesis</a>,\u201d which is that if you have a billion boxes only one of which contains a diamond (the truth), and your detectors only provide <a href=\"https://wiki.lesswrong.com/wiki/Amount_of_evidence\">1 bit of evidence</a> apiece, then it takes much more evidence to promote the truth to your particular attention\u2014to narrow it down to ten good possibilities, each deserving of our individual attention\u2014than it does to figure out <em>which</em> of those ten possibilities is true. It takes 27 bits to narrow it down to ten, and just another 4 bits will give us better than even odds of having the right answer.</p><p>Thus the detective, in calling Mortimer to the particular attention of the police, for no reason out of a million other people, is skipping over <em>most of the evidence</em> that needs to be supplied against Mortimer.</p><p>And the detective ought to have this evidence in their possession, at the first moment when they bring Mortimer to the police\u2019s attention <em>at all</em>. It may be mere rational evidence rather than legal evidence, but if there\u2019s <em>no evidence</em> then the detective is harassing and persecuting poor Mortimer.</p><p>During my recent <a href=\"http://bloggingheads.tv/videos/2220\">diavlog with Scott Aaronson on quantum mechanics</a>, I did manage to corner Scott to the extent of getting Scott to admit that there was no concrete evidence whatsoever that favors a <a href=\"https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ/p/xsZnufn3cQw7tJeQ3\">collapse postulate</a> or <a href=\"https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ/p/S8ysHqeRGuySPttrS\">single-world quantum mechanics</a>. But, said Scott, we might encounter <em>future</em> evidence in favor of single-world quantum mechanics, and many-worlds still has <a href=\"http://lesswrong.com/lw/py/the_born_probabilities/\">the open question of the Born probabilities</a>.</p><p>This is indeed what I would call the fallacy of privileging the hypothesis. There must be a trillion better ways to answer the Born question without adding a collapse postulate that would be the only non-linear, non-unitary, discontinous, non-differentiable, non-CPT-symmetric, non-local in the configuration space, Liouville\u2019s-Theorem-violating, privileged-space-of-simultaneity-possessing, faster-than-light-influencing, acausal, informally specified law in all of physics. Something that unphysical is not worth <em>saying out loud</em> or even <em>thinking about as a possibility</em>without a rather large weight of evidence\u2014far more than the current grand total of zero.</p><p>But because of a historical accident, collapse postulates and single-world quantum mechanics are indeed on everyone\u2019s lips and in everyone\u2019s mind to be thought of, and so the open question of the Born probabilities is offered up (by Scott Aaronson no less!) as evidence that many-worlds can\u2019t yet offer a complete picture of the world. Which is taken to mean that single-world quantum mechanics is still in the running somehow.</p><p>In the minds of human beings, if you can get them to think about this particular hypothesis rather than the trillion other possibilities that are no more complicated or unlikely, you really <em>have</em> done a huge chunk of the work of persuasion. Anything thought about is treated as \u201cin the running,\u201d and if other runners seem to fall behind in the race a little, it\u2019s assumed that this runner is edging forward or even entering the lead.</p><p>And yes, this is just the same fallacy committed, on a much more blatant scale, by the theist who points out that modern science does not offer an absolutely complete explanation of the entire universe, and takes this as evidence for the existence of Jehovah. Rather than Allah, the Flying Spaghetti Monster, or a trillion other gods no less complicated\u2014never mind the space of naturalistic explanations!</p><p>To talk about \u201cintelligent design\u201d whenever you point to a purported flaw or open problem in evolutionary theory is, again, privileging the hypothesis\u2014you must have evidence <em>already in hand</em> that points to intelligent design specifically in order to justify <em>raising that particular idea to our attention</em>, rather than a thousand others.</p><p>So that\u2019s the <em>sane</em> rule. And the corresponding <a href=\"https://wiki.lesswrong.com/wiki/Anti-epistemology\">anti-epistemology</a> is to talk endlessly of \u201cpossibility\u201d and how you \u201ccan\u2019t disprove\u201d an idea, to hope that future evidence may confirm it without presenting past evidence already in hand, to dwell and dwell on <em>possibilities</em> without evaluating possibly unfavorable evidence, to draw glowing word-pictures of confirming observations that <em>could</em> happen but haven\u2019t happened <em>yet</em>, or to try and show that piece after piece of negative evidence is \u201cnot conclusive.\u201d</p><p>Just as <a href=\"https://wiki.lesswrong.com/wiki/Occam%27s_razor\">Occam\u2019s Razor</a> says that more complicated propositions require more evidence to believe, more complicated propositions also ought to require more work to raise to attention. Just as the principle of <a href=\"https://wiki.lesswrong.com/wiki/Burdensome_details\">burdensome details</a> requires that each part of a belief be separately justified, it requires that each part be separately raised to attention.</p><p>As discussed in <a href=\"https://www.lesswrong.com/posts/zFuCxbY9E2E8HTbfZ/perpetual-motion-beliefs\">Perpetual Motion Beliefs</a>, faith and type 2 perpetual motion machines (water <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-ex-box-test {position: absolute; overflow: hidden; width: 1px; height: 60ex}\n.mjx-line-box-test {display: table!important}\n.mjx-line-box-test span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\u2192\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">\u2192</span></span></span></span></span></span> ice cubes <span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-ex-box-test {position: absolute; overflow: hidden; width: 1px; height: 60ex}\n.mjx-line-box-test {display: table!important}\n.mjx-line-box-test span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"+\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span></span></span></span></span> electricity) have in common that they purport to <em>manufacture improbability from nowhere</em>, whether the improbability of water forming ice cubes or the improbability of arriving at correct beliefs without observation. Sometimes most of the anti-work involved in manufacturing this improbability is getting us to <em>pay attention</em> to an unwarranted belief\u2014thinking on it, dwelling on it. In large answer spaces, attention without evidence is more than halfway to belief without evidence.</p><p>Someone who spends all day thinking about whether the <em>Trinity</em> does or does not exist, rather than Allah or Thor or the Flying Spaghetti Monster, is more than halfway to Christianity. If leaving, they\u2019re less than half departed; if arriving, they\u2019re more than halfway there.</p><p>An oft-encountered mode of privilege is to try to make uncertainty within a space, slop outside of that space onto the privileged hypothesis. For example, a creationist seizes on some (allegedly) debated aspect of contemporary theory, argues that scientists are <em>uncertain about evolution</em>, and then says, \u201cWe don\u2019t really know which theory is right, so maybe intelligent design is right.\u201d But the uncertainty is uncertainty <em>within</em> the realm of naturalistic theories of evolution\u2014we have no reason to believe that we\u2019ll need to leave that realm to deal with our uncertainty, still <em>less</em> that we would jump out of the realm of standard science and land <em>on Jehovah in particular</em>. That is privileging the hypothesis\u2014taking doubt <em>within</em> a normal space, and trying to slop doubt <em>out</em> of the normal space, onto a privileged (and usually discredited) <em>extremely</em> abnormal target.</p><p>Similarly, our uncertainty about where the Born statistics come from should be uncertainty <em>within</em> the space of quantum theories that are continuous, linear, unitary, slower-than-light, local, causal, naturalistic, et cetera\u2014the usual character of physical law. Some of that uncertainty might slop outside the standard space onto theories that violate <em>one</em> of these standard characteristics. It\u2019s indeed possible that we might have to think outside the box. But single-world theories violate <em>all</em> these characteristics, and there is no reason to privilege that hypothesis.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5hpGj9nDLgokfghvR": 2, "LDTSbmXtokYAsEq8e": 1, "ZpG9rheyAkgCoEQea": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X2AD2LgtKgkRNPj2a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 93, "extendedScore": null, "score": 0.00015, "legacy": true, "legacyId": "1642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "Kqs6GR7F5xziuSyGZ", "canonicalCollectionSlug": "rationality", "canonicalBookId": "ah2GqzZSeBpW9QHgb", "canonicalNextPostSlug": "living-in-many-worlds", "canonicalPrevPostSlug": "decoherence-is-falsifiable-and-testable", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 93, "bannedUserIds": null, "commentsLocked": false, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 131, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rmAbiEKQDpDnZzcRf", "xsZnufn3cQw7tJeQ3", "S8ysHqeRGuySPttrS", "3ZKvf9u2XEWddGZmS", "zFuCxbY9E2E8HTbfZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-29T05:22:48.366Z", "modifiedAt": null, "url": null, "title": "Why Many-Worlds Is Not The Rationally Favored Interpretation", "slug": "why-many-worlds-is-not-the-rationally-favored-interpretation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:02.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sLLR5u9NJTfL88Spx/why-many-worlds-is-not-the-rationally-favored-interpretation", "pageUrlRelative": "/posts/sLLR5u9NJTfL88Spx/why-many-worlds-is-not-the-rationally-favored-interpretation", "linkUrl": "https://www.lesswrong.com/posts/sLLR5u9NJTfL88Spx/why-many-worlds-is-not-the-rationally-favored-interpretation", "postedAtFormatted": "Tuesday, September 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Many-Worlds%20Is%20Not%20The%20Rationally%20Favored%20Interpretation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Many-Worlds%20Is%20Not%20The%20Rationally%20Favored%20Interpretation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLLR5u9NJTfL88Spx%2Fwhy-many-worlds-is-not-the-rationally-favored-interpretation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Many-Worlds%20Is%20Not%20The%20Rationally%20Favored%20Interpretation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLLR5u9NJTfL88Spx%2Fwhy-many-worlds-is-not-the-rationally-favored-interpretation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsLLR5u9NJTfL88Spx%2Fwhy-many-worlds-is-not-the-rationally-favored-interpretation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 923, "htmlBody": "<p>Eliezer recently posted an essay on <a href=\"http://wiki.lesswrong.com/wiki/Privilege_the_hypothesis\">\"the fallacy of privileging the hypothesis\"</a>. What it's really about is the fallacy of privileging an <em>arbitrary</em> hypothesis. In the fictional example, a detective proposes that the investigation of an unsolved murder should begin by investigating whether a particular, randomly chosen citizen was in fact the murderer. Towards the end, this is likened to the presumption that one particular religion, rather than any of the other existing or even merely possible religions, is especially worth investigating.</p>\n<p>However, in between the fictional and the supernatural illustrations of the fallacy, we have something more empirical: quantum mechanics. Eliezer writes, as he has previously, that the many-worlds interpretation is <em>the one</em> - the rationally favored interpretation, the picture of reality which rationally should be adopted given the empirical success of quantum theory. Eliezer has said this before, and I have argued against it before, back when this site was just part of a blog. This site is about rationality, not physics; and the quantum case is not essential to the exposition of this fallacy. But given the regularity with which many-worlds metaphysics shows up in discussion here, perhaps it is worth presenting a case for the opposition.</p>\n<p><a id=\"more\"></a>We can do this the easy way, or the hard way. The easy way is to argue that many-worlds is merely <em>not favored</em>, because we are nowhere near being able to <a href=\"http://wiki.lesswrong.com/wiki/Locate_the_hypothesis\">locate our hypotheses</a> in a way which permits a clean-cut judgment about their relative merits. The available hypotheses about the reality beneath quantum appearances are one and all unfinished muddles, and we should let their advocates get on with turning them into exact hypotheses without picking favorites first. (That is, if their advocates can be bothered turning them into exact hypotheses.)</p>\n<p>The hard way is to argue that many-worlds is actually <em>disfavored</em> - that we can already say it is unlikely to be true. But let's take the easy path first, and see how things stand at the end.</p>\n<p>The two examples of favoring an arbitrary hypothesis with which we have been provided - the murder investigation, the rivalry of religions - both present a situation in which the obvious hypotheses are homogeneous. They all have the form \"Citizen X did it\" or \"Deity Y did it\". It is easy to see that for particular values of X and Y, one is making an arbitrary selection from a large set of possibilities. This is not the case in quantum foundations. The <a href=\"http://en.wikipedia.org/wiki/Interpretation_of_quantum_mechanics\">well-known interpretations</a> are extremely heterogeneous. There has not been much of an effort made to express them in a common framework - something necessary if we want to apply Occam's razor in the form of <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">theoretical complexity</a> - nor has there been much of an attempt to discern the full \"space\" of possible theories from which they have been drawn - something necessary if we really do wish to avoid privileging the hypotheses we happen to have. Part of the reason is, again, that many of the known options are somewhat underdeveloped as exact theories. They subsist partly on rhetoric and handwaving; they are mathematical vaporware. And it's hard to benchmark vaporware.</p>\n<p>In his <a href=\"/lw/19m/privileging_the_hypothesis/\">latest article</a>, Eliezer presents the following argument:</p>\n<p>\"... there [is] no concrete evidence whatsoever that favors a collapse postulate or single-world quantum mechanics.&nbsp; But, said Scott, we might encounter <em>future</em> evidence in favor of single-world quantum mechanics, and many-worlds still has the open question of the Born probabilities... There must be a trillion better ways to answer the Born question without adding a collapse postulate...\"</p>\n<p>The basic wrong assumption being made is that quantum superposition <em>by default</em> equals multiplicity - that because the wavefunction in the double-slit experiment has two branches, one for each slit, there must be two of something there - and that a single-world interpretation has to add an extra postulate to this picture, such as a collapse process which removes one branch. But superposition-as-multiplicity really is just another hypothesis. When you use ordinary probabilities, you are not rationally obligated to believe that every outcome exists somewhere; and an electron wavefunction really may be describing a single object in a single state, rather than a multiplicity of them.</p>\n<p>A quantum amplitude, being a complex number, is not an ordinary probability; it is, instead, a mysterious quantity from which usable probabilities are derived. Many-worlds says, \"Let's view these amplitudes as realities, and try to derive the probabilities from them.\" But you can go the other way, and say, \"Let's view these amplitudes as derived from the probabilities of a more fundamental theory.\" Mathematical results like Bell's theorem show that this will require a little imagination - you won't be able to derive quantum mechanics as an approximation to a 19th-century type of physics. But we have the imagination; we just need to use it in a disciplined way.</p>\n<p>So that's the kernel of the argument that many worlds is <em>not favored</em>: the hypotheses under consideration are still too much of a mess to even be commensurable, and the informal argument for many worlds, quoted above, simply presupposes a multiplicity interpretation of quantum superposition. How about the argument that many worlds is actually <em>disfavored</em>? That would become a genuinely technical discussion, and when pressed, I would ultimately not insist upon it. We don't know enough about the theory-space yet. Single-world thinking looks more fruitful to me, when it comes to sub-quantum theory-building, but there are versions of many-worlds which I do occasionally like to think about. So the verdict for now has to be: <a href=\"http://en.wikipedia.org/wiki/Not_proven\">not proven</a>; and meanwhile, <a href=\"http://en.wikipedia.org/wiki/Hundred_Flowers_Campaign\">let a hundred schools of thought contend</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1c9": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sLLR5u9NJTfL88Spx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 13, "extendedScore": null, "score": 1.7e-05, "legacy": true, "legacyId": "1648", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["X2AD2LgtKgkRNPj2a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-29T07:56:47.836Z", "modifiedAt": null, "url": null, "title": "Intuitive differences: when to agree to disagree", "slug": "intuitive-differences-when-to-agree-to-disagree", "viewCount": null, "lastCommentedAt": "2020-07-19T20:43:21.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JF4mv4PbTp6ckN3vG/intuitive-differences-when-to-agree-to-disagree", "pageUrlRelative": "/posts/JF4mv4PbTp6ckN3vG/intuitive-differences-when-to-agree-to-disagree", "linkUrl": "https://www.lesswrong.com/posts/JF4mv4PbTp6ckN3vG/intuitive-differences-when-to-agree-to-disagree", "postedAtFormatted": "Tuesday, September 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intuitive%20differences%3A%20when%20to%20agree%20to%20disagree&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntuitive%20differences%3A%20when%20to%20agree%20to%20disagree%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJF4mv4PbTp6ckN3vG%2Fintuitive-differences-when-to-agree-to-disagree%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intuitive%20differences%3A%20when%20to%20agree%20to%20disagree%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJF4mv4PbTp6ckN3vG%2Fintuitive-differences-when-to-agree-to-disagree", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJF4mv4PbTp6ckN3vG%2Fintuitive-differences-when-to-agree-to-disagree", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1241, "htmlBody": "<p>Two days back, I had a rather frustrating disagreement with a friend. The debate rapidly hit a point where it seemed to be going nowhere, and we spent a while going around in circles before agreeing to change the topic. Yesterday, as I was riding the subway, things clicked. I suddenly realized not only what the disagreement had actually been about, but also what several previous disagreements we'd had were about. In all cases, our opinions and arguments had been grounded in opposite intuitions:</p>\n<ul>\n<li><strong>Kaj's intuition.</strong> In general, we can eventually learn to understand a phenomenon well enough to create a model that is flexible and robust. Coming up with the model is the hard part, but once that is done, adapting the general model to account for specific special cases is a relatively straightforward and basically mechanical process.</li>\n<li><strong>Friend's intuition.</strong> In general, there are some phenomena which are too complex to be accurately modeled. Any model you create for them is bristle and inflexible: adapting the general model to account for specific special cases takes almost as much work as creating the original model in the first place.</li>\n</ul>\n<p>You may notice that these intuitions are not mutually exclusive in the strict sense. They could both be right, one of them covering certain classes of things and the other the remaining ones. And neither one is obviously and blatantly false - both have evidence supporting them. So the disagreement is not about which one is right, as such. Rather, it's a question of which one is <em>more</em> right, which is the one with broader applicability.</p>\n<p>As soon as I realized this, I also realized two other things. One, whenever we would run into this difference in the future, we'd need to recognize it and stop that line of debate, for it wouldn't be resolved before the root disagreement had been solved. Two, actually resolving that core disagreement would take so much time and energy that it probably wouldn't be worth the effort.<a id=\"more\"></a></p>\n<p>The important thing to realize is that neither intuition rests on any particular piece of evidence. Instead, each one is a general outlook that has been formed over many years and countless pieces of evidence, most of which have already been forgotten. Before my realization, neither of us had even consciously known they existed. They are abstract patterns our minds have extracted from what must be hundreds of different cases we've encountered, very high-level hypotheses that have been repeatedly tested and found to be accurate.</p>\n<p>It would be impossible to find out which was the more applicable one by means of regular debate. Each of us would have to gather all the evidence that led to the formulation of the intuition in the first place. Pulling a number out of my hat, I'd guess that a comprehensive overview of that evidence (for one intuition) would run at least a hundred pages long. Furthermore, it wouldn't be sufficient for each of us to simply read the other side's overview, once it had been gathered. By this point, we would be interpreting the evidence in light of our already existing intuition. I wouldn't be surprised if simply reading through the summary would lead to both sides only being more certain of their own intuition being right. We would have to take the time to discuss each individual item in detail.</p>\n<p>And if a real attempt to sort out the difference is hard, resolving it in the middle of a debate about something else is impossible. Both sides in the debate will have an opinion they think is <em>obvious</em> and be puzzled as to why the other side can consistently fail to get something so obvious. At the same time, neither can access the evidence that leads them to consider their opinion so obvious, and both will grow increasingly frustrated at both the other side's bone-headedness and their own failure to properly communicate something that shouldn't even need explaining.</p>\n<p>In many cases, trying to resolve an intuitive difference simply isn't worth the effort. Learn to recognize your intuitive differences, and you'll know when to break off debates once they hit that difference. Putting those intuitions in words still helps understanding, though. When I told my friend the things I've just written here, she agreed, and we were able to have a constructive dialogue about those differences. (While doing so, and returning to the previous day's topic, we were able to identify at least five separate points of disagreement that were all rooted in the same intuitive difference.) Each one was also able to explain, on a rough level, some of the background that supported their intuition. In the end, we still didn't agree, but at least we understood each other's positions a little better.</p>\n<p>But what if the intuitive difference is about something really important? Me and my friend resolved to just wait things out and see whose hypothesis would turn out more accurate, but sometimes the difference might affect big decisions about the actions you want to take. (Robin's and Eliezer's disagreement on the nature of the Singularity comes to mind.) What if the disagreement really needs to be solved?</p>\n<p>I'm not sure how well it can be done, but one could try. First off, both need to realize that in all likelihood, both intuitions have a large grain of truth to them. Like with me and my friend, the question is often one of the breadth of applicability, not of a strict truth or falsehood. Once the basic positions have been formulated, both should ask <a href=\"/lw/vk/back_up_and_ask_whether_not_why/\">whether, not why</a>. Assign some certainty value on the likelyhood of your intuition being the more correct one, and then consider the fact that your \"opponent\" has spent many years analyzing evidence to reach this position and might very well be right. Adjust your certainty downwards to account for this realization. Then take a few weeks considering both the things that may have led you to formulate this intuition, as well as the things that might have led your opponent to theirs. Spend time gathering evidence for both sides of the view, and be sure to give each piece of evidence a balanced view: half of the time you'll first consider a case from the PoV of your opponent's hypothesis, then of your own. Half of the time you'll do it the other way around. Commit all such considerations in writing and present them to your opponent an regular intervals, taking the time to discuss them through. This is no time for <a href=\"http://wiki.lesswrong.com/wiki/Motivated_skepticism\">motivated skepticism</a> - both of you need to have genuine <a href=\"http://wiki.lesswrong.com/wiki/Crisis_of_faith\">crisis of faith</a> in order for things to get anywhere.</p>\n<p>Not every disagreement is an intuitive difference. Any disagreement that rests on particular pieces of evidence and can be easily resolved with the correct empirical evidence isn't one. If it feels like one of the intuitions is strictly false instead of having a large grain of truth to it, it's still an intuitive difference, but not the kind of one that I have been covering here. An intuitive difference is also kind of related to, but different from, an <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a>. In order to resolve it, a lot of information needs to be absorbed, but by both partners, not simply the other. It's not a question of having different evidence: theoretically, you might both even have <em>exactly the same </em>evidence, but gathered in a different order. The question is one of differing interpretations, not raw data as such.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "wzgcQCrwKfETcBpR9": 2, "AADZcNS24mmSfPp2w": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JF4mv4PbTp6ckN3vG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 29, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "1651", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nHRri2F49MZbcqmkY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-09-30T23:52:52.381Z", "modifiedAt": null, "url": null, "title": "NY-area OB/LW meetup Saturday 10/3 7 PM", "slug": "ny-area-ob-lw-meetup-saturday-10-3-7-pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:34.729Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/spuqrkxvbg8W7orvA/ny-area-ob-lw-meetup-saturday-10-3-7-pm", "pageUrlRelative": "/posts/spuqrkxvbg8W7orvA/ny-area-ob-lw-meetup-saturday-10-3-7-pm", "linkUrl": "https://www.lesswrong.com/posts/spuqrkxvbg8W7orvA/ny-area-ob-lw-meetup-saturday-10-3-7-pm", "postedAtFormatted": "Wednesday, September 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NY-area%20OB%2FLW%20meetup%20Saturday%2010%2F3%207%20PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANY-area%20OB%2FLW%20meetup%20Saturday%2010%2F3%207%20PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspuqrkxvbg8W7orvA%2Fny-area-ob-lw-meetup-saturday-10-3-7-pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NY-area%20OB%2FLW%20meetup%20Saturday%2010%2F3%207%20PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspuqrkxvbg8W7orvA%2Fny-area-ob-lw-meetup-saturday-10-3-7-pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fspuqrkxvbg8W7orvA%2Fny-area-ob-lw-meetup-saturday-10-3-7-pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>Since lots of us anticipate being in New York this weekend for <a href=\"http://www.singularitysummit.com/\">Singularity Summit 2009</a>, it seems like a great time to hold a NY-area Overcoming Bias/Less Wrong meetup! Whether you're attending the summit or not, do consider dropping by the 92nd Street Marriott any time after 7PM, where you will find a lively group of rationalists discussing and dissecting the day's events, and the ideas presented by the summit speakers. We'll be meeting in the hotel's internal restaurant, which should be open until well after midnight.</p>\n<p><a href=\"http://www.facebook.com/event.php?eid=164749662462#/event.php?eid=164749662462\">RSVP at the Facebook Group</a> or <a href=\"https://www.google.com/calendar/event?action=TEMPLATE&amp;tmeid=X2Nrb2pjZDFuNmdzamNkaGk2Z3IzNGczNmM1aG1hb2pmZHRsaXNvcmZkayBzaW5naW5zdC5vcmdfdm92b2tqNnVsN3JmNWxvMmJnOW0xZmk3c3NAZw&amp;tmsrc=c2luZ2luc3Qub3JnX3Zvdm9rajZ1bDdyZjVsbzJiZzltMWZpN3NzQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20\">see this event in Google Calendar</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "spuqrkxvbg8W7orvA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 5.262223912919381e-07, "legacy": true, "legacyId": "1659", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-01T12:44:38.676Z", "modifiedAt": null, "url": null, "title": "Regular NYC Meetups", "slug": "regular-nyc-meetups", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.684Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eHiicreYQBZovxtDr/regular-nyc-meetups", "pageUrlRelative": "/posts/eHiicreYQBZovxtDr/regular-nyc-meetups", "linkUrl": "https://www.lesswrong.com/posts/eHiicreYQBZovxtDr/regular-nyc-meetups", "postedAtFormatted": "Thursday, October 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Regular%20NYC%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARegular%20NYC%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeHiicreYQBZovxtDr%2Fregular-nyc-meetups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Regular%20NYC%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeHiicreYQBZovxtDr%2Fregular-nyc-meetups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeHiicreYQBZovxtDr%2Fregular-nyc-meetups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p><a href=\"/lw/1a3/nyarea_oblw_meetup_saturday_103_7_pm/155e\">Sayeth Jasen</a>:</p>\n<div id=\"body_t1_155e\" class=\"comment-content\">\n<div class=\"md\">\n<blockquote>\n<p>This is an excellent opportunity to announce that I recently organized an OB/LW discussion group that meets in NYC twice a month. We had been meeting sporadically ever since Robin's visit back in April. The regular meetings only started about a month ago and have been great fun. Here is the google group we've been using to organize them:</p>\n<p><a href=\"http://groups.google.com/group/overcomingbiasnyc\">http://groups.google.com/group/overcomingbiasnyc</a></p>\n<p>We meet every 2nd Saturday at 11:00am and every 4th Tuesday at 6:00pm at Georgia's Bake Shop (on the corner of 89th street and Broadway). The deal is that I show up every time and stay for at least two hours regardless of whether or not anyone else comes.</p>\n<p>I've been meaning to post this for a while but I don't have enough Karma...</p>\n</blockquote>\n<p>A couple thoughts:</p>\n<ul>\n<li>We're trying to build a community here, and meetups are wonderful, <em>wonderful</em> things. I would recommend that you all follow Jasen's <em>excellent</em> example and see whether OB/LW meetups are happening in your area, and if not, what you can do about that. I'd even suggest an if-you-build-it-they-will-come mentality. We don't really know how many voiceless readers there are for each active commenter. Even if you know of no other LW commenters in your area, <em>try</em> making an announcement that you'll be at such-and-such a place and <em>see what happens</em>.</li>\n<li>The karma system is all well and good, but if you have something of high, obvious value on your hands, like, say, regular LW meetups in a major city, you should feel <em>absolutely free</em> to pester one of us high-karma folk (we live in a convenient bar off to the side) to post it on your behalf. It seems plausible that there should also be a monthly thread in which low-karma folk can post these things as comments and high-karma folk can repost them as top-level posts. What say we?</li>\n</ul>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eHiicreYQBZovxtDr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 5.263526106262018e-07, "legacy": true, "legacyId": "1660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-01T12:49:19.084Z", "modifiedAt": null, "url": null, "title": "Open Thread: October 2009", "slug": "open-thread-october-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:59.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sihh7JxyE2Xa3RKA5/open-thread-october-2009", "pageUrlRelative": "/posts/sihh7JxyE2Xa3RKA5/open-thread-october-2009", "linkUrl": "https://www.lesswrong.com/posts/sihh7JxyE2Xa3RKA5/open-thread-october-2009", "postedAtFormatted": "Thursday, October 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20October%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20October%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsihh7JxyE2Xa3RKA5%2Fopen-thread-october-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20October%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsihh7JxyE2Xa3RKA5%2Fopen-thread-october-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsihh7JxyE2Xa3RKA5%2Fopen-thread-october-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<p>Hear ye, hear ye: commence the discussion of things which have not been discussed.</p>\n<p>As usual, if a discussion gets particularly good, spin it off into a posting.</p>\n<p>(For this Open Thread, I'm going to try something new: priming the pump with a few things I'd like to see discussed.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sihh7JxyE2Xa3RKA5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 5.263532296616477e-07, "legacy": true, "legacyId": "1635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 436, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-01T16:23:42.085Z", "modifiedAt": null, "url": null, "title": "Why Don\u2019t We Apply What We Know About Twins to Everybody Else?", "slug": "why-don-t-we-apply-what-we-know-about-twins-to-everybody", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:30.266Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2WmQMCFLeEJJ6CrnS/why-don-t-we-apply-what-we-know-about-twins-to-everybody", "pageUrlRelative": "/posts/2WmQMCFLeEJJ6CrnS/why-don-t-we-apply-what-we-know-about-twins-to-everybody", "linkUrl": "https://www.lesswrong.com/posts/2WmQMCFLeEJJ6CrnS/why-don-t-we-apply-what-we-know-about-twins-to-everybody", "postedAtFormatted": "Thursday, October 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20Don%E2%80%99t%20We%20Apply%20What%20We%20Know%20About%20Twins%20to%20Everybody%20Else%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20Don%E2%80%99t%20We%20Apply%20What%20We%20Know%20About%20Twins%20to%20Everybody%20Else%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2WmQMCFLeEJJ6CrnS%2Fwhy-don-t-we-apply-what-we-know-about-twins-to-everybody%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20Don%E2%80%99t%20We%20Apply%20What%20We%20Know%20About%20Twins%20to%20Everybody%20Else%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2WmQMCFLeEJJ6CrnS%2Fwhy-don-t-we-apply-what-we-know-about-twins-to-everybody", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2WmQMCFLeEJJ6CrnS%2Fwhy-don-t-we-apply-what-we-know-about-twins-to-everybody", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 335, "htmlBody": "<p><span style=\"font-size: 14px;\">I think our intuition might be miscalibrated when it comes to evaluating how much a person&rsquo;s genes impact how they turn out physically (which isn&rsquo;t surprising). What&rsquo;s a bit strange is that we seem to be closer to the truth when it comes to twins.</span></p>\n<p style=\"font-size: 1.05em;\">Nobody&rsquo;s surprised when identical twins turn out to have very similar bodies (weight, muscle mass, etc), even into adulthood.</p>\n<p style=\"font-size: 1.05em;\">But when it comes to non-twins, people seem to think that &ldquo;making the right choices&rdquo; and &ldquo;willpower&rdquo; are primary factors in how human bodies turn out, and that we can assign a good amount of personal credit or blame to individuals for good and bad outcomes.</p>\n<p style=\"font-size: 1.05em;\">There is a disconnect between these two visions, and I think that it&rsquo;s the latter that needs to be <a href=\"http://wiki.lesswrong.com/wiki/Updating\">updated</a>.</p>\n<p style=\"font-size: 1.05em;\">After all, even if we put aside the direct ways in which our genes build our bodies (encoding how our tissues grow) and instead look at our abilities to &ldquo;make the right choices&rdquo; and exert &ldquo;willpower&rdquo;, we find that those are also greatly determined by genetic factors. Identical twins probably turn out very similar in good part because they have almost identical amounts of those qualities of mind.<a id=\"more\"></a></p>\n<p style=\"font-size: 1.05em;\"><strong></strong>This doesn&rsquo;t mean that all is pre-determined and that if we all stop trying we&rsquo;ll turn out the same we would have otherwise, but rather that we are playing within certain parameters, and that the part we control is probably smaller than most people think (not non-existent &mdash; we still deserve some credit &mdash; just more modest).</p>\n<p style=\"font-size: 1.05em;\">To be clear, I&rsquo;m not saying the situation was white and we thought it was black, or even that it&rsquo;s a black &amp; white thing, but rather that most people&rsquo;s intuition might be the wrong shade of gray. Otherwise, I would think there would be a bigger variation between identical twins, but they spend their lives making different choices yet most stay very similar to each other (as far as I know &mdash; if you know of a study on this, please send it my way).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2WmQMCFLeEJJ6CrnS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 20, "extendedScore": null, "score": 5.263895819909666e-07, "legacy": true, "legacyId": "1662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-01T21:35:33.419Z", "modifiedAt": null, "url": null, "title": "Are you a Democrat singletonian, or a Republican singletonian?", "slug": "are-you-a-democrat-singletonian-or-a-republican-singletonian", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:36.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3JnuoQCN9x2zGo6yh/are-you-a-democrat-singletonian-or-a-republican-singletonian", "pageUrlRelative": "/posts/3JnuoQCN9x2zGo6yh/are-you-a-democrat-singletonian-or-a-republican-singletonian", "linkUrl": "https://www.lesswrong.com/posts/3JnuoQCN9x2zGo6yh/are-you-a-democrat-singletonian-or-a-republican-singletonian", "postedAtFormatted": "Thursday, October 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20you%20a%20Democrat%20singletonian%2C%20or%20a%20Republican%20singletonian%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20you%20a%20Democrat%20singletonian%2C%20or%20a%20Republican%20singletonian%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JnuoQCN9x2zGo6yh%2Fare-you-a-democrat-singletonian-or-a-republican-singletonian%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20you%20a%20Democrat%20singletonian%2C%20or%20a%20Republican%20singletonian%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JnuoQCN9x2zGo6yh%2Fare-you-a-democrat-singletonian-or-a-republican-singletonian", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3JnuoQCN9x2zGo6yh%2Fare-you-a-democrat-singletonian-or-a-republican-singletonian", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Some people say that the difference between Republicans and Democrats is that Republicans are conservative in the sense of opposing change, while Democrats are liberal in the sense of promoting change.&nbsp; But this isn't true - both parties want change; neither especially cares how things were done in the past.</p>\n<p>Some people say that Republicans are fiscally conservative, while Democrats are fiscally liberal.&nbsp; But this isn't true.&nbsp; Republicans and Democrats both run up huge deficits; they just spend the money on different things.</p>\n<p>Some people say Democrats are liberal in the sense of favoring liberty.&nbsp; But this isn't true.&nbsp; Republicans want freedom to own guns and run their businesses as they please, while Democrats want the freedom to have abortions and live as they please.</p>\n<p>Someone - it may have been George Lakoff - observed that Republicans want government to be their daddy, while Democrats want government to be their mommy.&nbsp; That's the most-helpful distinction that I've heard.&nbsp; Republicans want a government that's stern and and protects them from strangers.&nbsp; Democrats want a government that's forgiving and takes care of all their needs.<a id=\"more\"></a></p>\n<p>I was thinking about this because of singletons.&nbsp; Some people are in favor of creating a singleton AI to rule the universe.&nbsp; I assume that, as with party affiliation, people choose a position for emotional rather than rational reasons.&nbsp; So which type of person would want a singleton - a daddy-seeking Republican, or a mommy-seeking Democrat?</p>\n<p>I think the answer is, Both.&nbsp; Republicans and Democrats would both want a singleton to take care of them; just in different ways.&nbsp; Those who don't want a singleton at all would be Libertarians.</p>\n<p>Regardless of whether you think a singleton is a good idea or a bad idea - does this mean that Americans would overwhelmingly vote to construct a singleton, if they were given the choice?</p>\n<p>And would the ideas about how to design that singleton break down along party lines?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3JnuoQCN9x2zGo6yh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": -14, "extendedScore": null, "score": 5.264422234183127e-07, "legacy": true, "legacyId": "1663", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-02T06:40:11.245Z", "modifiedAt": null, "url": null, "title": "Scott Aaronson on Born Probabilities", "slug": "scott-aaronson-on-born-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:35.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p9g23Aka8kiPDpppy/scott-aaronson-on-born-probabilities", "pageUrlRelative": "/posts/p9g23Aka8kiPDpppy/scott-aaronson-on-born-probabilities", "linkUrl": "https://www.lesswrong.com/posts/p9g23Aka8kiPDpppy/scott-aaronson-on-born-probabilities", "postedAtFormatted": "Friday, October 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scott%20Aaronson%20on%20Born%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScott%20Aaronson%20on%20Born%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9g23Aka8kiPDpppy%2Fscott-aaronson-on-born-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scott%20Aaronson%20on%20Born%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9g23Aka8kiPDpppy%2Fscott-aaronson-on-born-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9g23Aka8kiPDpppy%2Fscott-aaronson-on-born-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1198, "htmlBody": "<p><!-- /* Font Definitions */ @font-face {font-family:\u5b8b\u4f53; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-alt:SimSun; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} @font-face {font-family:\"Cambria Math\"; panose-1:2 4 5 3 5 4 6 3 2 4; mso-font-charset:1; mso-generic-font-family:roman; mso-font-format:other; mso-font-pitch:variable; mso-font-signature:0 0 0 0 0 0;} @font-face {font-family:Calibri; panose-1:2 15 5 2 2 2 4 3 2 4; mso-font-charset:0; mso-generic-font-family:swiss; mso-font-pitch:variable; mso-font-signature:-520092929 1073786111 9 0 415 0;} @font-face {font-family:\"\\@\u5b8b\u4f53\"; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:\"\"; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} a:link, span.MsoHyperlink {mso-style-priority:99; color:blue; mso-themecolor:hyperlink; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {mso-style-noshow:yes; mso-style-priority:99; color:purple; mso-themecolor:followedhyperlink; text-decoration:underline; text-underline:single;} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoPapDefault {mso-style-type:export-only; margin-bottom:10.0pt; line-height:115%;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.0in 1.0in 1.0in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --></p>\n<!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]-->\n<p class=\"MsoNormal\">This post attempts to <a href=\"/lw/12y/the_popularization_bias/\">popularize</a> some of <a href=\"http://www.scottaaronson.com/\">Scott Aaronson</a>'s <a href=\"http://www.scottaaronson.com/democritus/lec9.html\">lectures</a> and <a href=\"http://arxiv.org/abs/quant-ph/0401062\">research</a> <a href=\"http://arxiv.org/abs/quant-ph/0412187\">results</a> relating to Born probabilities. I think they represent a significant step towards answering the question \"<a href=\"/lw/py/the_born_probabilities/\">Why Born's rule?</a>\" but do not seem to be very well known. Prof. Aaronson writes frequently on his popular blog, <a href=\"http://www.scottaaronson.com/blog/\">Shtetl-Optimized</a>, but is apparently too modest to use it to do much promotion of his own ideas. I hope he doesn&rsquo;t mind that I take up this task (and that he forgives any errors and misunderstandings I may have committed here).</p>\n<p class=\"MsoNormal\">Before I begin, I want to point out something that has been bugging me about the fictional <a href=\"/lw/ps/where_physics_meets_experience/\">Ebborian physics</a>, which will eventually lead us to Aaronson's ideas. So, let&rsquo;s first recall the following passage from Eliezer&rsquo;s story:</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a></p>\n<blockquote>\n<p class=\"MsoNormal\">\"And we also discovered,\" continues Po'mi, \"that our very planet of Ebbore, including all the people on it, has a four-dimensional thickness, and is constantly fissioning along that thickness, just as our brains do.&nbsp; Only the fissioned sides of our planet do not remain in contact, as our new selves do; the sides separate into the fourth-dimensional void.\"</p>\n<p class=\"MsoNormal\">&hellip;</p>\n<p class=\"MsoNormal\">\"Well,\" says Po'mi, \"when the world splits down its four-dimensional thickness, it does not always split exactly evenly.&nbsp; Indeed, it is not uncommon to see nine-tenths of the four-dimensional thickness in one side.\"</p>\n<p class=\"MsoNormal\">...</p>\n<p class=\"MsoNormal\">\"Now,\" says Po'mi, \"if fundamental physics has nothing to do with consciousness, can you tell me why the subjective probability of finding ourselves in a side of the split world, should be exactly proportional to the square of the thickness of that side?\"</p>\n</blockquote>\n<p class=\"MsoNormal\">Ok, so the part that&rsquo;s been bugging me is, suppose an Ebborian world splits twice, first into 1/3 and 2/3 of the original thickness (slices A and B respectively), then the B slice splits exactly in half, into two 1/3 thickness slices (C and D). Before the splitting, with what probability should you anticipate ending up in the slices A, C and D? Well, according to the squaring rule, you have 1/5 chance of ending up in A, and 4/5 chance of ending up in B. Those in B then have equal chance of ending up in C and D, so each of them gets a final probability of 2/5.</p>\n<p class=\"MsoNormal\">Well, that&rsquo;s not how quantum branching works! There, the probability of ending up in any branch depends only on the final amplitude of that branch, not on the order in which branching occurred. This makes perfect sense since decoherence is not a instantaneous process, and thinking of it as branching is only an approximation because worlds never completely split off and become totally independent of one another. In QM, the &ldquo;order of branching&rdquo; is not even well defined, so how can probabilities depend on it?</p>\n<p class=\"MsoNormal\">Suppose we want to construct an Ebborian physics where, like in QM, the probability of ending up in any slice depends only on the thickness of that slice, and not on the order in which splitting occurs, how do we go about doing that? Simple, we just make that probability a function of the absolute thickness of a slice, instead of having it depend on the relative thickness at each splitting.</p>\n<p class=\"MsoNormal\">So let&rsquo;s say that the subjective probability of ending up in any slice is proportional to the square of the absolute thickness of that slice, and consider the above example again. When the world splits into A and B, the probabilities are again 1/5 and 4/5 respectively. But when B splits again into C and D, A goes from probability 1/5 to 1/3, and C and D each get 1/3. That&rsquo;s pretty weird&hellip; what&rsquo;s going on this time?</p>\n<p class=\"MsoNormal\">To use <a href=\"http://www.scottaaronson.com/democritus/lec9.html\">Aaronson&rsquo;s language</a>, splitting is not a 2-norm preserving transformation; it only preserves the 1-norm. Or to state this more plainly, splitting conserves the sum of the individual slices&rsquo; thicknesses, but not the sum of the squares of the individual thicknesses. So in order to apply the squaring rule and get a set of probabilities that sum to 1 at the end, we have to renormalize, and this renormalizing can cause the probability of a slice to go up or down, depending purely on what happens to <em>other</em> slices that it otherwise would have nothing to do with.</p>\n<p class=\"MsoNormal\">Note that in quantum mechanics, the evolution of a wavefunction always preserves its 2-norm, not its 1-norm (nor p-norm for any p&ne;2). If we were to use any probability rule other than the squaring rule in QM, we would have to renormalize and thereby encounter this same issue: the probability of a branch would go up or down depending on other parts of the wavefunction that it otherwise would have little interaction with.</p>\n<p class=\"MsoNormal\">At this point you might ask, &ldquo;Ok, this seems unusual and counterintuitive, but lots of physics are counterintuitive. Is there some other argument that the probability rule shouldn&rsquo;t involve renormalization?&rdquo; And the answer to that is yes, because to live in a world with probability renormalization would be to have magical powers, including the ability to solve NP-complete problems in polynomial time. (And to turn this into a full anthropic explanation of the Born rule, similar to the anthropic explanations for other physical laws and constants, we just have to note that intelligence seems to have little evolutionary value in such a world. But that&rsquo;s my position, not Aaronson&rsquo;s, or at least he hasn&rsquo;t argued for this additional step in public, as far as I know.)</p>\n<p class=\"MsoNormal\">Aaronson actually proved that problems in <a href=\"http://en.wikipedia.org/wiki/PP_(complexity)\">PP</a>, which are commonly believed to be even harder than NP problems, can be solved in polynomial time using <a href=\"http://arxiv.org/abs/quant-ph/0412187\">&ldquo;fantasy&rdquo; quantum computers</a> that use variants of Born&rsquo;s rule where the exponent doesn't equal 2. But it turns out that the power of these computers has nothing to do with quantum computing, but instead has everything to do with probability renormalization. So here I&rsquo;ll show how we can solve NP-complete (instead of PP since it&rsquo;s easier to think about) problems in polynomial time using the modified Ebborian physics that I described above.</p>\n<p class=\"MsoNormal\">The idea is actually very easy to understand. Each time an Ebborian world slice splits, its descendant slices decrease in total probability, while every other slice increases in probability. (Recall how when B split, A&rsquo;s probability went from 1/5 to 1/3, and B&rsquo;s 4/5 became a total of 2/3 for C and D.) So to take advantage of this, we first split our world into an exponential number of slices of equal thickness, and let each slice try a different possible solution to the NP-complete problem. If a slice finds that its candidate solution is a correct one, then it does nothing, otherwise it splits itself a large number of times. Since that greatly decreases their own probabilities, and increases the probabilities of the slices that didn&rsquo;t split at the end, we should expect to find ourselves in one of the latter kind of slices when the computation finishes, which (surprise!) happens to be one that found a correct solution. Pretty neat, right?</p>\n<p class=\"MsoNormal\"><strong>ETA:</strong> The lecture notes and papers I linked to also give explanations for other aspects of quantum mechanics, such as why it is linear, and why it preserves the 2-norm and not some other p-norm. Read them to find out more.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p9g23Aka8kiPDpppy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 39, "extendedScore": null, "score": 0.000109, "legacy": true, "legacyId": "1661", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["57YjxPCRfZpXNJw49", "3ZKvf9u2XEWddGZmS", "WajiC3YWeJutyAXTn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-02T10:50:11.836Z", "modifiedAt": null, "url": null, "title": "'oy, girls on lw, want to get together some time?'", "slug": "oy-girls-on-lw-want-to-get-together-some-time", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:01.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xtEK9upWLnYykDxXj/oy-girls-on-lw-want-to-get-together-some-time", "pageUrlRelative": "/posts/xtEK9upWLnYykDxXj/oy-girls-on-lw-want-to-get-together-some-time", "linkUrl": "https://www.lesswrong.com/posts/xtEK9upWLnYykDxXj/oy-girls-on-lw-want-to-get-together-some-time", "postedAtFormatted": "Friday, October 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'oy%2C%20girls%20on%20lw%2C%20want%20to%20get%20together%20some%20time%3F'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'oy%2C%20girls%20on%20lw%2C%20want%20to%20get%20together%20some%20time%3F'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtEK9upWLnYykDxXj%2Foy-girls-on-lw-want-to-get-together-some-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='oy%2C%20girls%20on%20lw%2C%20want%20to%20get%20together%20some%20time%3F'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtEK9upWLnYykDxXj%2Foy-girls-on-lw-want-to-get-together-some-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxtEK9upWLnYykDxXj%2Foy-girls-on-lw-want-to-get-together-some-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>2:45:24 PM <a href=\"http://meteuphoric.wordpress.com/\">Katja Grace</a>: The main thing that puts me off in online dating profiles is lack of ambition to save the world<br />2:45:35 PM Katja Grace: Or do anything much<br />2:48:03 PM Michael Blume: *nods*<br />2:48:07 PM Michael Blume: this is indeed a problem<br />2:57:55 PM Katja Grace: Maybe there is a dating site for smart ambitious nerds somewhere<br />2:58:25 PM Katja Grace: Need to set up lw extension perhaps<br />2:59:02 PM Michael Blume: haha, yes ^^<br />3:00:40 PM Katja Grace: Plenty of discussion on why few girls, how to get girls, nobody ever says 'oy, girls on lw, want to get together some time?'<br />3:01:14 PM Michael Blume: somebody really should say that<br />3:01:34 PM Michael Blume: hell, I'm tempted to just copy that IM into a top-level post and click 'submit'<br />3:01:48 PM Katja Grace: Haha dare you to</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "W9aNkPwtPhMrcfgj7": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xtEK9upWLnYykDxXj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 63, "baseScore": 38, "extendedScore": null, "score": 6.2e-05, "legacy": true, "legacyId": "1664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 184, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-03T03:36:49.690Z", "modifiedAt": null, "url": null, "title": "When Willpower Attacks", "slug": "when-willpower-attacks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:02.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SNP6iGikzdf8k95Lq/when-willpower-attacks", "pageUrlRelative": "/posts/SNP6iGikzdf8k95Lq/when-willpower-attacks", "linkUrl": "https://www.lesswrong.com/posts/SNP6iGikzdf8k95Lq/when-willpower-attacks", "postedAtFormatted": "Saturday, October 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20Willpower%20Attacks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20Willpower%20Attacks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNP6iGikzdf8k95Lq%2Fwhen-willpower-attacks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20Willpower%20Attacks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNP6iGikzdf8k95Lq%2Fwhen-willpower-attacks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSNP6iGikzdf8k95Lq%2Fwhen-willpower-attacks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 880, "htmlBody": "<p>Less Wrong has held many discussions of willpower. All of them have focused on the cases where willpower fails, and its failure causes harm, such as procrastination, overeating and addiction. Collectively, we call these behaviors akrasia. Akrasia is any behavior that we believe is harmful, but do anyways due to a lack of willpower. Akrasia, however, represents only a small subset of the cases in which willpower fails, and focusing on it too much creates an availability bias that skews our perception of what willpower is, how it works and how much of it is desireable. To counter this bias, I present here some common special cases where strong willpower is harmful or even fatal.</p>\r\n<p><a id=\"more\"></a></p>\r\n<p>Consider the human sleep cycle. By default, we settle into an equilibrium in which we sleep for about eight hours per day, starting at about the same time each evening. We can override the normal sleep schedule, and stay awake when we should be asleep, or sleep when we should be awake, by applying willpower. For this example, we'll confine the definition of willpower to just this one ability. Willpower is the ability to control sleep times; we'll leave the ability to work hard and resist cake for later.</p>\r\n<p>We use sleep-willpower to arrange our hours strategically. However, we can only do so within certain constraints, which cannot be overriden by conscious choice. It isn't possible to refrain from sleeping entirely, because the amount of willpower required to stay awake increases with the amount sleep missed, until eventually it exceeds the amount of willpower available. Attempting to minimize sleep for a long enough period will eventually cause a shift to an alternative equilibrium, polyphasic sleep, in which all but the most important stage of sleep (REM) are skipped. It's harder to stay awake in some circumstances, like dark rooms and boring lectures, than in others. And it's impossible to sleep too much more than normal; the better rested we are, the harder it is to fall asleep. These restrictions are protective; when rats are artificially prevented from sleeping for too long, they die, and when humans use stimulants to stay awake too long, they die too. If a human had unlimited sleep-willpower, then they would risk serious injury every time they used it.</p>\r\n<p>Next, consider diet-willpower. Humans have a system that decides what is good to eat, which manifests as cravings and taste preferences. Many people try to override this mechanism, usually for health reasons, replacing them with advice from a perceived authority. Consider a hypothetical person who decides to follow a highly restrictive diet, to lose weight. Unfortunately, most of the diet advice available is bad; diet advice available in the recent past was very bad; and some of the diet advice floating around is <em>disastrously</em> bad. Restrictive diets tend to have harmful deficiencies. If that deficiency is one of the ones that our biochemical diet manager knows how to handle, then following the diet will require willpower. The more severe the deficiency, the more willpower will be required. Try to follow a no-fat diet, and lapses will be the only thing keeping you alive.</p>\r\n<p>In these examples, we have a mechanism which provides defaults, and a mechanism for overriding them. In general, we call the defaults \"System 1\", and the override mechanism \"System 2\"; and whatever influence System 2 has over System 1, we call willpower. System 1 represents a general policy, and System 2 adjusts it for the present circumstances. This pattern applies across many different and sometimes unrelated mechanisms, and we use the term \"willpower\" for all of them. The larger the adjustment, or the longer it is applied, the more willpower is required; and the different implementations of willpower - sleep-willpower, diet-willpower, work-willpower, etc - all require resources, such as glucose and focused attention, which are finite and fungible. When willpower fails, it usually \"breaks\" - that is, it fails all at once - rather than failing gradually. Deviating from innate or habitual behavior requires willpower. Willpower is reduced by pain, discomfort, and many types of biochemical imbalance. Sustained application of willpower may adjust habits and produce a new equilibrium, but if no such equilibrium exists, it will eventually fail and cause a reversion or overcorrection in the opposite direction.</p>\r\n<p>Willpower lets us adjust our behavior to match our decisions, and willpower failures protect us from mistakes. Generals often successfully convince their soldiers that they should be courageous and always fight to the death, but when stressed enough, they'll break, desert or surrender, and survive. For soldiers on the losing side of a battle, excess willpower is fatal. Teenagers sometimes pledge to stay celibate. Their psychology is designed to make sure they don't keep that pledge. Married couples pledge monogamy, but women with infertile husbands and men with infertile wives tend to cheat. When stubborn people argue, one of them has to give up and end the conversation eventually. The sooner that happens, the less time they waste and the fewer black eyes they get. Workers may be convinced to keep a frantic pace, but sheer laziness will keep them from working hard enough to injure themselves.</p>\r\n<p>The next time you see a willpower hack, remember that willpower isn't always a good thing. Sometimes, what we think of as akrasia is actually protecting us from harm.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YrLoz567b553YouZ2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SNP6iGikzdf8k95Lq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 22, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "1523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-05T02:45:02.960Z", "modifiedAt": "2020-07-04T22:41:33.198Z", "url": null, "title": "Dying Outside", "slug": "dying-outside", "viewCount": null, "lastCommentedAt": "2021-10-27T04:50:19.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HalFinney", "createdAt": "2009-02-27T06:39:24.867Z", "isAdmin": false, "displayName": "HalFinney"}, "userId": "yoBXvu9p4eYkPy7Qi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bshZiaLefDejvPKuS/dying-outside", "pageUrlRelative": "/posts/bshZiaLefDejvPKuS/dying-outside", "linkUrl": "https://www.lesswrong.com/posts/bshZiaLefDejvPKuS/dying-outside", "postedAtFormatted": "Monday, October 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dying%20Outside&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADying%20Outside%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbshZiaLefDejvPKuS%2Fdying-outside%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dying%20Outside%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbshZiaLefDejvPKuS%2Fdying-outside", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbshZiaLefDejvPKuS%2Fdying-outside", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 725, "htmlBody": "<p style=\"padding-left: 30px;\"><em>A man goes in to see his doctor, and after some tests, the doctor says, \"I'm sorry, but you have a fatal disease.\"</em></p>\n<p style=\"padding-left: 30px;\"><em>Man: \"That's terrible! How long have I got?\"</em></p>\n<p style=\"padding-left: 30px;\"><em>Doctor: \"Ten.\"</em></p>\n<p style=\"padding-left: 30px;\"><em>Man: \"Ten? What kind of answer is that? Ten months? Ten years? Ten what?\"</em></p>\n<p style=\"padding-left: 30px;\"><em>The doctor looks at his watch. \"Nine.\"</em></p>\n<p>Recently I received some bad medical news (although not as bad as in the joke). Unfortunately I have been diagnosed with a fatal disease, Amyotrophic Lateral Sclerosis or ALS, sometimes called Lou Gehrig's disease. ALS causes nerve damage, progressive muscle weakness and paralysis, and ultimately death. Patients lose the ability to talk, walk, move, eventually even to breathe, which is usually the end of life. This process generally takes about 2 to 5 years.</p>\n<p>There are however two bright spots in this picture. The first is that ALS normally does not affect higher brain functions. I will retain my abilities to think and reason as usual. Even as my body is dying outside, I will remain alive inside.</p>\n<p>The second relates to survival. Although ALS is generally described as a fatal disease, this is not quite true. It is only mostly fatal. When breathing begins to fail, ALS patients must make a choice. They have the option to either go onto invasive mechanical respiration, which involves a tracheotomy and breathing machine, or they can die in comfort. I was very surprised to learn that over 90% of ALS patients choose to die. And even among those who choose life, for the great majority this is an emergency decision made in the hospital during a medical respiratory crisis. In a few cases the patient will have made his wishes known in advance, but most of the time the procedure is done as part of the medical management of the situation, and then the ALS patient either lives with it or asks to have the machine disconnected so he can die. Probably fewer than 1% of ALS patients arrange to go onto ventilation when they are still in relatively good health, even though this provides the best odds for a successful transition.<a id=\"more\"></a></p>\n<p>With mechanical respiration, survival with ALS can be indefinitely extended. And the great majority of people living on respirators say that their quality of life is good and they are happy with their decision. (There may be a selection effect here.) It seems, then, that calling ALS a fatal disease is an oversimplification. ALS takes away your body, but it does not take away your mind, and if you are determined and fortunate, it does not have to take away your life.</p>\n<p>There are a number of practical and financial obstacles to successfully surviving on a ventilator, foremost among them the great load on caregivers. No doubt this contributes to the high rates of choosing death. But it seems that much of the objection is philosophical. People are not happy about being kept alive by machines. And they assume that their quality of life would be poor, without the ability to move and participate in their usual activities. This is despite the fact that most people on respirators describe their quality of life as acceptable to good. As we have seen in other contexts, people are surprisingly poor predictors of how they will react to changed circumstances. This seems to be such a case, contributing to the high death rates for ALS patients.</p>\n<p>I hope that when the time comes, I will choose life. ALS kills only motor neurons, which carry signals to the muscles. The senses are intact. And most patients retain at least some vestige of control over a few muscles, which with modern technology can offer a surprisingly effective mode of communication. Stephen Hawking, the world's longest surviving ALS patient at over 40 years since diagnosis, is said to be able to type at ten words per minute by twitching a cheek muscle. I hope to be able to read, browse the net, and even participate in conversations by email and messaging. Voice synthesizers allow local communications, and I am making use of a free service for ALS patients which will create a synthetic model of my own natural voice, for future use. I may even still be able to write code, and my dream is to contribute to open source software projects even from within an immobile body. That will be a life very much worth living.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jaf5zfcGgCB2REXGw": 2, "xHjy88N2uJvGdgzfw": 3, "rfZ6DY88ApBDXFpyW": 2, "vmvTYnmaKA73fYDe5": 2, "izp6eeJJEg9v5zcur": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bshZiaLefDejvPKuS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 262, "baseScore": 313, "extendedScore": null, "score": 0.000495, "legacy": true, "legacyId": "1667", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 313, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 23, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-10-05T02:45:02.960Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-05T03:51:15.695Z", "modifiedAt": null, "url": null, "title": "Don't Think Too Hard.", "slug": "don-t-think-too-hard", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:36.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/APiWE2wScHQgFkuR6/don-t-think-too-hard", "pageUrlRelative": "/posts/APiWE2wScHQgFkuR6/don-t-think-too-hard", "linkUrl": "https://www.lesswrong.com/posts/APiWE2wScHQgFkuR6/don-t-think-too-hard", "postedAtFormatted": "Monday, October 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don't%20Think%20Too%20Hard.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon't%20Think%20Too%20Hard.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPiWE2wScHQgFkuR6%2Fdon-t-think-too-hard%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don't%20Think%20Too%20Hard.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPiWE2wScHQgFkuR6%2Fdon-t-think-too-hard", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAPiWE2wScHQgFkuR6%2Fdon-t-think-too-hard", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 983, "htmlBody": "<p>I find it interesting that when we're asleep - supposedly unconscious - we're frequently fully conscious, mired in a nonsensical dreamworld of our own creation. There's currently no universally accepted theory for the purpose of dreams - they range from cleaning up mental detritus to subconscious problem solving to cognitive accidents. On the other hand, we DO know plenty about what goes on in the brain during the dream state.<br /><br /><a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6WD0-4F1GRCB-1&amp;_user=10&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1021862617&amp;_rerunOrigin=google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=f8bde623b4dc7db4c7b40c2af46ee5aa\">Studies</a> show that in dreams, our thought processes are largely the same as they ones we use when we're awake. The main difference seems to be that we don't notice the insane world that we're a part of. We reason perfectly normally based on our surroundings, we're just incapable of reasoning about those surroundings - we lack metacognition when we're dreaming. The culprit behind this is a brain area known as the dorsolateral prefrontal cortex (DLPFC). It's responsible for, among other things, executive function (directing other brain functions), as well as working memory and motor planning. This combined with the fact that it's the last brain area to develop (meaning it was the last brain area to evolve) suggests that it's key in creating conscious, directed thought. And during sleep, it's shut down, cutting off our ability to question the premises we're given. So, barring entering a lucid dream state, we lack the mental hardware to recognize we're in a hallucination when we dream - it seems perfectly normal.[1]</p>\n<p><a id=\"more\"></a>While we're dreaming, a number of other neurological events are taking place. Long term memories are being <a href=\"http://scienceblogs.com/cortex/2006/12/the_neuroscience_of_dreaming.php\">accessed and replayed</a>. Brain regions that are normally unconnected work in concert to unite disparate bits of <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19506253\">information</a>. Whatever the purpose of dreams is (if they indeed have a purpose at all), they appear to be a window to the strengthening of mental connections and creation of new ones that takes place while we're asleep. And this behavior is necessary to maintain high-level mental functioning. People do better on tests after getting REM sleep, and people who are REM sleep deprived show extremely impaired memory and <a href=\"http://www.news.harvard.edu/gazette/1996/02.08/ResearchLinksSl.html\">learning abilities</a>.<br /><br />Something similar occurs when our mind is wandering - unrelated brain areas are working together to develop new <a href=\"http://scienceblogs.com/cortex/2009/05/daydreams_1.php\">mental pathways</a> (which is why talking about the importance of daydreaming is currently <a title=\"all the rage\" href=\"http://www.google.com/search?q=the+importance+of+daydreaming&amp;ie=utf-8&amp;oe=utf-8&amp;aq=t&amp;rls=org.mozilla:en-US:official&amp;client=firefox-a\" target=\"_blank\">all the rage</a>). The same thing happens when consuming alcohol - daydreaming and mental connection formation <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/19422627\">increases</a>, and frontal cortex activity (and metacognition) decreases.<br /><br />The implication here is that the creation of new cognitive pathways is something that takes place in the absence of conscious, directed thought. It passes the plausibility test - we have a limited amount of cognitive resources, so focusing our thoughts leaves fewer mental resources left over for other tasks. And the formation of new mental connections is extremely important - it's essentially enlarging the search space our minds have access to when trying to solve a problem. Though it's not under conscious control, it's still a high-level function - <a href=\"http://psychology.ucsc.edu/dreams/Library/domhoff_2004b.html\">young children</a> and people with <a href=\"http://www.springerlink.com/content/th56g5u8w26528rt/\">autism</a> seem to have extremely muted dreams (if they dream at all), implying fewer mental connections are being formed. Our executive function is great at orchestrating different brain areas to find a solution to a problem, but it's only able to look through the space of possible solutions that's already been created.[2]<br /><br />Conscious, directed thought - amazing as it is - is not the end-all, be-all of mental function. Humans are at the top of the intellectual food chain (not to mention the actual food chain), but a number of the things that make us 'special' are things we share with other animals. Plenty of them can pass the mirror test. Plenty have language, use tools and have complex social structures. Physiologically, what sets us apart is our processing power - the sheer volume of cognitive pathways we can create and sort through. Half of solving a problem is having a search space that contains the answer, and the human mind can create an ENORMOUS search space. But it does so without directed thought.<br /><br />If a problem seems intractable, then, you may not be able to make headway by THINKING about it harder. That infamous burst of insight seldom seems to come while hunched over a desk or after that 10th straight hour in the lab - it comes \"in a moment of distraction or else burst forth from the subconscious while we <a href=\"http://books.google.com/books?id=9j0xJjHWqa8C&amp;pg=PA1&amp;lpg=PA1&amp;dq=poincare+burst+of+insight&amp;source=bl&amp;ots=oKpkUmV4-K&amp;sig=faYdsCL3vZAiSRKrOTdvOE43EHo&amp;hl=en&amp;ei=X_PISuz9EuKMtgfBkLW5AQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1#v=onepage&amp;q=&amp;f=false\">sleep</a>\" [3]. Though it's obviously important to put the hours in to understand your subject (the brain can only work with what it's given), a creative or insightful solution arises from those pseudo-random mental firings that are beyond our conscious control. The answer to a hard problem might be a mental path that your brain hasn't formed yet, making trying to think your way through to it a fruitless endeavor. At a certain point, it's important to step back, relax, and let your subconscious create more grist for the mill<br /><br /><br />[1] As an aside, the fact that the brain region responsible for working memory is shut down may be the reason why we generally don't remember our dreams, and why writing them down and trying to remember them is an important step in learning to enter a lucid dream state (when the DLPFC is thought to be activated).<br /><br />[2] Mind-wandering actually seems to be MOST effective when we're at least partially aware we're doing it - if you're not paying any attention at all, something important could easily slip right past you. Something similar probably takes place during the lucid dream state, where we're aware enough to direct the flow of the dream but not so aware that we wake ourselves from it (a frequent problem of beginning lucid dreamers).</p>\n<p>[3] Though I suspect there is a selection bias at work here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "APiWE2wScHQgFkuR6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 5.272359888658523e-07, "legacy": true, "legacyId": "1670", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-05T05:26:23.736Z", "modifiedAt": null, "url": null, "title": "The Presumptuous Philosopher's Presumptuous Friend", "slug": "the-presumptuous-philosopher-s-presumptuous-friend", "viewCount": null, "lastCommentedAt": "2018-06-26T13:36:23.530Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlaidX", "createdAt": "2009-06-28T10:50:31.260Z", "isAdmin": false, "displayName": "PlaidX"}, "userId": "Cgh6rBbvZe4Noxt5d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tZodkMtQ3Ao7anzNW/the-presumptuous-philosopher-s-presumptuous-friend", "pageUrlRelative": "/posts/tZodkMtQ3Ao7anzNW/the-presumptuous-philosopher-s-presumptuous-friend", "linkUrl": "https://www.lesswrong.com/posts/tZodkMtQ3Ao7anzNW/the-presumptuous-philosopher-s-presumptuous-friend", "postedAtFormatted": "Monday, October 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Presumptuous%20Philosopher's%20Presumptuous%20Friend&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Presumptuous%20Philosopher's%20Presumptuous%20Friend%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtZodkMtQ3Ao7anzNW%2Fthe-presumptuous-philosopher-s-presumptuous-friend%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Presumptuous%20Philosopher's%20Presumptuous%20Friend%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtZodkMtQ3Ao7anzNW%2Fthe-presumptuous-philosopher-s-presumptuous-friend", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtZodkMtQ3Ao7anzNW%2Fthe-presumptuous-philosopher-s-presumptuous-friend", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>One day, you and the presumptuous philosopher are walking along, arguing about the size of the universe, when suddenly Omega jumps out from behind a bush and knocks you both out with a crowbar. While you're unconscious, she builds two hotels, one with a million rooms, and one with just one room. Then she makes a million copies of both of you, sticks them all in rooms, and destroys the originals.</p>\n<p>You wake up in a hotel room, in bed with the presumptuous philosopher, with a note on the table from Omega, explaining what she's done.</p>\n<p>\"Which hotel are we in, I wonder?\" you ask.</p>\n<p>\"The big one, obviously\" says the presumptuous philosopher. \"Because of anthropic reasoning and all that. Million to one odds.\"</p>\n<p>\"Rubbish!\" you scream. \"Rubbish and poppycock! We're just as likely to be in any hotel omega builds, regardless of the number of observers in that hotel.\"</p>\n<p>\"Unless there are no observers, I assume you mean\" says the presumptuous philosopher.</p>\n<p>\"Right, that's a special case where the number of observers in the hotel matters. But except for that it's totally irrelevant!\"</p>\n<p>\"In that case,\" says the presumptuous philosopher, \"I'll make a deal with you. We'll go outside and check, and if we're at the small hotel I'll give you ten bucks. If we're at the big hotel, I'll just smile smugly.\"</p>\n<p>\"Hah!\" you say. \"You just lost an expected five bucks, sucker!\"</p>\n<p>You run out of the room to find yourself in a huge, ten thousand story attrium, filled with throngs of yourselves and smug looking presumptuous philosophers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tZodkMtQ3Ao7anzNW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 4, "extendedScore": null, "score": 5.272520922581716e-07, "legacy": true, "legacyId": "1672", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-06T20:59:41.195Z", "modifiedAt": null, "url": null, "title": "The First Step is to Admit That You Have a Problem", "slug": "the-first-step-is-to-admit-that-you-have-a-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pmfk7ruhWaHj9diyv/the-first-step-is-to-admit-that-you-have-a-problem", "pageUrlRelative": "/posts/Pmfk7ruhWaHj9diyv/the-first-step-is-to-admit-that-you-have-a-problem", "linkUrl": "https://www.lesswrong.com/posts/Pmfk7ruhWaHj9diyv/the-first-step-is-to-admit-that-you-have-a-problem", "postedAtFormatted": "Tuesday, October 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20First%20Step%20is%20to%20Admit%20That%20You%20Have%20a%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20First%20Step%20is%20to%20Admit%20That%20You%20Have%20a%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmfk7ruhWaHj9diyv%2Fthe-first-step-is-to-admit-that-you-have-a-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20First%20Step%20is%20to%20Admit%20That%20You%20Have%20a%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmfk7ruhWaHj9diyv%2Fthe-first-step-is-to-admit-that-you-have-a-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPmfk7ruhWaHj9diyv%2Fthe-first-step-is-to-admit-that-you-have-a-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 950, "htmlBody": "<p><em>This is part 1 of a sequence on problem solving.&nbsp; <a href=\"/lw/1bj/the_shadow_question/\">Here is part 2</a>.<br /></em></p>\n<p>It is a critical faculty to distinguish <em>tasks</em> from <em>problems</em>.&nbsp; A task is something you do because you predict it will get you from one state of affairs to another state of affairs that you prefer.&nbsp; A problem is an unacceptable/displeasing state of affairs, now or in the likely future.&nbsp; So a task is something you do, or can do, while a problem is something that is, or may be.&nbsp; For example:</p>\n<ul>\n<li>If you want a peanut butter sandwich, and you have the tools, ingredients, and knowhow that are required to make a peanut butter sandwich, you have a task on your hands.&nbsp; If you want a peanut butter sandwich, but you lack one or more of those items, you have a problem.</li>\n<li>If you are sad, and you know that this is because you have not seen your favorite cousin in a while, and you have the wherewithal to arrange to have your cousin over, then arranging to have your cousin over is a task.&nbsp; If you are sad, and you don't know why, then the sadness is a problem.</li>\n<li>If eight of your friends are involved in massive unpleasant social drama, but you have a forty-three-step surefire plan to calm down the angry and smooth over the ruffled and chew out the misbehaved and bring back the normalcy, you have forty-three subtasks of one big task.&nbsp; If you have no clue what the heck is up with the drama but it's on your last nerve, problem ahoy!</li>\n<li>If you are a mortal creature, you may already be a problem-haver.</li>\n</ul>\n<p>Problems are solved by turning them into tasks and carrying out those tasks.&nbsp; Turning problems into tasks can sometimes be problematic in itself, although small taskifications can be tasky.&nbsp; For instance, in the peanut butter sandwich case, if your only missing component for sandwich-making is bread, it doesn't take much mental acrobatics to determine that you now have two tasks to be conducted in order: 1. obtain bread, 2. make sandwich.&nbsp; Figuring out why you're sad, in case two, could be a task (if you're really good at introspecting accurately, or are very familiar with the cousin-missing type of sadness in particular) or could be a problem (if you're not good at that, or if you've never missed your favorite cousin before and have no prior experience with the precise feeling).&nbsp; And so on.</p>\n<p>Why draw this distinction with such care?&nbsp; Because treating problems like tasks will slow you down in solving them.&nbsp; You can't <em>just</em> become immortal any more than you can <em>just</em> make a peanut butter sandwich without any bread.&nbsp; And agonizing about \"why I can't <em>just</em> do this\" will produce the solution to very few problems.&nbsp; First, you have to figure out how to taskify the problem.&nbsp; And the first step is to understand that you have a problem.</p>\n<p><a id=\"more\"></a></p>\n<p>Identifying problems is surprisingly difficult.&nbsp; The language we use for them is almost precisely like the language we use for tasks: \"I have to help the exchange student learn English.\"&nbsp; \"I have to pick up milk on the way home from school.\"&nbsp; \"I have to clean the grout.\"&nbsp; \"I have to travel to Zanzibar.\"&nbsp; Some of these are more likely to be problems than others, but any of them <em>could</em> be, because problemhood and taskiness depend on factors other than what it is you're supposed to wind up with at the end.&nbsp; You can easily say what you want to wind up with after finishing doing any of the above \"have to's\": a bilingual student, a fridge that contains milk, clean grout, the property of being in Zanzibar.&nbsp; But for each outcome to unfold correctly, resources that you might or might not have will be called for.&nbsp; Does the exchange student benefit most from repetition, or having everything explained in song, or do you need to pepper your teaching with mnemonics?&nbsp; Do you have cash in your wallet for milk?&nbsp; Do you know what household items will clean grout and what items will dissolve it entirely?&nbsp; Where the hell is Zanzibar, anyway?&nbsp; The approximate ways in which a \"have to\" might be a problem are these:</p>\n<ul>\n<li>Lacking resources.&nbsp; Resources include tools, materials, cash, space to operate, cooperative other persons, time, physical capacities, and anything else that will contribute directly to the bringing about of the outcome.&nbsp; For instance, if you can't afford the milk you need to buy, carry it home once you've bought it because you have injured your elbow, or fit it into the fridge because the fridge is full of blueberry trifle, that's a resource problem.</li>\n<li>Lacking propositional knowledge.&nbsp; For instance, if you don't know where Zanzibar is, that's a propositional knowledge problem.</li>\n<li>Lacking procedural knowledge or skill.&nbsp; This overlaps somewhat with propositional knowledge, but roughly, it's data about how to go about applying your resources towards your goal.&nbsp; For instance, if you don't know how to best approach your exchange student's learning style, that's a procedural knowledge problem.</li>\n</ul>\n<p>So when you have to do something, you can tell whether it's a problem or a task by checking whether you have all of these things.&nbsp; That's not going to be foolproof: certain knowledge gaps can obscure themselves and other shortfalls too.&nbsp; If I mistakenly think that the store from which I want to purchase milk is open 24 hours a day, I have a milk-buying problem and may not realize it until I try to walk into the building and find it locked.</p>\n<p>Part 2 of this sequence will get into what to do when you have identified a problem.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1, "FtT2T9bRbECCGYxrL": 1, "wMPYFGmhcFg4bSb4Z": 1, "BzghQYM9GnkMHxZKb": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pmfk7ruhWaHj9diyv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 74, "extendedScore": null, "score": 0.000119, "legacy": true, "legacyId": "1674", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 75, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vEJBc6hfKntnQ2A7C"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-07T16:35:16.698Z", "modifiedAt": null, "url": null, "title": "Let them eat cake: Interpersonal Problems vs Tasks", "slug": "let-them-eat-cake-interpersonal-problems-vs-tasks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:30.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HughRistik", "createdAt": "2009-03-10T19:45:18.256Z", "isAdmin": false, "displayName": "HughRistik"}, "userId": "edjn5sXHGjFCn9Qr7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AhHhm63zdZSDLmb76/let-them-eat-cake-interpersonal-problems-vs-tasks", "pageUrlRelative": "/posts/AhHhm63zdZSDLmb76/let-them-eat-cake-interpersonal-problems-vs-tasks", "linkUrl": "https://www.lesswrong.com/posts/AhHhm63zdZSDLmb76/let-them-eat-cake-interpersonal-problems-vs-tasks", "postedAtFormatted": "Wednesday, October 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let%20them%20eat%20cake%3A%20Interpersonal%20Problems%20vs%20Tasks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet%20them%20eat%20cake%3A%20Interpersonal%20Problems%20vs%20Tasks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhHhm63zdZSDLmb76%2Flet-them-eat-cake-interpersonal-problems-vs-tasks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let%20them%20eat%20cake%3A%20Interpersonal%20Problems%20vs%20Tasks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhHhm63zdZSDLmb76%2Flet-them-eat-cake-interpersonal-problems-vs-tasks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAhHhm63zdZSDLmb76%2Flet-them-eat-cake-interpersonal-problems-vs-tasks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1281, "htmlBody": "<p>When I read Alicorn's post on <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">problems vs tasks</a>, I immediately realized that the proposed terminology helped express one of my pet peeves: the resistance in society to applying rationality to socializing and dating.</p>\n<p>In a thread long, long ago, SilasBarta described his experience with <a href=\"/lw/zt/mate_selection_for_the_men_here/s5i\">dating advice</a>:</p>\n<blockquote>\n<p>I notice all advice on finding a girlfriend glosses over the actual nuts-and-bolts of it.</p>\n</blockquote>\n<p>In Alicorn's terms, he would be saying that the advice he has encountered treats <em>problems</em> as if they were <em>tasks</em>. Alicorn defines these terms a particular way:</p>\n<blockquote>\n<p>It is a critical faculty to distinguish <em>tasks</em> from <em>problems</em>.&nbsp; A task is something you do because you predict it will get you from one state of affairs to another state of affairs that you prefer.&nbsp; A problem is an unacceptable/displeasing state of affairs, now or in the likely future.&nbsp; So a task is something you do, or can do, while a problem is something that is, or may be.</p>\n</blockquote>\n<p>Yet as she observes in her post, treating genuine problems as if they were defined tasks is a mistake:</p>\n<blockquote>\n<p>Because treating problems like tasks will slow you down in solving them.&nbsp; You can't <em>just</em> become immortal any more than you can <em>just</em> make a peanut butter sandwich without any bread.</p>\n</blockquote>\n<p>Similarly, many straight guys or queer women can't <em>just</em> find a girlfriend, and many straight women or queer men can't <em>just</em> find a boyfriend,&nbsp; any more than they can \"just become immortal.\"</p>\n<p><a id=\"more\"></a></p>\n<p>People having trouble in those areas may ask for advice, perhaps out of a latent effort to turn the problem into more of a task. Yet a lot of conventional advice doesn't really turn the problem into the task (at least, not for everyone), but rather poses new problems, due to difficulties that Alicorn mentioned, such as lack of resources, lack of propositional knowledge, or lack of procedural knowledge. <br /><br />Take, for example, \"just be yourself,\" or \"just meet potential partners through friends.\" For many people, these pieces of advice just open up new problems: being oneself is a <em>problem</em> of personal identity. It's not a <em>task</em> that you can execute as part of a step in solving the problem of dating. Having a social network, let alone one that will introduce you to potential partners, is also a <em>problem</em> for many people. Consequently, these pieces of advice sound like \"let them eat cake.\"<br /><br />Society in general resists the notion that socializing (dating and mating in particular) is a <em>problem</em>. Rather, society treats it as a solved task, yet the procedures it advocates are incomplete, dependent on unacknowledged contextual factors, big hairy problems of their own, or just plain wrong. (Or it gives advice that consists of true observations that are useless for taskification, like \"everyone is looking for something different\" in a mate. Imagine telling a budding chef: \"everyone has different tastes\" in food. It's true, but it isn't actually useful in taskifying a problem like \"how do I cook a meal?\")<br /><br />Even worse, society resists better attempts to taskify social interaction (especially dating and mating). People who attempt to taskify socializing and dating are often seen as inauthentic, manipulative, inhuman, mechanical, objectifying of others, or going to unnecessary lengths. <br /><br />While some particular attempts of taskifying those problems may indeed suffer from those flaws, some people seem like they object to any form of taskifying in those areas. There may be good reasons to be skeptical of the taskifiability of socializing and mating. Yet while socializing and dating may not be completely taskifiable due to the <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/15qm\">improvisational</a> and heavily context-dependent nature of those problems, they are actually taskifiable to a reasonably large degree.<br /><br />Many people seem to hold an idealistic view of socializing and dating, particularly dating, that places them on another plane of reality where things are just supposed to happen \"magically\" and \"naturally,\" free of planning or any other sort of deliberation. Ironically, this Romantic view can actually be counterproductive to romance. Taskifaction doesn't destroy romance any more than it destroys music or dance. Personally, I think musicians who can actually play their instruments are capable of creating more \"magical\" music than musicians who can't. The Romantic view only applies to those who are naturally adept; in other words, those for who mating is not a <em>problem</em>. For those who do experience romance as a problem, the Romantic view is garbage [Edit: while turning this into a top-level post, I've realized that I need more clarification of what I am calling the \"Romantic\" view]. <br /><br />The main problem with this Romantic view is that is that it conflates a requirement for a solution with the requirements for the task-process that leads to the solution. Just because many people want mating and dating to <em>feel</em> magical and spontaneous, it doesn't mean that every step in finding and attracting mates must be magical and spontaneous, lacking any sort of planning, causal thinking, or other elements of taskification. Any artist, whether in visual media, music, drama, or dance knows that the \"magic\" of their art is produced by mundane and usually heavily taskified processes. You can't \"just\" create a sublime work of art any more than you can \"just\" have a sublime romantic experience (well, some very talented and lucky people can, but it's a lot harder for everyone else). Actually, it is taskification itself which allows skill to flourish, creating a foundation for expression that can feel spontaneous and magical. It is the mundane that guides the magical, not the other way around.<br /><br />Sucking at stuff is not sublime. It's not sublime in art, it's not sublime in music, and it's not sublime in dance. In dating, there is nothing wrong with a little innocence and awkwardness, but the lack of procedural and propositional knowledge can get to the point where it intrudes ruins the \"magic.\" There is nothing \"magical\" about the experience of someone who is bumbling socially and romantically, and practically forcing other people to reject him or her, either for that person of for those around. Yet to preserve the perception of \"magic\" and \"spontaneity\" (an experience that is only accessible for those with natural attractiveness and popularity, or luck), society is actually denying that type of experience to those who experience dating as a <em>problem</em>. Of course, they might \"get lucky\" and eventually get together with someone who is a decent without totally screwing things up with that person... but why is society mandating that romance be a given for some people, but a matter of \"getting lucky\" for others?<br /><br />The sooner society figures out the following, the better:<br /><br />1. For many people, socializing and dating are <em>problems</em>, not yet tasks. <br /><br />2. Socializing and dating can be taskified to the extend that other problems with similar solutions requirements (e.g. improvisation, fast response to emotional impulses of oneself and others, high attention to context, connection to one's own instincts) can be taskified. Which is a lot of the way, but definitely not all the way.<br /><br />3. Taskification when applied to interpersonal behavior is not <em>inherently</em> immoral or dehumanizing to anyone, nor does it inherently steal the \"magic\" from romance any more than dance training steals the magic from dance.<br /><br />Until then, we will continue to have a social caste system of those for whom socializing and dating is a <em>task</em> (e.g. due to intuitive social skills), over those for whom those things are still <em>problems</em> (due to society's accepted taskifications not working for them, and being prevented from making better taskifications due to societal pressure and censure).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AhHhm63zdZSDLmb76", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 86, "extendedScore": null, "score": 0.000138, "legacy": true, "legacyId": "1675", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 575, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Pmfk7ruhWaHj9diyv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-07T20:35:09.646Z", "modifiedAt": null, "url": null, "title": "New Haven/Yale Less Wrong Meetup: 5 pm, Monday October 12", "slug": "new-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.070Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z45QjCjM48rPuMFkG/new-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "pageUrlRelative": "/posts/z45QjCjM48rPuMFkG/new-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "linkUrl": "https://www.lesswrong.com/posts/z45QjCjM48rPuMFkG/new-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "postedAtFormatted": "Wednesday, October 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Haven%2FYale%20Less%20Wrong%20Meetup%3A%205%20pm%2C%20Monday%20October%2012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Haven%2FYale%20Less%20Wrong%20Meetup%3A%205%20pm%2C%20Monday%20October%2012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz45QjCjM48rPuMFkG%2Fnew-haven-yale-less-wrong-meetup-5-pm-monday-october-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Haven%2FYale%20Less%20Wrong%20Meetup%3A%205%20pm%2C%20Monday%20October%2012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz45QjCjM48rPuMFkG%2Fnew-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz45QjCjM48rPuMFkG%2Fnew-haven-yale-less-wrong-meetup-5-pm-monday-october-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>Posted on behalf of Thomas McCabe:</p>\n<p>I&nbsp;(Thomas McCabe, a Yale math student) will be hosting a Less Wrong meetup in New Haven,&nbsp;Connecticut, on the Yale University campus. The meetup will take place&nbsp;at 5 PM on Monday, October 12th, at the Yorkside Pizza &amp; Restaurant at&nbsp;288 York St. (time and place are flexible if anyone has a conflict);&nbsp;please comment if you'd like to attend, or if you have any questions&nbsp;or ideas. The location can be found on Google Maps at <a href=\"http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=yorkside&amp;sll=41.310875,-72.930295&amp;sspn=0.006834,0.021136&amp;ie=UTF8&amp;hq=yorkside&amp;hnear=&amp;ll=41.313822,-72.930293&amp;spn=0.006834,0.021136&amp;z=16&amp;iwloc=A\">this link</a>.</p>\n<p>Some confirmed attendees include SIAI folk and Less Wrongers <a href=\"/user/AnnaSalamon/submitted/\">Anna Salamon</a>, <a href=\"/user/Steve_Rayhawk/\">Steve Rayhawk</a>, <a href=\"/user/CarlShulman\">Carl Shulman</a>, and <a href=\"/user/Roko/\">Roko Mijc</a>.</p>\n<p>Feel free to contact me at <a href=\"mailto:thomas.mccabe@yale.edu\">thomas.mccabe@yale.edu</a>, or at 518-248-5525.<br />Thanks, and see everyone there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z45QjCjM48rPuMFkG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 5.278941275995476e-07, "legacy": true, "legacyId": "1676", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-07T21:15:14.155Z", "modifiedAt": null, "url": null, "title": "Boston Area Less Wrong Meetup: 2 pm Sunday October 11th", "slug": "boston-area-less-wrong-meetup-2-pm-sunday-october-11th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.102Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KoQpJFdWkjMd2Rtzh/boston-area-less-wrong-meetup-2-pm-sunday-october-11th", "pageUrlRelative": "/posts/KoQpJFdWkjMd2Rtzh/boston-area-less-wrong-meetup-2-pm-sunday-october-11th", "linkUrl": "https://www.lesswrong.com/posts/KoQpJFdWkjMd2Rtzh/boston-area-less-wrong-meetup-2-pm-sunday-october-11th", "postedAtFormatted": "Wednesday, October 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boston%20Area%20Less%20Wrong%20Meetup%3A%202%20pm%20Sunday%20October%2011th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoston%20Area%20Less%20Wrong%20Meetup%3A%202%20pm%20Sunday%20October%2011th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoQpJFdWkjMd2Rtzh%2Fboston-area-less-wrong-meetup-2-pm-sunday-october-11th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boston%20Area%20Less%20Wrong%20Meetup%3A%202%20pm%20Sunday%20October%2011th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoQpJFdWkjMd2Rtzh%2Fboston-area-less-wrong-meetup-2-pm-sunday-october-11th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKoQpJFdWkjMd2Rtzh%2Fboston-area-less-wrong-meetup-2-pm-sunday-october-11th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>There will be a Less Wrong meet-up this Sunday, October 11th,&nbsp;2 pm, in Cambridge at the Central Square Starbucks Coffee at <a href=\"http://www.yelp.com/biz/starbucks-coffee-cambridge-7\">655 Massachusetts Avenue</a>&nbsp;(time and place are flexible if anyone has a conflict);&nbsp;please comment if you'd like to attend, or if you have any questions&nbsp;or ideas. Some confirmed attendees include SIAI folk and Less Wrongers <a href=\"http://lesswrong.com/user/AnnaSalamon/submitted/\"><span style=\"color: #6a8a6b;\">Anna Salamon</span></a>, <a href=\"http://lesswrong.com/user/Steve_Rayhawk/\"><span style=\"color: #6a8a6b;\">Steve Rayhawk</span></a>, <a href=\"http://lesswrong.com/user/CarlShulman\"><span style=\"color: #6a8a6b;\">Carl Shulman</span></a>, and <a href=\"http://lesswrong.com/user/Roko/\"><span style=\"color: #6a8a6b;\">Roko Mijc</span></a>. Also keep your eyes peeled for a probable appearance&nbsp;of expert reductionist Gary Drescher, and a&nbsp;rumored Scott Aaronson sighting.</p>\r\n<p>Feel free to contact me at&nbsp;my first name DOT my last name AT&nbsp;post.harvard.edu or 646-525-5383.<br />Thanks, and see everyone there!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KoQpJFdWkjMd2Rtzh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 8, "extendedScore": null, "score": 5.279009260310513e-07, "legacy": true, "legacyId": "1677", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-07T22:51:05.591Z", "modifiedAt": null, "url": null, "title": "LW Meetup Google Calendar", "slug": "lw-meetup-google-calendar", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:36.780Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KNkd7awTwYxaHqmgJ/lw-meetup-google-calendar", "pageUrlRelative": "/posts/KNkd7awTwYxaHqmgJ/lw-meetup-google-calendar", "linkUrl": "https://www.lesswrong.com/posts/KNkd7awTwYxaHqmgJ/lw-meetup-google-calendar", "postedAtFormatted": "Wednesday, October 7th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Meetup%20Google%20Calendar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Meetup%20Google%20Calendar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkd7awTwYxaHqmgJ%2Flw-meetup-google-calendar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Meetup%20Google%20Calendar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkd7awTwYxaHqmgJ%2Flw-meetup-google-calendar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNkd7awTwYxaHqmgJ%2Flw-meetup-google-calendar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 102, "htmlBody": "<p>I've set up a calendar on Google to track future Less Wrong meetups. I've included links to view the calendar in a couple time zones, but note that if you add the calendar to your own google account, events should be shown in your usual time zone (if someone can confirm this for me, I'd appreciate it). I'll do my best to add any meetups posted to LW, but feel free to e-mail me if you don't see them.</p>\n<p><a href=\"http://www.google.com/calendar/embed?src=singinst.org_vovokj6ul7rf5lo2bg9m1fi7ss%40group.calendar.google.com&amp;ctz=America/Los_Angeles\">Less Wrong Meetups: Pacific View</a><br /> <a href=\"http://www.google.com/calendar/embed?src=singinst.org_vovokj6ul7rf5lo2bg9m1fi7ss%40group.calendar.google.com&amp;ctz=America/New_York\">Less Wrong Meetups: Eastern View</a></p>\n<p><a href=\"http://www.google.com/calendar/ical/singinst.org_vovokj6ul7rf5lo2bg9m1fi7ss%40group.calendar.google.com/public/basic.ics\">Link for use in iCal or anything else supporting the ics format</a></p>\n<p><a href=\"https://www.google.com/calendar/feeds/singinst.org_vovokj6ul7rf5lo2bg9m1fi7ss%40group.calendar.google.com/private-605c29abbcf29601d53c0f1b10a25e9f/basic\">Raw XML Version</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KNkd7awTwYxaHqmgJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 14, "extendedScore": null, "score": 5.279171880859422e-07, "legacy": true, "legacyId": "1678", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-09T16:23:17.593Z", "modifiedAt": null, "url": null, "title": "I'm Not Saying People Are Stupid", "slug": "i-m-not-saying-people-are-stupid", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:03.571Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cs2nvx7ajkGr5kudk/i-m-not-saying-people-are-stupid", "pageUrlRelative": "/posts/cs2nvx7ajkGr5kudk/i-m-not-saying-people-are-stupid", "linkUrl": "https://www.lesswrong.com/posts/cs2nvx7ajkGr5kudk/i-m-not-saying-people-are-stupid", "postedAtFormatted": "Friday, October 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'm%20Not%20Saying%20People%20Are%20Stupid&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'm%20Not%20Saying%20People%20Are%20Stupid%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs2nvx7ajkGr5kudk%2Fi-m-not-saying-people-are-stupid%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'm%20Not%20Saying%20People%20Are%20Stupid%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs2nvx7ajkGr5kudk%2Fi-m-not-saying-people-are-stupid", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcs2nvx7ajkGr5kudk%2Fi-m-not-saying-people-are-stupid", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 312, "htmlBody": "<p>Razib <a href=\"http://scienceblogs.com/gnxp/2009/10/singularity_summit_2009.php\">summarized</a> my entire cognitive biases talk at the Singularity Summit 2009 as saying:&nbsp; \"Most people are stupid.\"</p>\n<p>Hey!&nbsp; That's a bit unfair.&nbsp; I never said during my talk that most people are stupid.&nbsp; In fact, I was very careful not to say, at any point, that people are stupid, because that's explicitly <em>not</em> what I believe.</p>\n<p>I don't think that people who believe in <a href=\"/lw/q6/collapse_postulates/\">single-world quantum mechanics</a> are <a href=\"/lw/q7/if_manyworlds_had_come_first/\">stupid</a>.&nbsp; John von Neumann believed in a collapse postulate.</p>\n<p>I don't think that philosophers who believe in the \"possibility\" of <a href=\"/lw/p7/zombies_zombies/\">zombies</a> are <a href=\"/lw/pn/zombies_the_movie/\">stupid</a>.&nbsp; David Chalmers believes in zombies.</p>\n<p>I don't even think that <a href=\"/lw/tv/excluding_the_supernatural/\">theists</a> are <a href=\"/lw/1e/raising_the_sanity_waterline/\">stupid</a>.&nbsp; Robert Aumann believes in Orthodox Judaism.</p>\n<p>And in the closing sentence of my talk on cognitive biases and existential risk, I did not say that humanity was devoting more resources to football than existential risk prevention because we were stupid.</p>\n<p>There's an old joke that runs as follows:</p>\n<p style=\"padding-left: 30px;\">A motorist is driving past a mental hospital when he gets a flat tire.<br />He goes out to change the tire, and sees that one of the patients is watching him through the fence.<br />Nervous, trying to work quickly, he jacks up the car, takes off the wheel, puts the lugnuts into the hubcap -<br />And steps on the hubcap, sending the lugnuts clattering into a storm drain.<br />The mental patient is still watching him through the fence.<br />The motorist desperately looks into the storm drain, but the lugnuts are gone.<br />The patient is still watching.<br />The motorist paces back and forth, trying to think of what to do -<br />And the patient says,<br />\"Take one lugnut off each of the other tires, and you'll have three lugnuts on each.\"<br />\"That's brilliant!\" says the motorist.&nbsp; \"What's someone like you doing in an asylum?\"<br />\"I'm here because I'm crazy,\" says the patient, \"not because I'm stupid.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "KWFhr6A2dHEb6wmWJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cs2nvx7ajkGr5kudk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 48, "extendedScore": null, "score": 7.6e-05, "legacy": true, "legacyId": "1689", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xsZnufn3cQw7tJeQ3", "WqGCaRhib42dhKWRL", "fdEWWr8St59bXLbQr", "fsDz6HieZJBu54Yes", "u6JzcFtPGiznFgDxP", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-10T10:56:46.960Z", "modifiedAt": null, "url": null, "title": "How to get that Friendly Singularity: a minority view", "slug": "how-to-get-that-friendly-singularity-a-minority-view", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:53.629Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3FXzDQ56Z5HdHaqAB/how-to-get-that-friendly-singularity-a-minority-view", "pageUrlRelative": "/posts/3FXzDQ56Z5HdHaqAB/how-to-get-that-friendly-singularity-a-minority-view", "linkUrl": "https://www.lesswrong.com/posts/3FXzDQ56Z5HdHaqAB/how-to-get-that-friendly-singularity-a-minority-view", "postedAtFormatted": "Saturday, October 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20get%20that%20Friendly%20Singularity%3A%20a%20minority%20view&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20get%20that%20Friendly%20Singularity%3A%20a%20minority%20view%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FXzDQ56Z5HdHaqAB%2Fhow-to-get-that-friendly-singularity-a-minority-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20get%20that%20Friendly%20Singularity%3A%20a%20minority%20view%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FXzDQ56Z5HdHaqAB%2Fhow-to-get-that-friendly-singularity-a-minority-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3FXzDQ56Z5HdHaqAB%2Fhow-to-get-that-friendly-singularity-a-minority-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1995, "htmlBody": "<p><em>Note: I know this is a rationality site, not a Singularity Studies site. But the Singularity issue is ever in the background here, and the local focus on decision theory fits right into the larger scheme - see below. </em></p>\n<p>There is a worldview which I have put together over the years, which is basically my approximation to Eliezer's master plan. It's not an attempt to reconstruct every last detail of Eliezer's actual strategy for achieving a Friendly Singularity, though I think it must have considerable resemblance to the real thing. It might be best regarded as Eliezer-<em>inspired</em>, or as \"what my Inner Eliezer thinks\". What I propose to do is to outline this quasi-mythical orthodoxy, this tenuous implicit consensus (tenuous consensus because there is in fact a great diversity of views in the world of thought about the Singularity, but implicit consensus because no-one else has a plan), and then state how I think it should be amended. The amended plan is the \"minority view\" promised in my title.</p>\n<p><a id=\"more\"></a><em>Elements Of The Worldview</em></p>\n<p><strong>There will be strongly superhuman intelligence in the historically immediate future, unless a civilization-ending technological disaster occurs first.</strong></p>\n<ul>\n<li>Implicit assumption: problem-solving entities (natural and artificial intelligences, and coalitions thereof) do possess an attribute, their \"general intelligence\", which is both objective and rankable. Theoretical computer science suggests that this is so, but that it takes a lot of conceptual work to arrive at a fully objective definition of general intelligence. </li>\n<li>The \"historically immediate future\" may be taken to mean, as an absolute upper bound, the rest of this century. Personally, I find it hard to see how twenty more years can pass without people being able to make planet-killing nanotechnology, so I give it twenty years maximum before we're in the endgame. </li>\n<li>I specify <em>technological</em> disaster in the escape clause, because a natural disaster sufficient to end civilization is extremely unlikely on this timescale, and it will require a cultural disruption of that order to halt the progression towards superhuman intelligence. </li>\n</ul>\n<p><strong>In a conflict of values among intelligences, the higher intelligence will win, so for human values / your values to survive after superintelligence, the best chance is for the seed from which the superintelligence grew to have already been \"human-friendly\".</strong></p>\n<ul>\n<li>Elementary but very important observation: for at least some classes of intelligence, such as the \"expected-utility maximizer\" (EUM), values or goals are utterly contingent. The component specifying the utility function is independent of the component which solves the problem of maximizing expected utility, and so literally <em>any</em> goal that can be parsed by the problem solver, no matter how absurd, can become its supreme value, just as a calculator will dutifully attempt to evaluate any expression that you throw at it. The contingency of AI core values means that neither utopia nor dystopia (from a human perspective) is guaranteed - though the latter is far more likely, if the values are specified carelessly. </li>\n<li>The seed might be an artificial intelligence, modifying itself, or a natural intelligence modifying itself, or some combination of these. But AI is generally considered to have the advantage over natural intelligence when it comes to self-modification.&nbsp;</li>\n</ul>\n<p><strong>The way to produce a human-friendly seed intelligence is to identify the analogue, in the cognitive architecture behind human decision-making, of the utility function of an EUM, and then to \"renormalize\" or \"reflectively idealize\" this, i.e. to produce an ideal moral agent as defined with respect to our species' particular \"utility function\".</strong></p>\n<ul>\n<li>Human beings are not EUMs, but we do belong to <em>some</em> abstract class of decision-making system, and there is going to be some component of that system which specifies the goals rather than figuring out how to achieve them. That component is the analogue of the utility function. </li>\n<li>This ideal moral agent has to have, not just the right values, but the attribute of superhuman intelligence, if its creation is to constitute a Singularity; and those values have to be stable during the period of self-modification which produces increasing intelligence. The solution of these problems - self-enhancement, and ethical stability under self-enhancement - is also essential for the attainment of a Friendly Singularity. But that is basically a technical issue of computer science and I won't talk further about it. </li>\n</ul>\n<p><strong>The truly fast way to produce a human-relative ideal moral agent is to create an AI with the interim goal of inferring the \"human utility function\" (but with a few safeguards built in, so it doesn't, e.g., kill off humanity while it solves that sub-problem), and which is programmed to then transform itself into the desired ideal moral agent once the exact human utility function has been identified.</strong></p>\n<ul>\n<li>Figuring out the human utility function is a problem of empirical cognitive neuroscience, and if our AI really is a potential superintelligence, it ought to be better at such a task than any human scientist.&nbsp;</li>\n<li>I am especially going out on a limb in asserting that this final proposition is part of the master plan, though I think traces of the idea can be found in recent writings. But anyway, it's a plausible way to round out the philosophy and the research program; it makes sense if you agree with everything else that came before. It's what my Inner Eliezer thinks. </li>\n</ul>\n<p><em>Commentary</em></p>\n<p>This is, somewhat remarkably, a well-defined research program for the creation of a Friendly Singularity. You could print it out right now and use it as the mission statement of your personal institute for benevolent superintelligence. There are very hard theoretical and empirical problems in there, but I do not see anything that is clearly nonsensical or impossible.</p>\n<p>So what's my problem? Why don't I just devote the rest of my life to the achievement of this vision? There are two, maybe three amendments I would wish to make. What I call <strong>the ontological problem</strong> has not been addressed; <strong>the problem of consciousness</strong>, which is the main subproblem of the ontological problem, is also passed over; and finally, it makes sense to advocate that <strong>human neuroscientists should be trying to identify the human utility function</strong>, rather than simply planning to delegate that task to an AI scientist.</p>\n<p>The problem of ontology and the problem of consciousness can be stated briefly enough: our physics is incomplete, and even worse, our general scientific ontology is incomplete, because inherently and by construction it excludes the reality of consciousness.</p>\n<p>The observation that quantum mechanics, when expressed in a form which makes \"measurement\" an undefined basic concept, does not provide an objective and self-sufficient account of reality, has led on this site to the advocacy of the many-worlds interpretation as the answer. I <a href=\"/lw/19s/why_manyworlds_is_not_the_rationally_favored/\">recently argued</a> that many worlds is <em>not</em> the clear favorite, to a somewhat mixed response, and I imagine that I will be greeted with almost immovable skepticism if I also assert that the very template of natural-scientific reduction - mathematical physics in all its forms - is inherently inadequate for the description of consciousness. Nonetheless, I do so assert. Maybe I will make the case at greater length in a future article. But the situation is more or less as follows. We have invented a number of abstract disciplines, such as logic, mathematics, and computer science, by means of which we find ourselves able to think in a rigorously exact fashion about a variety of abstract possible objects. These objects constitute the theoretical ontology in terms of which we seek to understand and identify the nature of the actual world. I suppose there is also a minimal \"worldly\" ontology still present in all our understandings of the actual world, whereby concepts such as \"thing\" and \"cause\" still play a role, in conjunction with the truly abstract ideas. But this is how it is if you attempt to literally identify the world with any form of physics that we have, whether it's classical atoms in a void, complex amplitudes stretching across a multiverse configuration space, or even a speculative computational physics, based perhaps on cellular automata or equivalence classes of Turing machines.</p>\n<p>Having adopted such a framework, how does one then understand one's own conscious experience? Basically, through a combination of outright denial with a stealth dualism that masquerades as identity. Thus a person could say, for example, that the passage of time is an illusion (that's denial) and that perceived qualities are just neuronal categorizations (stealth dualism). I call the latter identification a stealth dualism because it blithely asserts that one thing is another thing when in fact they are nothing like each other. Stealth dualisms are unexamined habitual associations of a bit of physico-computational ontology with a bit of subjective phenomenology which allow materialists to feel that the mind does not pose a philosophical problem for them.</p>\n<p>My stance, therefore, is that intellectually we are in a much much worse position, when it comes to understanding consciousness, than most scientists, and especially most computer scientists, think. Not only is it an unsolved problem, but we are trying to solve it in the wrong way: presupposing the desiccated ontology of our mathematical physics, and trying to fit the diversities of phenomenological ontology into that framework. This is, I submit, entirely the wrong way round. One should instead proceed as follows: I exist, and among my properties are that I experience what I am experiencing, and that there is a sequence of such experiences. If I can free my mind from the assumption that the known classes of abstract object are all that can possibly exist, what sort of entity do I appear to be? Phenomenology - self-observation - thereby turns into an ontology of the self, and if you've done it correctly (I'm not saying this is easy), you have the beginning of a new ontology which by design accommodates the manifest realities of consciousness. The task then becomes to reconstitute or reinterpret the world according to mathematical physics in a way which does not erase anything you think you established in the phenomenological phase of your theory-building.</p>\n<p>I'm sure this program can be pursued in a variety of ways. My way is to emphasize the phenomenological unity of consciousness as indicating the ontological unity of the self, and to identify the self with what, in current physical language, we would call a large irreducible tensor factor in the quantum state of the brain. Again, the objective is not to reduce consciousness to quantum mechanics, but rather to reinterpret the formal ontology of quantum mechanics in a way which is not outright inconsistent with the bare appearances of experience. However, I'm not today insisting upon the correctness of my particular approach (or even trying very hard to explain it); only emphasizing my conviction that there remains an incredibly profound gap in our understanding of the world, and it has radical implications for any technically detailed attempt to bring about a human-friendly outcome to the race towards superintelligence. In particular, all the disciplines (e.g. theoretical computer science, empirical cognitive neuroscience) which play a part in cashing out the principles of a Friendliness strategy would need to be conceptually reconstructed in a way founded upon the true ontology.</p>\n<p>Having said all that, it's a lot simpler to spell out the meaning of my other amendment to the \"orthodox\" blueprint for a Friendly Singularity. It is advisable to not just think about how to delegate the empirical task of determining the human utility function to an AI scientist, but also to encourage existing human scientists to tackle this problem. The basic objective is to understand what sort of decision-making system we are. We're not expected utility maximizers; well, what are we then? This is a conceptual problem, though it requires empirical input, and research by merely human cognitive neuroscientists and decision theorists should be capable of producing conceptual progress, which will in turn help us to find the correct concepts which I have merely approximated here in talking about \"utility functions\" and \"ideal moral agents\".</p>\n<p>Thanks to anyone who read this far. :-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3FXzDQ56Z5HdHaqAB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 17, "extendedScore": null, "score": 5.285295328353028e-07, "legacy": true, "legacyId": "1693", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Note: I know this is a rationality site, not a Singularity Studies site. But the Singularity issue is ever in the background here, and the local focus on decision theory fits right into the larger scheme - see below. </em></p>\n<p>There is a worldview which I have put together over the years, which is basically my approximation to Eliezer's master plan. It's not an attempt to reconstruct every last detail of Eliezer's actual strategy for achieving a Friendly Singularity, though I think it must have considerable resemblance to the real thing. It might be best regarded as Eliezer-<em>inspired</em>, or as \"what my Inner Eliezer thinks\". What I propose to do is to outline this quasi-mythical orthodoxy, this tenuous implicit consensus (tenuous consensus because there is in fact a great diversity of views in the world of thought about the Singularity, but implicit consensus because no-one else has a plan), and then state how I think it should be amended. The amended plan is the \"minority view\" promised in my title.</p>\n<p><a id=\"more\"></a><em>Elements Of The Worldview</em></p>\n<p><strong id=\"There_will_be_strongly_superhuman_intelligence_in_the_historically_immediate_future__unless_a_civilization_ending_technological_disaster_occurs_first_\">There will be strongly superhuman intelligence in the historically immediate future, unless a civilization-ending technological disaster occurs first.</strong></p>\n<ul>\n<li>Implicit assumption: problem-solving entities (natural and artificial intelligences, and coalitions thereof) do possess an attribute, their \"general intelligence\", which is both objective and rankable. Theoretical computer science suggests that this is so, but that it takes a lot of conceptual work to arrive at a fully objective definition of general intelligence. </li>\n<li>The \"historically immediate future\" may be taken to mean, as an absolute upper bound, the rest of this century. Personally, I find it hard to see how twenty more years can pass without people being able to make planet-killing nanotechnology, so I give it twenty years maximum before we're in the endgame. </li>\n<li>I specify <em>technological</em> disaster in the escape clause, because a natural disaster sufficient to end civilization is extremely unlikely on this timescale, and it will require a cultural disruption of that order to halt the progression towards superhuman intelligence. </li>\n</ul>\n<p><strong id=\"In_a_conflict_of_values_among_intelligences__the_higher_intelligence_will_win__so_for_human_values___your_values_to_survive_after_superintelligence__the_best_chance_is_for_the_seed_from_which_the_superintelligence_grew_to_have_already_been__human_friendly__\">In a conflict of values among intelligences, the higher intelligence will win, so for human values / your values to survive after superintelligence, the best chance is for the seed from which the superintelligence grew to have already been \"human-friendly\".</strong></p>\n<ul>\n<li>Elementary but very important observation: for at least some classes of intelligence, such as the \"expected-utility maximizer\" (EUM), values or goals are utterly contingent. The component specifying the utility function is independent of the component which solves the problem of maximizing expected utility, and so literally <em>any</em> goal that can be parsed by the problem solver, no matter how absurd, can become its supreme value, just as a calculator will dutifully attempt to evaluate any expression that you throw at it. The contingency of AI core values means that neither utopia nor dystopia (from a human perspective) is guaranteed - though the latter is far more likely, if the values are specified carelessly. </li>\n<li>The seed might be an artificial intelligence, modifying itself, or a natural intelligence modifying itself, or some combination of these. But AI is generally considered to have the advantage over natural intelligence when it comes to self-modification.&nbsp;</li>\n</ul>\n<p><strong id=\"The_way_to_produce_a_human_friendly_seed_intelligence_is_to_identify_the_analogue__in_the_cognitive_architecture_behind_human_decision_making__of_the_utility_function_of_an_EUM__and_then_to__renormalize__or__reflectively_idealize__this__i_e__to_produce_an_ideal_moral_agent_as_defined_with_respect_to_our_species__particular__utility_function__\">The way to produce a human-friendly seed intelligence is to identify the analogue, in the cognitive architecture behind human decision-making, of the utility function of an EUM, and then to \"renormalize\" or \"reflectively idealize\" this, i.e. to produce an ideal moral agent as defined with respect to our species' particular \"utility function\".</strong></p>\n<ul>\n<li>Human beings are not EUMs, but we do belong to <em>some</em> abstract class of decision-making system, and there is going to be some component of that system which specifies the goals rather than figuring out how to achieve them. That component is the analogue of the utility function. </li>\n<li>This ideal moral agent has to have, not just the right values, but the attribute of superhuman intelligence, if its creation is to constitute a Singularity; and those values have to be stable during the period of self-modification which produces increasing intelligence. The solution of these problems - self-enhancement, and ethical stability under self-enhancement - is also essential for the attainment of a Friendly Singularity. But that is basically a technical issue of computer science and I won't talk further about it. </li>\n</ul>\n<p><strong id=\"The_truly_fast_way_to_produce_a_human_relative_ideal_moral_agent_is_to_create_an_AI_with_the_interim_goal_of_inferring_the__human_utility_function___but_with_a_few_safeguards_built_in__so_it_doesn_t__e_g___kill_off_humanity_while_it_solves_that_sub_problem___and_which_is_programmed_to_then_transform_itself_into_the_desired_ideal_moral_agent_once_the_exact_human_utility_function_has_been_identified_\">The truly fast way to produce a human-relative ideal moral agent is to create an AI with the interim goal of inferring the \"human utility function\" (but with a few safeguards built in, so it doesn't, e.g., kill off humanity while it solves that sub-problem), and which is programmed to then transform itself into the desired ideal moral agent once the exact human utility function has been identified.</strong></p>\n<ul>\n<li>Figuring out the human utility function is a problem of empirical cognitive neuroscience, and if our AI really is a potential superintelligence, it ought to be better at such a task than any human scientist.&nbsp;</li>\n<li>I am especially going out on a limb in asserting that this final proposition is part of the master plan, though I think traces of the idea can be found in recent writings. But anyway, it's a plausible way to round out the philosophy and the research program; it makes sense if you agree with everything else that came before. It's what my Inner Eliezer thinks. </li>\n</ul>\n<p><em>Commentary</em></p>\n<p>This is, somewhat remarkably, a well-defined research program for the creation of a Friendly Singularity. You could print it out right now and use it as the mission statement of your personal institute for benevolent superintelligence. There are very hard theoretical and empirical problems in there, but I do not see anything that is clearly nonsensical or impossible.</p>\n<p>So what's my problem? Why don't I just devote the rest of my life to the achievement of this vision? There are two, maybe three amendments I would wish to make. What I call <strong>the ontological problem</strong> has not been addressed; <strong>the problem of consciousness</strong>, which is the main subproblem of the ontological problem, is also passed over; and finally, it makes sense to advocate that <strong>human neuroscientists should be trying to identify the human utility function</strong>, rather than simply planning to delegate that task to an AI scientist.</p>\n<p>The problem of ontology and the problem of consciousness can be stated briefly enough: our physics is incomplete, and even worse, our general scientific ontology is incomplete, because inherently and by construction it excludes the reality of consciousness.</p>\n<p>The observation that quantum mechanics, when expressed in a form which makes \"measurement\" an undefined basic concept, does not provide an objective and self-sufficient account of reality, has led on this site to the advocacy of the many-worlds interpretation as the answer. I <a href=\"/lw/19s/why_manyworlds_is_not_the_rationally_favored/\">recently argued</a> that many worlds is <em>not</em> the clear favorite, to a somewhat mixed response, and I imagine that I will be greeted with almost immovable skepticism if I also assert that the very template of natural-scientific reduction - mathematical physics in all its forms - is inherently inadequate for the description of consciousness. Nonetheless, I do so assert. Maybe I will make the case at greater length in a future article. But the situation is more or less as follows. We have invented a number of abstract disciplines, such as logic, mathematics, and computer science, by means of which we find ourselves able to think in a rigorously exact fashion about a variety of abstract possible objects. These objects constitute the theoretical ontology in terms of which we seek to understand and identify the nature of the actual world. I suppose there is also a minimal \"worldly\" ontology still present in all our understandings of the actual world, whereby concepts such as \"thing\" and \"cause\" still play a role, in conjunction with the truly abstract ideas. But this is how it is if you attempt to literally identify the world with any form of physics that we have, whether it's classical atoms in a void, complex amplitudes stretching across a multiverse configuration space, or even a speculative computational physics, based perhaps on cellular automata or equivalence classes of Turing machines.</p>\n<p>Having adopted such a framework, how does one then understand one's own conscious experience? Basically, through a combination of outright denial with a stealth dualism that masquerades as identity. Thus a person could say, for example, that the passage of time is an illusion (that's denial) and that perceived qualities are just neuronal categorizations (stealth dualism). I call the latter identification a stealth dualism because it blithely asserts that one thing is another thing when in fact they are nothing like each other. Stealth dualisms are unexamined habitual associations of a bit of physico-computational ontology with a bit of subjective phenomenology which allow materialists to feel that the mind does not pose a philosophical problem for them.</p>\n<p>My stance, therefore, is that intellectually we are in a much much worse position, when it comes to understanding consciousness, than most scientists, and especially most computer scientists, think. Not only is it an unsolved problem, but we are trying to solve it in the wrong way: presupposing the desiccated ontology of our mathematical physics, and trying to fit the diversities of phenomenological ontology into that framework. This is, I submit, entirely the wrong way round. One should instead proceed as follows: I exist, and among my properties are that I experience what I am experiencing, and that there is a sequence of such experiences. If I can free my mind from the assumption that the known classes of abstract object are all that can possibly exist, what sort of entity do I appear to be? Phenomenology - self-observation - thereby turns into an ontology of the self, and if you've done it correctly (I'm not saying this is easy), you have the beginning of a new ontology which by design accommodates the manifest realities of consciousness. The task then becomes to reconstitute or reinterpret the world according to mathematical physics in a way which does not erase anything you think you established in the phenomenological phase of your theory-building.</p>\n<p>I'm sure this program can be pursued in a variety of ways. My way is to emphasize the phenomenological unity of consciousness as indicating the ontological unity of the self, and to identify the self with what, in current physical language, we would call a large irreducible tensor factor in the quantum state of the brain. Again, the objective is not to reduce consciousness to quantum mechanics, but rather to reinterpret the formal ontology of quantum mechanics in a way which is not outright inconsistent with the bare appearances of experience. However, I'm not today insisting upon the correctness of my particular approach (or even trying very hard to explain it); only emphasizing my conviction that there remains an incredibly profound gap in our understanding of the world, and it has radical implications for any technically detailed attempt to bring about a human-friendly outcome to the race towards superintelligence. In particular, all the disciplines (e.g. theoretical computer science, empirical cognitive neuroscience) which play a part in cashing out the principles of a Friendliness strategy would need to be conceptually reconstructed in a way founded upon the true ontology.</p>\n<p>Having said all that, it's a lot simpler to spell out the meaning of my other amendment to the \"orthodox\" blueprint for a Friendly Singularity. It is advisable to not just think about how to delegate the empirical task of determining the human utility function to an AI scientist, but also to encourage existing human scientists to tackle this problem. The basic objective is to understand what sort of decision-making system we are. We're not expected utility maximizers; well, what are we then? This is a conceptual problem, though it requires empirical input, and research by merely human cognitive neuroscientists and decision theorists should be capable of producing conceptual progress, which will in turn help us to find the correct concepts which I have merely approximated here in talking about \"utility functions\" and \"ideal moral agents\".</p>\n<p>Thanks to anyone who read this far. :-)</p>", "sections": [{"title": "There will be strongly superhuman intelligence in the historically immediate future, unless a civilization-ending technological disaster occurs first.", "anchor": "There_will_be_strongly_superhuman_intelligence_in_the_historically_immediate_future__unless_a_civilization_ending_technological_disaster_occurs_first_", "level": 1}, {"title": "In a conflict of values among intelligences, the higher intelligence will win, so for human values / your values to survive after superintelligence, the best chance is for the seed from which the superintelligence grew to have already been \"human-friendly\".", "anchor": "In_a_conflict_of_values_among_intelligences__the_higher_intelligence_will_win__so_for_human_values___your_values_to_survive_after_superintelligence__the_best_chance_is_for_the_seed_from_which_the_superintelligence_grew_to_have_already_been__human_friendly__", "level": 1}, {"title": "The way to produce a human-friendly seed intelligence is to identify the analogue, in the cognitive architecture behind human decision-making, of the utility function of an EUM, and then to \"renormalize\" or \"reflectively idealize\" this, i.e. to produce an ideal moral agent as defined with respect to our species' particular \"utility function\".", "anchor": "The_way_to_produce_a_human_friendly_seed_intelligence_is_to_identify_the_analogue__in_the_cognitive_architecture_behind_human_decision_making__of_the_utility_function_of_an_EUM__and_then_to__renormalize__or__reflectively_idealize__this__i_e__to_produce_an_ideal_moral_agent_as_defined_with_respect_to_our_species__particular__utility_function__", "level": 1}, {"title": "The truly fast way to produce a human-relative ideal moral agent is to create an AI with the interim goal of inferring the \"human utility function\" (but with a few safeguards built in, so it doesn't, e.g., kill off humanity while it solves that sub-problem), and which is programmed to then transform itself into the desired ideal moral agent once the exact human utility function has been identified.", "anchor": "The_truly_fast_way_to_produce_a_human_relative_ideal_moral_agent_is_to_create_an_AI_with_the_interim_goal_of_inferring_the__human_utility_function___but_with_a_few_safeguards_built_in__so_it_doesn_t__e_g___kill_off_humanity_while_it_solves_that_sub_problem___and_which_is_programmed_to_then_transform_itself_into_the_desired_ideal_moral_agent_once_the_exact_human_utility_function_has_been_identified_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "69 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sLLR5u9NJTfL88Spx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-10T14:05:50.422Z", "modifiedAt": null, "url": null, "title": "The Argument from Witness Testimony", "slug": "the-argument-from-witness-testimony", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.167Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ioy3DmZduwrNor3hC/the-argument-from-witness-testimony", "pageUrlRelative": "/posts/ioy3DmZduwrNor3hC/the-argument-from-witness-testimony", "linkUrl": "https://www.lesswrong.com/posts/ioy3DmZduwrNor3hC/the-argument-from-witness-testimony", "postedAtFormatted": "Saturday, October 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Argument%20from%20Witness%20Testimony&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Argument%20from%20Witness%20Testimony%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fioy3DmZduwrNor3hC%2Fthe-argument-from-witness-testimony%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Argument%20from%20Witness%20Testimony%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fioy3DmZduwrNor3hC%2Fthe-argument-from-witness-testimony", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fioy3DmZduwrNor3hC%2Fthe-argument-from-witness-testimony", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 971, "htmlBody": "<p>(Note: This is essentially a rehash/summarization of <a href=\"http://www.utsc.utoronto.ca/~sobel/\">Jordan Sobel</a>'s <a href=\"http://www.utsc.utoronto.ca/~sobel/OnL_T/BigLotteries.pdf\">Lotteries and Miracles</a>&nbsp;- you may prefer the original.)</p>\n<p><a href=\"http://141.211.177.75/philosophy/philosophy_detail/0,2874,19979%255Fpeople%255F17504680,00.html\"> George Mavrodes</a> wrote an interesting analogy. Scenario 1: Suppose you read a newspaper report claiming that a particular individual (say, Henry Plushbottom of Topeka, Kansas) has won a very large lottery. Before reading the newspaper, you would have given quite low odds that Henry in particular had won the lottery. However, the newspaper report flips your beliefs quite drastically. Afterward, you would give quite high odds that Henry in particular had won the lottery. Scenario 2: You have read various claims that a particular individual (<a href=\"http://en.wikipedia.org/wiki/Jesus\">Jesus of Nazareth</a>) arose from the dead. Before hearing those claims, you would have given quite low odds of anything so unlikely happening. However (since you are reading <a href=\"/\">LessWrong</a>) you presumably do not give quite high odds that Jesus arose from the dead.</p>\n<p>What is it about the second scenario which makes it different from the first?</p>\n<p><a id=\"more\"></a></p>\n<p>Let's model Scenario 1 as a simple Bayes net. There are two nodes, one representing whether Henry wins, and one representing whether Henry is reported to win, and one arrow, from first to the second.</p>\n<p><img src=\"http://www.johnicholas.com-a.googlepages.com/HenryBayesNet.png.png\" alt=\"A two-node bayes net.\" width=\"400\" height=\"105\" /></p>\n<p>What are the parameters of the conditional probability tables? Before any information came in, it seemed very unlikely that Henry was the winner - perhaps he had a one in a million chance. Given that Henry did win, what is the chance that he would be reported to have won? Pretty likely - newspapers do err, but it's reasonable to believe that 9 times out of 10, they get the name of the lottery winner correct. Now suppose that Henry didn't win. What is the chance that he would be reported to have won by mistake? There's nothing in particular to single him out from the other non-winners - being misreported is just as unlikely as winning, maybe even more unlikely.</p>\n<p>So we have (using w to abbreviate \"Henry Wins\" and r to abbreviate \"Henry is reported\"):</p>\n<ul>\n</ul>\n<ul>\n<li>P(w)=10<sup>-6</sup> - Henry has a one-in-a-million-chance of winning.</li>\n<li>P(!w)=1-10<sup>-6</sup> </li>\n<li>P(r|w)=0.9 - Reporters are pretty careful about names in this kind of story.</li>\n<li>P(!r|w)=0.1</li>\n<li>P(r|!w)=10<sup>-7</sup> - Not everyone plays, so there are even more people \"competing\" to be misreported, and Henry is supposed to be undistinguished.</li>\n<li>P(!r|!w)=1-10<sup>-7</sup> </li>\n</ul>\n<ul>\n</ul>\n<p>With a simple computation, we can verify that this model replicates the phenomenon in question. After reading the report, one's estimated probability should be:</p>\n<ul>\n<li>P( w | r ) = (by Bayes' Theorem)</li>\n<li>P( w ) * P( r | w ) / P( r ) = (expand P( r ) by cases)</li>\n<li>P( w ) * P( r | w ) / ( P( r | w ) * P( w ) + P( r | !w ) * P( !w ) ) = (substitute the numerical values)</li>\n<li>10<sup>-6</sup> * 0.9 / ( 0.9 * 10<sup>-6</sup> + 10<sup>-7</sup> * (1 - 10<sup>-6</sup>) ) = (approximately)</li>\n<li>0.9</li>\n</ul>\n<p>Of course, Scenario 2 could be modeled with two nodes and one arrow in exactly the same way. If it is rational to come to a different conclusion, then the parameters must be different. How would you justify setting the parameters differently in the second case?</p>\n<p>Somewhat relatedly, Douglas Walton has an \"argumentation scheme\" for Argument from Witness Testimony. An argumentation scheme is (roughly) a useful pattern of \"presumptive\" reasoning - that is, uncertain reasoning. In general, the argumentation/defeasible reasoning/non-monotonic logic community seems strangely isolated from the Bayesian inference community, though nominally they're both associated with artificial intelligence. Despite how odd each approach seems from the other side, there is a possibility of cross-fertilization here. Here are the so-called \"premises\" of the scheme (from <a href=\"http://www.amazon.com/Argumentation-Schemes-Douglas-Walton/dp/0521723744\">Argumentation Schemes</a>, p. 310):</p>\n<ul>\n<li><em>Position to Know Premise</em>: Witness W is in a position to know whether A is true or not.</li>\n<li><em>Truth Telling Premise</em>: Witness W is telling the truth.</li>\n<li><em>Statement Premise</em>: Witness W states that A is true.</li>\n<li><em>Conclusion</em>: A may be plausibly taken to be true.</li>\n</ul>\n<p>Here are the so-called \"critical questions\" associated with the argument from witness testimony:</p>\n<ol>\n<li>Is what the witness said internally consistent?</li>\n<li>Is what the witness said consistent with the known facts of the case (based on evidence apart from what the witness testified to)?</li>\n<li>Is what the witness said consistent with what other witnesses have (independently) testified to?</li>\n<li>Is there some kind of bias that can be attributed to the account given by the witness?</li>\n<li>How plausible is the statement A asserted by the witness?</li>\n</ol>\n<p>As I understand it, argumentation schemes are something like <a href=\"http://en.wikipedia.org/wiki/Rule_of_inference\">inference rules</a> for plausible reasoning but the actual premises (including both the scheme's \"premises\" and its \"critical questions\") are treated differently. I have not yet been able to unpack Walton's description of how they ought to be treated differently into the language of single agent reasoning. Usually argumentation theory is phrased and targeted for dialog between differing agents (for example, legal advocates), but it certainly can be applied to single agent reasoning. For example, <a href=\"http://oscarhome.soc-sci.arizona.edu/pollock.html\">Pollack</a>'s <a href=\"http://oscarhome.soc-sci.arizona.edu/ftp/OSCAR-web-page/oscar.html\">OSCAR</a> is based on defeasible reasoning.</p>\n<p>(Spoiler)</p>\n<p>Jordan Sobel's answer is that the key aspect of the sudden flip is P(r|!w), the probability of observing a false report. In Scenario 1, the probability of a false report of Henry's having won is even less likely than the probability of Henry winning. Given that humans are known to self-deceive regarding the things that are miraculous and wonderful, you should not carry that parameter through the analogy unchanged. Small increases in P(r|!w) lead to large reductions in P(w|r). For example, if P(r|!w) were equal to P(w), then the posterior probability that Henry won would drop below 0.5. If P(r|!w) were one in a hundred thousand, the posterior probability would drop below 0.1.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ioy3DmZduwrNor3hC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 5.28561674676874e-07, "legacy": true, "legacyId": "1685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-11T21:51:52.943Z", "modifiedAt": null, "url": null, "title": "Link: PRISMs, Gom Jabbars, and Consciousness (Peter Watts)", "slug": "link-prisms-gom-jabbars-and-consciousness-peter-watts", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.206Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JulianMorrison", "createdAt": "2009-02-27T12:57:27.471Z", "isAdmin": false, "displayName": "JulianMorrison"}, "userId": "CeZ67D5YxAKemKhYL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FuqELHc3kHnJ4gzbo/link-prisms-gom-jabbars-and-consciousness-peter-watts", "pageUrlRelative": "/posts/FuqELHc3kHnJ4gzbo/link-prisms-gom-jabbars-and-consciousness-peter-watts", "linkUrl": "https://www.lesswrong.com/posts/FuqELHc3kHnJ4gzbo/link-prisms-gom-jabbars-and-consciousness-peter-watts", "postedAtFormatted": "Sunday, October 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Link%3A%20PRISMs%2C%20Gom%20Jabbars%2C%20and%20Consciousness%20(Peter%20Watts)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALink%3A%20PRISMs%2C%20Gom%20Jabbars%2C%20and%20Consciousness%20(Peter%20Watts)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuqELHc3kHnJ4gzbo%2Flink-prisms-gom-jabbars-and-consciousness-peter-watts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Link%3A%20PRISMs%2C%20Gom%20Jabbars%2C%20and%20Consciousness%20(Peter%20Watts)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuqELHc3kHnJ4gzbo%2Flink-prisms-gom-jabbars-and-consciousness-peter-watts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFuqELHc3kHnJ4gzbo%2Flink-prisms-gom-jabbars-and-consciousness-peter-watts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p><a href=\"http://www.rifters.com/crawl/?p=791\">http://www.rifters.com/crawl/?p=791</a></p>\n<blockquote>\n<p>Morsella has gone back to basics. Forget art, symphonies, science. Forget the step-by-step learning of complex tasks. Those may be some of the things we use consciousness for <em>now</em> but that doesn&rsquo;t mean that&rsquo;s what it evolved for, any more than the cones in our eyes evolved to give kaleidoscope makers something to do. What&rsquo;s the primitive, bare-bones, nuts-and-bolts <em>thing</em> that consciousness does once we&rsquo;ve stripped away all the self-aggrandizing bombast?</p>\n<p>Morsella&rsquo;s answer is delightfully mundane: it mediates conflicting motor commands to the skeletal muscles.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FuqELHc3kHnJ4gzbo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 14, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "1695", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-12T00:29:19.218Z", "modifiedAt": null, "url": null, "title": "What Program Are You?", "slug": "what-program-are-you", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:05.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinHanson", "createdAt": "2009-02-26T13:46:26.443Z", "isAdmin": false, "displayName": "RobinHanson"}, "userId": "P4HT9AG3PuXjZv5Mw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5r7jgoZN6eDN7M5hK/what-program-are-you", "pageUrlRelative": "/posts/5r7jgoZN6eDN7M5hK/what-program-are-you", "linkUrl": "https://www.lesswrong.com/posts/5r7jgoZN6eDN7M5hK/what-program-are-you", "postedAtFormatted": "Monday, October 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Program%20Are%20You%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Program%20Are%20You%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5r7jgoZN6eDN7M5hK%2Fwhat-program-are-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Program%20Are%20You%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5r7jgoZN6eDN7M5hK%2Fwhat-program-are-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5r7jgoZN6eDN7M5hK%2Fwhat-program-are-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 575, "htmlBody": "<p>I've been trying for a while to make sense of the various alternate decision theories discussed here at LW, and have kept quiet until I thought I understood something well enough to make a clear contribution.&nbsp; Here goes.</p>\n<p>You simply cannot reason about what to do by referring to what program you run, and considering the other instances of that program, for the simple reason that: <em>there is no unique program that corresponds to any physical object. </em></p>\n<p>Yes, you can think of many physical objects O as running a program P on data D, but there are many many ways to decompose an object into program and data, as in O = &lt;P,D&gt;.&nbsp; At one extreme you can think of every physical object as running exactly the same program, i.e., the laws of physics, with its data being its particular arrangements of particles and fields.&nbsp; At the other extreme, one can think of each distinct physical state as a distinct program, with an empty unused data structure.&nbsp; Inbetween there are an astronomical range of other ways to break you into your program P and your data D.</p>\n<p>Eliezer's descriptions of his \"Timeless Decision Theory\", however refer often to \"the computation\" as distinguished from \"its input\" in this \"instantiation\" as if there was some unique way to divide a physical state into these two components.&nbsp; <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">For example</a>:</p>\n<p style=\"padding-left: 30px;\">The one-sentence version is:&nbsp; Choose as though controlling the logical output of the abstract computation you implement, including the output of all other instantiations and simulations of that computation.<br /><br />The three-sentence version is:&nbsp; Factor your uncertainty over (impossible) possible worlds into a causal graph that includes nodes corresponding to the unknown outputs of known computations; condition on the known initial conditions of your decision computation to screen off factors influencing the decision-setup; compute the counterfactuals in your expected utility formula by surgery on the node representing the logical output of that computation.</p>\n<p><a href=\"/lw/164/timeless_decision_theory_and_metacircular/\">And also</a>:</p>\n<p style=\"padding-left: 30px;\">Timeless decision theory, in which the (Godelian diagonal) expected utility formula is written as follows:&nbsp; Argmax[A in Actions] in Sum[O in Outcomes](Utility(O)*P(this computation yields A []-&gt; O|rest of universe))&nbsp; ... which is why TDT one-boxes on Newcomb's Problem - both your current self's physical act, and Omega's physical act in the past, are logical-causal descendants of the computation, and are recalculated accordingly inside the counterfactual. ...&nbsp; Timeless decision theory can state very definitely how it treats the various facts, within the interior of its expected utility calculation.&nbsp; It does not update any physical or logical parent of the logical output - rather, it conditions on the initial state of the computation, in order to screen off outside influences; then no further inferences about them are made.</p>\n<p>These summaries give the strong impression that one cannot use this decision theory to figure out what to decide until one has first decomposed one's physical state into one's \"computation\" as distinguished from one's \"initial state\" and its followup data structures eventually leading to an \"output.\"&nbsp; And since there are many many ways to make this decomposition, there can be many many decisions recommended by this decision theory.&nbsp;</p>\n<p>The advice to \"choose as though controlling the logical output of the abstract computation you implement\" might have you choose as if you controlled the actions of all physical objects, if you viewed the laws of physics as your program, or choose as if you only controlled the actions of the particular physical state that you are, if every distinct physical state is a different program.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "MP8NqPNATMqPrij4n": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5r7jgoZN6eDN7M5hK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 36, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "1696", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szfxvS8nsxTgJLBHs", "fQv85Rd3pw789MHaX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-12T22:27:37.461Z", "modifiedAt": null, "url": null, "title": "Do the 'unlucky' systematically underestimate high-variance strategies?", "slug": "do-the-unlucky-systematically-underestimate-high-variance", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.803Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gsYjMui5yD7ePkTY6/do-the-unlucky-systematically-underestimate-high-variance", "pageUrlRelative": "/posts/gsYjMui5yD7ePkTY6/do-the-unlucky-systematically-underestimate-high-variance", "linkUrl": "https://www.lesswrong.com/posts/gsYjMui5yD7ePkTY6/do-the-unlucky-systematically-underestimate-high-variance", "postedAtFormatted": "Monday, October 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20the%20'unlucky'%20systematically%20underestimate%20high-variance%20strategies%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20the%20'unlucky'%20systematically%20underestimate%20high-variance%20strategies%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsYjMui5yD7ePkTY6%2Fdo-the-unlucky-systematically-underestimate-high-variance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20the%20'unlucky'%20systematically%20underestimate%20high-variance%20strategies%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsYjMui5yD7ePkTY6%2Fdo-the-unlucky-systematically-underestimate-high-variance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgsYjMui5yD7ePkTY6%2Fdo-the-unlucky-systematically-underestimate-high-variance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p>From the UK Telegraph:</p>\n<blockquote>\n<p>A decade ago, I set out to investigate luck. I wanted to examine the impact on people's lives of chance opportunities, lucky breaks and being in the right place at the right time. After many experiments, I believe that I now understand why some people are luckier than others and that it is possible to become luckier.</p>\n<p>To launch my study, I placed advertisements in national newspapers and magazines, asking for people who felt consistently lucky or unlucky to contact me. Over the years, 400 extraordinary men and women volunteered for my research from all walks of life: the youngest is an 18-year-old student, the oldest an 84-year-old retired accountant.</p>\n</blockquote>\n<p><a href=\"http://www.telegraph.co.uk/technology/3304496/Be-lucky---its-an-easy-skill-to-learn.html\">Be lucky -- it's an easy skill to learn</a></p>\n<p>On reading the article, the takeaway message seems to be that the 'unlucky' systematically fail to take advantage of high-expected-but-low-median value opportunities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"gHCNhqxuJq2bZ2akb": 1, "nwcnHxrxcgnwJ878t": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gsYjMui5yD7ePkTY6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 25, "extendedScore": null, "score": 5.291371990967199e-07, "legacy": true, "legacyId": "1701", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-13T00:10:47.818Z", "modifiedAt": null, "url": null, "title": "Anticipation vs. Faith: At What Cost Rationality?", "slug": "anticipation-vs-faith-at-what-cost-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:38.562Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pePKcEd4HXfwtBptQ/anticipation-vs-faith-at-what-cost-rationality", "pageUrlRelative": "/posts/pePKcEd4HXfwtBptQ/anticipation-vs-faith-at-what-cost-rationality", "linkUrl": "https://www.lesswrong.com/posts/pePKcEd4HXfwtBptQ/anticipation-vs-faith-at-what-cost-rationality", "postedAtFormatted": "Tuesday, October 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anticipation%20vs.%20Faith%3A%20At%20What%20Cost%20Rationality%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnticipation%20vs.%20Faith%3A%20At%20What%20Cost%20Rationality%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpePKcEd4HXfwtBptQ%2Fanticipation-vs-faith-at-what-cost-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anticipation%20vs.%20Faith%3A%20At%20What%20Cost%20Rationality%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpePKcEd4HXfwtBptQ%2Fanticipation-vs-faith-at-what-cost-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpePKcEd4HXfwtBptQ%2Fanticipation-vs-faith-at-what-cost-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p class=\"MsoNormal\">Anticipation and faith are both aspects of the human decision process, in a sense just subroutines of a larger program, but they also generate subjective experiences (qualia) that we value for their own sake. Suppose you ask a religious friend why he doesn&rsquo;t give up religion, he might say something like &ldquo;Having faith in God comforts me and I think it is a central part of the human experience. Intellectually I know it&rsquo;s irrational, but I want to keep my faith anyway. My friends and the government will protect me from making any truly serious mistakes as a result of having too much faith (like falling into dangerous cults or refusing to give medical treatment to my children).\"</p>\n<p>Personally I've never been religious, so this is just a guess of what someone might say. But these are the kinds of <a href=\"/lw/19d/the_anthropic_trilemma/14rq\">thoughts</a> I have when faced with the prospect of giving up the anticipation of future experiences (after being <a href=\"/lw/19d/the_anthropic_trilemma/14rj\">prompted</a> by Dan Armak). We don't know for sure yet that anticipation is irrational, but it's hard to see how it can be patched up to work in an environment where <a href=\"/lw/19d/the_anthropic_trilemma/\">mind copying and merging</a> are possible, and in the mean time, we have a decision theory (<a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>) that seems to work fine, but does not involve any notion of anticipation.</p>\n<p>What would you do if true rationality requires giving up something even more fundamental to the human experience than faith? I wonder if anyone is actually willing to take this step, or is this the limit of human rationality, the end of a short journey across the space of possible minds?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pePKcEd4HXfwtBptQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 5.291547748174322e-07, "legacy": true, "legacyId": "1700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 106, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["y7jZ9BLEeuNTzgAE5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-13T01:07:35.908Z", "modifiedAt": null, "url": null, "title": "The power of information?", "slug": "the-power-of-information", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:38.116Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K8ovvvMKPvY6wPXAJ/the-power-of-information", "pageUrlRelative": "/posts/K8ovvvMKPvY6wPXAJ/the-power-of-information", "linkUrl": "https://www.lesswrong.com/posts/K8ovvvMKPvY6wPXAJ/the-power-of-information", "postedAtFormatted": "Tuesday, October 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20power%20of%20information%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20power%20of%20information%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK8ovvvMKPvY6wPXAJ%2Fthe-power-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20power%20of%20information%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK8ovvvMKPvY6wPXAJ%2Fthe-power-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK8ovvvMKPvY6wPXAJ%2Fthe-power-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1069, "htmlBody": "<p>I'm thinking about how to model an ecosystem of recursively self-improving computer programs.&nbsp; The model I have in mind assumes finite CPU cycles/second and finite memory as resources, and that these resources are already allocated at time zero.&nbsp; It models the rate of production of new information by a program given its current resources of information, CPU cycles, and memory; the conversion of information into power to take resources from other programs; and a decision rule by which a program chooses which other program to take resources from.&nbsp; The objective is to study the system dynamics, in particular looking for attractors and bifurcations/catastrophes, and to see what range of initial conditions don't lead to a singleton.</p>\n<p>(A more elaborate model would also represent the fraction of ownership one program had of another program, that being a weight to use to blend the decision rules of the owning programs with the decision rule of the owned program.&nbsp; It may also be desirable to model trade of information.&nbsp; I think that modeling Moore's law wrt CPU speed and memory size would make little difference, if we assume the technologies developed would be equally available to all agents.&nbsp; I'm interested in the shapes of the attractors, not the rate of convergence.)</p>\n<p>Problem: I don't know how to model power as a function of information.</p>\n<p><a id=\"more\"></a>I have a rough model of how information grows over time; so I can estimate the relative amounts of information in a single real historical society at two points in time.&nbsp; If I can say that society X had tech level T at time A, and society Y had tech level T at time B, I can use this model to estimate what tech level society Y had at time A.</p>\n<p>Therefore, I can gather historical data about military conflicts between societies at different tech levels, estimate the information ratio between those societies, and relate it to the manpower ratios between the armies involved and the outcome of the conflict, giving a system of inequalities.</p>\n<p>You can help me in 3 ways:</p>\n<ul>\n<li>Tell me why this is a bad idea.</li>\n<li>Tell me some better way to relate information to power.</li>\n<li>Gather historical data of the type I just described.</li>\n</ul>\n<p>If you choose the last option, choose a historical conflict between sides of uneven tech level, and post here as many as you can find of the following details:</p>\n<ul>\n<li>Identifying details: Year, opponents involved, name of conflict</li>\n<li>Number of combatants on each side</li>\n<li>Duration of the conflict</li>\n<li>Outcome of the conflict</li>\n<li>Identification of technological advantages of either side, especially ones that proved key (this can include non-material technologies, such as training or ideologies)</li>\n<li>How long the side with a technological advantage had possessed that technology at the time of the battle</li>\n<li>Estimation of the tech levels of each side in terms of a \"standard\" Western-Equivalent era (WE) attached to the most-advanced \"Western\" nation of the time (or in the conflict), and the facts used to make these estimates.&nbsp; (We might trace the WE timeline starting with Sumeria, then Egypt, Greece, Rome, etc.&nbsp; If enough datapoints are available for the 2 societies involved to model their information growth, they need not be mapped to WE.)</li>\n</ul>\n<p>For example:</p>\n<ul>\n<li>Battle of Agincourt, English vs. French, October 1415.</li>\n<li>English combatants: Given as <a href=\"http://en.wikipedia.org/wiki/Battle_of_Agincourt\">5900</a>, or <a href=\"http://en.wikipedia.org/wiki/Battle_of_Agincourt#Modern_re-assessment_of_Agincourt\">8000</a></li>\n<li>French: Given as <a href=\"http://en.wikipedia.org/wiki/Battle_of_Agincourt\">20,000-36,000</a>, or <a href=\"http://en.wikipedia.org/wiki/Battle_of_Agincourt#Modern_re-assessment_of_Agincourt\">12,000</a></li>\n<li>Duration: 3 hours</li>\n<li>English win; <a href=\"http://en.wikipedia.org/wiki/Battle_of_Agincourt\">4,000&ndash;10,000 French dead, with up to 1,600 English dead</a></li>\n<li>Most historians identify the English longbow as the key technology responsible for the English win (reference needed).&nbsp; The English longbow was used at least as early as <a href=\"http://en.wikipedia.org/wiki/English_longbow\">1346</a>.&nbsp; However, the Wikipedia article on Agincourt, as well as <a href=\"http://rosspruden.blogspot.com/2009/05/amazing-true-story-of-battle-of.html\">this site</a>, indicate that the crucial technology was not just the longbow, but the use of palings (long stakes driven into the ground to protect bowmen from cavalry).&nbsp; A third key innovation of the English was being dedicated to winning battles rather than to fighting for glory; the French disdained longbows as unchivalrous, and were given to drawing up battle plans that would give them personal glory in the retelling rather than battle plans that would win (described briefly <a href=\"http://rosspruden.blogspot.com/2009/05/amazing-true-story-of-battle-of.html\">here</a>, and in more detail in a book, I think \"A Distant Mirror\" by Barbara Tuchman.)&nbsp; This may be why the French deployed their archers behind their cavalry, where they were unable to take part in most of the battle.</li>\n<li>I assign the French a tech-level of 1346 WE, based on this being the date when the English first used the longbow heavily in a major battle.</li>\n</ul>\n<p>Using the two dates 1415 and 1346 leads to some tech-level (or information) ratio R.&nbsp; For example, under a simple model assuming that tech level doubled every 70 years in this era, we would give the English a tech-level ratio over the French of 2, and then say that the tech-level ratio enjoyed by the English produced a power multiplier greater than the manpower ratio enjoyed by the french:&nbsp;&nbsp; P(2) &gt; 30000/5900.&nbsp; This ignores the many advances shared by the English and French between 1346 and 1415; but most of them were not relevant to the battle.&nbsp; It also ignores the claim that the main factor was that the French had heavy armour, which was a disadvantage rather than an advantage in the deep mud on that rainy day.&nbsp; Oh well.&nbsp; (Let's hope for enough data that the law of large numbers kicks in.)</p>\n<p>After gathering a few dozen datapoints, it may be possible to discern a shape for the function P.&nbsp; (Making P a multiplying force that is a function of a ratio assumes P is linear, since eg. P(8) = P(8/4)*P(4/2)*P(2/1) = 4*P(2); the data can reject this assumption.)&nbsp; There may be a way to factor the battle duration and the casualty outcome into the equation as well; or at least to see if they correlate with the distance of the datapoint's manpower ratio from the estimated value of P(information ratio) for that datapoint.</p>\n<p>(I tried to construct another example from the Battle of Little Bighorn to show a case where the lower-level technology won, but found that the Indians had more rifles than the Army did, and that there is no agreement as to whether the Indians' repeating rifles or the Army's longer-ranged single-shot Springfield rifles were better.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K8ovvvMKPvY6wPXAJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 0, "extendedScore": null, "score": 5.291644515358498e-07, "legacy": true, "legacyId": "1702", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-13T16:10:14.847Z", "modifiedAt": null, "url": null, "title": "Quantifying ethicality of human actions", "slug": "quantifying-ethicality-of-human-actions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.931Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bogus", "createdAt": "2009-07-17T17:58:52.405Z", "isAdmin": false, "displayName": "bogus"}, "userId": "ChXHsXmDQFWZH638i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2CtfZDpSymBt33AJb/quantifying-ethicality-of-human-actions", "pageUrlRelative": "/posts/2CtfZDpSymBt33AJb/quantifying-ethicality-of-human-actions", "linkUrl": "https://www.lesswrong.com/posts/2CtfZDpSymBt33AJb/quantifying-ethicality-of-human-actions", "postedAtFormatted": "Tuesday, October 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Quantifying%20ethicality%20of%20human%20actions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuantifying%20ethicality%20of%20human%20actions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2CtfZDpSymBt33AJb%2Fquantifying-ethicality-of-human-actions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Quantifying%20ethicality%20of%20human%20actions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2CtfZDpSymBt33AJb%2Fquantifying-ethicality-of-human-actions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2CtfZDpSymBt33AJb%2Fquantifying-ethicality-of-human-actions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1350, "htmlBody": "<p><em><strong>Background:</strong>&nbsp; This article </em><em> is licensed under the <a href=\"http://www.gnu.org/copyleft/fdl.html\">GNU Free Documentation License</a> and <a href=\"http://creativecommons.org/licenses/by-sa/3.0/\">Creative Commons Attributions-Share-Alike Unported</a>. It </em><em>was posted to Wikipedia by an author who wished to remain anonymous, known variously as \"24\" and \"142\".&nbsp; It was subsequently removed from view on Wikipedia, but its text has been preserved by a number of mirrors.&nbsp; While it could be seen as no more than a basic primer in moral philosophy, it is arguably required reading to anyone unfamiliar with the philosophical background of such concepts as <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a> and <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>.</em><br /><br />The search for a <strong>formal method for evaluating and quantifying ethicality and morality of human actions</strong> stretches back to ancient times. While any simple view of right, wrong and dispute resolution relies on some linguistic and cultural norms, a 'formal' method presumably cannot, and must rely instead on knowledge of more basic human nature, and symbolic methods that allow for only very simple evidence.<a id=\"more\"></a><br />By contrast, modern systems of criminal justice and civil law evaluate and quantify social and moral norms (usually as a fine or sentence or ruling on damages) rely usually on adversarial process and forensic method, combined using some quasi-empirical methods and many outright appeal to authority and ad hominem arguments. These would all be unacceptable in a formal method based on something more resembling axiomatic proof, which by definion relies on some axioms of morality.<br /><br />Religious moral codes provide such axioms in most societies, and to some degree, following those strictly could be considered formal in that no more trusted or respected method existed. But our modern concept of what is formal and thus universally trustworthy and transparent is derived from that of the ancient Greeks:<br /><br />Pythagoras and Plato sought to combine moral and mathematical elements of reality in their work on ontology. This was very influential and the work of both is still consulted to this day, although, the social and political implications of their methods are often rejected by more modern philosophers.<br /><br />Thomas Aquinas, Francis Bacon and some of the Asharite philosophers shared a belief in some kind of over-arching ethical reality provided by a deity. But while Aquinas and Bacon integrated this with methods of Aristotle and ultimately inspired Jesuit and other Catholic methods of assessing and dispensing justice, resulting in Catholic canon law and other forms of Christian church law, the Asharite influence on Islam rejected parallel Mutazilite work on Aristotle, and eventually resulted in the \"classical fiqh\" and the shariah now being revived in some parts of the Islamic World. Thus it could reasonably be said that Catholic and Islamic thought diverged on Aristotle's ideas in the middle ages.<br /><br />Some consider the debate to continue to this day in economics, with the neoclassical economics based firmly on Aristotle's methods via Friedrich Hayek and Karl Popper, against Islamic economics and feminist economics which reject some aspects of Aristotle's logic, e.g. law of excluded middle, and seek to build on some intuitive and morally defensible ontology, as Plato did. This is probably no less of a controversy today than it was in Plato's time, or among the Asharites:<br /><br />Today, few accept that economics is a means to any ethical or moral end, but more of a technology that serves the ends of those who control and refine it. It remains however that economics does \"evaluate and quantify\" relationships of such importance, e.g. food, labour, that most humans literally cannot live without an economy around them. Thus an economy embodies assumptions about ethics and morality, and Karl Marx thought that this was itself proof that capitalist economics had subsumed the role of the old feudal methods. This view is current to this day in Marxist economics.<br /><br />However, the longest-lived view of formal methods as applied to morality comes not from Western but Eastern traditions. Confucianism with its stress on honesty and transparency and etiquette, and moral example of rulers and elders, has at times been seen as a formal method among the Chinese, its \"axioms\" often respected as much or more than any from science.<br /><br />Buddhism also stresses notions of right livelihood which seem to be possible to measure and compare in a quasi-formal manner. The Noble Eightfold Path is a set of priorities, ordinal not cardinal, not strictly quantities, but still, a useful framework for any more formal or weighted value theory.<br /><br />During The Enlightenment the various traditions became more unified:<br /><br />Immanuel Kant, in his \"categorical imperative\", sought to define moral duty reflectively, in that everyone was obligated to anticipate and limit the impacts of one's own actions, and \"not act as one would not have everyone act.\". This can be seen as a restated Golden Rule. In the 20th century it was restated as the ecological footprint, a measure of one's use of the Earth's natural capital, which later became a keystone of green economics.<br /><br />Other related practices are means of measuring well-being and assessing the implied value of life of various professional ethical codes and infrastructure decisions. While these systems rely on empirical methods for gathering data, and are more interested in \"is\" than \"should\", they are at least \"transparent\" and \"repeatable\" in a sense that could be called \"pre-formal\" or \"pre-requisite to formal\". Some think that they verge at times on the reliability of the quasi-empirical methods in mathematics, in that no conceivable disproof seems possible, but evidence \"for\" is not disputed - an example being the observation of Marilyn Waring that actions which prepare for war have measurably higher economic values than those within family.<br /><br />A formal method could reconcile many points of view by excluding forensic or audit methods which passed morally-undesirable outcomes, e.g. war or genocide, or worse which valued them highly. It could not validate any one view a \"true\" but it could find a \"best\" or \"best next step\" for some given time horizon or limited list of models or choices to evaluate. Most proposals for moral purchasing employ some such process. Given a very large number of socially-shared semi-formal economically-committed methods, one might take a mean or other stochastic measure of ethical and moral acceptability to those participating, and thus produce very nearly a species-wide informal method that would have as much reliability as one could expect from any \"formal\" method. Such are the goals of some NGOs in civil society and peace movement and labour movement and anti-globalization movement circles.<br /><br />An alternative but less popular view is that \"human nature\" can be so well understood and modelled mathematically that it becomes possible to assess with formal and mathematical methods, the cognitive bias or moral instinct, e.g. altruism of humans in general, perhaps with measurable variations due to genetics. This view has been popular since the emergence of the theory of evolution, and E. O. Wilson and George Lakoff are among those who have asserted a strong \"biological basis\" for \"morality\" and \"cognitive science of mathematics\" respectively.<br /><br />Some combination of these views may effective at posing a starting point for models of moral cores and instincts and aesthetics in human beings. However few see them as routes to new moral codes that would be more reliable than the traditional religious ones. A notable exception is B. F. Skinner who proposed exactly such replacement in his \"Walden Two\", a sort of behaviorist utopia which had many characteristics in common with modern eco-anarchism and eco-villages. Most advocates of such co-housing and extended family living situations, e.g. Daniel Quinn or William Thomas, consider informal, political, \"tribal\" methods sufficient or more desirable than those involving any kind of \"proof\".<br /><br />If so, the long search for a formal method to evaluate and quantify ethical outcomes, even in economics, may come to be seen as a sort of mathematical fetishism, or scientism, or even commodity fetishism to the degree it requires the reduction of quality of life to a series of simple quantities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2CtfZDpSymBt33AJb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -14, "extendedScore": null, "score": 5.293182691446305e-07, "legacy": true, "legacyId": "1706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-13T20:09:18.348Z", "modifiedAt": null, "url": null, "title": "BHTV: Eliezer Yudkowsky and Andrew Gelman", "slug": "bhtv-eliezer-yudkowsky-and-andrew-gelman", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NXpnf6nJfsgmtfvCN/bhtv-eliezer-yudkowsky-and-andrew-gelman", "pageUrlRelative": "/posts/NXpnf6nJfsgmtfvCN/bhtv-eliezer-yudkowsky-and-andrew-gelman", "linkUrl": "https://www.lesswrong.com/posts/NXpnf6nJfsgmtfvCN/bhtv-eliezer-yudkowsky-and-andrew-gelman", "postedAtFormatted": "Tuesday, October 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20Eliezer%20Yudkowsky%20and%20Andrew%20Gelman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20Eliezer%20Yudkowsky%20and%20Andrew%20Gelman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXpnf6nJfsgmtfvCN%2Fbhtv-eliezer-yudkowsky-and-andrew-gelman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20Eliezer%20Yudkowsky%20and%20Andrew%20Gelman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXpnf6nJfsgmtfvCN%2Fbhtv-eliezer-yudkowsky-and-andrew-gelman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXpnf6nJfsgmtfvCN%2Fbhtv-eliezer-yudkowsky-and-andrew-gelman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9, "htmlBody": "<p>\n<object width=\"380\" height=\"288\" data=\"http://static.bloggingheads.tv/maulik/offsite/offsite_flvplayer.swf\" type=\"application/x-shockwave-flash\">\n<param name=\"flashvars\" value=\"playlist=http%3A%2F%2Fbloggingheads%2Etv%2Fdiavlogs%2Fliveplayer%2Dplaylist%2F23065%2F00%3A00%2F57%3A59\" />\n<param name=\"src\" value=\"http://static.bloggingheads.tv/maulik/offsite/offsite_flvplayer.swf\" />\n</object>\n</p>\n<p><span style=\"font-family: Verdana; font-size: 12px; color: #666666; font-weight: bold; line-height: 18px;\">Percontations: The Nature of Probability</span></p>\n<p><a href=\"http://bloggingheads.tv/diavlogs/23065\">Source</a>.</p>\n<p><a href=\"http://www.stat.columbia.edu/~gelman/\">Background on Gelman</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NXpnf6nJfsgmtfvCN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 5.293590195843033e-07, "legacy": true, "legacyId": "1707", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-13T23:06:46.947Z", "modifiedAt": null, "url": null, "title": "We're in danger. I must tell the others...", "slug": "we-re-in-danger-i-must-tell-the-others", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:37.969Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AllanCrossman", "createdAt": "2009-03-13T20:51:34.435Z", "isAdmin": false, "displayName": "AllanCrossman"}, "userId": "uHa4S4jNkPPrEd8ha", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MEyqjpSFogiEaoixn/we-re-in-danger-i-must-tell-the-others", "pageUrlRelative": "/posts/MEyqjpSFogiEaoixn/we-re-in-danger-i-must-tell-the-others", "linkUrl": "https://www.lesswrong.com/posts/MEyqjpSFogiEaoixn/we-re-in-danger-i-must-tell-the-others", "postedAtFormatted": "Tuesday, October 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We're%20in%20danger.%20I%20must%20tell%20the%20others...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe're%20in%20danger.%20I%20must%20tell%20the%20others...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMEyqjpSFogiEaoixn%2Fwe-re-in-danger-i-must-tell-the-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We're%20in%20danger.%20I%20must%20tell%20the%20others...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMEyqjpSFogiEaoixn%2Fwe-re-in-danger-i-must-tell-the-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMEyqjpSFogiEaoixn%2Fwe-re-in-danger-i-must-tell-the-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>... Oh, no! I've been shot!</p>\n<p style=\"padding-left: 90px;\">&mdash; C3PO</p>\n<p>A strange sort of paralysis can occur when risk-averse people (like me) decide that we're going to play it safe. We imagine the worst thing that could happen if we go ahead with our slightly risky plan, and this stops us from carrying it out.</p>\n<p>One possible way of overcoming such paralysis is to remind yourself just how much danger you're actually in.</p>\n<p>Humanity could be mutilated by nuclear war, biotechnology disasters, societal meltdown, environmental collapse, oppressive governments, disagreeable AI, or other horrors. On an individual level, anybody's life could turn sour for more mundane reasons, from disease to bereavement to divorce to unemployment to depression. The terrifying scenarios depend on your values, and differ from person to person. Those here who hope to live forever may die of old age, and then cryonics turns out not to work.</p>\n<p>There must be some number X which is the probability of Really Bad Things happening to you. X is probably not a tiny figure, but instead significantly above zero, which encourages you to go ahead with whatever slightly risky plan you were contemplating, as long as it only nudges X upwards a little.</p>\n<p>Admittedly, this tactic seems like a cheap hack that relies on an error in human reasoning - is nudging your danger level from .2 to .201 actually more acceptable than nudging it from 0 to .001? Perhaps not. Needless to say, a real rationalist ought to ignore all this and take the action with the highest expected value.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MEyqjpSFogiEaoixn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 5.293892762499094e-07, "legacy": true, "legacyId": "1708", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-14T00:08:43.863Z", "modifiedAt": null, "url": null, "title": "PredictionBook.com - Track your calibration", "slug": "predictionbook-com-track-your-calibration", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.485Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ofSYgmMby7iqxJqi6/predictionbook-com-track-your-calibration", "pageUrlRelative": "/posts/ofSYgmMby7iqxJqi6/predictionbook-com-track-your-calibration", "linkUrl": "https://www.lesswrong.com/posts/ofSYgmMby7iqxJqi6/predictionbook-com-track-your-calibration", "postedAtFormatted": "Wednesday, October 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PredictionBook.com%20-%20Track%20your%20calibration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APredictionBook.com%20-%20Track%20your%20calibration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FofSYgmMby7iqxJqi6%2Fpredictionbook-com-track-your-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PredictionBook.com%20-%20Track%20your%20calibration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FofSYgmMby7iqxJqi6%2Fpredictionbook-com-track-your-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FofSYgmMby7iqxJqi6%2Fpredictionbook-com-track-your-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<p>Our hosts at <a href=\"http://tricycledevelopments.com/it-development\">Tricycle Developments</a> have created <a href=\"http://predictionbook.com/\">PredictionBook.com</a>, which lets you make predictions and then track your calibration - see whether things you assigned a 70% probability happen 7 times out of 10.</p>\n<p>The major challenge with a tool like this is (a) coming up with good short-term predictions to track (b) maintaining your will to keep on tracking yourself even if the results are discouraging, as they probably will be.</p>\n<p>I think the main motivation to actually use it, would be rationalists challenging each other to put a prediction on the record and track the results - I'm going to try to remember to do this the next time Michael Vassar says \"X%\" and I assign a different probability.&nbsp; (Vassar would have won quite a few points for his superior predictions of Singularity Summit 2009 attendance - I was pessimistic, Vassar was accurate.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1, "E8PHMuf7tsr8teXAe": 1, "8daMDi9NEShyLqxth": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ofSYgmMby7iqxJqi6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 41, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "1709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-14T01:40:56.490Z", "modifiedAt": null, "url": null, "title": "The Shadow Question", "slug": "the-shadow-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:32.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vEJBc6hfKntnQ2A7C/the-shadow-question", "pageUrlRelative": "/posts/vEJBc6hfKntnQ2A7C/the-shadow-question", "linkUrl": "https://www.lesswrong.com/posts/vEJBc6hfKntnQ2A7C/the-shadow-question", "postedAtFormatted": "Wednesday, October 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Shadow%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Shadow%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEJBc6hfKntnQ2A7C%2Fthe-shadow-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Shadow%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEJBc6hfKntnQ2A7C%2Fthe-shadow-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvEJBc6hfKntnQ2A7C%2Fthe-shadow-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1053, "htmlBody": "<p><em>This is part 2 of a sequence on problem solving.&nbsp; <a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">Here's part 1</a>, which introduces the vocabulary of \"problems\" versus \"tasks\".&nbsp; This post's title is a reference<sup>1</sup> worth 15 geek points if you get it without Googling, and 20 if you can also get it without reading the rest of the post.</em></p>\n<p>You have to <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">be careful what you wish for</a>.&nbsp; You can't just look at a problem, say \"That's not okay,\" and set about changing the world to contain <em>something, anything, other than that.</em>&nbsp; The easiest way to change things is <em>usually</em> to make them worse.&nbsp; If I owe the library fifty cents that I don't have lying around, I can't go, \"That's not okay!&nbsp; I don't <em>want</em> to owe the library fifty cents!\" and consider my problem solved when I set the tardy book on fire and now owe them, not money, but a new copy of the book.&nbsp; Or you could make things, not worse in the specific domain of your original problem, but bad in some tangentially related department: I could solve my library fine problem by stealing fifty cents from my roommate and giving it to the library.&nbsp; I'd no longer be indebted to the library.&nbsp; But then I'd be a thief, and my roommate might find out and be mad at me.&nbsp; Calling that a <em>solution</em> to the library fine problem would be, if not an outright abuse of the word \"solution\", at least a bit misleading.</p>\n<p>So what kind of solutions <em>are</em> we looking for?&nbsp; How do we answer the Shadow Question?&nbsp; It's hard to turn a complex problem into doable tasks without some idea of what you want the world to look like when you've completed those tasks.&nbsp; You could just say that you want to optimize according to your utility function, but that's a little like saying that your goal is to achieve your goals: no <em>duh, </em>but <em>now</em> what?&nbsp; You probably don't even know <a href=\"/lw/zv/post_your_utility_function/\">what your utility function is</a>; it's not a luminous feature of your mind.<a id=\"more\"></a></p>\n<p>For little problems, the answer to the Shadow Question may not be <em>complete.&nbsp; </em>For instance, I have never before thought to mentally specify, when making a peanut butter sandwich, that I'd prefer that my act of sandwich-making not lead to the destruction of the Everglades.&nbsp; But it's complete enough.&nbsp; The Everglades aren't close enough to my sandwich for me to think they're worth explicitly acting to protect, even now that Everglades-destruction has occurred to me as an undesirable potential side effect.&nbsp; But for big problems, well - we may have a problem...</p>\n<p>Here's a few broad approaches you could take in trying to answer the Shadow Question.&nbsp; Somebody please medicate me for my addiction to cutesy reference-y titles for things:</p>\n<ul>\n<li><strong>First, Do No Harm</strong>: Your top priority is to avoid making anything <em>worse</em> than the present status quo.&nbsp; This is the strategy to apply if the status quo is more-or-less acceptable but <em>precarious</em>, or if you're in a particularly hazardous location relative to your problem (i.e. you can very easily make something go very pear-shaped if you don't tread carefully).&nbsp; For instance, you don't move somebody who's just been flung at high speed from a Prius and landed on the shoulder of the highway and isn't moving, if you aren't a paramedic.&nbsp; However she's doing, you're most likely to make it worse if you try to drag her somewhere.</li>\n<li><strong>Cherry on Top</strong>: Your top priority is to make things <em>better</em> than the present status quo.&nbsp; When your problem is mostly independent from the rest of the world, and you have some direct control over it, this is a safe bet: pick what you can mess with, and mess with it so it gets better.&nbsp; It's a worse choice when anything you do will probably have a heap of side effects.&nbsp; For instance, if you're not feeling well, you could drink a glass of water and take a nap.&nbsp; This pretty definitely won't cure you, but it's got a good shot of helping a little.</li>\n<li><strong>Lottery Ticket</strong>: Your top priority is to <em>enable</em> a <em>best case scenario</em>.&nbsp; When the best case scenario is easy and straightforward to attain, this isn't a long shot - but it's also not much of a problem.&nbsp; This is the strategy to employ when you have a <em>really awesome</em> best case on your hands, or when the worse cases are fairly safe and you're comfortable risking them.&nbsp; This is distinct from \"Cherry on Top\" because CoT doesn't allow a large chance for worsening the status quo; it requires the predictable outcome to be an improvement, even if it's not the most fantastic thing that could happen.&nbsp; As an example, you could sign up for cryonics.&nbsp; This is guaranteed to cost you financially, but if a string of \"ifs\" turns out nicely, it <em>might</em> let you be an immortal undead ice zombie with a flying car, which would be very cool (pun intended).</li>\n<li><strong>Turn Disasters Off</strong>: Your top priority is to <em>disable</em> a <em>worst case scenario</em> (or a family of them).&nbsp; This is the go-to strategy when the disaster in question is really, horrendously awful and you aren't comfortable with it having any appreciable chance of realization.&nbsp; You might tolerate a guaranteed reduction in the quality of the situation in order to stave off a worse one, and so it's different from \"First, Do No Harm\".&nbsp; For instance, you could <a href=\"/lw/12o/the_trolley_problem_in_popular_culture_torchwood/\">hand over children to evil aliens</a> in order to avert global catastrophe.</li>\n</ul>\n<p>These strategies tolerate plenty of overlap, but in general, the more overlap available in a situation, the less problematic a problem you have.&nbsp; If you can simultaneously enable the best case, disable the worst case, make it unlikely that anything will deteriorate, and nearly guarantee that things will improve - uh - go ahead and do that, then!&nbsp; Sometimes, though, it seems like you have to organize these strategies and narrow down your plan in order.&nbsp; Arrange them however you like, and in the search space each one leaves behind, optimize for the next.</p>\n<p>Part 3 of this sequence will conclude it, and will talk about resource evaluation.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>\"The Shadow Question\" refers to the question \"What do you want?\", which was repeatedly asked by creatures called Shadows and their agents during the course of the splendid television show Babylon 5.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "BzghQYM9GnkMHxZKb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vEJBc6hfKntnQ2A7C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 39, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "1711", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Pmfk7ruhWaHj9diyv", "4ARaTpNX62uaL86j6", "MvwdPfYLX866vazFJ", "REpzLJaQjJ2hJb6sR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-14T16:52:35.107Z", "modifiedAt": null, "url": null, "title": "Information theory and FOOM", "slug": "information-theory-and-foom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:00:03.140Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KqhHhsBRzbf7eckTS/information-theory-and-foom", "pageUrlRelative": "/posts/KqhHhsBRzbf7eckTS/information-theory-and-foom", "linkUrl": "https://www.lesswrong.com/posts/KqhHhsBRzbf7eckTS/information-theory-and-foom", "postedAtFormatted": "Wednesday, October 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Information%20theory%20and%20FOOM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInformation%20theory%20and%20FOOM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqhHhsBRzbf7eckTS%2Finformation-theory-and-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Information%20theory%20and%20FOOM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqhHhsBRzbf7eckTS%2Finformation-theory-and-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKqhHhsBRzbf7eckTS%2Finformation-theory-and-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1433, "htmlBody": "<p>Information is power.&nbsp; But how much power?&nbsp; This question is vital when considering the speed and the limits of post-singularity development.&nbsp; To address this question, consider 2 other domains in which information accumulates, and is translated into an ability to solve problems:&nbsp; Evolution, and science.</p>\n<h2><a id=\"more\"></a><strong>DNA Evolution</strong></h2>\n<p>Genes code for proteins.&nbsp; Proteins are composed of modules called \"domains\"; a protein contains from 1 to dozens of domains.&nbsp; We classify genes into gene \"families\", which can be loosely defined as sets of genes that on average share &gt;25% of their amino acid sequence and have a good alignment for &gt;75% of their length.&nbsp; The number of genes and gene families known doubles every 28 months; but most \"new\" genes code for proteins that recombine previously-known domains in different orders.</p>\n<p>Almost all of the information content of a genome resides in the amino-acid sequence of its domains; the rest mostly indicates what order to use domains in individual genes, and how genes regulate other genes.&nbsp; About 64% of domains (and 84% of those found in eukaryotes) evolved before eukaryotes split from prokaryotes about 2 billion years ago. (Michael Levitt, PNAS July 7 2009, \"Nature of the protein universe\"; D. Yooseph et al. \"The Sorcerer II global ocean sampling expedition\", PLoS Bio 5:e16.)&nbsp; (Prokaryotes are single-celled organisms lacking a nucleus, mitochondria, or gene introns.&nbsp; All multicellular organisms are eukaryotes.)</p>\n<p>It's therefore accurate to say that most of the information generated by evolution was produced in the first one or two billion years; the development of more-complex organisms seems to have nearly stopped evolution of protein domains.&nbsp; (Multi-cellular organisms are much larger and live much longer; therefore there are many orders of magnitude fewer opportunities for selection in a given time period.)&nbsp; Similarly, most evolution within eukaryotes seems to have occurred during a period of about 50 million years leading up to the Cambrian explosion, half a billion years ago.</p>\n<p>My first observation is that evolution has been slowing down in information-theoretic terms, while speeding up in terms of the intelligence produced.&nbsp; This means that adding information to the gene pool increases the effective intelligence that can be produced using that information by a more-than-linear amount.</p>\n<p>In the first of several irresponsible assumptions I'm going to make, let's assume that the information evolved in time <em>t</em> is proportional to<em> i</em> = log(t), while the intelligence evolved is proportional to e<sup>t</sup> = e<sup>e^i</sup>.&nbsp; I haven't done the math to support those particular functions; but I'm confident that they fit the data better than linear functions would.&nbsp; (This assumption is key, and the data should be studied more closely before taking my analysis too seriously.)</p>\n<p>My second observation is that evolution occurs in spurts.&nbsp; There's a lot of data to support this, including data from simulated evolution; see in particular the theory of punctuated equilibrium, and the data from various simulations of evolution in <em>Artificial Life</em> and <em>Artificial Life II</em>.&nbsp; But I want to single out the eukaryote-to-Cambrian-explosion spurt.&nbsp; The evolution of the first eukaryotic cell suddenly made a large subset of organism-space more accessible; and the speed of evolution, which normally decreases over time, instead <em>increased</em> for tens of millions of years.</p>\n<h2><strong>Science!</strong></h2>\n<p>The following discussion relies largely on de Solla Price's <em>Little Science, Big Science</em> (1963), Nicholas Rescher's <em>Scientific Progress:&nbsp;A Philosophical Essay on the Economics of Research in Natural Science</em> (1978), and the data I presented in my 2004 TransVision talk, \"<a href=\"http://www.transhumanism.org/tv/2004/GoetzAccelChange.ppt\">The myth of accelerating change</a>\".</p>\n<p>The growth of \"raw\" scientific knowledge is exponential by most measures: Number of scientists, number of degrees granted, number of journals, number of journal articles, number of dollars spent.&nbsp; Most of these measures have a doubling time of 10-15 years.&nbsp; (GDP has a doubling time closer to 20 years, suggesting that the ultimate limits on knowledge may be economic.)</p>\n<p>The growth of \"important\" scientific knowledge, measured by journal citations, discoveries considered worth mentioning in histories of science, and perceived social change, is much slower; if it is exponential, it appears IMHO to have had a doubling time of 50-100 years between 1600 and 1940.&nbsp; (It can be argued that this growth began slowing down at the onset of World War II, and more dramatically around 1970).&nbsp; Nicholas Rescher argues that important knowledge = log(raw information).</p>\n<p>A simple argument supporting this is that \"important\" knowledge is the number of distinctions you can make in the world; and the number of distinctions you can draw based on a set of examples is of course proportional to the log of the size of your data set, assuming that the different distinctions are independent and equiprobable, and your data set is random.&nbsp; However, an opposing argument is that log(i) is simply the amount of non-redundant information present in a database with uncompressed information <em>i</em>.&nbsp; (This appears to be approximately the case for genetic sequences.&nbsp; IMHO it is unlikely that scientific knowledge is that redundant; but that's just a guess.)&nbsp; Therefore, important knowledge is somewhere between O(log(information)) and O(information), depending whether information is closer to O(raw information) or O(log(raw information)).</p>\n<h2>Analysis</h2>\n<p>We see two completely-opposite pictures:&nbsp; In evolution, the efficaciousness of information increases more-than-exponentially with the amount of information.&nbsp; In science, it increases somewhere between logarithmically and linearly.</p>\n<p>My final irreponsible assumption will be that the production of ideas, concepts, theories, and inventions (\"important knowledge\") from raw information, is analogous to the production of intelligence from gene-pool information.&nbsp; Therefore, evolution's efficacy at using the information present in the gene pool can give us a lower bound on the amount of useful knowledge that <em>could</em> be extracted from our raw scientific knowledge.</p>\n<p>I argued above that the amount of intelligence produced from a given gene-information-pool <em>i</em> is approximately e^e<sup>i</sup>, while the amount of useful knowledge we extract from raw information <em>i</em> is somewhere between O(i) and O(log(i)).&nbsp; The implication is that the fraction of discoveries that we have made, out of those that could be made from the information we already have, has an upper bound between O(1/e^e^i) and O(1/e^e^e^i).</p>\n<p>One key question in asking what the shape of AI takeoff will be, is therefore: Will AI's efficiency at drawing inferences from information be closer to that of humans, or that of evolution?</p>\n<p>If the latter, then the number of important discoveries that an AI could make, using only the information we already have, may be between e^e^i and e^e^e^i times the number of important discoveries that we have made from it.&nbsp; <em>i</em> is a large number representing the total information available to humanity.&nbsp; e^e^i is a goddamn large number.&nbsp; e^e^e^i is an awful goddamn large number.&nbsp; Where before, we predicted FOOM, we would then predict FOOM^FOOM^FOOM^FOOM.</p>\n<p>Furthermore, the development of the first AI will be, I think, analogous to the evolution of the first eukaryote, in terms of suddenly making available a large space of possible organisms.&nbsp; I therefore expect the pace of information generation by evolution to suddenly switch from falling, to increasing, even before taking into account recursive self-improvement.&nbsp; This means that the rate of information increase will be much greater than can be extrapolated from present trends.&nbsp; Supposing that the rate of acquisition of important knowledge will change from log(i=e<sup>t</sup>) to e<sup>t</sup> gives us FOOM^FOOM^FOOM^FOOM^FOOM, or <a href=\"http://en.wikipedia.org/wiki/Tetration\"><sup>4</sup>FOOM</a>.</p>\n<p>This doesn't necessarily mean a hard takeoff.&nbsp; \"Hard takeoff\" means, IMHO, FOOM in less than 6 months.&nbsp; Reaching the e^e^e^i level of efficiency would require vast computational resources, even given the right algorithms; an analysis might find that the universe doesn't have enough computronium to even represent, let alone reason over, that space.&nbsp; (In fact, this brings up the interesting possibility that the ultimate limits of knowledge will be storage capacity:&nbsp; Our AI descendants will eventually reach the point where they need to delete knowledge from their collective memory in order to have the space to learn something new.)</p>\n<p>However, I think this does mean FOOM.&nbsp; It's just a question of when.</p>\n<p>ADDED:&nbsp; Most commenters are losing sight of the overall argument.&nbsp; This is the argument:</p>\n<ol>\n<li> Humans have diminishing returns on raw information when trying to produce knowledge.&nbsp; It takes more dollars, more data, and more scientists to produce a publication or discovery today than in 1900.</li>\n<li>Evolution has increasing returns on information when producing intelligence.&nbsp; With 51% of the information in a human's DNA, you could build at best a bacteria.&nbsp; With 95-99%, you could build a chimpanzee.</li>\n<li>Producing knowledge from information is like producing intelligence from information. (Weak point.)</li>\n<li>Therefore, the knowledge that could be inferred from the knowledge that we have is much, much larger than the knowledge that we have.</li>\n<li>An artificial intelligence may be much more able than us to infer what is implied by what it knows.</li>\n<li>Therefore, the Singularity may not go FOOM, but FOOM<sup>FOOM</sup>.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KqhHhsBRzbf7eckTS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 5, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "1714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Information is power.&nbsp; But how much power?&nbsp; This question is vital when considering the speed and the limits of post-singularity development.&nbsp; To address this question, consider 2 other domains in which information accumulates, and is translated into an ability to solve problems:&nbsp; Evolution, and science.</p>\n<h2 id=\"DNA_Evolution\"><a id=\"more\"></a><strong>DNA Evolution</strong></h2>\n<p>Genes code for proteins.&nbsp; Proteins are composed of modules called \"domains\"; a protein contains from 1 to dozens of domains.&nbsp; We classify genes into gene \"families\", which can be loosely defined as sets of genes that on average share &gt;25% of their amino acid sequence and have a good alignment for &gt;75% of their length.&nbsp; The number of genes and gene families known doubles every 28 months; but most \"new\" genes code for proteins that recombine previously-known domains in different orders.</p>\n<p>Almost all of the information content of a genome resides in the amino-acid sequence of its domains; the rest mostly indicates what order to use domains in individual genes, and how genes regulate other genes.&nbsp; About 64% of domains (and 84% of those found in eukaryotes) evolved before eukaryotes split from prokaryotes about 2 billion years ago. (Michael Levitt, PNAS July 7 2009, \"Nature of the protein universe\"; D. Yooseph et al. \"The Sorcerer II global ocean sampling expedition\", PLoS Bio 5:e16.)&nbsp; (Prokaryotes are single-celled organisms lacking a nucleus, mitochondria, or gene introns.&nbsp; All multicellular organisms are eukaryotes.)</p>\n<p>It's therefore accurate to say that most of the information generated by evolution was produced in the first one or two billion years; the development of more-complex organisms seems to have nearly stopped evolution of protein domains.&nbsp; (Multi-cellular organisms are much larger and live much longer; therefore there are many orders of magnitude fewer opportunities for selection in a given time period.)&nbsp; Similarly, most evolution within eukaryotes seems to have occurred during a period of about 50 million years leading up to the Cambrian explosion, half a billion years ago.</p>\n<p>My first observation is that evolution has been slowing down in information-theoretic terms, while speeding up in terms of the intelligence produced.&nbsp; This means that adding information to the gene pool increases the effective intelligence that can be produced using that information by a more-than-linear amount.</p>\n<p>In the first of several irresponsible assumptions I'm going to make, let's assume that the information evolved in time <em>t</em> is proportional to<em> i</em> = log(t), while the intelligence evolved is proportional to e<sup>t</sup> = e<sup>e^i</sup>.&nbsp; I haven't done the math to support those particular functions; but I'm confident that they fit the data better than linear functions would.&nbsp; (This assumption is key, and the data should be studied more closely before taking my analysis too seriously.)</p>\n<p>My second observation is that evolution occurs in spurts.&nbsp; There's a lot of data to support this, including data from simulated evolution; see in particular the theory of punctuated equilibrium, and the data from various simulations of evolution in <em>Artificial Life</em> and <em>Artificial Life II</em>.&nbsp; But I want to single out the eukaryote-to-Cambrian-explosion spurt.&nbsp; The evolution of the first eukaryotic cell suddenly made a large subset of organism-space more accessible; and the speed of evolution, which normally decreases over time, instead <em>increased</em> for tens of millions of years.</p>\n<h2 id=\"Science_\"><strong>Science!</strong></h2>\n<p>The following discussion relies largely on de Solla Price's <em>Little Science, Big Science</em> (1963), Nicholas Rescher's <em>Scientific Progress:&nbsp;A Philosophical Essay on the Economics of Research in Natural Science</em> (1978), and the data I presented in my 2004 TransVision talk, \"<a href=\"http://www.transhumanism.org/tv/2004/GoetzAccelChange.ppt\">The myth of accelerating change</a>\".</p>\n<p>The growth of \"raw\" scientific knowledge is exponential by most measures: Number of scientists, number of degrees granted, number of journals, number of journal articles, number of dollars spent.&nbsp; Most of these measures have a doubling time of 10-15 years.&nbsp; (GDP has a doubling time closer to 20 years, suggesting that the ultimate limits on knowledge may be economic.)</p>\n<p>The growth of \"important\" scientific knowledge, measured by journal citations, discoveries considered worth mentioning in histories of science, and perceived social change, is much slower; if it is exponential, it appears IMHO to have had a doubling time of 50-100 years between 1600 and 1940.&nbsp; (It can be argued that this growth began slowing down at the onset of World War II, and more dramatically around 1970).&nbsp; Nicholas Rescher argues that important knowledge = log(raw information).</p>\n<p>A simple argument supporting this is that \"important\" knowledge is the number of distinctions you can make in the world; and the number of distinctions you can draw based on a set of examples is of course proportional to the log of the size of your data set, assuming that the different distinctions are independent and equiprobable, and your data set is random.&nbsp; However, an opposing argument is that log(i) is simply the amount of non-redundant information present in a database with uncompressed information <em>i</em>.&nbsp; (This appears to be approximately the case for genetic sequences.&nbsp; IMHO it is unlikely that scientific knowledge is that redundant; but that's just a guess.)&nbsp; Therefore, important knowledge is somewhere between O(log(information)) and O(information), depending whether information is closer to O(raw information) or O(log(raw information)).</p>\n<h2 id=\"Analysis\">Analysis</h2>\n<p>We see two completely-opposite pictures:&nbsp; In evolution, the efficaciousness of information increases more-than-exponentially with the amount of information.&nbsp; In science, it increases somewhere between logarithmically and linearly.</p>\n<p>My final irreponsible assumption will be that the production of ideas, concepts, theories, and inventions (\"important knowledge\") from raw information, is analogous to the production of intelligence from gene-pool information.&nbsp; Therefore, evolution's efficacy at using the information present in the gene pool can give us a lower bound on the amount of useful knowledge that <em>could</em> be extracted from our raw scientific knowledge.</p>\n<p>I argued above that the amount of intelligence produced from a given gene-information-pool <em>i</em> is approximately e^e<sup>i</sup>, while the amount of useful knowledge we extract from raw information <em>i</em> is somewhere between O(i) and O(log(i)).&nbsp; The implication is that the fraction of discoveries that we have made, out of those that could be made from the information we already have, has an upper bound between O(1/e^e^i) and O(1/e^e^e^i).</p>\n<p>One key question in asking what the shape of AI takeoff will be, is therefore: Will AI's efficiency at drawing inferences from information be closer to that of humans, or that of evolution?</p>\n<p>If the latter, then the number of important discoveries that an AI could make, using only the information we already have, may be between e^e^i and e^e^e^i times the number of important discoveries that we have made from it.&nbsp; <em>i</em> is a large number representing the total information available to humanity.&nbsp; e^e^i is a goddamn large number.&nbsp; e^e^e^i is an awful goddamn large number.&nbsp; Where before, we predicted FOOM, we would then predict FOOM^FOOM^FOOM^FOOM.</p>\n<p>Furthermore, the development of the first AI will be, I think, analogous to the evolution of the first eukaryote, in terms of suddenly making available a large space of possible organisms.&nbsp; I therefore expect the pace of information generation by evolution to suddenly switch from falling, to increasing, even before taking into account recursive self-improvement.&nbsp; This means that the rate of information increase will be much greater than can be extrapolated from present trends.&nbsp; Supposing that the rate of acquisition of important knowledge will change from log(i=e<sup>t</sup>) to e<sup>t</sup> gives us FOOM^FOOM^FOOM^FOOM^FOOM, or <a href=\"http://en.wikipedia.org/wiki/Tetration\"><sup>4</sup>FOOM</a>.</p>\n<p>This doesn't necessarily mean a hard takeoff.&nbsp; \"Hard takeoff\" means, IMHO, FOOM in less than 6 months.&nbsp; Reaching the e^e^e^i level of efficiency would require vast computational resources, even given the right algorithms; an analysis might find that the universe doesn't have enough computronium to even represent, let alone reason over, that space.&nbsp; (In fact, this brings up the interesting possibility that the ultimate limits of knowledge will be storage capacity:&nbsp; Our AI descendants will eventually reach the point where they need to delete knowledge from their collective memory in order to have the space to learn something new.)</p>\n<p>However, I think this does mean FOOM.&nbsp; It's just a question of when.</p>\n<p>ADDED:&nbsp; Most commenters are losing sight of the overall argument.&nbsp; This is the argument:</p>\n<ol>\n<li> Humans have diminishing returns on raw information when trying to produce knowledge.&nbsp; It takes more dollars, more data, and more scientists to produce a publication or discovery today than in 1900.</li>\n<li>Evolution has increasing returns on information when producing intelligence.&nbsp; With 51% of the information in a human's DNA, you could build at best a bacteria.&nbsp; With 95-99%, you could build a chimpanzee.</li>\n<li>Producing knowledge from information is like producing intelligence from information. (Weak point.)</li>\n<li>Therefore, the knowledge that could be inferred from the knowledge that we have is much, much larger than the knowledge that we have.</li>\n<li>An artificial intelligence may be much more able than us to infer what is implied by what it knows.</li>\n<li>Therefore, the Singularity may not go FOOM, but FOOM<sup>FOOM</sup>.</li>\n</ol>", "sections": [{"title": "DNA Evolution", "anchor": "DNA_Evolution", "level": 1}, {"title": "Science!", "anchor": "Science_", "level": 1}, {"title": "Analysis", "anchor": "Analysis", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "95 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-15T04:30:49.778Z", "modifiedAt": null, "url": null, "title": "Waterloo, ON, Canada Meetup: 6pm Sun Oct 18 '09!", "slug": "waterloo-on-canada-meetup-6pm-sun-oct-18-09", "viewCount": null, "lastCommentedAt": "2009-10-20T16:51:42.717Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j93FEgdau5YBvt4BY/waterloo-on-canada-meetup-6pm-sun-oct-18-09", "pageUrlRelative": "/posts/j93FEgdau5YBvt4BY/waterloo-on-canada-meetup-6pm-sun-oct-18-09", "linkUrl": "https://www.lesswrong.com/posts/j93FEgdau5YBvt4BY/waterloo-on-canada-meetup-6pm-sun-oct-18-09", "postedAtFormatted": "Thursday, October 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Waterloo%2C%20ON%2C%20Canada%20Meetup%3A%206pm%20Sun%20Oct%2018%20'09!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWaterloo%2C%20ON%2C%20Canada%20Meetup%3A%206pm%20Sun%20Oct%2018%20'09!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj93FEgdau5YBvt4BY%2Fwaterloo-on-canada-meetup-6pm-sun-oct-18-09%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Waterloo%2C%20ON%2C%20Canada%20Meetup%3A%206pm%20Sun%20Oct%2018%20'09!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj93FEgdau5YBvt4BY%2Fwaterloo-on-canada-meetup-6pm-sun-oct-18-09", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj93FEgdau5YBvt4BY%2Fwaterloo-on-canada-meetup-6pm-sun-oct-18-09", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>Michael Vassar and I will be attending the <a href=\"http://www.q2cfestival.com\">Quantum to Cosmos Festival</a> of the Perimeter Institute.&nbsp; Is anyone interested in meeting up at the Symposium Cafe on 4 King St N, Waterloo, ON, Canada, on Sunday at 6pm on October 18th 2009?&nbsp; I might duck out at around 8pm, but Michael Vassar seems more likely to stick around.&nbsp; If we get at least two more positive reply comments (plus the one person who suggested the meetup) then it'll be on and I'll take the question mark off the title.&nbsp; If that time doesn't work for you but you're in the area, feel free to email me about meeting informally.</p>\n<p><strong>Result:</strong>&nbsp; Okay, we have exactly two people RSVPing (I was hoping for three).&nbsp; We'll show up at the Symposium Cafe at 6pm, possibly walk around or head out to elsewhere by 6:30pm (i.e., join the meetup by 6:30pm if you want to be part of the walking-around group), and we've both got a talk we want to attend at 8:00pm.&nbsp; Either stuff will happen, or not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j93FEgdau5YBvt4BY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 5.296901819444488e-07, "legacy": true, "legacyId": "1717", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-15T09:37:33.643Z", "modifiedAt": null, "url": null, "title": "How to think like a quantum monadologist", "slug": "how-to-think-like-a-quantum-monadologist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RKEepbECEXQXqXFNm/how-to-think-like-a-quantum-monadologist", "pageUrlRelative": "/posts/RKEepbECEXQXqXFNm/how-to-think-like-a-quantum-monadologist", "linkUrl": "https://www.lesswrong.com/posts/RKEepbECEXQXqXFNm/how-to-think-like-a-quantum-monadologist", "postedAtFormatted": "Thursday, October 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20think%20like%20a%20quantum%20monadologist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20think%20like%20a%20quantum%20monadologist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKEepbECEXQXqXFNm%2Fhow-to-think-like-a-quantum-monadologist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20think%20like%20a%20quantum%20monadologist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKEepbECEXQXqXFNm%2Fhow-to-think-like-a-quantum-monadologist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRKEepbECEXQXqXFNm%2Fhow-to-think-like-a-quantum-monadologist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2466, "htmlBody": "<p>Half the responses to my last article focused on the subject of consciousness, understandably so. Back when LW was still part of OB, I stated my views in more detail (e.g. <a href=\"/lw/qe/do_scientists_already_know_this_stuff/k52\">here</a>, <a href=\"/lw/p7/zombies_zombies/u28\">here</a>, <a href=\"/lw/p7/zombies_zombies/u2a\">here</a>, and <a href=\"/lw/ps/where_physics_meets_experience/jo3\">here</a>); and I also think it's just obvious, once you allow yourself to notice, that the physics we have does not even contain the everyday phenomenon of color, so something has to change. However, it also seems that people won't change their minds until a concrete alternative to physics-as-usual and <em>de facto</em> property dualism actually comes along. Therefore, I have set out to explain how to think like a quantum monadologist, which is what I will call myself.<br /><a id=\"more\"></a><br />Fortunately, this new outlook agrees with the existing outlook far more than it disagrees. For example, even though I'm a quantum monadologist, I'm still seeking to identify the self and its experiences with some part of the physical brain. And I'm not seeking to add big new annexes to the physical formalism that we have, just in order to house the mind; though I may feel the need to impose a certain structure on that formalism, for ontological reasons, and that may or may not have empirical consequences in the macro-quantum realm.<br /><br />So what <em>are</em> the distinctive novelties of this other approach to the problem? There is an ontological hypothesis, that conscious states are states of a single physical entity, which we may call the self. There is a preferred version of the quantum formalism, in which the world is described by quantum jumps between spacelike tensor products of abstract quantum states (more on this below). The self is represented by one of the tensor factors appearing in these products. There is an inversion of attitude with respect to the mathematical formalism; we do not say that the self is actually a vector in a Hilbert space, we say that the nature of the self is as revealed by phenomenology, and the mathematics is just a way of describing its structure and dynamics. Finally, it is implied that significant quantum effects are functionally relevant to cognition, though so far this tells us nothing about where or how.<br /><br /><em>Quantum Jumps Between Tensor Products? </em><br /><br />For this audience, I think it's best that I start by explaining the quantum formalism I propose, even though the formalism has been chosen solely to match the ontology. I will assume familiarity with the basics of quantum mechanics, including superposition, entanglement, and the fact that we only ever see one outcome, even though the wavefunction describes many.<br /><br />Suppose we have three qubits, allegedly in a state like |011&gt; + |101&gt; + |110&gt;. In a many-worlds interpretation, we suppose that all three components are equally real. In a one-world interpretation, we normally assume that reality is just one of the three, e.g. |011&gt;, which can be expanded as |0&gt; x |1&gt; x |1&gt;: the first qubit is <em>actually</em> in the 0 state, the second and third qubits in the 1 state.<br /><br />However, we may, with just as much mathematical validity, express the original state as {|01&gt;+|10&gt;}|1&gt; + |110&gt;. If we look at that first term, how many <em>things</em> are present in it? If the defining property of a thing is that it has a state of its own, then we only have two things, and not three, because two of our qubits are entangled and don't have independent states. It is logically possible to have a one-world interpretation according to which there are two things actually there - one with quite a few degrees of freedom, in the state |01&gt;+|10&gt;, and the other in the much simpler state |1&gt; (and with |110&gt; being unreal, an artefact of the Schrodinger formalism, as must be all the unreal \"branches\" and \"worlds\" according to any single-world interpretation).<br /><br />And there you have it. This is, in its essence, the quantum formalism or quantum interpretation I want to use, as a neo-monadologist. At any time, the universe consists of a number of entities whose formal states inhabit Hilbert spaces of various dimension (thus |01&gt;+|10&gt; comes from a four-dimensional Hilbert space, while |1&gt; comes from a two-dimensional Hilbert space), and the true dynamics consists of repeatedly jumping from one such set of entity-states to another set of entity-states. Models like this exist <a href=\"http://arxiv.org/abs/hep-th/0302111\">in the physics literature</a> (see especially Figure 1; you may think of the points as qubits, and the ovals around them as indicating potential entanglement). For those who think in terms of \"collapse interpretations\", this may be regarded as a \"partial collapse theory\" in which most things, at any given time, are completely disentangled; actually realized entanglements are relatively local and transient. However, from the monadological perspective, we want to get away from the idea of entanglement, somewhat. We don't want to think of this as a world in which there are two entangled qubits and one un-entangled qubit, but rather a world in which there is one monad with four degrees of freedom, and another monad with two degrees of freedom. (The degrees of freedom correspond to the number of complex amplitudes required to specify the quantum state.)<br /><br /><em>The Actual Ontology of the Self and Its Relationship to the Formalism </em><br /><br />I've said that the self, the entity which you are and which is experiencing what you experience, is to be formally represented by one of these tensor factors; like |01&gt;+|10&gt;, though much much bigger. But I've also said that this is still just formalism; I'm not saying that the actual state of the self consists of a vector in a Hilbert space or a big set of complex numbers. So what is the actual state of the self, and how does it relate to the mathematics? <br /><br />The actual nature of the self I take to be at least partly revealed by phenomenology. You are, when awake, experiencing sensations; and you are experiencing them <em>as</em> something - there is a conceptual element to experience. Thoughts and emotions also, I think, conform somewhat to this dual description; there is an aspect of veridical awareness, and an aspect of conceptual projection. If we adopt <a href=\"http://en.wikipedia.org/wiki/Edmund_Husserl\">Husserl</a>'s quasi-Cartesian method of investigating consciousness - neither believing in that which is not definitely there, nor outright rejecting any of the stream of suppositions which make up the conceptual side of experience - we find that a specific consciousness, whatever else may be true about it, is partly characterized by this stream of double-sided states: on one side, the \"data\", the \"raw sensations\" and even \"raw thoughts\"; on the other side, the \"interpretation\", all the things which are posited to be true about the data. <br /><br />Husserl says all this much better than I do, and says much more as well, and he has a precise technical vocabulary in which to say it. As phenomenology, what I just wrote is crude and elementary. But I do want to point out one thing, which is that there is a phenomenology of thought and not just a phenomenology of sensation. Because sensations are so noticeable, philosophers of consciousness generally accept that they are there, and that a description of consciousness must include sensations; but there is a tendency (not universal) to regard thought, cognition, as unconscious. I see this as just footdragging on the part of materialist philosophers who have at length been compelled to admit that colors, et cetera, are there, somewhere; if you were setting out to describe your experience without ontological prejudice, of course you would say something about what you think and not just what you sense, and you would say that you have at least partial awareness of what you're thinking. <br /><br />But this poses a minor ontological challenge. So long as the ontology of consciousness is restricted to sensation, you can get away with saying that the contents of consciousness consist of a visual sensory field in a certain state, an auditory sensory field in another state, and so on through all the senses, and then all of these integrated in a unitary spatiotemporal meta-perception. A thought, however, is a rather different thing; it is something like a consciously apprehended conceptual structure. There are at least two ontological challenges here: what is a \"conceptual structure\", and how does it unite with raw sensory data to produce an interpreted experience, such as an experience of seeing an apple? The philosophers who limit consciousness to raw sensation alone don't face these problems; they can describe concepts and thinking in a purely computational and unconscious fashion. However, in reality there clearly is such a thing as conceptual phenomenology (or else we wouldn't talk about beliefs and thoughts and awareness of them), and the actual ontology of the self must reflect this. <br /><br />A crude way to proceed here, which I introduce more as a suggestion than as the answer, is to distinguish between presence and interpretation as aspects of consciousness. It's almost just terminology; but it's terminology constructed to resemble the reality. So, we say there is a self, whatever that is; everything \"raw\" is \"present\" to that self; and everything with a conceptual element is some raw presence that is being \"interpreted\". And since interpretations are themselves processes occurring within the self, logically they are themselves potentially present to it; and their presence may itself be conceptually interpreted. Thus we have the possibility of iteratively more complex \"higher-order thoughts\", thoughts about thoughts. <br /><br />Enough with the poetics for a moment. Is there a natural <em>formalism</em> for talking about such an entity? It would seem to require a conjunction of qualitative continua and sentential structure. For example, a standard way of talking about the raw visual field specifies hue, saturation, and intensity at every point in that field. But we also want to be able to say that a particular substructure within that field is being \"seen as a square\" or even \"seen as an apple\". We might build up these complex concepts <strong>square</strong> or <strong>apple</strong> combinatorially from a <a href=\"http://en.wikipedia.org/wiki/Natural_semantic_metalanguage\">set of primitive concepts</a>; and then we need a further notation to say that raw sensory structure X is currently being experienced as a Y. I emphasize again that I am not talking about the computation whereby input X is processed or categorized as a Y, but the conscious experience of interpreting sensation X as an object Y. It can be a slippery idea to hold onto, but I maintain that the situation is analogous to how it was with sensation. You can't say that a particular shade of red is <em>really</em> some colorless physical entity; you have to turn it around and say that the entity in your theory, which hitherto you only knew formally and mathematically, is actually a shade of red. And similarly, we are going to have to say that certain states and certain transitions of state, which we only knew formally and computationally, are actually conceptually interpreted perceptions, reflectively driven thought processes, and so forth. <br /><br />Returning to the second part of the question with which we started - how does the actual ontology of the self relate to the quantum mathematics - I have supposed that there is a mapping (maybe not 1-to-1, we may be overlooking other aspects of the self) from states of the self to descriptions of those states in a hybrid qualitative/sentential formalism. The implication is that there is a further mapping from this intermediate formalism into the quantum formalism of Hilbert spaces. This isn't actually so amazing. One way to do it is to have a separate basis state for each state in the intermediate formalism - so the basis states are formally labelled by the qualitative/sentential structures - and to also postulate that superpositions of these basis states never actually show up (as we would be unable to interpret them as states of consciousness). But there may be more subtle ways to do it which take advantage of more of the structure of Hilbert space. <br /><br /><em>What About Unconscious Matter?</em>&nbsp; <br /><br />If I continue to use this terminology of \"monads\" to describe the entities whose quantum states, tensored together, form the formal state of the universe from moment to moment, then my basic supposition is that conscious minds, e.g. as known from within to adult humans, correspond to monads with very many degrees of freedom, and that these are causally surrounded by (and interact with) many lesser monads in simpler, unconscious states. I'm not saying that complexity causes consciousness, but rather that conscious states, on account of having a minimum internal structure of a certain complexity, cannot be found in (say) a two-qubit monad, and that these simple monads make up the vast majority of them in nature. <br /><br />In fact, this might be an apt moment to say something about the relationship between these \"monads\" and the elementary particles in terms of which physics is normally described. I think of this in terms of string theory; not to be dogmatic about it, but it just concretely illustrates a way of thinking. There is a formulation of string theory in which everything is made up of entangled \"D0-branes\". An individual D0-brane, as I understand it, has just one scalar internal degree of freedom. A particular spatial geometry can be formed by a quantum condensate of D0-branes, and particles in that geometry are themselves individual D0-branes or are lesser condensates (e.g. a string would be, I suppose, a 1-dimensional D0-brane condensate). Living matter is made up of electrons and quarks; but these are themselves just D0-brane composites. So here we have the answer. The D0-branes are the fundamental degrees of freedom - the qubits of nature, so to speak - and their entanglements and disentanglements define the boundaries of the monads. <br /><br /><em>Abrupt Conclusion </em><br /><br />This is obviously more of a research program than a theory. About a dozen separate instances of handwaving need to be turned into concrete propositions before it has produced an actual theory. The section on how to talk about the actual nature of consciousness without implicitly falling back into the habit of treating the formalism as the reality may seem especially slippery and mystical; but in the end I think it's just another problem we have to face and solve. However, the point of this article is not to carry out the research program, but just to suggest what I'm actually on about. It will be interesting to see how much sense people are able to extract from it. <br /><br />P.S. I will get around to responding to comments from the previous article soon.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RKEepbECEXQXqXFNm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": -17, "extendedScore": null, "score": -3e-05, "legacy": true, "legacyId": "1720", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 268, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-19T07:29:34.468Z", "modifiedAt": null, "url": null, "title": "Localized theories and conditional complexity", "slug": "localized-theories-and-conditional-complexity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:38.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimmy", "createdAt": "2009-02-27T18:23:27.410Z", "isAdmin": false, "displayName": "jimmy"}, "userId": "JKdbpXHkv9AsuazJ3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8f7sXMEiKRWQdBRna/localized-theories-and-conditional-complexity", "pageUrlRelative": "/posts/8f7sXMEiKRWQdBRna/localized-theories-and-conditional-complexity", "linkUrl": "https://www.lesswrong.com/posts/8f7sXMEiKRWQdBRna/localized-theories-and-conditional-complexity", "postedAtFormatted": "Monday, October 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Localized%20theories%20and%20conditional%20complexity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALocalized%20theories%20and%20conditional%20complexity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f7sXMEiKRWQdBRna%2Flocalized-theories-and-conditional-complexity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Localized%20theories%20and%20conditional%20complexity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f7sXMEiKRWQdBRna%2Flocalized-theories-and-conditional-complexity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8f7sXMEiKRWQdBRna%2Flocalized-theories-and-conditional-complexity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 885, "htmlBody": "<p>Suppose I hand you a series of data points without providing the context. Consider the theory v = a*t for t&lt;&lt;1, v = b for t&gt;&gt;1. Without knowing anything a priori about the shapes of the curves, one must have enough data to make sure that v follows the right lines at the two limits since there is complexity that must be justified. &nbsp;Here we have two one-parameter curves, so we need at least two data points to pick the right slope and offset, as well as at least a couple more to make sure it follows the right shape.</p>\n<p>This is what I&rsquo;ll call a completely local theory &ndash; see data, fit curve. &nbsp;Dealing with problems at this level does not leave much room for human bias or error, but it also does not allow for improvement by including &nbsp;background knowledge.</p>\n<p><a id=\"more\"></a></p>\n<p>Now consider the case where v = velocity of a rocket sled, a = thrust/mass of sled, &nbsp;and b = sqrt(thrust/(1/2*rho*Cd*Af)). If you have a theory explaining rockets and aerodynamics, the equation v=a*t and v = b are just the limiting cases for small and large t. In this case, you only need two data points to find a and b since you already know the shape (over the full range) from solving the differential equations. If you understand the aerodynamics well enough, and know the shape and mass of the sled, you don&rsquo;t even need to do the experiment! &nbsp;The &ldquo;<a href=\"http://en.wikipedia.org/wiki/Conditional_entropy\">conditional complexity</a>&rdquo; is 0 since it is directly predicted from what we already know. This is the magic of keeping track of the dependencies between theories.</p>\n<p>We can take this a step further and derive a theory of aerodynamics from a theory of air molecules- and so on until we have one massively connected <a href=\"http://en.wikipedia.org/wiki/Theory_of_everything\">TOE</a>.</p>\n<p>Now step back to the beginning. &nbsp;If all I tell you is that when t = 1e-5, v = 2e-5 and when t = 1e-3, v = 2e-3, &nbsp;you&rsquo;re going to come up with the equationv = 2*t. If someone, with no further information, suggested that v = 2*t was only a small t approximation, and that for large t, v = 5.32, you&rsquo;d think that &nbsp;he&rsquo;s nutso (and rightfully so), with all that unnecessary complexity.</p>\n<p>As a wannabe Bayesian, you need to update on all evidence, so we're almost never trying to fit data without knowing what it means. We prefer globally simple theories, not theories where each local section is simple but they don't want to fit together.</p>\n<p>I suspect that one of the main reasons people fail to <a href=\"http://en.wikipedia.org/wiki/Occams_razor#Anti-razors\">understand/accept</a>&nbsp;Occam's razor comes from trying to apply it to theories locally and then noticing that by importing information from a more general theory, they can do better. Of course you do better with more information than you do with a wisely chosen ignorance prior. You need to apply Occams razor to the whole bundle. Since all of the background theory is the same, you can reduce this to the <a href=\"http://en.wikipedia.org/wiki/Entropy\">entropy</a> of the local theories that is left after <a href=\"http://en.wikipedia.org/wiki/Conditional_entropy\">conditioning</a> on the background theory.</p>\n<p>When Eliezer says that he doesn't expect humans to have <a href=\"/lw/lq/fake_utility_functions/\">simple utility functions</a>, its not because it is a magical case where Occam's razor doesn't apply. It's that it would take more bits overall to explain evolution creating a simple utility function than it would be to explain evolution creating a particular <a href=\"/lw/l3/thou_art_godshatter\">locally complex utility function</a>. This is very different than concluding that Occam's razor doesn't fit to real life. If Occam's razor seems to be giving you bad answers, you're <a href=\"http://www.doingitwrong.com/\">doing it wrong</a>.</p>\n<p>What does this imply for the future? Those with poor memories and/or a poor understanding of history will answer &ldquo;much like the present&rdquo; based on a single point and the locally simplest fit. You can find people one step up from that who notice improvements over time and fit it to a line, which again isn't a bad guess if that's all you know (you almost always know more- actually using the rest of your information efficiently is the trick). Another step ahead and you get people who hypothesize an exponential growth based on their understanding of improvements feeding improvements, or at least a wider spanning dataset. This is where you&rsquo;ll find Ray Kurzweil and the 'accellerating improvement singularity' folk. The last step I know of is where you'll find Eliezer Yudkowsky and other &lsquo;hard takeoff&rsquo; folks. This is where you say &ldquo;yes, I know my theory is locally more complex- I know that it isn&rsquo;t obvious from looking at the curve, quite the opposite. However, my theory is &nbsp;less complex after conditioning on the rest of my knowledge that <em>doesn't show up on this plot</em>, and for that reason, &nbsp;I believe it to be true&rdquo;.</p>\n<p>This might sound like saying &ldquo;Emeralds are <a href=\"http://en.wikipedia.org/wiki/Grue_and_Bleen\">Grue</a> not green&rdquo;, but while &ldquo;X until arbitrary date, then Y&rdquo; fares worse when applying Occam&rsquo;s razor locally, if our theory of color indicated a special &lsquo;turning point&rsquo;, then we would have to conclude &ldquo;Emeralds are grue, not green&rdquo;, and we would conclude this beacuse of Occam's razor, not inspite of it.</p>\n<p>I chose this example because it is important and well known at LW, but not for lack of other examples. In my experience, this is a very common mistake, even for otherwise intelligent individuals. This makes getting it right a quite fun rationality &lsquo;<a href=\"/lw/qs/einsteins_superpowers/\">superpower</a>&rsquo;.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8f7sXMEiKRWQdBRna", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 5.307054223182045e-07, "legacy": true, "legacyId": "1733", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NnohDYHNnKDtbiMyp", "cSXZpvqpa9vbGGLtG", "5o4EZJyqmHY4XgRCY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-19T12:36:30.730Z", "modifiedAt": null, "url": null, "title": "Applying Double Standards to \u2018\u2018Divisive\u2019\u2019 Ideas", "slug": "applying-double-standards-to-divisive-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:38.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlaidX", "createdAt": "2009-06-28T10:50:31.260Z", "isAdmin": false, "displayName": "PlaidX"}, "userId": "Cgh6rBbvZe4Noxt5d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/queYtEHA9mPLeehSK/applying-double-standards-to-divisive-ideas", "pageUrlRelative": "/posts/queYtEHA9mPLeehSK/applying-double-standards-to-divisive-ideas", "linkUrl": "https://www.lesswrong.com/posts/queYtEHA9mPLeehSK/applying-double-standards-to-divisive-ideas", "postedAtFormatted": "Monday, October 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applying%20Double%20Standards%20to%20%E2%80%98%E2%80%98Divisive%E2%80%99%E2%80%99%20Ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplying%20Double%20Standards%20to%20%E2%80%98%E2%80%98Divisive%E2%80%99%E2%80%99%20Ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqueYtEHA9mPLeehSK%2Fapplying-double-standards-to-divisive-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applying%20Double%20Standards%20to%20%E2%80%98%E2%80%98Divisive%E2%80%99%E2%80%99%20Ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqueYtEHA9mPLeehSK%2Fapplying-double-standards-to-divisive-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqueYtEHA9mPLeehSK%2Fapplying-double-standards-to-divisive-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>This is a commentary by Linda Gottfredson on a paper by Hunt and Carlson about a paper by Richard Nisbett regarding studies done by Arthur Jensen. It's ultimately about race and intelligence, but it seemed meta enough to link to here.</p>\n<p>Warning: PDF</p>\n<p><a href=\"http://www.udel.edu/educ/gottfredson/reprints/2007doublestandards.pdf\">Applying Double Standards to &lsquo;&lsquo;Divisive&rsquo;&rsquo; Ideas</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "queYtEHA9mPLeehSK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 5.307579874818081e-07, "legacy": true, "legacyId": "1735", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-19T13:16:44.043Z", "modifiedAt": null, "url": null, "title": "Near and far skills", "slug": "near-and-far-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:41.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DD5bZCWDGaEogGuTM/near-and-far-skills", "pageUrlRelative": "/posts/DD5bZCWDGaEogGuTM/near-and-far-skills", "linkUrl": "https://www.lesswrong.com/posts/DD5bZCWDGaEogGuTM/near-and-far-skills", "postedAtFormatted": "Monday, October 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Near%20and%20far%20skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANear%20and%20far%20skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDD5bZCWDGaEogGuTM%2Fnear-and-far-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Near%20and%20far%20skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDD5bZCWDGaEogGuTM%2Fnear-and-far-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDD5bZCWDGaEogGuTM%2Fnear-and-far-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 414, "htmlBody": "<p>Robin Hanson has repeatedly <a href=\"http://www.overcomingbias.com/2008/11/abstractdistant.html\">pointed</a> <a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\">out</a>&nbsp;the difference between near and far modes of thinking, and Alicorn has repeatedly <a href=\"/lw/gf/saturation_distillation_improvisation_a_story/\">pointed</a>&nbsp;<a href=\"/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/\">out</a> the difference between procedural and propositional knowledge. Just occurred to me that it's pretty much the same difference. I will now proceed to play some obvious riffs on the theme.</p>\n<p>Do you ever feel yourself to be an expert on some topic just from reading about it a lot? Every young enthusiastic programmer just entering the workforce feels like that. (I certainly did.) Every wannabe entrepreneur who hangs out on Hacker News and has read enough Paul Graham essays feels they can take on the world by just applying those valuable insights. (I certainly did.) It's soooo fun to feel knowledgeable, maybe even project it outward by giving Internet advice to newbies. The public relations department of my brain doesn't seem to care that I have no actual experience: reading stuff is quite enough to change my self-image.<a id=\"more\"></a></p>\n<p>The problem is, excessively liking propositional knowledge over procedural is a bias that harms us every day. Though some information is directly useful, most of it is worthless. A couple days ago I had to give advice to a classmate of mine who wants to start his own \"thing\" but isn't sure. See, he has this theory that one should accumulate propositional knowledge until one reaches critical mass, at which point the successful venture happens by itself. Being the wise and experienced mentor that I am (hah... on my second \"thing\", without much success), I told him outright that his theory was bullshit. Propositional knowledge doesn't spontaneously turn into procedural.</p>\n<p>(Digression: come to think, I'm not even sure why we need the kinds of propositional knowledge that we tend to accumulate. It reeks of a <a href=\"http://wiki.lesswrong.com/wiki/Superstimulus\">superstimulus</a>. I know more about programming that I'll ever need for work or play, but don't remember the birthdays of all my acquaintances, which would obviously be more useful. Memorizing birthdays just isn't as exciting as reading about comonads or whatever.)</p>\n<p>At this point I sincerely wish I had a recipe for overcoming this bias. Like pjeby, he always has a recipe. Well, I don't. Maybe perceiving the bias will turn out to be enough; maybe some kind of social software thing can help cure it on a mass scale, like <a href=\"http://en.wikipedia.org/wiki/Meetup.com\">meetup.com</a> is trying to cure <a href=\"http://en.wikipedia.org/wiki/Bowling_Alone\">Bowling Alone</a>; or maybe each of us will have to apply force, the oldskool way. It's too early to say.</p>\n<p>Thank me for never mentioning the example of riding a bicycle.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x3zyEPFaJANB2BHmP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DD5bZCWDGaEogGuTM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 22, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "1730", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MT85svcEweuryr2sn", "Pmfk7ruhWaHj9diyv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-20T01:13:07.428Z", "modifiedAt": null, "url": null, "title": "Shortness is now a treatable condition", "slug": "shortness-is-now-a-treatable-condition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:40.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TxFhRaZjcXZXtbcmq/shortness-is-now-a-treatable-condition", "pageUrlRelative": "/posts/TxFhRaZjcXZXtbcmq/shortness-is-now-a-treatable-condition", "linkUrl": "https://www.lesswrong.com/posts/TxFhRaZjcXZXtbcmq/shortness-is-now-a-treatable-condition", "postedAtFormatted": "Tuesday, October 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shortness%20is%20now%20a%20treatable%20condition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShortness%20is%20now%20a%20treatable%20condition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxFhRaZjcXZXtbcmq%2Fshortness-is-now-a-treatable-condition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shortness%20is%20now%20a%20treatable%20condition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxFhRaZjcXZXtbcmq%2Fshortness-is-now-a-treatable-condition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTxFhRaZjcXZXtbcmq%2Fshortness-is-now-a-treatable-condition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>There was some talk here about height taxes, but there's a better solution - <a href=\"http://www.nypost.com/p/news/local/manhattan/kids_dope_up_to_get_high_gtNeJJ7lzkwDFcPeEJ8zLJ/0\">redefine shortness as a treatable condition</a> and use HGH to cure it. They even got FDA on board with that, at least for 1.2% shortest people.</p>\n<p>Unsatisfactory sexual performance became a treatable condition with Viagra. Depression and hyperactivity became treatable conditions with SSRIs. Being ugly is already almost considered a treatable condition, at least one can get that impression from cosmetic surgery ads. Being overweight is universally considered an illness, even though we don't have too many effective treatment options (surgery is unpopular, and effective drugs like fen-phen and ECA are not officially prescribed any more). If we ever figure out how to increase IQ, you can be certain low IQ will be considered a treatable condition too. Almost everything undesirable gets redefined as an illness as soon as an effective way to fix it is developed.</p>\n<p>I welcome these changes. Yes, redefining large parts of normal human variability as illness is a lie, but if that's what society needs to work around its taboos against human enhancement, so be it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TxFhRaZjcXZXtbcmq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "1736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-20T16:10:38.877Z", "modifiedAt": null, "url": null, "title": "Lore Sj\u00f6berg's Life-Hacking FAQK", "slug": "lore-sjoeberg-s-life-hacking-faqk", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:38.731Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlaidX", "createdAt": "2009-06-28T10:50:31.260Z", "isAdmin": false, "displayName": "PlaidX"}, "userId": "Cgh6rBbvZe4Noxt5d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LeXfq8NsaHXzTDNcu/lore-sjoeberg-s-life-hacking-faqk", "pageUrlRelative": "/posts/LeXfq8NsaHXzTDNcu/lore-sjoeberg-s-life-hacking-faqk", "linkUrl": "https://www.lesswrong.com/posts/LeXfq8NsaHXzTDNcu/lore-sjoeberg-s-life-hacking-faqk", "postedAtFormatted": "Tuesday, October 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lore%20Sj%C3%B6berg's%20Life-Hacking%20FAQK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALore%20Sj%C3%B6berg's%20Life-Hacking%20FAQK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeXfq8NsaHXzTDNcu%2Flore-sjoeberg-s-life-hacking-faqk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lore%20Sj%C3%B6berg's%20Life-Hacking%20FAQK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeXfq8NsaHXzTDNcu%2Flore-sjoeberg-s-life-hacking-faqk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLeXfq8NsaHXzTDNcu%2Flore-sjoeberg-s-life-hacking-faqk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p><a href=\"http://www.wired.com/culture/lifestyle/commentary/alttext/2007/08/alttext_0822\">Lore Sj&ouml;berg's Life-hacking FAQK</a></p>\n<p>Pretty self-explanatory. Also available as a <a href=\"http://audio.sonibyte.com/audio/wired/8110.mp3\">podcast</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LeXfq8NsaHXzTDNcu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 1, "extendedScore": null, "score": 5.310414281308608e-07, "legacy": true, "legacyId": "1742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-20T16:35:52.427Z", "modifiedAt": null, "url": null, "title": "Why the beliefs/values dichotomy?", "slug": "why-the-beliefs-values-dichotomy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:41.932Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nW2EGC9XmsjEGsn9r/why-the-beliefs-values-dichotomy", "pageUrlRelative": "/posts/nW2EGC9XmsjEGsn9r/why-the-beliefs-values-dichotomy", "linkUrl": "https://www.lesswrong.com/posts/nW2EGC9XmsjEGsn9r/why-the-beliefs-values-dichotomy", "postedAtFormatted": "Tuesday, October 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20the%20beliefs%2Fvalues%20dichotomy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20the%20beliefs%2Fvalues%20dichotomy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnW2EGC9XmsjEGsn9r%2Fwhy-the-beliefs-values-dichotomy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20the%20beliefs%2Fvalues%20dichotomy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnW2EGC9XmsjEGsn9r%2Fwhy-the-beliefs-values-dichotomy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnW2EGC9XmsjEGsn9r%2Fwhy-the-beliefs-values-dichotomy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 686, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!-- /* Font Definitions */ @font-face {font-family:Wingdings; panose-1:5 0 0 0 0 0 0 0 0 0; mso-font-charset:2; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:0 268435456 0 0 -2147483648 0;} @font-face {font-family:\u5b8b\u4f53; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-alt:SimSun; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} @font-face {font-family:\"Cambria Math\"; panose-1:2 4 5 3 5 4 6 3 2 4; mso-font-charset:0; mso-generic-font-family:roman; mso-font-pitch:variable; mso-font-signature:-1610611985 1107304683 0 0 415 0;} @font-face {font-family:Calibri; panose-1:2 15 5 2 2 2 4 3 2 4; mso-font-charset:0; mso-generic-font-family:swiss; mso-font-pitch:variable; mso-font-signature:-520092929 1073786111 9 0 415 0;} @font-face {font-family:\"\\@\u5b8b\u4f53\"; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:\"\"; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:.5in; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:0in; margin-left:.5in; margin-bottom:.0001pt; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:0in; margin-left:.5in; margin-bottom:.0001pt; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:.5in; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoPapDefault {mso-style-type:export-only; margin-bottom:10.0pt; line-height:115%;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.0in 1.0in 1.0in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} /* List Definitions */ @list l0 {mso-list-id:1464956144; mso-list-type:hybrid; mso-list-template-ids:-261441802 -960706562 67698691 67698693 67698689 67698691 67698693 67698689 67698691 67698693;} @list l0:level1 {mso-level-start-at:0; mso-level-number-format:bullet; mso-level-text:\uf0b7; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; font-family:Symbol; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} ol {margin-bottom:0in;} ul {margin-bottom:0in;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]--><!--[if gte mso 9]><xml> <o:shapedefaults v:ext=\"edit\" spidmax=\"1026\" /> </xml><![endif]--><!--[if gte mso 9]><xml> <o:shapelayout v:ext=\"edit\"> <o:idmap v:ext=\"edit\" data=\"1\" /> </o:shapelayout></xml><![endif]--></p>\n<p class=\"MsoNormal\">I'd like to suggest that the fact that human preferences can be decomposed into beliefs and values is one that deserves greater scrutiny and explanation. It seems intuitively obvious to us that rational preferences must decompose like that (even if not exactly into a probability distribution and a utility function), but it&rsquo;s less obvious <em>why</em>.</p>\n<p class=\"MsoNormal\">The importance of this question comes from our tendency to see beliefs as being more objective than values. We think that beliefs, but not values, can be right or wrong, or at least that the notion of right and wrong applies to a greater degree to beliefs than to values. One dramatic illustration of this is in Eliezer Yudkowsky&rsquo;s proposal of <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>, where an AI extrapolates the preferences of an ideal humanity, in part by replacing their \"wrong&rdquo; beliefs with &ldquo;right&rdquo; ones. On the other hand, the AI treats their values with much more respect.</p>\n<p class=\"MsoNormal\">Since beliefs and values seem to correspond roughly to the probability distribution and the utility function in expected utility theory, and expected utility theory is convenient to work with due to its mathematical simplicity and the fact that it&rsquo;s been the subject of extensive studies, it seems useful as a first step to transform the question into &ldquo;why can human decision making be approximated as expected utility maximization?&rdquo;</p>\n<p class=\"MsoNormal\">I can see at least two parts to this question:</p>\n<ul>\n<li>Why this mathematical structure?</li>\n<li>Why this representation of the mathematical structure?</li>\n</ul>\n<p class=\"MsoNormal\">Not knowing how to answer these questions yet, I&rsquo;ll just write a bit more about why I find them puzzling.</p>\n<h4 class=\"MsoNormal\"><a id=\"more\"></a>Why this mathematical structure?</h4>\n<p class=\"MsoNormal\">It&rsquo;s well know that expected utility maximization can be derived from a number of different sets of assumptions (the so called axioms of rationality) but they all include the assumption of Independence in some form. Informally, Independence says that what you prefer to happen in one possible world doesn&rsquo;t depend on what you think happens in other possible worlds. In other words, if you prefer A&amp;C to B&amp;C, then you must prefer A&amp;D to B&amp;D, where A and B are what happens in one possible world, and C and D are what happens in another.</p>\n<p class=\"MsoNormal\">This assumption is central to establishing the mathematical structure of expected utility maximization, where you value each possible world separately using the utility function, then take their weighted average. If your preferences were such that A&amp;C &gt; B&amp;C but A&amp;D &lt; B&amp;D, then you wouldn&rsquo;t be able to do this.</p>\n<p class=\"MsoNormal\">It seems clear that our preferences do satisfy Independence, at least approximately. But why? (In this post I exclude indexical uncertainty from the discussion, because in that case I think Independence definitely <a href=\"/lw/102/indexical_uncertainty_and_the_axiom_of/\"><em>doesn't</em> apply</a>.) One <a href=\"/lw/my/the_allais_paradox/\">argument</a> that Eliezer has made (in a somewhat different context) is that if our preferences didn&rsquo;t satisfy Independence, then we would become money pumps. But that argument seems to assume agents who violate Independence, but try to use expected utility maximization anyway, in which case it wouldn&rsquo;t be surprising that they behave inconsistently. In general, I think being a money pump requires having <em>circular</em> (i.e., intransitive) preferences, and it's quite possible to have transitive preferences that don't satisfy Independence (which is why Transitivity and Independence are listed as separate axioms in the axioms of rationality).</p>\n<h4 class=\"MsoNormal\">Why this representation?</h4>\n<p class=\"MsoNormal\">Vladimir Nesov has <a href=\"/lw/148/bayesian_utility_representing_preference_by/\">pointed out</a> that if a set of preferences can be represented by a probability function and a utility function, then it can also be represented by two probability functions. And furthermore we can &ldquo;mix&rdquo; these two probability functions together so that it&rsquo;s no longer clear which one can be considered &ldquo;beliefs&rdquo; and which one &ldquo;values&rdquo;. So why do we have the particular representation of preferences that we do?</p>\n<p class=\"MsoNormal\">Is it possible that the dichotomy between beliefs and values is just an accidental byproduct of our evolution, perhaps a consequence of the specific environment that we&rsquo;re adapted to, instead of a common feature of all rational minds? Unlike the case with <a href=\"/lw/1b8/anticipation_vs_faith_at_what_cost_rationality/\">anticipation</a>, I don&rsquo;t claim that this is true or even likely here, but it seems to me that we don&rsquo;t understand things well enough yet to say that it&rsquo;s definitely false and why that's so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "xexCWMyds6QLWognu": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nW2EGC9XmsjEGsn9r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 28, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "1741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!-- /* Font Definitions */ @font-face {font-family:Wingdings; panose-1:5 0 0 0 0 0 0 0 0 0; mso-font-charset:2; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:0 268435456 0 0 -2147483648 0;} @font-face {font-family:\u5b8b\u4f53; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-alt:SimSun; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} @font-face {font-family:\"Cambria Math\"; panose-1:2 4 5 3 5 4 6 3 2 4; mso-font-charset:0; mso-generic-font-family:roman; mso-font-pitch:variable; mso-font-signature:-1610611985 1107304683 0 0 415 0;} @font-face {font-family:Calibri; panose-1:2 15 5 2 2 2 4 3 2 4; mso-font-charset:0; mso-generic-font-family:swiss; mso-font-pitch:variable; mso-font-signature:-520092929 1073786111 9 0 415 0;} @font-face {font-family:\"\\@\u5b8b\u4f53\"; panose-1:2 1 6 0 3 1 1 1 1 1; mso-font-charset:134; mso-generic-font-family:auto; mso-font-pitch:variable; mso-font-signature:3 680460288 22 0 262145 0;} /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-unhide:no; mso-style-qformat:yes; mso-style-parent:\"\"; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:.5in; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:0in; margin-left:.5in; margin-bottom:.0001pt; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:0in; margin-left:.5in; margin-bottom:.0001pt; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast {mso-style-priority:34; mso-style-unhide:no; mso-style-qformat:yes; mso-style-type:export-only; margin-top:0in; margin-right:0in; margin-bottom:10.0pt; margin-left:.5in; mso-add-space:auto; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoChpDefault {mso-style-type:export-only; mso-default-props:yes; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} .MsoPapDefault {mso-style-type:export-only; margin-bottom:10.0pt; line-height:115%;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.0in 1.0in 1.0in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} /* List Definitions */ @list l0 {mso-list-id:1464956144; mso-list-type:hybrid; mso-list-template-ids:-261441802 -960706562 67698691 67698693 67698689 67698691 67698693 67698689 67698691 67698693;} @list l0:level1 {mso-level-start-at:0; mso-level-number-format:bullet; mso-level-text:\uf0b7; mso-level-tab-stop:none; mso-level-number-position:left; text-indent:-.25in; font-family:Symbol; mso-fareast-font-family:\u5b8b\u4f53; mso-fareast-theme-font:minor-fareast; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} ol {margin-bottom:0in;} ul {margin-bottom:0in;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]--><!--[if gte mso 9]><xml> <o:shapedefaults v:ext=\"edit\" spidmax=\"1026\" /> </xml><![endif]--><!--[if gte mso 9]><xml> <o:shapelayout v:ext=\"edit\"> <o:idmap v:ext=\"edit\" data=\"1\" /> </o:shapelayout></xml><![endif]--></p>\n<p class=\"MsoNormal\">I'd like to suggest that the fact that human preferences can be decomposed into beliefs and values is one that deserves greater scrutiny and explanation. It seems intuitively obvious to us that rational preferences must decompose like that (even if not exactly into a probability distribution and a utility function), but it\u2019s less obvious <em>why</em>.</p>\n<p class=\"MsoNormal\">The importance of this question comes from our tendency to see beliefs as being more objective than values. We think that beliefs, but not values, can be right or wrong, or at least that the notion of right and wrong applies to a greater degree to beliefs than to values. One dramatic illustration of this is in Eliezer Yudkowsky\u2019s proposal of <a href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a>, where an AI extrapolates the preferences of an ideal humanity, in part by replacing their \"wrong\u201d beliefs with \u201cright\u201d ones. On the other hand, the AI treats their values with much more respect.</p>\n<p class=\"MsoNormal\">Since beliefs and values seem to correspond roughly to the probability distribution and the utility function in expected utility theory, and expected utility theory is convenient to work with due to its mathematical simplicity and the fact that it\u2019s been the subject of extensive studies, it seems useful as a first step to transform the question into \u201cwhy can human decision making be approximated as expected utility maximization?\u201d</p>\n<p class=\"MsoNormal\">I can see at least two parts to this question:</p>\n<ul>\n<li>Why this mathematical structure?</li>\n<li>Why this representation of the mathematical structure?</li>\n</ul>\n<p class=\"MsoNormal\">Not knowing how to answer these questions yet, I\u2019ll just write a bit more about why I find them puzzling.</p>\n<h4 class=\"MsoNormal\" id=\"Why_this_mathematical_structure_\"><a id=\"more\"></a>Why this mathematical structure?</h4>\n<p class=\"MsoNormal\">It\u2019s well know that expected utility maximization can be derived from a number of different sets of assumptions (the so called axioms of rationality) but they all include the assumption of Independence in some form. Informally, Independence says that what you prefer to happen in one possible world doesn\u2019t depend on what you think happens in other possible worlds. In other words, if you prefer A&amp;C to B&amp;C, then you must prefer A&amp;D to B&amp;D, where A and B are what happens in one possible world, and C and D are what happens in another.</p>\n<p class=\"MsoNormal\">This assumption is central to establishing the mathematical structure of expected utility maximization, where you value each possible world separately using the utility function, then take their weighted average. If your preferences were such that A&amp;C &gt; B&amp;C but A&amp;D &lt; B&amp;D, then you wouldn\u2019t be able to do this.</p>\n<p class=\"MsoNormal\">It seems clear that our preferences do satisfy Independence, at least approximately. But why? (In this post I exclude indexical uncertainty from the discussion, because in that case I think Independence definitely <a href=\"/lw/102/indexical_uncertainty_and_the_axiom_of/\"><em>doesn't</em> apply</a>.) One <a href=\"/lw/my/the_allais_paradox/\">argument</a> that Eliezer has made (in a somewhat different context) is that if our preferences didn\u2019t satisfy Independence, then we would become money pumps. But that argument seems to assume agents who violate Independence, but try to use expected utility maximization anyway, in which case it wouldn\u2019t be surprising that they behave inconsistently. In general, I think being a money pump requires having <em>circular</em> (i.e., intransitive) preferences, and it's quite possible to have transitive preferences that don't satisfy Independence (which is why Transitivity and Independence are listed as separate axioms in the axioms of rationality).</p>\n<h4 class=\"MsoNormal\" id=\"Why_this_representation_\">Why this representation?</h4>\n<p class=\"MsoNormal\">Vladimir Nesov has <a href=\"/lw/148/bayesian_utility_representing_preference_by/\">pointed out</a> that if a set of preferences can be represented by a probability function and a utility function, then it can also be represented by two probability functions. And furthermore we can \u201cmix\u201d these two probability functions together so that it\u2019s no longer clear which one can be considered \u201cbeliefs\u201d and which one \u201cvalues\u201d. So why do we have the particular representation of preferences that we do?</p>\n<p class=\"MsoNormal\">Is it possible that the dichotomy between beliefs and values is just an accidental byproduct of our evolution, perhaps a consequence of the specific environment that we\u2019re adapted to, instead of a common feature of all rational minds? Unlike the case with <a href=\"/lw/1b8/anticipation_vs_faith_at_what_cost_rationality/\">anticipation</a>, I don\u2019t claim that this is true or even likely here, but it seems to me that we don\u2019t understand things well enough yet to say that it\u2019s definitely false and why that's so.</p>", "sections": [{"title": "Why this mathematical structure?", "anchor": "Why_this_mathematical_structure_", "level": 1}, {"title": "Why this representation?", "anchor": "Why_this_representation_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "156 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 156, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qij9v3YqPfyur2PbX", "zJZvoiwydJ5zvzTHK", "kYgWmKJnqq8QkbjFj", "pePKcEd4HXfwtBptQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-22T15:16:39.346Z", "modifiedAt": null, "url": null, "title": "Biking Beyond Madness (link)", "slug": "biking-beyond-madness-link", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:39.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hirvinen", "createdAt": "2009-03-02T02:24:49.879Z", "isAdmin": false, "displayName": "hirvinen"}, "userId": "z4Nr6dBgHWfpe6NJd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jjGBv4iXpcprDc9DC/biking-beyond-madness-link", "pageUrlRelative": "/posts/jjGBv4iXpcprDc9DC/biking-beyond-madness-link", "linkUrl": "https://www.lesswrong.com/posts/jjGBv4iXpcprDc9DC/biking-beyond-madness-link", "postedAtFormatted": "Thursday, October 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biking%20Beyond%20Madness%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiking%20Beyond%20Madness%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjGBv4iXpcprDc9DC%2Fbiking-beyond-madness-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biking%20Beyond%20Madness%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjGBv4iXpcprDc9DC%2Fbiking-beyond-madness-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjjGBv4iXpcprDc9DC%2Fbiking-beyond-madness-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 404, "htmlBody": "<blockquote>\n<p>&lsquo;&lsquo;During race, I am going crazy, definitely,&rsquo;&rsquo; he says, smiling in bemused despair. &lsquo;&lsquo;I cannot explain why is that, but it is true.&rsquo;&rsquo;<br /><br />The craziness is methodical, however, and Robic and his crew know its pattern by heart. Around Day 2 of a typical weeklong race, his speech goes staccato. By Day 3, he is belligerent and sometimes paranoid. His short-term memory vanishes, and he weeps uncontrollably. The last days are marked by hallucinations: bears, wolves and aliens prowl the roadside; asphalt cracks rearrange themselves into coded messages. Occasionally, Robic leaps from his bike to square off with shadowy figures that turn out to be mailboxes. In a 2004 race, he turned to see himself pursued by a howling band of black-bearded men on horseback.<br /><br />&lsquo;&lsquo;Mujahedeen, shooting at me,&rsquo;&rsquo; he explains. &lsquo;&lsquo;So I ride faster.&rsquo;&rsquo;</p>\n</blockquote>\n<p>This <a href=\"http://www.nytimes.com/2006/02/05/sports/playmagazine/05robicpm.html?_r=1&amp;pagewanted=all\">2006 New York Times story</a> is about Jure Robic, a Slovenian ultra long distance bicycler who goes seriously insane when he pushes himself far enough during the races. At the point he feels like dying out of fatigue he still has a major portion (estimated 50 % by his team) of his strength left. So he hands over control to his team and with their help, pushes himself into the realm of insanity and gives up control to the team:<a id=\"more\"></a></p>\n<blockquote>\n<p>&nbsp;For Robic, his support crew serves as a second brain, consisting of a well-drilled cadre of a half-dozen fellow Slovene soldiers. It resembles other crews in that it feeds, hydrates, guides and motivates &mdash; but with an important distinction. The second brain, not Robic&rsquo;s, is in charge.</p>\n<p>&nbsp;&lsquo;&lsquo;By the third day, we are Jure&rsquo;s software,&rsquo;&rsquo; says Lt. Miran Stanovnik, Robic&rsquo;s crew chief. &lsquo;&lsquo;He is the hardware, going down the road.&rsquo;&rsquo;</p>\n</blockquote>\n<p>&nbsp;His success isn't because of exceptional physiology or training:</p>\n<blockquote>\n<p>&nbsp;On rare occasions when he permits himself to be tested in a laboratory, his ability to produce power and transport oxygen ranks on a par with those of many other ultra-endurance athletes. He wins for the most fundamental of reasons: he refuses to stop.</p>\n</blockquote>\n<p>&nbsp;The whole thing is an intriguing example of making an <a href=\"/lw/uo/make_an_extraordinary_effort/\">extraordinary, desperate effort</a> by knowing that even when his body and brain scream for him to stop, he can go further, and doing so. Also, pushing one's self to become insane isn't the sensible thing to do, but for him, it is <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">the path that wins</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"iNqgMuoewHKMhhXAp": 1, "ymWzfKxBchRvmCTNX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jjGBv4iXpcprDc9DC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 25, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "1751", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GuEsfTpSDSbXFiseH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-22T16:06:42.037Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: October 2009", "slug": "rationality-quotes-october-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:06.882Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/K5L2DX5SBmGE23NoC/rationality-quotes-october-2009", "pageUrlRelative": "/posts/K5L2DX5SBmGE23NoC/rationality-quotes-october-2009", "linkUrl": "https://www.lesswrong.com/posts/K5L2DX5SBmGE23NoC/rationality-quotes-october-2009", "postedAtFormatted": "Thursday, October 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20October%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20October%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5L2DX5SBmGE23NoC%2Frationality-quotes-october-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20October%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5L2DX5SBmGE23NoC%2Frationality-quotes-october-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FK5L2DX5SBmGE23NoC%2Frationality-quotes-october-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p style=\"margin: 0px 0px 1em;\">A monthly thread for posting rationality-related quotes you've seen recently (or had stored in your quotesfile for ages).</p>\n<ul style=\"margin: 10px 2em; padding: 0px; list-style-type: disc; list-style-position: outside;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then fine, post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "K5L2DX5SBmGE23NoC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "1752", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 284, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-23T03:48:08.860Z", "modifiedAt": null, "url": null, "title": "The continued misuse of the Prisoner's Dilemma", "slug": "the-continued-misuse-of-the-prisoner-s-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:42.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "SilasBarta", "createdAt": "2009-03-01T00:03:34.864Z", "isAdmin": false, "displayName": "SilasBarta"}, "userId": "zDPSZfarhLM7Gehug", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xq89jpDo6NZexWQLr/the-continued-misuse-of-the-prisoner-s-dilemma", "pageUrlRelative": "/posts/xq89jpDo6NZexWQLr/the-continued-misuse-of-the-prisoner-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/xq89jpDo6NZexWQLr/the-continued-misuse-of-the-prisoner-s-dilemma", "postedAtFormatted": "Friday, October 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20continued%20misuse%20of%20the%20Prisoner's%20Dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20continued%20misuse%20of%20the%20Prisoner's%20Dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq89jpDo6NZexWQLr%2Fthe-continued-misuse-of-the-prisoner-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20continued%20misuse%20of%20the%20Prisoner's%20Dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq89jpDo6NZexWQLr%2Fthe-continued-misuse-of-the-prisoner-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxq89jpDo6NZexWQLr%2Fthe-continued-misuse-of-the-prisoner-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 727, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/tn/the_true_prisoners_dilemma/\">The True Prisoner's Dilemma</a>, <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem and Regret of Rationality</a></p>\n<p>In The True Prisoner's Dilemma, Eliezer Yudkowsky pointed out a critical problem with the way the Prisoner's Dilemma is taught: the distinction between utility and avoided-jail-time is not made clear.&nbsp; The payoff matrix is supposed to represent the former, even as its numerical values happen to coincidentally match the latter.&nbsp; And worse, people don't naturally assign utility as per the standard payoff matrix: their compassion for the friend in the \"accomplice\" role means they wouldn't feel quite so good about a \"successful\" backstabbing, nor quite so bad about being backstabbed.&nbsp; (\"Hey, at least I didn't rat out a friend.\")</p>\n<p>For that reason, you rarely encounter a <em>true</em> Prisoner's Dilemma, even an iterated one.&nbsp; The above complications prevent real-world payoff matrices from working out that way.</p>\n<p>Which brings us to another unfortunate example of this misunderstanding being taught.</p>\n<p><a id=\"more\"></a>Recently, on the <em>New York Times</em>'s \"Freakonomics\" blog, Professor Daniel Hamermesh <a href=\"http://freakonomics.blogs.nytimes.com/2009/10/22/cheating-for-a-20/\">gleefully recounts</a> a recent experiment he performed (which he says he does often) on students in his intro economics course, which is basically the same as the Prisoner's Dilemma (henceforth, PD).</p>\n<p>Now, before going further, let me make clear that Hamermesh is no small player.&nbsp; Just take a look at all the accolades and accomplishments listed on his <a href=\"http://en.wikipedia.org/wiki/Daniel_S._Hamermesh\">Wikipedia page</a> or his <a href=\"http://www.eco.utexas.edu/faculty/Hamermesh/VITA.htm\">university page CV</a>.&nbsp; So, this is a teaching of a professor at the top of his field, so it's only with hesitation that I proceed further to allege that he's Doing It Wrong</p>\n<p>Hamermesh's variant of the PD is to pick eight students and auction off a $20 bill to them, with the money split evenly across the winners if there are multiple highest bids.&nbsp; Here, cooperation corresponds to adhering to a conspiracy where everyone agrees to make the same low bid and thus a big profit.&nbsp; Defecting corresponds to breaking the agreement and making a slightly higher bid so you can take everything for yourself.&nbsp; If the others continue to cooperate, their bid is lower and they get nothing.</p>\n<p>Here is how Hamermesh describes the result (italics mine, bold in the original):</p>\n<blockquote>\n<p>Today seven of the students stuck to the collusive agreement, and each bid $.01. They figured they would split the $20 eight ways, netting $2.49 each. <strong>Ashley</strong>, bless her heart, broke the agreement, bid $0.05, and collected $19.95. The other 7 students booed her, but I got the class to join me in applauding her, as <em>she was the only one who understood the game</em>.</p>\n</blockquote>\n<p><em>The</em> game?&nbsp; <em>Which</em> game?&nbsp; There's more than one game going on here!&nbsp; There's the neat little well-defined, artificial setup that Professor Hamermesh has laid out.&nbsp; On top of that, there's the game we better know as \"life\", in which the later consequences of superficially PD-like scenarios cause us to assign different utilities to successful backstabbing (defecting when others cooperate).&nbsp; There's also the game of becoming the high-status professor's <em>Wunderkind</em>.&nbsp; And while <strong>Ashley</strong> (whose name he bolded for some reason) may have won the narrow, artificial game, she also told everyone there that, \"Trusting me isn't such a good idea.\"&nbsp; In other words, the kind of consequence we normally worry about in our everyday lives.</p>\n<p>For this reason, I left the following <a href=\"http://freakonomics.blogs.nytimes.com/2009/10/22/cheating-for-a-20/#comment-508091\">comment</a>:</p>\n<blockquote>\n<p>No, she learned how to game a very narrow instance of that type of scenario, and got lucky that someone else didn&rsquo;t bid $0.06.</p>\n<p>Try that kind of thing in real life, and you&rsquo;ll get the social equivalent of a horse&rsquo;s head in your bed.</p>\n<p>Incidentally, how many friends did <strong>Ashley</strong> make out of this event?</p>\n</blockquote>\n<p>I probably came off as more \"anticapitalist\" or \"collectivist\" than I really am, but the point is important: betraying your partners has long-term consequences which aren't apparent when you only look at the narrow version of this game.</p>\n<p>Hamermesh's point was actually to show the difficulty of collusion in a free market.&nbsp; However, to the extent that markets can pose barriers to collusion, it's certainly <em>not</em> because going back on your word will consistently work out in just the right way as to divert a huge amount of utility to yourself -- which happens to be the very reason <strong>Ashley</strong> \"succeded\" (with the professor's applause) in this scenario.&nbsp; Rather, it's because the incentives for making such agreements fundamentally change; you are <em>still</em> better off maintaining a good reputation.</p>\n<p>Ultimately, the students learned the wrong lesson from an unrealistic game.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"be2Mh2bddQ6ZaBcti": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xq89jpDo6NZexWQLr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 33, "extendedScore": null, "score": 5.316551871319673e-07, "legacy": true, "legacyId": "1754", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFyWNBnDNEDsDNLrZ", "6ddcsdA2c2XpNpE5x"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-23T12:59:12.901Z", "modifiedAt": null, "url": null, "title": "Better thinking through experiential games", "slug": "better-thinking-through-experiential-games", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:02.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p66HNYv6an3eAcq5w/better-thinking-through-experiential-games", "pageUrlRelative": "/posts/p66HNYv6an3eAcq5w/better-thinking-through-experiential-games", "linkUrl": "https://www.lesswrong.com/posts/p66HNYv6an3eAcq5w/better-thinking-through-experiential-games", "postedAtFormatted": "Friday, October 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Better%20thinking%20through%20experiential%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABetter%20thinking%20through%20experiential%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp66HNYv6an3eAcq5w%2Fbetter-thinking-through-experiential-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Better%20thinking%20through%20experiential%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp66HNYv6an3eAcq5w%2Fbetter-thinking-through-experiential-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp66HNYv6an3eAcq5w%2Fbetter-thinking-through-experiential-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 762, "htmlBody": "<p>A few years ago I came across <em>The Logic of Failure</em> by Dietrich Doerner (previously mentioned <a href=\"/lw/bx/great_books_of_failure/8gx\">on LW</a>) which discusses cognitive failures in people dealing with \"complex situations\".<br /><br />One section (p.1 28) discusses a little simulation game, where participants are told to \"steer\" the temperature of a refrigerated storeroom with a defective thermostat, the exact equation governing how the thermostat setting affects actual temperature being unknown. Players control a dial with settings numbered 0 through 100, and can read actual temperature off a thermometer display. The <em>only</em> complications in this task are a) that there is a delay between changing the dial and the effects of the new setting; b) the possibility of \"overshoot\".<br /><br />I found the section's title chilling as well as fascinating: \"Twenty-eight is a good number.\" Doerner says this statement is typical of what participants faced with this type of situation tend to say. People don't just make ineffective use of the data they are presented with: they make up magical hypotheses, cling to superstitions or even call into question the very basis of the experiment, that there is a systematic link between thermostat setting and temperature.<br /><br />Reading about it is one thing, and actually playing the game quite another, so I got a group of colleagues together and we gave it a try. We were all involved in one way or another with managing software projects, which are systems way more complex than the simple thermostat system; our interest was to confirm Doerner's hypothesis that humans are generally inept at even simple management tasks. <strong>By the reports of all involved it was one of the most effective learning experiences they'd had.</strong> Since then, I have had a particular interest in this type of situation, which I have learned is sometimes called \"experiential learning\".<a id=\"more\"></a><br /><br />As I conceive of it, experiential learning consists of setting up a problematic situation, in such a way that the students (\"players\") should rely on their own wits to explore the situation, invent ways of dealing with it (sometimes by incorporating conceptual tools provided by an instructor), and test their newfound insights against the original problem - or against real-world situations. My preferred setting for experiential learning is a small-group format, with individual or group interaction with the situation, and group discussion for the debrief.<br /><br />In experiential learning there is no \"right\" or \"wrong\" lesson to be taken from a game or simulation. Everything that happens, not just the ostensible game but also the myriad meta-games that accompany it, is fodder for observation and analysis. Neither is realism a requirement for experiential learning; it is an understood convention of the genre that such games present an abstraction of some \"real world\" situation that necessarily deviates from it in many respects.<br /><br />The important part of an experiential learning situation is the debrief. In the debrief, you initially refrain from drawing conclusions about the experiment. The first thing you want from the session is <strong>data</strong>. A good question to ask is \"What happened in this session that stood out for you?\"<br /><br />Because you want to map the game back to the real world, perhaps in unforeseen ways, another thing you want from the session is <strong>analogies</strong>. A good question to ask is \"What did the experiences of this session remind you of?\"<br /><br />For learning to take place there should also be some <strong>puzzles</strong> arising from either the observations made during the game, or their transposition to real life. For instance, your preexisting mental model - derived from real life interactions - would have led you to different predictions about the game.<br /><br />The intended outcome of experiential learning is for students (and, sometimes, teacher) to construct an updated mental model that resolves these tensions and can be transposed back to real world situations and applied there. A constructivist approach doesn't expect students to draw exactly the same conclusions as the teacher, even when the teacher makes available the ingredients out of which students build their updated model. Knowledge obtained in that way is more <a href=\"/lw/la/truly_part_of_you/\">truly a part of you</a> - it sticks better than anything the teacher could have merely <em>told</em> you.<br /><br />An experiential learning game focusing on the basics of Bayesian reasoning might be a valuable design goal for this community - and a game I'd definitely have an interest in playing. Such is my \"hidden agenda\" in publishing this post...<br /><br />Any takers ?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"tk4R4LrX88gmFeMmY": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p66HNYv6an3eAcq5w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 32, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "1758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fg9fXrHpeaDD6pEPL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-23T14:40:48.084Z", "modifiedAt": null, "url": null, "title": "Extreme risks: when not to use expected utility", "slug": "extreme-risks-when-not-to-use-expected-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:41.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kmjCaq66MDkfvZpFX/extreme-risks-when-not-to-use-expected-utility", "pageUrlRelative": "/posts/kmjCaq66MDkfvZpFX/extreme-risks-when-not-to-use-expected-utility", "linkUrl": "https://www.lesswrong.com/posts/kmjCaq66MDkfvZpFX/extreme-risks-when-not-to-use-expected-utility", "postedAtFormatted": "Friday, October 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Extreme%20risks%3A%20when%20not%20to%20use%20expected%20utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExtreme%20risks%3A%20when%20not%20to%20use%20expected%20utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmjCaq66MDkfvZpFX%2Fextreme-risks-when-not-to-use-expected-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Extreme%20risks%3A%20when%20not%20to%20use%20expected%20utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmjCaq66MDkfvZpFX%2Fextreme-risks-when-not-to-use-expected-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmjCaq66MDkfvZpFX%2Fextreme-risks-when-not-to-use-expected-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 746, "htmlBody": "<p style=\"margin-bottom: 0.0001pt; line-height: normal;\">Would you prefer a 50% chance of gaining &euro;10, one chance in a million off gaining &euro;5 million, or a guaranteed &euro;5? The standard position on Less Wrong is that the answer depends solely on the difference between cash and utility. If your utility scales less-than-linearly with money, you are risk averse and should choose the last option; if it scales more-than-linearly, you are risk-loving and should choose the second one. If we replaced &euro;&rsquo;s with utils in the example above, then it would simply be irrational to prefer one option over the others.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">There are mathematical proofs of that result, but there are also strong intuitive arguments for it. What&rsquo;s the best way of seeing this? Imagine that X<sub>1</sub> and X<sub>2</sub> were two probability distributions, with mean u<sub>1</sub> and u<sub>2</sub> and variances v<sub>1</sub> and v<sub>2</sub>. If the two distributions are independent, then the sum X<sub>1</sub> + X<sub>2</sub> has mean u<sub>1</sub> + u<sub>2</sub>, and variance v<sub>1</sub> + v<sub>2</sub>.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">Now if we multiply the returns of any distribution by a constant r, the mean scales by r and variance scales by r2. Consequently if we have n probability distributions X<sub>1</sub>, X<sub>2</sub>, ... , X<sub>n</sub> representing n equally expensive investments, the expected average return is (&Sigma;<sup>n</sup><sub>i=1</sub> u<sub>i</sub>)/n, while the variance of this average is (&Sigma;<sup>n</sup><sub>i=1</sub> v<sub>i</sub>)/n<sup>2</sup>. If the v<sub>n</sub> are bounded, then once we make n large enough, that variance must tend to zero. So if you have many investments, your averaged actual returns will be, with high probability, very close to your expected returns.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;<!--[if gte vml 1]><v:shapetype id=\"_x0000_t75\" coordsize=\"21600,21600\" o:spt=\"75\" o:preferrelative=\"t\" path=\"m@4@5l@4@11@9@11@9@5xe\" filled=\"f\" stroked=\"f\"> <v:stroke joinstyle=\"miter\" /> <v:formulas> <v:f eqn=\"if lineDrawn pixelLineWidth 0\" /> <v:f eqn=\"sum @0 1 0\" /> <v:f eqn=\"sum 0 0 @1\" /> <v:f eqn=\"prod @2 1 2\" /> <v:f eqn=\"prod @3 21600 pixelWidth\" /> <v:f eqn=\"prod @3 21600 pixelHeight\" /> <v:f eqn=\"sum @0 0 1\" /> <v:f eqn=\"prod @6 1 2\" /> <v:f eqn=\"prod @7 21600 pixelWidth\" /> <v:f eqn=\"sum @8 21600 0\" /> <v:f eqn=\"prod @7 21600 pixelHeight\" /> <v:f eqn=\"sum @10 21600 0\" /> </v:formulas> <v:path o:extrusionok=\"f\" gradientshapeok=\"t\" o:connecttype=\"rect\" /> <o:lock v:ext=\"edit\" aspectratio=\"t\" /> </v:shapetype><v:shape id=\"_x0000_i1025\" type=\"#_x0000_t75\" alt=\"\" style='width:.75pt; height:.75pt'> <v:imagedata src=\"file:///C:\\Users\\StuartA\\AppData\\Local\\Temp\\msohtml1\\01\\clip_image001.gif\" mce_src=\"file:///C:\\Users\\StuartA\\AppData\\Local\\Temp\\msohtml1\\01\\clip_image001.gif\" o:href=\"http://lesswrong.com/static/tiny_mce/plugins/summarybreak/img/trans.gif\" /> </v:shape><![endif]--><!--[if !vml]--><a id=\"more\"></a><!--[endif]--></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">Thus there is no better strategy than to always follow expected utility. There is no such thing as sensible risk-aversion under these conditions, as there is no actual risk: you expect your returns to be your expected returns. Even if you yourself do not have enough investment opportunities to smooth out the uncertainty in this way, you could always aggregate your own money with others, through insurance or index funds, and achieve the same result. Buying a triple-rollover lottery ticket may be unwise; but being part of a consortium that buys up every ticket for a triple rollover lottery is just a dull, safe investment. If you have altruistic preferences, you can even aggregate results across the planet simply by encouraging more people to follow expected returns. So, case closed it seems; departing from expected returns is irrational.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">But the devil&rsquo;s detail is the condition &lsquo;once we make n large enough&rsquo;. Because there are risk distributions so skewed that no-one will ever be confronted with enough of them to reduce the variance to manageable levels. Extreme risks to humanity are an example; killer asteroids, rogue stars going supernova, unfriendly AI, nuclear war: even totalling all these risks together, throwing in a few more exotic ones, and generously adding every single other decision of our existence, we are nowhere near a neat probability distribution tightly bunched around its mean.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">To consider an analogous situation, imagine having to choose between a project that gave one util to each person on the planet, and one that handed slightly over twelve billion utils to a randomly chosen human and took away one util from everyone else. If there were trillions of such projects, then it wouldn&rsquo;t matter what option you chose. But if you only had one shot, it would be peculiar to argue that there are no rational grounds to prefer one over the other, simply because the trillion-iterated versions are identical. In the same way, our decision when faced with a single planet-destroying event should not be constrained by the behaviour of a hypothetical being who confronts such events trillions of times over.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">So where does this leave us? The independence axiom of the von Neumann-Morgenstern &nbsp;utility formalism should be ditched, as it implies that large variance distributions are identical to sums of low variance distributions. This axiom should be replaced by a weaker version which reproduces expected utility in the limiting case of many distributions. Since there is no single rational path available, we need to fill the gap with other axioms &ndash; values &ndash; that reflect our genuine tolerance towards extreme risk. As when we first discovered probability distributions in childhood, we may need to pay attention to medians, modes, variances, skewness, kurtosis or the overall shapes of the distributions. <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast\">Pascal's mugger</a> and his whole family can be confronted head-on rather than hoping the probabilities neatly cancel out.<br /> <!--[if !supportLineBreakNewLine]--><br /> <!--[endif]--></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">In these extreme cases, exclusively following the expected value is an arbitrary decision rather than a logical necessity.</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: 0.0001pt; line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"line-height: normal;\">&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kmjCaq66MDkfvZpFX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 6, "extendedScore": null, "score": 5.317674959112306e-07, "legacy": true, "legacyId": "1759", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-23T17:48:43.336Z", "modifiedAt": null, "url": null, "title": "Pound of Feathers, Pound of Gold", "slug": "pound-of-feathers-pound-of-gold", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:41.556Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h9WAEigBrbiimDHWA/pound-of-feathers-pound-of-gold", "pageUrlRelative": "/posts/h9WAEigBrbiimDHWA/pound-of-feathers-pound-of-gold", "linkUrl": "https://www.lesswrong.com/posts/h9WAEigBrbiimDHWA/pound-of-feathers-pound-of-gold", "postedAtFormatted": "Friday, October 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pound%20of%20Feathers%2C%20Pound%20of%20Gold&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APound%20of%20Feathers%2C%20Pound%20of%20Gold%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9WAEigBrbiimDHWA%2Fpound-of-feathers-pound-of-gold%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pound%20of%20Feathers%2C%20Pound%20of%20Gold%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9WAEigBrbiimDHWA%2Fpound-of-feathers-pound-of-gold", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9WAEigBrbiimDHWA%2Fpound-of-feathers-pound-of-gold", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 666, "htmlBody": "<p><em>Which weighs more:&nbsp; a pound of feathers, or a pound of gold?</em></p>\n<p>Close consideration of this riddle - and the conditions under which people tend to get it wrong - is helpful in understanding the limits of human rationality.&nbsp; It is a specific example which leads us to general principles of rationality failure.<a id=\"more\"></a></p>\n<p>These sorts of riddles and similar interpersonal language tricks (such as \"Stupid says what?\") are especially popular among children but not among adults.&nbsp; Why is this the case?&nbsp; Partly because adults are more likely to have previously encountered and become familiar with their patterns, but there are other factors - including one very relevant one.&nbsp; Children tend to have less-developed capacities of impulse control.</p>\n<p>It takes very little analysis to discover the 'trick' in the question; the concepts involved are relatively simple.&nbsp; But we're confronted with the fact that people <em>do</em> answer it incorrectly, and that by manipulating aspects of the context in which the question is delivered, we can significantly increase the chance people will fall for it.&nbsp; What does this imply?&nbsp; That analysis is not being conducted in the erroneous cases, and that context is a contributing factor to whether people successfully engage in conceptual analysis.&nbsp; Specifically, that context determines whether people will counter their impulses long enough for analysis to be completed.</p>\n<p>The key to these sorts of riddles is <em>time pressure</em>.&nbsp; If people feel free to take as much time as they like thinking over the question, they rarely fall for the trick.&nbsp; But if they're trying to answer rapidly, they'll screw up.&nbsp; Examples of situations that often result in such behavior include:&nbsp; competing against others to see who can be correct first, trying to demonstrate competence by investing little effort in answering, or encountering the question as part of a limited-duration examination.&nbsp; If several superficially-similar questions whose answer depends on retrieving facts from memory rather than performing logical analysis of the question are asked before the riddle is presented, that also tends to result in a wrong response.</p>\n<p>The error occurs because of our weight-related associations with the concepts of 'feathers' and 'gold', our conditioned assumptions about the sorts of questions people are likely to ask, and a failure to inhibit the first impulses towards response.&nbsp; Feathers are far less dense than gold; any given volume of feathers will weigh far less than the same volume of the metal.&nbsp; Questions about a property rarely contain their own answers in a trivial way - we do not expect the defined quantities in the question to be equivalent relative to the property being asked about.&nbsp; And - this is the most vital aspect - it takes longer for our brains to process the question at a conceptual level than it does to activate our associations.</p>\n<p>In the state of nature, organisms are often under intense pressure to produce results quickly.&nbsp; If they take too long, the resource they're trying to exploit may be taken by a competitor - or worse, they may become exploited resources by a predator.&nbsp; So stimulus-response methods which produce generally-useful reactions tend to be favored over extremely accurate and precisely analysis that takes longer.&nbsp; As a consequence, natural modes of though available to humans favor rapid responses more than rigorous correctness - and in much the same way that the limits of our visual processing systems lead to optical illusions, which can be understood and thus constructed, the limits of our conceptual processing lead to inherent tendencies towards fallacies of reason, which can be exploited to produce riddles and language gags.</p>\n<p>Just as other aspects of our behavioral response involve the repression of rudimentary reflexes, our thinking involves the inhibition of associational activation and reflexive reactions.&nbsp; The \"more advanced\" cognitive functions can take place only because the simpler, less resource-intensive, and faster functions are prevented from initiating responses before them.</p>\n<p>In the wrestling match between the modern functions and the ancient ones they try to control, the more subtle and advanced features are at a distinct disadvantage.&nbsp; Which brings us to the next post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h9WAEigBrbiimDHWA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 5, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "1760", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-24T20:46:07.942Z", "modifiedAt": null, "url": null, "title": "Arrow's Theorem is a Lie", "slug": "arrow-s-theorem-is-a-lie", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:51.681Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hqpSutKLfBQtffs72/arrow-s-theorem-is-a-lie", "pageUrlRelative": "/posts/hqpSutKLfBQtffs72/arrow-s-theorem-is-a-lie", "linkUrl": "https://www.lesswrong.com/posts/hqpSutKLfBQtffs72/arrow-s-theorem-is-a-lie", "postedAtFormatted": "Saturday, October 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arrow's%20Theorem%20is%20a%20Lie&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArrow's%20Theorem%20is%20a%20Lie%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqpSutKLfBQtffs72%2Farrow-s-theorem-is-a-lie%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arrow's%20Theorem%20is%20a%20Lie%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqpSutKLfBQtffs72%2Farrow-s-theorem-is-a-lie", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqpSutKLfBQtffs72%2Farrow-s-theorem-is-a-lie", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1421, "htmlBody": "<p>Suppose that we, a group of rationalists, are trying to come to an agreement on which course of action to take. All of us have the same beliefs about the world, as Aumann's Agreement Theorem requires, but everyone has different preferences. What decision-making procedure should we use? We want our decision-making procedure to satisfy a number of common-sense criteria:</p>\n<p>- Non-dictatorship: No one person should be able to dictate what we should do next. The decision-making process must take multiple people's preferences into account.</p>\n<p>- Determinism: Given the same choices, and the same preferences, we should always make the same decisions.</p>\n<p>- Pareto efficiency: If every member of the group prefers action A to action B, the group as a whole should also prefer A to B.</p>\n<p>- Independence of irrelevant alternatives: If we, as a group, prefer A to B, and a new option C is introduced, then we should still prefer A to B, regardless of what we think about C.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Arrow's_Theorem\">Arrow's Theorem</a> says that: Suppose we have a list of possible courses of action, {A, B, C, D... Q}. Everyone gets a pencil and a piece of paper, and writes down what their preferences are. Eg., if you thought that Q was the best, D was the second best, and A was the worst, you would write down Q &gt; D &gt; (...) &gt; A. We then collect all the slips of paper, and put them in a big bin. However, we can't take all these slips of paper and produce a set of preferences for the group as a whole, eg., {H &gt; E &gt; I &gt; C &gt; ... &gt; D &gt; A}, in a way that satisfies all of the above criteria.</p>\n<p><a id=\"more\"></a>Therefore, (so the story goes), much woe be unto us, for there is no way we can make decisions that satisfies this entirely reasonable list of requirements. Except, this isn't actually true. Suppose that, instead of writing down a list of which actions are preferred to which other actions, we write down a score next to each action, on a scale from 0-10. (For simplicity, we allow irrationals as well as integers, so, say, sqrt(2) or 7/3 are perfectly valid scores). We add up the numbers that each person puts down for each action. Say, if person A puts down 5 for action A, and person B puts down 7 for action A, then A has a total score of 12. We then decree: we will, as a group, prefer any action with a higher total score to any action with a lower total score. This procedure <em>does </em>satisfy all of the desired criteria:</p>\n<p>- Non-dictatorship: Suppose there are two possible actions, A and B, and ten people in the group. Each person gives a score of zero to A and a ten to B, so B has a total score of 100, as compared to A's score of zero. If any one person changes their score for A to X, and their score for B to Y, then the total score for A is now X, and the score for B is now 90 + Y. Since we know that 0 &lt;= X &lt;= 10 and 0 &lt;= Y &lt;= 10, we can derive that 90 + Y &gt;= 90 &gt; 10 &gt;= X, so B is still preferable to A. Therefore, no one person can switch the group's decision from B to A, so no one person is a dictator.</p>\n<p>- Determinism: All we do is add and rank the scores, and adding and ranking are both deterministic operations, so we know this procedure is deterministic.</p>\n<p>- Pareto efficiency: Suppose that there are N people in the group, and two possible actions, A and B. Every person prefers A to B, so person X will assign some score to B, B_X, and some score to A, A_X, where A_X &gt; B_X. Let C_X = A_X - B_X; we know that C_X &gt; 0. The total score for B is (B_1 + B_2 + B_3 + ... + B_N), and the total score for A is (A_1 + A_2 + A_3 + ... + A_N) = (B_1 + B_2 + B_3 + ... + B_N) + (C_1 + C_2 + C_3 + ... + C_N) &gt; (B_1 + B_2 + B_3 + ... + B_N), so the group as a whole also prefers A to B.</p>\n<p>- Independence of irrelevant alternatives: If we prefer A to B as a group, and we all assign the same scores to A and B when C is introduced, then the total score of A must still be higher than that of B, even after C is brought in. Hence, our decision between A and B does not depend on C.</p>\n<p>What happened here? Arrow's Theorem isn't wrong, in the usual sense; <a href=\"http://en.wikipedia.org/wiki/Arrow%27s_Theorem#Informal_proof\">the proof</a> is perfectly valid mathematically. However, the theorem only covers a specific subset of decision algorithms: those where everyone writes down a simple preference ordering, of the form {A &gt; B &gt; C &gt; D ... &gt; Q}. It doesn't cover <em>all</em> collective decision procedures.</p>\n<p>I think what's happening here, in general, is that a). there is a great deal of demand for theorems which show counterintuitive results, as these tend to be the most interesting, and b). there are a large number of theorems which give counterintuitive results about things in the world of mathematics, or physics. For instance, many mathematicians were surprised when Cantor showed that the number of all rational numbers is equal to the number of all integers.</p>\n<p>Therefore, every time someone finds a theorem which appears to show something counterintuitive about real life, there is a tendency to stretch the definition: to say, wow, this theorem shows that thing X is actually impossible, when it usually just says \"thing X is impossible if you use method Y, under conditions A, B and C\". And it's unlikely you'll get called on it, because people are used to theorems which show things like 0.999... = 1, and we apparently treat this as evidence that a theorem which (supposedly) says \"you can't design a sane collective decision-making procedure\" or \"you can't do electromagnetic levitation\" is more likely to be correct.</p>\n<p>More examples of this type of fallacy:</p>\n<p>- <a href=\"http://en.wikipedia.org/wiki/Earnshaw%27s_theorem\">Earnshaw's theorem</a> states that no stationary collection of charges, or magnets, is stable. Hence, you cannot put a whole bunch of magnets in a plate, put the plate over a table with some more magnets in it, and expect it to stay where it is, regardless of the configuration of the magnets. This theorem was taken to mean that electromagnetic levitation of any sort was impossible, which caused a great deal of grief, as scientists then still thought that atoms were governed by electromagnetism rather than quantum mechanics, and so the theorem would appear to imply that no collection of atoms is stable. However, we now know that atoms stay stable by virtue of the Pauli exclusion principle, not classical E&amp;M. And, it turns out, it <em>is</em> possible to levitate things with magnets: you just have to keep the thing spinning, instead of leaving it in one place. Or, you can use <a href=\"http://en.wikipedia.org/wiki/Diamagnetism\">diamagnetism</a>, but that's another discussion.</p>\n<p>- <a href=\"/lw/pq/the_socalled_heisenberg_uncertainty_principle/\">Heisenberg's Student Confusion Principle</a>, which does <em>not </em>say \"we can't find out what the position and momentum of a particle is\", has already been covered earlier, by our own Eliezer Yudkowsky.</p>\n<p>- <a href=\"http://en.wikipedia.org/wiki/Free_will_theorem\">Conway's Free Will Theorem</a> says that it's impossible to predict the outcomes of certain quantum experiments ahead of time. This is a pretty interesting fact about quantum physics. However, it is <em>only</em> a fact about quantum physics; it is <em>not </em>a fact about this horribly entangled thing that we call \"free will\". There are also numerous classical, fully deterministic computations that we can't predict the output ahead of time; eg., if you write a Turing Machine to output every <a href=\"http://en.wikipedia.org/wiki/Twin_prime_conjecture\">twin prime</a>, will it halt after some finite number of steps? If we could predict all such computations ahead of time, it would violate the Halting Theorem, and so there must be some that we can't predict.</p>\n<p>- There are numerous <a href=\"http://en.wikipedia.org/wiki/Category:Economics_laws\">law of economics</a> which appear to fall into this category; however, so far as I can tell, few of them have been mathematically formalized. I am not entirely sure whether this is a good thing or a bad thing, as formalizing them would make them more precise, and clarify the conditions under which they apply, but also might cause people to believe in a law \"because it's been mathematically proven!\", even well outside of the domain it was originally intended for.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mPuSAzJN7CyrMiKrf": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hqpSutKLfBQtffs72", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 36, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "1762", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eWuuznxeebcjWpdnH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-25T18:14:43.990Z", "modifiedAt": null, "url": null, "title": "The Value of Nature and Old Books", "slug": "the-value-of-nature-and-old-books", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:53.196Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PGyJ5Kczc77AYLSKZ/the-value-of-nature-and-old-books", "pageUrlRelative": "/posts/PGyJ5Kczc77AYLSKZ/the-value-of-nature-and-old-books", "linkUrl": "https://www.lesswrong.com/posts/PGyJ5Kczc77AYLSKZ/the-value-of-nature-and-old-books", "postedAtFormatted": "Sunday, October 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Value%20of%20Nature%20and%20Old%20Books&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Value%20of%20Nature%20and%20Old%20Books%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGyJ5Kczc77AYLSKZ%2Fthe-value-of-nature-and-old-books%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Value%20of%20Nature%20and%20Old%20Books%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGyJ5Kczc77AYLSKZ%2Fthe-value-of-nature-and-old-books", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPGyJ5Kczc77AYLSKZ%2Fthe-value-of-nature-and-old-books", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 751, "htmlBody": "<p>People have always had a religious or quasi-religious reverence for nature. In modern times, some people have started to see nature more as an enemy to be conquered than as a god to be worshiped. Such people point out that uncontrolled nature causes a tremendous amount of human suffering (to say nothing of all the misery that it causes other creatures), and that vast improvements to human welfare have largely been the result of us ceasing to love and fear nature and starting to control it.<br /><br />There are several common responses to this. One response is that it is solipsistic for humans to measure the value of nature in terms of what is and is not good for us. This strikes me as right only insofar as it ignores the welfare of non-human creatures who have enough going on in terms of consciousness and/or sentience to matter; I think the objection would be without merit if one were to broaden the scope of concern to something like all  creatures, present and future, capable of having experiences (who else is there to care about?). A&nbsp; second response is that seeing ourselves as highly effective lords over  nature leads to dangerous overconfidence, which leads to costly mistakes in how we deal with nature. This is a very fair point, but what it really amounts to is a claim that we shouldn't underestimate the enemy, not that the enemy is really a friend. Anyway, the solution to that problem is to become better rationalists and get better at being skeptical regarding our powers, not to retreat into quasi-mystical Gaia worship. A third response is that getting into a \"conquer nature\" frame of mind puts people into a \"conquer everything\" frame of mind and leads to aggression against other people. This might have merit historically, but that problem is also best confronted directly, in this case by more effectively promulgating liberal humanistic values.<br /><a id=\"more\"></a><br />So what, if anything, is left to the idea that there is something special about nature worthy of particular regard? And by special I mean something beyond the fact that many people just plain enjoy it the way they enjoy lots of other things that nevertheless have no claim to any special status. I would say that the main thing that makes nature special in this sense is that when you are in nature or contemplating nature, you can be confident that the resulting thoughts and feelings are uncontaminated by all of the (visible and invisible) ideas and biases and assumptions that are present in  your particular time and place. When you look at a waterfall and you like it, you can be pretty sure that: (i) it wasn't put there by anyone with an agenda; (ii) you weren't manipulated into liking it by contemporary ideology or social pressure or persuasive advertising or whatever; and (iii) the thoughts that you think while contemplating it aren't the thoughts anyone is trying to lead you into. In other words, nature is a way of guaranteeing that there is a little corner of experience that we are instinctively drawn to and that we can be confident doesn't represent anyone else's attempt to control us. And since other people are trying to control us all the time, even in relatively free societies (all the more so in oppressive ones), this is of real value.<br /><br />I think the same basic point applies to some other things besides nature. Why do people still read old books* even when the knowledge in them has been refined and improved-upon in the meantime? In many subjects,  we don't. Nobody learns geometry by reading Euclid, because there would be no point. But people do still read ancient works of philosophy. It seems to me that one good reason to do so is that for all the ways that these works have been analyzed and surpassed in the intervening years, the reader can be sure that what is written there is not the product of manipulation by the forces that are at work in the reader's own time and place. So it represents another way to gain valuable freedom and distance.<br /><br />*Here I'm talking about non-fiction books. The merits of old creative works even when the innovations in them have become widespread in newer works is a different story. Often a point like the one in this post still applies, and sometimes the old stuff really is still just the best.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PGyJ5Kczc77AYLSKZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 11, "extendedScore": null, "score": 5.322994820275032e-07, "legacy": true, "legacyId": "1765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-26T01:43:16.174Z", "modifiedAt": null, "url": null, "title": "Circular Altruism vs. Personal Preference", "slug": "circular-altruism-vs-personal-preference", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:42.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/swSBpXET5cRbJBFFY/circular-altruism-vs-personal-preference", "pageUrlRelative": "/posts/swSBpXET5cRbJBFFY/circular-altruism-vs-personal-preference", "linkUrl": "https://www.lesswrong.com/posts/swSBpXET5cRbJBFFY/circular-altruism-vs-personal-preference", "postedAtFormatted": "Monday, October 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Circular%20Altruism%20vs.%20Personal%20Preference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACircular%20Altruism%20vs.%20Personal%20Preference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswSBpXET5cRbJBFFY%2Fcircular-altruism-vs-personal-preference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Circular%20Altruism%20vs.%20Personal%20Preference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswSBpXET5cRbJBFFY%2Fcircular-altruism-vs-personal-preference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FswSBpXET5cRbJBFFY%2Fcircular-altruism-vs-personal-preference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 459, "htmlBody": "<p>Suppose there is a diagnostic procedure that allows to catch a relatively rare disease with absolute precision. If left untreated, the disease if fatal, but when diagnosed it's easily treatable (I suppose there are some real-world approximations). The diagnostics involves an uncomfortable procedure and inevitable loss of time. At what a priori probability would you not care to take the test, leaving this outcome to chance? Say, you decide it's 0.0001%.</p>\n<p>Enter <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a>. Your decision to take or not take the test may be as well considered a decision for the whole population (let's also <a href=\"http://wiki.lesswrong.com/wiki/Least_convenient_possible_world\">assume</a> you are typical and everyone is similar in this decision). By deciding to personally not take the test, you've decided that most people won't take the test, and thus, for example, with 0.00005% of the population having the condition, about 3000 people will die. While personal tradeoff is fixed, this number obviously depends on the size of the population.</p>\n<p>It seems like a horrible thing to do, making a decision that results in 3000 deaths. Thus, taking the test seems like a small personal sacrifice for this gift to others. Yet this is circular: everyone would be thinking that, reversing decision solely to help others, not benefiting personally. Nobody benefits.</p>\n<p>Obviously, together with 3000 lives saved, there is a factor of 6 billion accepting the test, and that harm is also part of the outcome chosen by the decision. If everyone personally prefers to not take the test, then inflicting the opposite on the whole population is only so much worse.</p>\n<p>Or is it?<a id=\"more\"></a> What if you care more about other people's lives in proportion to their comfort than you care about your own life in proportion to your own comfort? How can caring about other people be in <em>exact</em> harmony with caring about yourself? It may be the case that you prefer other people to take the test, even if you don't want to take the test yourself, and that is the position of the whole population. What is the right thing to do then? What wins: personal preference, or this \"circular altruism\", preference about other people that not a single person accepts for oneself?</p>\n<p>If altruism wins, then it seems that the greater the population, the less personal preference should matter, and the more the structure of altruistic preference takes over the personal decision-making. Person disappears, with everyone going through the motions of implementing the perfect play for their ancestral spirits.</p>\n<p>P.S. This thought experiment is an example of <a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">Pascal's mugging</a> closer to real-world scale. As with specks, I assume that there is no <a href=\"/lw/13t/andrewhs_observation_and_opportunity_costs/\">opportunity cost</a> in lives from taking the test. The experiment also confronts <a href=\"http://wiki.lesswrong.com/wiki/Utilitarianism\">utilitarian analysis</a> of caring about other people depending on structure of the population, comparing that criterion against personal preference.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "swSBpXET5cRbJBFFY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "1766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XpXQ4KNzLa9ZHYw8p"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-26T22:06:38.945Z", "modifiedAt": null, "url": null, "title": "Computer bugs and evolution", "slug": "computer-bugs-and-evolution", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:42.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NcY2K27z6DYoLpdYa/computer-bugs-and-evolution", "pageUrlRelative": "/posts/NcY2K27z6DYoLpdYa/computer-bugs-and-evolution", "linkUrl": "https://www.lesswrong.com/posts/NcY2K27z6DYoLpdYa/computer-bugs-and-evolution", "postedAtFormatted": "Monday, October 26th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Computer%20bugs%20and%20evolution&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComputer%20bugs%20and%20evolution%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcY2K27z6DYoLpdYa%2Fcomputer-bugs-and-evolution%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Computer%20bugs%20and%20evolution%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcY2K27z6DYoLpdYa%2Fcomputer-bugs-and-evolution", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcY2K27z6DYoLpdYa%2Fcomputer-bugs-and-evolution", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p>Last Friday, I finally packaged up the quarterly release of JCVI's automatic prokaryote functional annotation pipeline and distributed it to the other 3 sequencing centers for the Human Microbiome Project.&nbsp; It looks at genes found in newly-sequenced bacterial genomes and guesses what they do.&nbsp; As often happens when I release a new version, shortly afterwards, I discovered a major bug that had been hiding in the code for years.<a id=\"more\"></a></p>\n<p>The program takes each new gene and runs BLAST against a database of known genes, and produces a list of identifiers of genes resembling the new genes.&nbsp; It then takes these identifiers, and calls a program to look in a database for all of the synonyms for these identifiers used in different gene databases.&nbsp; This lookup step takes 90% of the program's runtime.</p>\n<p>I found that the database lookup usually failed, because most identifiers didn't match the regular expression used in the lookup program to retrieve identifiers.&nbsp; Nobody had noticed this, because nobody had checked the database log files.&nbsp; I fixed the program so that the database lookup would always work correctly, and re-ran the program.&nbsp; It produced exactly the same output as before, but took five times as long to run.</p>\n<p>So instead of going dancing as I'd planned, I spent Friday evening figuring out why this happened.&nbsp; It turned out that the class of identifiers that failed to match the regular expression were a subset of a set of identifiers for which the lookup didn't have to be done, because some previously-cached data would give the same results.&nbsp; Once I realized this, I was able to speed the program up more, by excluding more such identifiers, and avoiding the overhead of about a million subroutine calls that would eventually fail when the regular expression failed to match.&nbsp; (Lest you think that the regular expression was intentionally written that way to produce that behavior, the regular expression was inside a library that was written by someone else.&nbsp; Also, the previously-cached data would not have given the correct results prior to a change that I made a few months ago.)</p>\n<p>A bug in a program is like a mutation.&nbsp; Bugs in a computer program are almost always bad.&nbsp; But this was a beneficial bug, which had no effect other than to make the program run much faster than it had been designed to.&nbsp; I was delighted to see this proof of the central non-intuitive idea of evolution:&nbsp; A random change can sometimes be beneficial, even to something as complex and brittle as a 10,000-line Perl program.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NcY2K27z6DYoLpdYa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 55, "extendedScore": null, "score": 0.00011366212145081655, "legacy": true, "legacyId": "1770", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 48, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-27T00:28:12.781Z", "modifiedAt": null, "url": null, "title": "Doing Your Good Deed For The Day", "slug": "doing-your-good-deed-for-the-day-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/reG3g4wwzwJcKnFfh/doing-your-good-deed-for-the-day-0", "pageUrlRelative": "/posts/reG3g4wwzwJcKnFfh/doing-your-good-deed-for-the-day-0", "linkUrl": "https://www.lesswrong.com/posts/reG3g4wwzwJcKnFfh/doing-your-good-deed-for-the-day-0", "postedAtFormatted": "Tuesday, October 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20Your%20Good%20Deed%20For%20The%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20Your%20Good%20Deed%20For%20The%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreG3g4wwzwJcKnFfh%2Fdoing-your-good-deed-for-the-day-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20Your%20Good%20Deed%20For%20The%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreG3g4wwzwJcKnFfh%2Fdoing-your-good-deed-for-the-day-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FreG3g4wwzwJcKnFfh%2Fdoing-your-good-deed-for-the-day-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 913, "htmlBody": "<p>A few months ago, researchers at the University of Toronto published a <a href=\"http://blog.newsweek.com/blogs/nurtureshock/archive/2009/10/12/is-good-behavior-a-license-to-misbehave.aspx\">very interesting</a> <a href=\"http://www.google.ie/url?sa=t&amp;source=web&amp;ct=res&amp;cd=6&amp;ved=0CBoQFjAF&amp;url=http%3A%2F%2Fwww.rotman.utoronto.ca%2Fnewthinking%2Fgreenproducts.pdf&amp;ei=BTnmSoSLJaPajQe6yNShBA&amp;usg=AFQjCNEmQvSQvbVgEJOzxl4oZw7Z8zaq4w&amp;sig2=y2Z6-kG-5cwBJY_zXHHLiA\">study</a> on moral behavior. The one sentence summary of the most interesting part is that people who did one good deed were less likely to do another good deed in the near future. They had, quite literally, done their good deed for the day.</p>\n<p>In the first part of the study, they showed that people exposed to environmentally friendly, \"green\" products were more likely to behave nicely. Subjects were asked to rate products in an online store; unbeknownst to them, half were in a condition where the products were environmentally friendly, and the other half in a condition where the products were not. Then they played a <a href=\"http://en.wikipedia.org/wiki/Dictator_Game\">Dictator Game</a>. Subjects who had seen environmentally friendly products shared more of their money.<br /><br />In the second part, instead of just rating the products, they were told to select $25 worth of products to buy from the store. One in twenty five subjects would actually receive the products they'd purchased. Then they, too, played the Dictator Game. Subjects who had bought environmentally friendly products shared less of their money.<br /><br />In the third part, subjects bought products as before. Then, they participated in a \"separate, completely unrelated\" experiment \"on perception\" in which they earned money by identifying dot patterns. The experiment was designed such that participants could lie about their perceptions to earn more. People who purchased the green products were more likely to do so.<br /><br />This does not prove that environmentalists are actually bad people - remember that whether a subject purchased green products or normal products was completely randomized. It does suggest that people who have done one nice thing feel less of an obligation to do another.</p>\n<p><a id=\"more\"></a></p>\n<p>This meshes nicely with a self-signalling conception of morality. If part of the point of behaving morally is to convince yourself that you're a good person, then once you're convinced, behaving morally loses a lot of its value.<br /><br />By coincidence, a few days after reading this study, I found <a href=\"http://experimentaltheology.blogspot.com/2009/08/bait-and-switch-of-contemporary.html\">this article</a> by Dr. Beck, a theologian, complaining about the behavior of churchgoers on Sunday afternoon lunches. He says that in his circles, it's well known that people having lunch after church tend to abuse the waitstaff and tip poorly. And he blames the same mechanism identified by Mazar and Zhong in their Dictator Game. He says that, having proven to their own satisfaction that they are godly and holy people, doing something <em>else </em>godly and holy like being nice to others would be overkill.<br /><br />It sounds...strangely plausible.<br /><br />If this is true, then anything that makes people feel moral without actually doing good is no longer a harmless distraction. All those biases that lead people to give time and money and thought to causes that don't really merit them waste not only time and money, but an exhaustible supply of moral fiber (compare to Baumeister's idea of <a href=\"http://www.psychologytoday.com/files/attachments/584/baumeisteretal1998.pdf\">willpower as a limited resource</a>).<br /><br />People here probably don't have to worry about church. But some of the other activities Dr. Beck mentions as morality sinkholes seem appropriate, with a few of the words changed:</p>\n<blockquote>\n<p>Bible study<br />Voting Republican<br />Going on spiritual retreats<br />Reading religious books<br />Arguing with evolutionists<br />Sending your child to a Christian school or providing education at home<br />Using religious language<br />Avoiding R-rated movies<br />Not reading Harry Potter.</p>\n</blockquote>\n<p>I've seen <a href=\"http://michaelkeenan.blogspot.com/2009/02/dont-vote.html\">Michael Keenan</a> and Patri Friedman make exactly the same point regarding voting, and I would add to the less religion-o-centric list:</p>\n<blockquote>\n<p>Joining \"<a href=\"http://gl-es.facebook.com/group.php?gid=6812867316\">1000000 STRONG AGAINST WORLD HUNGER</a>\" type Facebook groups<br />Reading a book about the struggles faced by poor people, and telling people how emotional it made you<br />\"Raising awareness of problems\" without raising awareness of any practical solution<br />Taking (or teaching) college courses about the struggles of the less fortunate<br /><a href=\"/lw/181/solutions_to_political_problems_as_counterfactuals/14mh\">Many</a> forms of political, religious, and philosophical arguments</p>\n</blockquote>\n<p>My preferred solution to this problem is to consciously try not to count anything I do as charitable or morally relevant except actually donating money to organizations. It is a bit extreme, but, like Eliezer's utilitarian foundation for deontological ethics, sometimes to escape the problems inherent in <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">running on corrupted hardware</a> you have to jettison all the bathwater, even knowing it contains a certain number of babies. A lot probably slips by subconsciously, but I find it better than nothing. Your mileage may vary.</p>\n<p>It may be tempting to go from here to a society where we talk much less about morality, especially little bits of morality that have no importance on their own. That might have unintended consequences. Remember that the participants in the study who saw lots of environmentally friendly products but couldn't buy any ended up nicer. The urge to be moral seems to build up by anything priming us with thoughts of morality. <br /><br />But to prevent that urge from being discharged, we need to plug up the moral sinkholes Dr. Beck mentions, and any other moral sinkholes we can find. We need to give people less moral recognition and acclaim for performing only slightly moral acts. Only then can we concentrate our limited moral fiber on truly improving the world.<br /><br />And by, \"we\", I mean \"you\". I've done my part just by writing this essay.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "reG3g4wwzwJcKnFfh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "1771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9ZaZXDnL3SEmYZqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-27T00:36:20.553Z", "modifiedAt": null, "url": null, "title": "Doing Your Good Deed For The Day", "slug": "doing-your-good-deed-for-the-day-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MxyRNd6qJsYAcXKuw/doing-your-good-deed-for-the-day-1", "pageUrlRelative": "/posts/MxyRNd6qJsYAcXKuw/doing-your-good-deed-for-the-day-1", "linkUrl": "https://www.lesswrong.com/posts/MxyRNd6qJsYAcXKuw/doing-your-good-deed-for-the-day-1", "postedAtFormatted": "Tuesday, October 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20Your%20Good%20Deed%20For%20The%20Day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20Your%20Good%20Deed%20For%20The%20Day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxyRNd6qJsYAcXKuw%2Fdoing-your-good-deed-for-the-day-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20Your%20Good%20Deed%20For%20The%20Day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxyRNd6qJsYAcXKuw%2Fdoing-your-good-deed-for-the-day-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMxyRNd6qJsYAcXKuw%2Fdoing-your-good-deed-for-the-day-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 929, "htmlBody": "<p>A few months ago, researchers at the University of Toronto published a <a href=\"http://blog.newsweek.com/blogs/nurtureshock/archive/2009/10/12/is-good-behavior-a-license-to-misbehave.aspx\">very interesting</a> <a href=\"http://www.google.ie/url?sa=t&amp;source=web&amp;ct=res&amp;cd=6&amp;ved=0CBoQFjAF&amp;url=http%3A%2F%2Fwww.rotman.utoronto.ca%2Fnewthinking%2Fgreenproducts.pdf&amp;ei=BTnmSoSLJaPajQe6yNShBA&amp;usg=AFQjCNEmQvSQvbVgEJOzxl4oZw7Z8zaq4w&amp;sig2=y2Z6-kG-5cwBJY_zXHHLiA\">study</a> on moral behavior. The one sentence summary of the most interesting part is that people who did one good deed were less likely to do another good deed in the near future. They had, quite literally, done their good deed for the day.</p>\n<p>In the first part of the study, they showed that people exposed to environmentally friendly, \"green\" products were more likely to behave nicely. Subjects were asked to rate products in an online store; unbeknownst to them, half were in a condition where the products were environmentally friendly, and the other half in a condition where the products were not. Then they played a <a href=\"http://en.wikipedia.org/wiki/Dictator_Game\">Dictator Game</a>. Subjects who had seen environmentally friendly products shared more of their money.<br /><br />In the second part, instead of just rating the products, they were told to select $25 worth of products to buy from the store. One in twenty five subjects would actually receive the products they'd purchased. Then they, too, played the Dictator Game. Subjects who had bought environmentally friendly products shared less of their money.<br /><br />In the third part, subjects bought products as before. Then, they participated in a \"separate, completely unrelated\" experiment \"on perception\" in which they earned money by identifying dot patterns. The experiment was designed such that participants could lie about their perceptions to earn more. People who purchased the green products were more likely to do so.<br /><br />This does not prove that environmentalists are actually bad people - remember that whether a subject purchased green products or normal products was completely randomized. It does suggest that people who have done one nice thing feel less of an obligation to do another.</p>\n<p><a id=\"more\"></a></p>\n<p>This meshes nicely with a self-signalling conception of morality. If part of the point of behaving morally is to convince yourself that you're a good person, then once you're convinced, behaving morally loses a lot of its value.<br /><br />By coincidence, a few days after reading this study, I found <a href=\"http://experimentaltheology.blogspot.com/2009/08/bait-and-switch-of-contemporary.html\">this article</a> by Dr. Beck, a theologian, complaining about the behavior of churchgoers on Sunday afternoon lunches. He says that in his circles, it's well known that people having lunch after church tend to abuse the waitstaff and tip poorly. And he blames the same mechanism identified by Mazar and Zhong in their Dictator Game. He says that, having proven to their own satisfaction that they are godly and holy people, doing something <em>else </em>godly and holy like being nice to others would be overkill.<br /><br />It sounds...strangely plausible.<br /><br />If this is true, then anything that makes people feel moral without actually doing good is no longer a harmless distraction. All those biases that lead people to give time and money and thought to causes that don't really merit them waste not only time and money, but an exhaustible supply of moral fiber (compare to Baumeister's idea of <a href=\"http://www.psychologytoday.com/files/attachments/584/baumeisteretal1998.pdf\">willpower as a limited resource</a>).<br /><br />People here probably don't have to worry about church. But some of the other activities Dr. Beck mentions as morality sinkholes seem appropriate, with a few of the words changed:</p>\n<blockquote>\n<p>Bible study<br />Voting Republican<br />Going on spiritual retreats<br />Reading religious books<br />Arguing with evolutionists<br />Sending your child to a Christian school or providing education at home<br />Using religious language<br />Avoiding R-rated movies<br />Not reading Harry Potter.</p>\n</blockquote>\n<p>I've seen <a href=\"http://michaelkeenan.blogspot.com/2009/02/dont-vote.html\">Michael Keenan</a> and Patri Friedman make exactly the same point regarding voting, and I would add to the less religion-o-centric list:</p>\n<blockquote>\n<p>Joining \"<a href=\"http://gl-es.facebook.com/group.php?gid=6812867316\">1000000 STRONG AGAINST WORLD HUNGER</a>\" type Facebook groups<br />Reading a book about the struggles faced by poor people, and telling people how emotional it made you<br />\"Raising awareness of problems\" without raising awareness of any practical solution<br />Taking (or teaching) college courses about the struggles of the less fortunate<br /><a href=\"/lw/181/solutions_to_political_problems_as_counterfactuals/14mh\">Many</a> forms of political, religious, and philosophical arguments</p>\n</blockquote>\n<p>My preferred solution to this problem is to consciously try not to count anything I do as charitable or morally relevant except actually donating money to organizations. It is a bit extreme, but, like Eliezer's utilitarian foundation for deontological ethics, sometimes to escape the problems inherent in <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">running on corrupted hardware</a> you have to jettison all the bathwater, even knowing it contains a certain number of babies. A lot probably slips by subconsciously, but I find it better than nothing (at least, I did when I was actually making money; I've had to suspend it recently). Your mileage may vary.</p>\n<p>It may be tempting to go from here to a society where we talk much less about morality, especially little bits of morality that have no importance on their own. That might have unintended consequences. Remember that the participants in the study who saw lots of environmentally friendly products but couldn't buy any ended up nicer. The urge to be moral seems to build up by anything priming us with thoughts of morality. <br /><br />But to prevent that urge from being discharged, we need to plug up the moral sinkholes Dr. Beck mentions, and any other moral sinkholes we can find. We need to give people less moral recognition and acclaim for performing only slightly moral acts. Only then can we concentrate our limited moral fiber on truly improving the world.<br /><br />And by, \"we\", I mean \"you\". I've done my part just by writing this essay.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MxyRNd6qJsYAcXKuw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "1772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9ZaZXDnL3SEmYZqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-27T00:45:24.144Z", "modifiedAt": null, "url": null, "title": "Doing your good deed for the day", "slug": "doing-your-good-deed-for-the-day", "viewCount": null, "lastCommentedAt": "2019-08-19T21:17:35.836Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r8stxYL29NF9w53am/doing-your-good-deed-for-the-day", "pageUrlRelative": "/posts/r8stxYL29NF9w53am/doing-your-good-deed-for-the-day", "linkUrl": "https://www.lesswrong.com/posts/r8stxYL29NF9w53am/doing-your-good-deed-for-the-day", "postedAtFormatted": "Tuesday, October 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Doing%20your%20good%20deed%20for%20the%20day&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoing%20your%20good%20deed%20for%20the%20day%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8stxYL29NF9w53am%2Fdoing-your-good-deed-for-the-day%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Doing%20your%20good%20deed%20for%20the%20day%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8stxYL29NF9w53am%2Fdoing-your-good-deed-for-the-day", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr8stxYL29NF9w53am%2Fdoing-your-good-deed-for-the-day", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 975, "htmlBody": "<p><a href=\"http://blog.newsweek.com/blogs/nurtureshock/archive/2009/10/12/is-good-behavior-a-license-to-misbehave.aspx\">Interesting</a> new <a href=\"http://www.google.ie/url?sa=t&amp;source=web&amp;ct=res&amp;cd=6&amp;ved=0CBoQFjAF&amp;url=http%3A%2F%2Fwww.rotman.utoronto.ca%2Fnewthinking%2Fgreenproducts.pdf&amp;ei=WkHmSqbdPMOMjAfGmtyhBA&amp;usg=AFQjCNEmQvSQvbVgEJOzxl4oZw7Z8zaq4w&amp;sig2=iOMp6AtL_6QF0IzQsvf67g\">study</a> out on moral behavior. The one sentence summary of the most interesting part is that people who did one good deed were less likely to do another good deed in the near future. They had, quite literally, done their good deed for the day.<br /><br />In the first part of the study, they showed that people exposed to environmentally friendly, \"green\" products were more likely to behave nicely. Subjects were asked to rate products in an online store; unbeknownst to them, half were in a condition where the products were environmentally friendly, and the other half in a condition where the products were not. Then they played a Dictator Game. Subjects who had seen environmentally friendly products shared more of their money.<br /><br />In the second part, instead of just rating the products, they were told to select $25 worth of products to buy from the store. One in twenty five subjects would actually receive the products they'd purchased. Then they, too, played the <a href=\"http://en.wikipedia.org/wiki/Dictator_Game\">Dictator Game</a>. Subjects who had bought environmentally friendly products shared less of their money.<br /><br />In the third part, subjects bought products as before. Then, they participated in a \"separate, completely unrelated\" experiment \"on perception\" in which they earned money by identifying dot patterns. The experiment was designed such that participants could lie about their perceptions to earn more. People who purchased the green products were more likely to do so.<br /><br />This does not prove that environmentalists are actually bad people - remember that whether a subject purchased green products or normal products was completely randomized. It does suggest that people who have done one nice thing feel less of an obligation to do another.<br /><br />This meshes nicely with a self-signalling conception of morality. If part of the point of behaving morally is to convince yourself that you're a good person, then once you're convinced, behaving morally loses a lot of its value.<a id=\"more\"></a><br /><br />By coincidence, a few days after reading this study, I found <a href=\"http://experimentaltheology.blogspot.com/2009/08/bait-and-switch-of-contemporary.html\">this article</a> by Dr. Beck, a theologian, complaining about the behavior of churchgoers on Sunday afternoon lunches. He says that in his circles, it's well known that people having lunch after church tend to abuse the waitstaff and tip poorly. And he blames the same mechanism identified by Mazar and Zhong in their Dictator Game. He says that, having proven to their own satisfaction that they are godly and holy people, doing something <em>else</em> godly and holy like being nice to others would be overkill.<br /><br />It sounds...strangely plausible.<br /><br />If this is true, then anything that makes people feel moral without actually doing good is no longer a harmless distraction. All those biases that lead people to give time and money and thought to causes that don't really merit them waste not only time and money, but an exhaustible supply of moral fiber (compare to Baumeister's idea of <a href=\"http://www.psychologytoday.com/files/attachments/584/baumeisteretal1998.pdf\">willpower as a limited resource</a>).<br /><br />People here probably don't have to worry about church. But some of the other activities Dr. Beck mentions as morality sinkholes seem appropriate, with a few of the words changed:</p>\n<blockquote>\n<p>Bible study<br />Voting Republican<br />Going on spiritual retreats<br />Reading religious books<br />Arguing with evolutionists<br />Sending your child to a Christian school or providing education at home<br />Using religious language<br />Avoiding R-rated movies<br />Not reading Harry Potter.</p>\n</blockquote>\n<p><br />Let's not get too carried away with the evils of spiritual behavior - after all, data do show that religious people still give more to non-religious charities than the nonreligious do. But the points in and of themselves are valid. I've seen <a href=\"http://michaelkeenan.blogspot.com/2009/02/dont-vote.html\">Michael Keenan</a> and Patri Friedman say exactly the same thing regarding voting, and I would add to the less religion-o-centric list:</p>\n<blockquote>\n<p>Joining \"<a href=\"http://www.google.ie/url?sa=t&amp;source=web&amp;ct=res&amp;cd=4&amp;ved=0CBkQFjAD&amp;url=http%3A%2F%2Fgl-es.facebook.com%2Fgroup.php%3Fgid%3D6812867316&amp;ei=MELmSqrvEqK5jAeap9ShBA&amp;usg=AFQjCNHZx3sQ9MBnDQWe0_MDY7ckffPrgQ&amp;sig2=X6txcBNPuCaU0AjIPWskew\">1000000 STRONG AGAINST WORLD HUNGER</a>\" type Facebook groups<br />Reading a book about the struggles faced by poor people, and telling people how emotional it made you<br />\"Raising awareness of problems\" without raising awareness of any practical solution<br />Taking (or teaching) college courses about the struggles of the less fortunate<br />Many forms of political, religious, and philosophical arguments</p>\n</blockquote>\n<p>My preferred solution to this problem is to consciously try not to count anything I do as charitable or morally relevant except actually donating money to organizations. It is a bit extreme, but, like Eliezer's utilitarian foundation for deontological ethics, sometimes to escape the problems inherent in <a href=\"http://www.google.ie/url?sa=t&amp;source=web&amp;ct=res&amp;cd=2&amp;ved=0CBcQFjAB&amp;url=http%3A%2F%2Flesswrong.com%2Flw%2Fuv%2Fends_dont_justify_means_among_humans%2F&amp;ei=T0LmSsqnB6PbjQeBusShBA&amp;usg=AFQjCNHMc-TZlS8SyK_xwyYRuI6GsaPwaw&amp;sig2=HMuT4NtYXYWoRuqn0AbXfw\">running on corrupted hardware</a> you have to jettison all the bathwater, even knowing it contains a certain number of babies. A lot probably slips by subconsciously, but I find it better than nothing (at least, I did when I was actually making money; it hasn't worked since I went back to school. Your mileage may vary.<br /><br />It may be tempting to go from here to a society where we talk much less about morality, especially little bits of morality that have no importance on their own. That might have unintended consequences. Remember that the participants in the study who saw lots of environmentally friendly products but couldn't buy any ended up nicer. The urge to be moral seems to build up by anything priming us with thoughts of morality.<br /><br />But to prevent that urge from being discharged, we need to plug up the moral sinkholes Dr. Beck mentions, and any other moral sinkholes we can find. We need to give people less moral recognition and acclaim for performing only slightly moral acts. Only then can we concentrate our limited moral fiber on truly improving the world.<br /><br />And by, \"we\", I mean \"you\". I've done my part just by writing this essay.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ksdiAMKfgSyEeKMo6": 1, "ZzxvopS4BwLuQy42n": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r8stxYL29NF9w53am", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 141, "baseScore": 152, "extendedScore": null, "score": 0.000247, "legacy": true, "legacyId": "1773", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 153, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-28T14:40:08.464Z", "modifiedAt": null, "url": null, "title": "Expected utility without the independence axiom", "slug": "expected-utility-without-the-independence-axiom", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:42.325Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tGhz4aKyNzXjvnWhX/expected-utility-without-the-independence-axiom", "pageUrlRelative": "/posts/tGhz4aKyNzXjvnWhX/expected-utility-without-the-independence-axiom", "linkUrl": "https://www.lesswrong.com/posts/tGhz4aKyNzXjvnWhX/expected-utility-without-the-independence-axiom", "postedAtFormatted": "Wednesday, October 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Expected%20utility%20without%20the%20independence%20axiom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExpected%20utility%20without%20the%20independence%20axiom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGhz4aKyNzXjvnWhX%2Fexpected-utility-without-the-independence-axiom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Expected%20utility%20without%20the%20independence%20axiom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGhz4aKyNzXjvnWhX%2Fexpected-utility-without-the-independence-axiom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtGhz4aKyNzXjvnWhX%2Fexpected-utility-without-the-independence-axiom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1180, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/John_von_Neumann\">John von Neumann</a> and <a href=\"http://en.wikipedia.org/wiki/Oskar_Morgenstern\">Oskar Morgenstern</a> developed a system of four axioms that they claimed any rational decision maker must follow. The major consequence of these axioms is that when faced with a decision, you should always act solely to increase your expected utility. All four axioms have been attacked at various times and from various directions; but three of them are very solid. The fourth - independence - is the most controversial.</p>\n<p>To understand the axioms, let A, B and C be lotteries - processes that result in different outcomes, positive or negative, with a certain probability of each. For 0&lt;p&lt;1, the mixed lottery pA + (1-p)B implies that you have p chances of being in lottery A, and (1-p) chances of being in lottery B. Then writing A&gt;B means that you prefer lottery A to lottery B, A&lt;B is the reverse and A=B means that you are indifferent between the two. Then the von Neumann-Morgenstern <a href=\"http://en.wikipedia.org/wiki/Expected_utility\">axioms</a> are:</p>\n<ul type=\"disc\">\n<li class=\"MsoNormal\">(Completeness) For every A and B either A&lt;B, A&gt;B or A=B.</li>\n<li class=\"MsoNormal\">(Transitivity) For every A, B and C with A&gt;B and B&gt;C, then A&gt;C.</li>\n<li class=\"MsoNormal\">(Continuity) For every A&gt;B&gt;C then there exist a probability p with B=pA + (1-p)C.</li>\n<li class=\"MsoNormal\">(Independence) For every A, B and C with A&gt;B, and for every 0&lt;t&le;1, then tA + (1-t)C &gt; tB + (1-t)C.</li>\n</ul>\n<p>In this post, I'll try and prove that even without the Independence axiom, you should continue to use expected utility in most situations. This requires some mild extra conditions, of course. The problem is that although these conditions are considerably weaker than Independence, they are harder to phrase. So please bear with me here.</p>\n<p>The whole insight in this post rests on the fact that a lottery that has 99.999% chance of giving you &pound;1 is very close to being a lottery that gives you &pound;1 with certainty. I want to express this fact by looking at the narrowness of the probability distribution, using the standard deviation. However, this narrowness is not an intrinsic property of the distribution, but of our utility function. Even in the example above, if I decide that receiving &pound;1 gives me a utility of one, while receiving zero gives me a utility of minus ten billion, then I no longer have a narrow distribution, but a wide one. So, unlike the traditional set-up, we have to assume a utility function as being given. Once this is chosen, this allows us to talk about the mean and standard deviation of a lottery.</p>\n<p>Then if you define c(&mu;) as the lottery giving you a certain return of &mu;, you can use the following axiom instead of independence:</p>\n<ul type=\"disc\">\n<li class=\"MsoNormal\">(Standard deviation bound) For all &epsilon;&gt;0, there exists a &delta;&gt;0 such that for all &mu;&gt;0, then any lottery B with mean &mu; and standard deviation less that &mu;&delta; has B&gt;c((1-&epsilon;)&mu;).</li>\n</ul>\n<p>This seems complicated, but all that it says, in mathematical terms, is that if we have a probability distribution that is \"narrow enough\" around its mean &mu;, then we should value it are being very close to a certain return of &mu;. The narrowness is expressed in terms of its standard deviation - a lottery with zero SD is a guaranteed return of &mu;, and as the SD gets larger, the distribution gets wider, and the chances of getting values far away from &mu; increases. So risk, in other words, scales (approximately) with the SD.</p>\n<p><a id=\"more\"></a>We also need to make sure that we are not risk loving - if we are inveterate gamblers for the point of being gamblers, our behaviour may be a lot more complicated.</p>\n<ul type=\"disc\">\n<li class=\"MsoNormal\">(Not risk loving) If A has mean &mu;&gt;0, then A&le;c(&mu;).</li>\n</ul>\n<p>I.e. we don't love a worse rate of return just because of the risk. This axiom can and maybe should be weakened, but it's a good approximation for the moment - most people are not risk loving with huge risks.</p>\n<p><em>Assume you are going to be have to choose n different times whether to accept independent lotteries with fixed mean &beta;&gt;0, and all with SD less than a fixed upper-bound K. Then if you are not risk loving and n is large enough, you must accept an arbitrarily large proportion of the lotteries.</em></p>\n<p>Proof: From now on, I'll use a different convention for adding and scaling lotteries. Treating them as random variables, A+B will mean the lottery consisting of A and B together, while xA will mean the same lottery as A, but with all returns (positive or negative) scaled by x.</p>\n<p>Let X<sub>1</sub>, X<sub>2</sub>, ... , X<sub>n</sub> be these n independent lotteries, with means &beta; and variances v<sub>j</sub>. The since the standard deviations are less than K, the variances must be less than K<sup>2</sup>.</p>\n<p>Let Y = X<sub>1</sub> + X<sub>2</sub> + ... + X<sub>n</sub>. The mean of Y is n&beta;. The variance of Y is the sum of the v<sub>j</sub>, which is less than nK<sup>2</sup>. Hence the SD of Y is less than K&radic;(n). Now pick an &epsilon;&gt;0, and the resulting &delta;&gt;0 from the standard deviation bound axiom. For large enough n, n&beta;&delta; must be larger than K&radic;(n); hence, for large enough n, Y &gt; c((1-&epsilon;)n&beta;). Now, if we were to refuse more that &epsilon;n of the lotteries, we would be left with a distribution with mean &le; (1-&epsilon;)n&beta;, which, since we are not risk loving, is worse than c((1-&epsilon;)n&beta;), which is worse than Y. Hence we must accept more than a proportion (1-&epsilon;) of the lotteries on offer. <strong>&diams;</strong></p>\n<p>This only applies to lotteries that share the same mean, but we can generalise the result as:</p>\n<p><em>Assume you are going to be have to choose n different times whether to accept independent lotteries all with means greater than a fixed &beta;&gt;0, and all with SD less than a fixed upper-bound K. Then if you are not risk loving and n is large enough, you must accept lotteries whose means represent an arbitrarily large proportion of the total mean of all lotteries on offer.</em></p>\n<p>Proof: The same proof works as before, with n&beta; now being a lower bound on the true mean &mu; of Y. Thus we get Y &gt; c((1-&epsilon;)&mu;), and we must accept lotteries whose total mean is greater than (1-&epsilon;)&mu;. <strong>&diams;</strong></p>\n<p>&nbsp;</p>\n<p><strong>Analysis:</strong> Since we rejected independence, we must now consider the lotteries when taken as a whole, rather than just seeing them individually. When considered as a whole, \"reasonable\" lotteries are more tightly bunched around their total mean than they are individually. Hence the more lotteries we consider, the more we should treat them as if only their mean mattered. So if we are not risk loving, and expect to meet many lotteries with bounded SD in our lives, we should follow expected utility. Deprived of independence, expected utility sneaks in via aggregation.</p>\n<p><strong>Note:</strong> This restates the first half of my previous <a href=\"/lw/1cv/extreme_risks_when_not_to_use_expected_utility/\">post</a> - a post so confusingly written it should be staked through the heart and left to die on a crossroad at noon.</p>\n<p><strong>Edit:</strong> Rewrote a part to emphasis the fact that a utility function needs to be chosen in advance - thanks to <a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/17p9\">Peter de Blanc</a> and <a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/17p8\">Nick Hay</a> for bringing this up.</p>\n<p class=\"MsoNormal\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "Zs4nYLkNr7Rbo4mAP": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tGhz4aKyNzXjvnWhX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 20, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "1769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kmjCaq66MDkfvZpFX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-29T12:06:04.105Z", "modifiedAt": null, "url": null, "title": "Post retracted: If you follow expected utility, expect to be money-pumped", "slug": "post-retracted-if-you-follow-expected-utility-expect-to-be", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:42.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TRcfweoArnnx8zCX5/post-retracted-if-you-follow-expected-utility-expect-to-be", "pageUrlRelative": "/posts/TRcfweoArnnx8zCX5/post-retracted-if-you-follow-expected-utility-expect-to-be", "linkUrl": "https://www.lesswrong.com/posts/TRcfweoArnnx8zCX5/post-retracted-if-you-follow-expected-utility-expect-to-be", "postedAtFormatted": "Thursday, October 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Post%20retracted%3A%20If%20you%20follow%20expected%20utility%2C%20expect%20to%20be%20money-pumped&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APost%20retracted%3A%20If%20you%20follow%20expected%20utility%2C%20expect%20to%20be%20money-pumped%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRcfweoArnnx8zCX5%2Fpost-retracted-if-you-follow-expected-utility-expect-to-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Post%20retracted%3A%20If%20you%20follow%20expected%20utility%2C%20expect%20to%20be%20money-pumped%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRcfweoArnnx8zCX5%2Fpost-retracted-if-you-follow-expected-utility-expect-to-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTRcfweoArnnx8zCX5%2Fpost-retracted-if-you-follow-expected-utility-expect-to-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>This post has been retracted because it is in error. Trying to shore it up just involved a variant of the St Petersburg Paradox and a small point on pricing contracts that is not enough to make a proper blog post.</p>\n<p>I apologise.</p>\n<p><strong>Edit:</strong> Some people have asked that I keep the original up to illustrate the confusion I was under. I unfortunately don't have a copy, but I'll try and recreate the idea, and illustrate where I went wrong.</p>\n<p>The original idea was that if I were to offer you a contract L that gained &pound;1 with 50% probability or &pound;2 with 50% probability, then if your utility function wasn't linear in money, you would generally value L at having a value other that &pound;1.50. Then I could sell or buy large amounts of these contracts from you at your stated price, and use the law of large number to ensure that I valued each contract at &pound;1.50, thus making a certain profit.</p>\n<p>The first flaw consisted in the case where your utility is concave in cash (\"risk averse\"). In that case, I can't buy L from you unless you already have L. And each time I buy it from you, the mean quantity of cash you have goes down, but your utility goes up, since you do not like the uncertainty inherent in L. So I get richer, but you get more utility, and once you've sold all L's you have, I cannot make anything more out of you.</p>\n<p>If your utility is convex in cash (\"risk loving\"), then I can sell you L forever, at more than &pound;1.50. And your money will generally go down, as I drain it from you. However, though the median amount of cash you have goes down, your utility goes up, since you get a chance - however tiny - of huge amounts of cash, and the utility generated by this sum swamps the fact you are most likely ending up with nothing. If I could go on forever, then I can drain you entirely, as this is a biased random walk on a one-dimensional axis. But I would need infinite ressources to do this.</p>\n<p>The major error was to reason like an investor, rather than a utility maximiser. Investors are very interested in putting prices on objects. And if you assign the wrong price to L while investing, someone will take advantage of you and arbitrage you. I might return to this in a subsequent post; but the issue is that even if your utility is concave or convex in money, you would put a price of &pound;1.50 on L <strong>if</strong> L were an easily traded commodity with a lot of investors also pricing it at &pound;1.50.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TRcfweoArnnx8zCX5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 1, "extendedScore": null, "score": 5.332292225218359e-07, "legacy": true, "legacyId": "1784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-29T15:04:30.286Z", "modifiedAt": null, "url": null, "title": "A Less Wrong Q&A with Eliezer (Step 1: The Proposition)", "slug": "a-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:43.281Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/psuphwRHuJu9T9PPa/a-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "pageUrlRelative": "/posts/psuphwRHuJu9T9PPa/a-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "linkUrl": "https://www.lesswrong.com/posts/psuphwRHuJu9T9PPa/a-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "postedAtFormatted": "Thursday, October 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Less%20Wrong%20Q%26A%20with%20Eliezer%20(Step%201%3A%20The%20Proposition)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Less%20Wrong%20Q%26A%20with%20Eliezer%20(Step%201%3A%20The%20Proposition)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsuphwRHuJu9T9PPa%2Fa-less-wrong-q-and-a-with-eliezer-step-1-the-proposition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Less%20Wrong%20Q%26A%20with%20Eliezer%20(Step%201%3A%20The%20Proposition)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsuphwRHuJu9T9PPa%2Fa-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsuphwRHuJu9T9PPa%2Fa-less-wrong-q-and-a-with-eliezer-step-1-the-proposition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p>I don't know if I'm the only one, but I've always been a bit frustrated by Eliezer's <a href=\"http://bloggingheads.tv/search/?participant1=Yudkowsky,%20Eliezer\">BloggingHeadsTV episodes</a>. I find myself wishing that Eliezer would get more speaking time and could address more directly some of the things we discuss at LW/OB.</p>\n<p>Some of you will surely think: \"If you want more Eliezer, just read his blog posts and papers!\"</p>\n<p>Sensible advice, no doubt, but I think that there's something special (because of how our brains evolved) about actually <em>seeing</em> and <em>hearing</em>&nbsp;a teacher, and I find it very helpful to see how he applies rationality techniques in \"real time\".&nbsp;But I'm not just looking for a dry lecture, I want more Eliezer because I enjoy listening to him (gotta love that <a href=\"http://www.acceleratingfuture.com/people/gallery/eliezer-yudkowsky/specks.jpg\" target=\"_blank\">sense of humor</a>), and I bet many of you do too.</p>\n<p><strong>Here is my suggestion</strong>:</p>\n<p><span style=\"font-family: mceinline;\"><em>If the Less Wrong community thinks it's a good idea and if Eliezer agrees</em></span>, I will create a \"Step 2: Ask Your Questions\" post in which the comments section will be used to gather questions for Eliezer. Each question should be submitted as an individual comment to allow more granularity in the ranking based on the voting system.</p>\n<p>After at least 7 days, to give everybody enough time to submit their questions and vote, Eliezer will sit down in front of a camera&nbsp;and answers however many questions he fells like answering&nbsp;(at a time of his choosing,&nbsp;<a href=\"http://www.zazzle.co.uk/bayes_theorem_2_tshirt-235701011749930737\" target=\"_blank\">t-shirt</a>&nbsp;or hot cocoa in a mug imprinted with&nbsp;<a href=\"http://plato.stanford.edu/entries/bayes-theorem/\">Bayes' theorem</a>&nbsp;are optional), in descending order from most to least&nbsp;votes, skipping the ones he doesn't want to answer. If Eliezer feels uncomfortable speaking alone for an extended period of time, he can get someone to read him the questions so that it feels more like an interview.</p>\n<p>The video can then be uploaded to a hosting service like Youtube or Vimeo, and posted to Less Wrong.</p>\n<p>In short, it would be a kind of Less Wrong podcast, something that many other sites do successfully.</p>\n<p>Yay or nay?</p>\n<p><strong>Update: </strong>Eliezer <a href=\"/lw/1di/a_less_wrong_qa_with_eliezer_step_1_the/17t1\">says</a> \"I'll do it.\"</p>\n<p><strong>Update 2: </strong>The thread where questions were submitted can be found <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/\">here</a>.</p>\n<p><strong>Update 3: </strong>Eliezer's video answers can be found <a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "psuphwRHuJu9T9PPa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 19, "extendedScore": null, "score": 5.332600374131576e-07, "legacy": true, "legacyId": "1782", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iLGrNTwTZivTX5774", "Qyix5Z5YPSGYxf7GG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-30T00:05:48.203Z", "modifiedAt": null, "url": null, "title": "David Deutsch: A new way to explain explanation", "slug": "david-deutsch-a-new-way-to-explain-explanation", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:46.900Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "marc", "createdAt": "2009-03-04T18:06:15.966Z", "isAdmin": false, "displayName": "marc"}, "userId": "W3fo3KxJttGJEi6oK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jzibHBxczkZuTKY93/david-deutsch-a-new-way-to-explain-explanation", "pageUrlRelative": "/posts/jzibHBxczkZuTKY93/david-deutsch-a-new-way-to-explain-explanation", "linkUrl": "https://www.lesswrong.com/posts/jzibHBxczkZuTKY93/david-deutsch-a-new-way-to-explain-explanation", "postedAtFormatted": "Friday, October 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20David%20Deutsch%3A%20A%20new%20way%20to%20explain%20explanation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADavid%20Deutsch%3A%20A%20new%20way%20to%20explain%20explanation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzibHBxczkZuTKY93%2Fdavid-deutsch-a-new-way-to-explain-explanation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=David%20Deutsch%3A%20A%20new%20way%20to%20explain%20explanation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzibHBxczkZuTKY93%2Fdavid-deutsch-a-new-way-to-explain-explanation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzibHBxczkZuTKY93%2Fdavid-deutsch-a-new-way-to-explain-explanation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 237, "htmlBody": "<p>I'm sure this <a href=\"http://www.ted.com/talks/david_deutsch_a_new_way_to_explain_explanation.html\">talk</a> will be of interest, even if most of the ideas that he talks about will be familiar to readers here.</p>\n<p>[edit]</p>\n<p>In this talk David Deutsch discusses \"the most important discovery in human history\"; how humanity moved beyond a few hundred thousand years of complete ignorance about the universe. Deutsch attempts to be specific about what led to this change - he concludes that it is the insistence that an explanation be 'hard to vary'.</p>\n<p>Whilst a 'hard to vary' explanation is functionally the same as a, more commonly known, Occam's Razor explanation (since fewer parameters necessarily make a fit harder to vary) the slightly different emphasis might be a useful pedagogical tool. A 'hard to vary' explanation will perhaps lead more naturally to questions about strong predictions and falsifiability than Occam's razor. It also seems harder to misunderstand. As we know, Occam's razor suffers because of the difference between actual complexity and linguistic complexity, so an explanation like \"it's magic\" can appear to be simple. Magic might appear simple, but it will never appear 'hard to vary', so students of rationality would have one less pitfall awaiting them.</p>\n<p>Deutsch also touches on what constitutes understanding and knowledge and cautions us not to trust predictions that are purely of an extrapolated empirical nature as there is no true understanding contained there.</p>\n<p>[/edit]&nbsp;</p>\n<p>If you haven't already read Deutsch's book \"<a href=\"http://www.amazon.co.uk/Fabric-Reality-Towards-Theory-Everything/dp/0140146903/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1256655204&amp;sr=8-1\">The Fabric of Reality</a>\" I'd highly recommend that as well.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jzibHBxczkZuTKY93", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 5.333535357510477e-07, "legacy": true, "legacyId": "1777", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-10-30T04:47:06.192Z", "modifiedAt": null, "url": null, "title": "Less Wrong / Overcoming Bias meet-up groups", "slug": "less-wrong-overcoming-bias-meet-up-groups", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:22.126Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QrYkJbE3Qtrg3FDbj/less-wrong-overcoming-bias-meet-up-groups", "pageUrlRelative": "/posts/QrYkJbE3Qtrg3FDbj/less-wrong-overcoming-bias-meet-up-groups", "linkUrl": "https://www.lesswrong.com/posts/QrYkJbE3Qtrg3FDbj/less-wrong-overcoming-bias-meet-up-groups", "postedAtFormatted": "Friday, October 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20%2F%20Overcoming%20Bias%20meet-up%20groups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20%2F%20Overcoming%20Bias%20meet-up%20groups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrYkJbE3Qtrg3FDbj%2Fless-wrong-overcoming-bias-meet-up-groups%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20%2F%20Overcoming%20Bias%20meet-up%20groups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrYkJbE3Qtrg3FDbj%2Fless-wrong-overcoming-bias-meet-up-groups", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQrYkJbE3Qtrg3FDbj%2Fless-wrong-overcoming-bias-meet-up-groups", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>Hi from Michael Vassar, the president of SIAI. With my wife and Singularity Summit co-director Aruna, I'll be traveling over the next few months to meet with rationality and singularity enthusiasts throughout the U.S.&nbsp; Specifically, I'll be in Boston from November 14-18, Philadelphia from December 1st to 10th and December 15th through January 4th, New Orleans from December 11th to 14th, Orlando on January 5th and 6th, Sarasota on January 7th through 12th, and in Tampa on the 11th if there is substantial interest in a meet-up there.&nbsp;</p>\n<p>Please comment if you are interested in attending a meet-up in any of the cities in question and we can start planning.</p>\n<p>I hope to find that there are thriving communities of rationalists in each of those cities already, but I'm traveling there to try to seed their precipitation from the local populace.&nbsp; If things go really well the groups and their respective cities will be on SIAI radar and who knows, maybe eventually there will be a Singularity Summit near Tomorrow-Land, a global catastrophic risk conference by the New Orleans levies, or a FAI extrapolation dynamics exploratory workshop near Independence Hall.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QrYkJbE3Qtrg3FDbj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 5.334021360222852e-07, "legacy": true, "legacyId": "1786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-02T00:44:46.686Z", "modifiedAt": null, "url": null, "title": "Our House, My Rules", "slug": "our-house-my-rules", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.026Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mhm2MNEuG2FcrMTH6/our-house-my-rules", "pageUrlRelative": "/posts/Mhm2MNEuG2FcrMTH6/our-house-my-rules", "linkUrl": "https://www.lesswrong.com/posts/Mhm2MNEuG2FcrMTH6/our-house-my-rules", "postedAtFormatted": "Monday, November 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Our%20House%2C%20My%20Rules&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOur%20House%2C%20My%20Rules%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhm2MNEuG2FcrMTH6%2Four-house-my-rules%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Our%20House%2C%20My%20Rules%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhm2MNEuG2FcrMTH6%2Four-house-my-rules", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMhm2MNEuG2FcrMTH6%2Four-house-my-rules", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 615, "htmlBody": "<p>People debate all the time about how strictly children should be disciplined. Obviously, this is a worthwhile debate to have, as there must be some optimal amount of discipline that is greater than zero. The debate's nominal focus is usually on what's best for the child, with even the advocates for greater strictness arguing that it's \"for their own good.\" It might also touch on what's good for other family members or for society at large. What I think is missing from the usual debate is that it assumes nothing but honorable motives on the part of the arguers. That is, it assumes that the arguments in favor of greater strictness are completely untainted by any element of authoritarianism or cruelty. But people are sometimes authoritarian and cruel! Just for fun! And the only people who you can be consistently cruel to without them slugging you, shunning you, suing you, or calling the police on you are your children. This is a reason for more than the usual amount of skepticism of arguments that say that strict parenting is necessary. If there were no such thing as cruelty in the world, people would still argue about the optimal level of strictness, and sometimes the more strict position would be the correct one, and parents would chose the optimal level of strictness on the basis of these arguments. But what we actually have is a world with lots and lots of cruelty lurking just under the surface, which cannot help but show up in the form of pro-strictness arguments in parenting debates. This should cause us to place less weight on pro-strictness arguments than we otherwise would.*&nbsp; Note that this is basically the same idea as Bertrand Russell's argument against the idea of <a title=\"The Very Worst Kind of Bias\" href=\"http://www.overcomingbias.com/2007/03/the_very_worst_.html\" target=\"_blank\">sin</a>: its true function is to allow people to exercise their natural cruelty while at the same time maintaining their opinion of themselves as moral.<a id=\"more\"></a><br /><br />One example of authoritarianism masquerading as sound discipline (even among otherwise good parents) is the idea of \"My House, My Rules.\" I've even heard parents go so far as to say things like: \"it's not <em>your</em> room, it's the room in <em>my</em> house that I allow you to live in.\" This attitude makes little sense on its own terms, as it suggests that parents would have no legitimate authority over, say, a famous child actor whose earnings paid for the house. Worse, it's a relatively minor manifestation of the broader notion that the child has a fundamentally lower status in the family just for being a child, that they deserve less weight in the family's utility function. I don't think this is what parents would be saying if recreational authoritarianism really were not a factor. They would still say that they, by virtue of their superior experience and judgment, get to make the rules (i.e., decide how to go about maximizing the family's utility function, though even this might be done with more authoritarianism than is necessary). But you wouldn't be hearing this \"I'm higher than you in the pecking order and don't you dare forget it\" attitude that is so very common.<br /><br />*Some might argue that arguments should be evaluated solely on their merit, and not on the motives with which they were offered. This is correct when the validity of the arguments can be finally determined. For most kinds of persuasive argumentation, especially in complicated and emotionally laden subjects like child rearing, arguments work on us without us ever being able to fully evaluate their merit. And in that world, it does make sense to down-weight arguments that have some bias built into them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ogWsaHQKwa6ddidRC": 1, "Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mhm2MNEuG2FcrMTH6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 42, "extendedScore": null, "score": 0.000116, "legacy": true, "legacyId": "1792", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 232, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-02T01:18:42.048Z", "modifiedAt": null, "url": null, "title": "Open Thread: November 2009", "slug": "open-thread-november-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:06.017Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZjikPZ67CvR64YvMK/open-thread-november-2009", "pageUrlRelative": "/posts/ZjikPZ67CvR64YvMK/open-thread-november-2009", "linkUrl": "https://www.lesswrong.com/posts/ZjikPZ67CvR64YvMK/open-thread-november-2009", "postedAtFormatted": "Monday, November 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20November%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20November%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjikPZ67CvR64YvMK%2Fopen-thread-november-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20November%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjikPZ67CvR64YvMK%2Fopen-thread-november-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZjikPZ67CvR64YvMK%2Fopen-thread-november-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. Feel free to rid yourself of cached thoughts by doing so in Old Church Slavonic. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</p>\n<p>If you're new to Less Wrong, check out <a href=\"/lw/b9/welcome_to_less_wrong/\">this welcome post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZjikPZ67CvR64YvMK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 5.341133935902408e-07, "legacy": true, "legacyId": "1793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 551, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CG9AEXwSjdrXPBEZ9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-02T01:22:07.732Z", "modifiedAt": null, "url": null, "title": "Open Thread: November 2009", "slug": "open-thread-november-2009-0", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:43.298Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kpreid", "createdAt": "2009-04-28T03:07:40.133Z", "isAdmin": false, "displayName": "kpreid"}, "userId": "rKiev4kfnTWRmCJit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sdSoCLQiqdQqBNHLi/open-thread-november-2009-0", "pageUrlRelative": "/posts/sdSoCLQiqdQqBNHLi/open-thread-november-2009-0", "linkUrl": "https://www.lesswrong.com/posts/sdSoCLQiqdQqBNHLi/open-thread-november-2009-0", "postedAtFormatted": "Monday, November 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20November%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20November%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdSoCLQiqdQqBNHLi%2Fopen-thread-november-2009-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20November%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdSoCLQiqdQqBNHLi%2Fopen-thread-november-2009-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsdSoCLQiqdQqBNHLi%2Fopen-thread-november-2009-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>This is, hopefully, the usual monthly thread for&nbsp;discussion of Less Wrong topics that have not appeared in recent posts.</p>\n<p><small>(My apologies and promise to fix if I'm doing this wrong; I have not previously submitted an article and am creating this thread so that I can post a comment in it.)</small></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sdSoCLQiqdQqBNHLi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "1794", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-03T02:58:04.599Z", "modifiedAt": null, "url": null, "title": "Re-understanding Robin Hanson\u2019s \u201cPre-Rationality\u201d", "slug": "re-understanding-robin-hanson-s-pre-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:43.385Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ahX9485o4kufEwCsY/re-understanding-robin-hanson-s-pre-rationality", "pageUrlRelative": "/posts/ahX9485o4kufEwCsY/re-understanding-robin-hanson-s-pre-rationality", "linkUrl": "https://www.lesswrong.com/posts/ahX9485o4kufEwCsY/re-understanding-robin-hanson-s-pre-rationality", "postedAtFormatted": "Tuesday, November 3rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Re-understanding%20Robin%20Hanson%E2%80%99s%20%E2%80%9CPre-Rationality%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARe-understanding%20Robin%20Hanson%E2%80%99s%20%E2%80%9CPre-Rationality%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahX9485o4kufEwCsY%2Fre-understanding-robin-hanson-s-pre-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Re-understanding%20Robin%20Hanson%E2%80%99s%20%E2%80%9CPre-Rationality%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahX9485o4kufEwCsY%2Fre-understanding-robin-hanson-s-pre-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FahX9485o4kufEwCsY%2Fre-understanding-robin-hanson-s-pre-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 794, "htmlBody": "<p class=\"MsoNormal\">I&rsquo;ve read Robin&rsquo;s paper &ldquo;<a href=\"http://hanson.gmu.edu/prior.pdf\">Uncommon Priors Require Origin Disputes</a>&rdquo; several times over the years, and I&rsquo;ve always struggled to understand it. Each time I would think that I did, but then I would forget my understanding, and some months or years later, find myself being puzzled by it all over again. So this time I&rsquo;m going to write down my newly re-acquired understanding, which will let others check that it is correct, and maybe help people (including my future selves) who are interested in Robin's idea but find the paper hard to understand.</p>\n<p>Here&rsquo;s the paper&rsquo;s abstract, in case you aren&rsquo;t already familiar with it.</p>\n<blockquote>\n<p class=\"MsoNormal\">In standard belief models, priors are always common knowledge. This prevents such models from representing agents&rsquo; probabilistic beliefs about the origins of their priors. By embedding standard models in a larger standard model, however, <em>pre-priors </em>can describe such beliefs. When an agent&rsquo; s prior and pre-prior are mutually consistent, he must believe that his prior would only have been different in situations where relevant event chances were different, but that variations in other agents&rsquo; priors are otherwise completely unrelated to which events are how likely. Due to this, Bayesians who agree enough about the origins of their priors must have the same priors.</p>\n</blockquote>\n<p class=\"MsoNormal\"><a id=\"more\"></a>I think my main difficulty with understanding the paper is the lack of a worked out example. So I&rsquo;ll take a simplified version of an example given in the paper and try to work out how it should be treated under the proposed formalism. Quoting the paper:</p>\n<blockquote>\n<p class=\"MsoNormal\">For example, if there were such a thing as a gene for optimism versus pessimism, you might believe that you had an equal chance of inheriting your mother&rsquo;s optimism gene or your father&rsquo;s pessimism gene.</p>\n</blockquote>\n<p class=\"MsoNormal\">Instead of talking about optimism vs pessimism in general, I&rsquo;ll use the example of an AI which has a prior on just the outcome of one coin toss (A) which will occur after it is created. The AI programmer will program it with one of two priors. The &ldquo;optimistic&rdquo; prior O says that the coin will land heads with probability .6, and the &ldquo;pessimistic&rdquo; prior P says that the coin will land heads with probability .4. For some reason, the programmer has decided to choose the prior based on an independent coin toss (B), which corresponds to the random Mendelian inheritance in the original example.</p>\n<p class=\"MsoNormal\">Suppose an &ldquo;optimistic&rdquo; AI wakes up and then reads Robin&rsquo;s paper. How would it reason? First, it needs a pre-prior (denoted p~ [EDIT: actually denoted q, as Hal pointed out in a comment] in the paper, but I&rsquo;ll use r here) that explains how it got its prior. So it asks the programmer how it got its prior, and the programmer tells it about coin toss B. (I&rsquo;m using the AI as an explanatory prop here, not saying that an actual AI would reason this way.) One plausible pre-prior at this point might be:</p>\n<ul>\n<li>r(p=O) = r(B=heads) = 0.5</li>\n<li>r(A=heads) = 0.6</li>\n<li>r(p=O AND A=heads) = 0.3</li>\n</ul>\n<p class=\"MsoNormal\">But unfortunately, this pre-prior doesn&rsquo;t satisfy Robin&rsquo;s <em>pre-rationality </em>condition (equation 2 on page 4), which when applied to this example says that</p>\n<ul>\n<li>O(A=heads) = r(A=heads | p=O) and</li>\n<li>P(A=heads) = r(A=heads | p=P)</li>\n</ul>\n<p class=\"MsoNormal\">The first equality holds, but the second one doesn&rsquo;t, because P(A=heads) = 0.4, and r(A=heads | p=P) = 0.6.</p>\n<p class=\"MsoNormal\">And actually there seems no way to construct any pre-prior that satisfies pre-rationality, unless the coin tosses A and B are correlated in the pre-prior (I think this is what Robin meant in the paper by &ldquo;track truth&rdquo;). But the programmer insists that the two coin tosses are not correlated, so that&rsquo;s that. What should the AI do at this point? Robin&rsquo;s paper doesn&rsquo;t say. It assumes pre-rationality and derives consequences from it, but doesn&rsquo;t talk about what we should do if we find ourselves lacking in pre-rationality, except that we might want to do something to correct this defect.</p>\n<p class=\"MsoNormal\">One obvious solution here is for the AI <em>programmer </em>to not have picked different priors for the AI based on an independent coin toss in the first place, and perhaps it could be argued that it was irrational, according to ordinary rationality, for the programmer to have done that. If it had been the case that O=P, then the AI can easily construct a pre-rational pre-prior. But our own priors depend partly on our genes, which were picked by evolution, so this solution doesn&rsquo;t seem to apply to us. And if we create any Bayesian AIs, the priors of those AIs will also be inevitably influenced (indirectly via us) by the randomness inherent in evolution.</p>\n<p class=\"MsoNormal\">So what should we (or our AIs) do? I think I have some ideas about that, but first, is my understanding of pre-rationality correct?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5oDii8KKW53n4HSx4": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ahX9485o4kufEwCsY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 33, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "1797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-04T21:31:00.512Z", "modifiedAt": null, "url": null, "title": "Rolf Nelson's \"The Rational Entrepreneur\"", "slug": "rolf-nelson-s-the-rational-entrepreneur", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:43.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DW7yzwCj76srESrtN/rolf-nelson-s-the-rational-entrepreneur", "pageUrlRelative": "/posts/DW7yzwCj76srESrtN/rolf-nelson-s-the-rational-entrepreneur", "linkUrl": "https://www.lesswrong.com/posts/DW7yzwCj76srESrtN/rolf-nelson-s-the-rational-entrepreneur", "postedAtFormatted": "Wednesday, November 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rolf%20Nelson's%20%22The%20Rational%20Entrepreneur%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARolf%20Nelson's%20%22The%20Rational%20Entrepreneur%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDW7yzwCj76srESrtN%2Frolf-nelson-s-the-rational-entrepreneur%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rolf%20Nelson's%20%22The%20Rational%20Entrepreneur%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDW7yzwCj76srESrtN%2Frolf-nelson-s-the-rational-entrepreneur", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDW7yzwCj76srESrtN%2Frolf-nelson-s-the-rational-entrepreneur", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>New blog up by longtime OB/LWer Rolf Nelson, \"<a href=\"http://www.rolfnelson.com/\">The Rational Entrepreneur</a>\" at <a href=\"http://www.rolfnelson.com/\">rolfnelson.com</a>.&nbsp; On \"the overlap between entrepreneurship and the modern tools of rationality\".&nbsp; Rolf will post daily through November, and after that it will depend on how much traction the blog gets.</p>\n<p>First posts:&nbsp; <a href=\"http://www.rolfnelson.com/2009/11/pay-more-attention-to-statistics.html\">Pay more attention to the statistics!</a> (which say to do what you know, not what you love) and <a href=\"http://www.rolfnelson.com/2009/11/having-co-founders-is-valuable-but-not.html\">Having co-founders is valuable but not crucial</a> (according to a study).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DW7yzwCj76srESrtN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 5.348229821022018e-07, "legacy": true, "legacyId": "1805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-05T11:23:10.873Z", "modifiedAt": null, "url": null, "title": "Money pumping: the axiomatic approach", "slug": "money-pumping-the-axiomatic-approach", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:46.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZTN6bLWqpwWn2i4qZ/money-pumping-the-axiomatic-approach", "pageUrlRelative": "/posts/ZTN6bLWqpwWn2i4qZ/money-pumping-the-axiomatic-approach", "linkUrl": "https://www.lesswrong.com/posts/ZTN6bLWqpwWn2i4qZ/money-pumping-the-axiomatic-approach", "postedAtFormatted": "Thursday, November 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Money%20pumping%3A%20the%20axiomatic%20approach&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMoney%20pumping%3A%20the%20axiomatic%20approach%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTN6bLWqpwWn2i4qZ%2Fmoney-pumping-the-axiomatic-approach%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Money%20pumping%3A%20the%20axiomatic%20approach%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTN6bLWqpwWn2i4qZ%2Fmoney-pumping-the-axiomatic-approach", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZTN6bLWqpwWn2i4qZ%2Fmoney-pumping-the-axiomatic-approach", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1552, "htmlBody": "<p>This post gets somewhat technical and mathematical, but the point can be summarised as:</p>\n<ul>\n<li>You are vulnerable to money pumps only to the extent to which you deviate from the <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">von Neumann-Morgenstern</a> axioms of expected utility.</li>\n</ul>\n<p>In other words, using alternate decision theories is bad for your wealth.</p>\n<p>But what is a money pump? Intuitively it is a series of trades that I propose to you, that end up bringing you back to where you started. All the trades must be indifferent or advantageous to you, so that you will accept them. And if even one of those trades is advantageous, then this is a money pump: I can charge you a tiny amount for that trade, making free money out of you. You are now strictly poorer than if you had not accepted the tradesat all.</p>\n<p>A <em>strict money pump</em> happens when every deal is advantageous to you, not simply indifferent. In most situations, there is no difference between a money pump and a strict money pump: I can offer you a tiny trinket at each indifferent deal to make it advantageous, and get these back later. There are odd preference systems out there, though, so the distinction is needed.</p>\n<p>The condition \"bringing you back to where you started\" needs to be examined some more. Thus define:</p>\n<p><strong>A strong money pump</strong> is a money pump which returns us both to exactly the same situations as when we started: in possession of the same assets and lotteries, with none of them having come due in the meantime.</p>\n<p><strong>A weak money pump</strong> is a money pump that returns us to the same situation that would have happened if we had never traded at all. Lotteries may have come due in the course of the trades.</p>\n<p><a id=\"more\"></a></p>\n<p>What is the difference? Quite simply with a strong money pump, since we return to exactly the same setup, I can then money pump you again, and again, charging you a penny at each round, and draining you of cash until you run out completely or your preferences change. Remember that I charge you that penny only for a deal that is strictly to your advantage, so even with that charge, you are coming out ahead on every deal. It's just at the end of the loop that you're losing out.</p>\n<p>For a weak money pump, since we don't return exactly to the same setup, I cannot just money pump you again instantly. I can get cash from you only once for each setup. It might never happen again; or it may happen regularly. But it's not as reusable as a strong money pump.</p>\n<p>Now if your preferences are <em>inconsistent</em>, I can certainly money pump you. In his post <a href=\"/lw/mz/zut_allais/\">Zut Allais</a> (French puns remain the property and responsibility of their authors) Eliezer presents a fun version of this, one where subjects prefer A to B <em>and </em>prefer B to A, depending on how they are phrased. Thus I will assume that your preferences are consistent - that you will only invert your preferences under objectively different conditions. I will also assume that you follow the completeness and continuity axioms of the <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">von Neumann-Morgenstern</a> formulation (completeness assumes you are actually capable of deciding between options, while continuity is a technical assumption). Note that the axioms on the Wikipedia article seemed to be incorrect, based on sources such as this <a href=\"http://books.google.co.uk/books?id=7pZV5O5n_WwC&amp;pg=PA21&amp;lpg=PA21&amp;dq=von+neumann+morgenstern+expected+utility+proof&amp;source=bl&amp;ots=JkJw9UJn7J&amp;sig=2kdpTvv59OORVDPhexvH9jaOMqg&amp;hl=en&amp;ei=C-TvSqL3DZHSjAeeqcDOCA&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=4&amp;ved=0CB8Q6AEwAw#v=onepage&amp;q=von%20neumann%20morgenstern%20expected%20utility%20proof&amp;f=false\">book</a>; I have corrected them by replacing &gt;'s with &ge; where appropriate. A lot of the complexities of this post revolve around the difference between these two symbols.</p>\n<p>The third von Neumann-Morgenstern axiom is <strong>transitivity</strong>: that if you like A at least as much as B (designated A &ge; B) and B at least as much as C, then you also like A at least as much as C. <strong>Strict transitivity</strong> is the weaker statement replacing &ge; with &gt; in the above. This is precisely where the strong money pumps come in:</p>\n<p><strong>If your preferences are consistent, complete and continuous, then you are immune to a strong money pump if and only if your preferences are transitive. You are immune to a strict strong money pump if and only if your preferences are strictly transitive.<br /></strong></p>\n<p style=\"padding-left: 30px;\"><em>Proof</em>: A strong money pump is equivalent with a sequence of preferences A &ge; B &ge; ... &ge; <strong>S &gt;</strong><strong> T</strong> &ge; ... &ge; Z &ge; A. Such a sequence can only exist if your preferences are not transitive (note the strict inequality in the middle).</p>\n<p style=\"padding-left: 30px;\">A strict strong money pump is equivalent with a sequence of preferences A &gt; B &gt; ... &gt; Z &gt; A. Such a sequence can only exist if your preferences are not strictly transitive.<strong>&diams;</strong></p>\n<p>The fourth von Neumann-Morgenstern axiom is <strong>independence</strong>: that if A &ge; B, then for all C, pA + (1-p)C &ge; pB + (1-p)C for all 0 &le; p&nbsp;&le; 1. It means essentially that all lotteries can be considered in isolation from each other. This is where the weak money pumps come in:</p>\n<p><strong>If your preferences are consistent, complete, continuous and transitive, then you are immune to a weak money pump if and only if your preferences are independent. You are immune to a strict weak money pump if and only if your preferences are not strictly dependent.</strong></p>\n<p>The rest of the post is a proof of this, and can be skipped for those unkeen on mathematics.</p>\n<p>First of all, we need to explain \"strictly dependent\" (there is no canonical definition of the term). Given all the other axioms, we can replace independence with the following equivalent axiom:</p>\n<ul>\n<li> (Independence II) For all A, B, C, D and 0 &le; p &le; 1, if pA + (1-p)C &gt; pB + (1-p)D, then A &gt; B or C &gt; D.</li>\n</ul>\n<p>The four standard axioms together imply the expected utility hypothesis, and Independence II is a simple consequence of that. Conversely, if we take C and D to be the same lottery, then the axiom states that pA + (1-p)C &gt; pB + (1-p)C implies A &gt; B (since C &lt; C is inconsistent). The <a href=\"http://en.wikipedia.org/wiki/Contrapositive\">contrapositive</a> of this statement is that if A &le; B, then pA + (1-p)C &le; pB + (1-p)C. This is just standard independence once more. Thus we can equivalently replace Independence with Independence II.</p>\n<p>A decision theory is dependent if it is not independent. Dependency means that there exists p, A, B, C and D such that pA + (1-p)C &gt; pB + (1-p)D while A &le; B and C &le; D. A decision theory is <strong>strictly dependent</strong> if we replace all &le; in that expression with &lt;. We're now ready to prove the result.</p>\n<p style=\"padding-left: 30px;\"><em>Proof</em>: Assume you have the lottery L, and that there will be a draw to determine one of the random elements. This means L can be written as pA + (1-p)B, where it will end up in as A or B after the draw. Then if I were to trade you L for M = pC + (1-p)D, with M &gt; L, independence implies that A &gt; C or B &gt; D.</p>\n<p style=\"padding-left: 30px;\">Consequently, if you start with lottery L and I trade it for M &gt; L, then for at least one outcome after the random draw, you are left with a lottery strictly better than what would have had if we had not traded. This result continues to be true no matter how often the draws happen, and hence I will not be able to trade you back to your initial situation with an advantageous or indifferent deal. Thus independence implies that you cannot be weakly money pumped with certainty.</p>\n<p style=\"padding-left: 30px;\">Conversely, if your preferences are dependent, then there are p, A, B, C and D such that pA + (1-p)C &gt; pB + (1-p)D and yet A &le; B and C &le; D. Then I can weakly money pump you, building on Eliezer's example. Assume you are in possession of pB + (1-p)D, with the first draw being to determine which of B or D you have. Then I can propose a binding contract: I will trade A for B and C for D after that first draw, replacing your current lottery with pA + (1-p)C. This is advantageous to you, so you will accept it. Then, after the first draw, I will propose to trade back B for A or D for C, another advantageous trade that you will accept. Congratulations! You have just been (weakly) money pumped.</p>\n<p style=\"padding-left: 30px;\">In the strict dependency situation, the proof works out exactly the same way, with&nbsp;&ge; replacing &gt; where appropriate, and proving that you cannot be strictly weakly money pumped if your preferences are consistent, transitive, complete, and not strictly dependent.<strong>&diams;</strong></p>\n<p><strong>Note: </strong>Of course, if you do not follow independence, you truly do not follow independence. You can play present lotteries off against future lotteries; you can look ahead and see that I will attempt to money-pump you, and compensate for that. You can therefore behave as if you were following independence in these situations, even if you do not. This sort of \"arbitraged independence\" will be the subject of a future post.</p>\n<p><strong>Addendum:</strong> Following a discussion with <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/18cq\">MendelShmiedekamp</a>, I realised that the continuity axiom is not needed for any of the results, as long as one uses independence II instead of independence. Anything that violates independence also violates independence II, so the \"if you violate independence, you can be weakly money-pumped\" result goes straight through to the non-continuous case.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2YcmB6SLtHnHRe3uX": 9}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZTN6bLWqpwWn2i4qZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "1791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post gets somewhat technical and mathematical, but the point can be summarised as:</p>\n<ul>\n<li>You are vulnerable to money pumps only to the extent to which you deviate from the <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">von Neumann-Morgenstern</a> axioms of expected utility.</li>\n</ul>\n<p>In other words, using alternate decision theories is bad for your wealth.</p>\n<p>But what is a money pump? Intuitively it is a series of trades that I propose to you, that end up bringing you back to where you started. All the trades must be indifferent or advantageous to you, so that you will accept them. And if even one of those trades is advantageous, then this is a money pump: I can charge you a tiny amount for that trade, making free money out of you. You are now strictly poorer than if you had not accepted the tradesat all.</p>\n<p>A <em>strict money pump</em> happens when every deal is advantageous to you, not simply indifferent. In most situations, there is no difference between a money pump and a strict money pump: I can offer you a tiny trinket at each indifferent deal to make it advantageous, and get these back later. There are odd preference systems out there, though, so the distinction is needed.</p>\n<p>The condition \"bringing you back to where you started\" needs to be examined some more. Thus define:</p>\n<p><strong>A strong money pump</strong> is a money pump which returns us both to exactly the same situations as when we started: in possession of the same assets and lotteries, with none of them having come due in the meantime.</p>\n<p><strong>A weak money pump</strong> is a money pump that returns us to the same situation that would have happened if we had never traded at all. Lotteries may have come due in the course of the trades.</p>\n<p><a id=\"more\"></a></p>\n<p>What is the difference? Quite simply with a strong money pump, since we return to exactly the same setup, I can then money pump you again, and again, charging you a penny at each round, and draining you of cash until you run out completely or your preferences change. Remember that I charge you that penny only for a deal that is strictly to your advantage, so even with that charge, you are coming out ahead on every deal. It's just at the end of the loop that you're losing out.</p>\n<p>For a weak money pump, since we don't return exactly to the same setup, I cannot just money pump you again instantly. I can get cash from you only once for each setup. It might never happen again; or it may happen regularly. But it's not as reusable as a strong money pump.</p>\n<p>Now if your preferences are <em>inconsistent</em>, I can certainly money pump you. In his post <a href=\"/lw/mz/zut_allais/\">Zut Allais</a> (French puns remain the property and responsibility of their authors) Eliezer presents a fun version of this, one where subjects prefer A to B <em>and </em>prefer B to A, depending on how they are phrased. Thus I will assume that your preferences are consistent - that you will only invert your preferences under objectively different conditions. I will also assume that you follow the completeness and continuity axioms of the <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">von Neumann-Morgenstern</a> formulation (completeness assumes you are actually capable of deciding between options, while continuity is a technical assumption). Note that the axioms on the Wikipedia article seemed to be incorrect, based on sources such as this <a href=\"http://books.google.co.uk/books?id=7pZV5O5n_WwC&amp;pg=PA21&amp;lpg=PA21&amp;dq=von+neumann+morgenstern+expected+utility+proof&amp;source=bl&amp;ots=JkJw9UJn7J&amp;sig=2kdpTvv59OORVDPhexvH9jaOMqg&amp;hl=en&amp;ei=C-TvSqL3DZHSjAeeqcDOCA&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=4&amp;ved=0CB8Q6AEwAw#v=onepage&amp;q=von%20neumann%20morgenstern%20expected%20utility%20proof&amp;f=false\">book</a>; I have corrected them by replacing &gt;'s with \u2265 where appropriate. A lot of the complexities of this post revolve around the difference between these two symbols.</p>\n<p>The third von Neumann-Morgenstern axiom is <strong>transitivity</strong>: that if you like A at least as much as B (designated A \u2265 B) and B at least as much as C, then you also like A at least as much as C. <strong>Strict transitivity</strong> is the weaker statement replacing \u2265 with &gt; in the above. This is precisely where the strong money pumps come in:</p>\n<p><strong id=\"If_your_preferences_are_consistent__complete_and_continuous__then_you_are_immune_to_a_strong_money_pump_if_and_only_if_your_preferences_are_transitive__You_are_immune_to_a_strict_strong_money_pump_if_and_only_if_your_preferences_are_strictly_transitive_\">If your preferences are consistent, complete and continuous, then you are immune to a strong money pump if and only if your preferences are transitive. You are immune to a strict strong money pump if and only if your preferences are strictly transitive.<br></strong></p>\n<p style=\"padding-left: 30px;\"><em>Proof</em>: A strong money pump is equivalent with a sequence of preferences A \u2265 B \u2265 ... \u2265 <strong>S &gt;</strong><strong> T</strong> \u2265 ... \u2265 Z \u2265 A. Such a sequence can only exist if your preferences are not transitive (note the strict inequality in the middle).</p>\n<p style=\"padding-left: 30px;\">A strict strong money pump is equivalent with a sequence of preferences A &gt; B &gt; ... &gt; Z &gt; A. Such a sequence can only exist if your preferences are not strictly transitive.<strong>\u2666</strong></p>\n<p>The fourth von Neumann-Morgenstern axiom is <strong>independence</strong>: that if A \u2265 B, then for all C, pA + (1-p)C \u2265 pB + (1-p)C for all 0 \u2264 p&nbsp;\u2264 1. It means essentially that all lotteries can be considered in isolation from each other. This is where the weak money pumps come in:</p>\n<p><strong id=\"If_your_preferences_are_consistent__complete__continuous_and_transitive__then_you_are_immune_to_a_weak_money_pump_if_and_only_if_your_preferences_are_independent__You_are_immune_to_a_strict_weak_money_pump_if_and_only_if_your_preferences_are_not_strictly_dependent_\">If your preferences are consistent, complete, continuous and transitive, then you are immune to a weak money pump if and only if your preferences are independent. You are immune to a strict weak money pump if and only if your preferences are not strictly dependent.</strong></p>\n<p>The rest of the post is a proof of this, and can be skipped for those unkeen on mathematics.</p>\n<p>First of all, we need to explain \"strictly dependent\" (there is no canonical definition of the term). Given all the other axioms, we can replace independence with the following equivalent axiom:</p>\n<ul>\n<li> (Independence II) For all A, B, C, D and 0 \u2264 p \u2264 1, if pA + (1-p)C &gt; pB + (1-p)D, then A &gt; B or C &gt; D.</li>\n</ul>\n<p>The four standard axioms together imply the expected utility hypothesis, and Independence II is a simple consequence of that. Conversely, if we take C and D to be the same lottery, then the axiom states that pA + (1-p)C &gt; pB + (1-p)C implies A &gt; B (since C &lt; C is inconsistent). The <a href=\"http://en.wikipedia.org/wiki/Contrapositive\">contrapositive</a> of this statement is that if A \u2264 B, then pA + (1-p)C \u2264 pB + (1-p)C. This is just standard independence once more. Thus we can equivalently replace Independence with Independence II.</p>\n<p>A decision theory is dependent if it is not independent. Dependency means that there exists p, A, B, C and D such that pA + (1-p)C &gt; pB + (1-p)D while A \u2264 B and C \u2264 D. A decision theory is <strong>strictly dependent</strong> if we replace all \u2264 in that expression with &lt;. We're now ready to prove the result.</p>\n<p style=\"padding-left: 30px;\"><em>Proof</em>: Assume you have the lottery L, and that there will be a draw to determine one of the random elements. This means L can be written as pA + (1-p)B, where it will end up in as A or B after the draw. Then if I were to trade you L for M = pC + (1-p)D, with M &gt; L, independence implies that A &gt; C or B &gt; D.</p>\n<p style=\"padding-left: 30px;\">Consequently, if you start with lottery L and I trade it for M &gt; L, then for at least one outcome after the random draw, you are left with a lottery strictly better than what would have had if we had not traded. This result continues to be true no matter how often the draws happen, and hence I will not be able to trade you back to your initial situation with an advantageous or indifferent deal. Thus independence implies that you cannot be weakly money pumped with certainty.</p>\n<p style=\"padding-left: 30px;\">Conversely, if your preferences are dependent, then there are p, A, B, C and D such that pA + (1-p)C &gt; pB + (1-p)D and yet A \u2264 B and C \u2264 D. Then I can weakly money pump you, building on Eliezer's example. Assume you are in possession of pB + (1-p)D, with the first draw being to determine which of B or D you have. Then I can propose a binding contract: I will trade A for B and C for D after that first draw, replacing your current lottery with pA + (1-p)C. This is advantageous to you, so you will accept it. Then, after the first draw, I will propose to trade back B for A or D for C, another advantageous trade that you will accept. Congratulations! You have just been (weakly) money pumped.</p>\n<p style=\"padding-left: 30px;\">In the strict dependency situation, the proof works out exactly the same way, with&nbsp;\u2265 replacing &gt; where appropriate, and proving that you cannot be strictly weakly money pumped if your preferences are consistent, transitive, complete, and not strictly dependent.<strong>\u2666</strong></p>\n<p><strong>Note: </strong>Of course, if you do not follow independence, you truly do not follow independence. You can play present lotteries off against future lotteries; you can look ahead and see that I will attempt to money-pump you, and compensate for that. You can therefore behave as if you were following independence in these situations, even if you do not. This sort of \"arbitraged independence\" will be the subject of a future post.</p>\n<p><strong>Addendum:</strong> Following a discussion with <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/18cq\">MendelShmiedekamp</a>, I realised that the continuity axiom is not needed for any of the results, as long as one uses independence II instead of independence. Anything that violates independence also violates independence II, so the \"if you violate independence, you can be weakly money-pumped\" result goes straight through to the non-continuous case.</p>", "sections": [{"title": "If your preferences are consistent, complete and continuous, then you are immune to a strong money pump if and only if your preferences are transitive. You are immune to a strict strong money pump if and only if your preferences are strictly transitive.", "anchor": "If_your_preferences_are_consistent__complete_and_continuous__then_you_are_immune_to_a_strong_money_pump_if_and_only_if_your_preferences_are_transitive__You_are_immune_to_a_strict_strong_money_pump_if_and_only_if_your_preferences_are_strictly_transitive_", "level": 1}, {"title": "If your preferences are consistent, complete, continuous and transitive, then you are immune to a weak money pump if and only if your preferences are independent. You are immune to a strict weak money pump if and only if your preferences are not strictly dependent.", "anchor": "If_your_preferences_are_consistent__complete__continuous_and_transitive__then_you_are_immune_to_a_weak_money_pump_if_and_only_if_your_preferences_are_independent__You_are_immune_to_a_strict_weak_money_pump_if_and_only_if_your_preferences_are_not_strictly_dependent_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "93 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zNcLnqHF5rvrTsQJx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-06T03:54:49.881Z", "modifiedAt": null, "url": null, "title": "Light Arts", "slug": "light-arts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:09.545Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9RMYL4qfYpgukLuHf/light-arts", "pageUrlRelative": "/posts/9RMYL4qfYpgukLuHf/light-arts", "linkUrl": "https://www.lesswrong.com/posts/9RMYL4qfYpgukLuHf/light-arts", "postedAtFormatted": "Friday, November 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Light%20Arts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALight%20Arts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RMYL4qfYpgukLuHf%2Flight-arts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Light%20Arts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RMYL4qfYpgukLuHf%2Flight-arts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9RMYL4qfYpgukLuHf%2Flight-arts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 943, "htmlBody": "<p><strong>tl;dr: </strong>It is worthwhile to convince people that they <em>already, </em>by their own lights, have reasons to believe true things, as this is faster, easier, nicer, and more effective than helping them create from scratch reasons to believe those things.</p>\n<p><em>This is not part of the problem-solving sequence.&nbsp; I do plan to finish that, but the last post is eluding me.</em></p>\n<p><strong>Related:</strong> Whatever it is I was thinking of <a href=\"/lw/1dt/open_thread_november_2009/18fz\">here</a> (let me know if you can dig up what it was).</p>\n<p>Today, while waiting for a bus, I heard the two girls sitting on the bench next to mine talking about organ donation.&nbsp; One said that she was thinking of ceasing to be an organ donor, because she'd heard that doctors don't try as hard to save donors in hopes of using their organs to save other lives.</p>\n<p>My bus was approaching.&nbsp; I didn't know the girl and could hardly follow up later with an arsenal of ironclad counterarguments.&nbsp; There was no time, and probably no receptivity, to engage in a lengthy discussion of why this medical behavior <em>wouldn't</em> happen.&nbsp; No chance to fire up my computer, try to get on the nearest wireless, and pull up empirical stats that say it <em>doesn't</em> happen.</p>\n<p>So I chuckled and interjected, at a convenient gap in her ramble, \"That's why you carry a blood donor card, too, so they think if you stay alive they'll keep getting blood from you!\"</p>\n<p>Some far-off potential tragic crisis averted?&nbsp; Maybe.&nbsp; She looked thoughtful, nodded, said that she did have a blood donor card, and that my suggestion made sense.&nbsp; I boarded my bus and it carried me away.&nbsp; I hope she's never hit by a cement truck.&nbsp; I hope that if she <em>is</em> hit by a cement truck, a stupid rumor she heard once doesn't turn it into as complete a waste as it would have to be without the wonders of organ transplant.<a id=\"more\"></a></p>\n<p>And even maintaining those twin hopes and feeling I'd done something to improve their conjoined chance of realization, I began to feel like perhaps I'd done wrong.&nbsp; I could conjure up a defense - hey, I laughed first, and I'd used the exact same words before as a mere joke (with people better-informed than this who I'd expected to get it on their own).&nbsp; It's not <em>strictly</em> my fault that she didn't take it as a joke too.&nbsp; And hey, I <em>would</em> have gone ahead and had the whole knock-down drag-out argument with her if there had only been time, if I could only have had her ear for long enough to spit out more than a soundbite, if only she hadn't been a complete stranger I'll never see again.</p>\n<p>But even without time and social pressure preventing you from having a great long knock-down drag-out argument, it can be devastatingly ineffectual to present the reasons you think are the <em>right</em> ones to believe some proposition P or take some action A.&nbsp; And presenting other reasons seems dishonest, somehow - just lining up soldier-arguments in favor of P or A because they're well-equipped against this opponent, and not because they're the best and soundest and strongest according to <em>objective</em> (read: <em>your</em>) standards.</p>\n<p>Here's a related story: in my midterm paper for my Plato's Republic class, my thesis statement was \"Plato's position on falsehood in the kallipolis is inconsistent\".&nbsp; Bam!&nbsp; Plato would have a heart attack!&nbsp; <em>Dreaded inconsistency!</em>&nbsp; But after I got comments back, I agreed with the professor that what I'd really shown was something weaker: \"Plato has good reason, by his own lights, to reject the Noble Lie\".&nbsp; No utter logical malady infects his city so thoroughly that I can demonstrate a rejection of modus ponens on the subject at hand.&nbsp; But the revision... is still pretty strong.&nbsp; Inconsistency is a general, powerful case of having reason to reject something.&nbsp; Inconsistency brings with it the <em>guarantee</em> of being wrong in at least one place.&nbsp; But so too, in a gentler and narrower way,<em> </em>does having reason to reject something by your own lights, even if it's not an airtight reason.&nbsp; And this gentleness is more non-threateningly persuasive, and this narrowness demands less background from your interlocutor in logic, and beginning from this preexisting background saves more time, than beginning with a priori principles and proceeding from there to proposition P or action A.</p>\n<p>The girl at the bus stop began by having nasty suspicions that doctors are twisted creatures who all walked straight out of an ethics textbook, evil consequentialist plots devoid of professionalism or commonsense morality fully-formed in their minds, and who would see her ambulance-borne self as a sack of valuable organs they could use to salvage numerous other lives if only they made the slightest wrong twitch with a scalpel.&nbsp; But <em>even if</em> we grant that falsehood, she <em>still</em> does not have adequate reason to withdraw her consent for organ donation, as long as she can present proof to evil consequentialist doctors that she's worth more alive than dead.&nbsp; And she can.</p>\n<p>Offering arguments like this - ones which use premises you don't hold that opponent does, and which aren't <em>reductios</em> in form - is only dishonest if your conclusion is meant to be, \"Objectively and all things considered, you should perform action A or believe proposition P.\"&nbsp; Those arguments (assuming the premises your opponent holds and you don't are false) show nothing of the kind.&nbsp; But pointing out that people have reasons by their own lights to believe P or perform A - concluding instead \"given these premises which you accept, A or P is reasonable\" - is not, I contend, underhanded.&nbsp; Conveniently, it's also not slow, mean, or difficult.&nbsp; And maybe these light arts saved a life today.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9RMYL4qfYpgukLuHf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 17, "extendedScore": null, "score": 5.351397626165816e-07, "legacy": true, "legacyId": "1812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-06T07:24:31.000Z", "modifiedAt": null, "url": null, "title": "News: Improbable Coincidence Slows LHC Repairs", "slug": "news-improbable-coincidence-slows-lhc-repairs", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:44.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zack_M_Davis", "createdAt": "2009-10-16T19:59:15.209Z", "isAdmin": false, "displayName": "Zack_M_Davis"}, "userId": "mPipmBTniuABY5PQy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y24ue9LLJdTmpPZnz/news-improbable-coincidence-slows-lhc-repairs", "pageUrlRelative": "/posts/y24ue9LLJdTmpPZnz/news-improbable-coincidence-slows-lhc-repairs", "linkUrl": "https://www.lesswrong.com/posts/y24ue9LLJdTmpPZnz/news-improbable-coincidence-slows-lhc-repairs", "postedAtFormatted": "Friday, November 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20News%3A%20Improbable%20Coincidence%20Slows%20LHC%20Repairs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANews%3A%20Improbable%20Coincidence%20Slows%20LHC%20Repairs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy24ue9LLJdTmpPZnz%2Fnews-improbable-coincidence-slows-lhc-repairs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=News%3A%20Improbable%20Coincidence%20Slows%20LHC%20Repairs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy24ue9LLJdTmpPZnz%2Fnews-improbable-coincidence-slows-lhc-repairs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy24ue9LLJdTmpPZnz%2Fnews-improbable-coincidence-slows-lhc-repairs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/u5/how_many_lhc_failures_is_too_many/\">How Many LHC Failures is Too Many?</a></p>\n<p>My first reaction to this was that it had to be a joke, but I thought <em>Less Wrong</em> readers would like to know that <em>The Times</em> of London is <a href=\"http://www.timesonline.co.uk/tol/news/science/physics/article6905250.ece\">reporting that repairs on the Large Hadron Collider have been delayed</a> by overheating caused by a piece of bread, possibly dropped by a bird:</p>\n<blockquote>\n<p>The rehabilitation of the beleaguered Large Hadron Collider was on hold  tonight after the failure of one of its powerful cooling units caused by an  errant chunk of baguette.</p>\n<p>The &pound;4 billion particle-collider faced more than a year of delays after a  helium leak stymied the project in its first few days of operation. It is  gradually being switched back on over the coming months but suffered a new  setback on Tuesday morning.</p>\n<p>Scientists at the CERN particle physics laboratory in Geneva noticed that the  system&rsquo;s carefully monitored temperatures were creeping up.</p>\n<p>\n<script src=\"http://www.timesonline.co.uk/tol/js/picture-gallery.js\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\nfunction slideshowPopUp(url)\n{\npictureGalleryPopupPic(url);\nreturn false;\n}\n</script>\n</p>\n<p>Further investigation into the failure of a cryogenic cooling plant revealed  an unusual impediment. A piece of crusty bread had paralysed a high voltage  installation that should have been powering the cooling unit. [...]</p>\n<p>A spokeswoman for CERN confirmed that baguette was responsible for the latest  hiatus, but she conceded that mystery surrounded the way it got into the  vital power installation, which is protected by high security fences.</p>\n<p>&ldquo;Nobody knows how it got there,&rdquo; she told <em>The Times</em>. &ldquo;The  best guess is that it was dropped by a bird, either that or it was thrown  out of a passing aeroplane.&rdquo;</p>\n<p>&ldquo;Obviously this was slightly surprising. Within the team there was some  amusement once they had relaxed after initial concerns.&rdquo;</p>\n</blockquote>\n<!--#include file=\"m63-article-related-attachements.html\"--> <!-- BEGIN: Module - M63 - Article Related Attachements -->\n<p>I'm <em>rather confident</em> that this is just a meaningless coincidence, but in light of the <a href=\"http://www.aleph.se/andart/archives/2008/09/bayes_moravec_and_the_lhc_quantum_suicide_subjective_probability_and_conspiracies.html\">anthropic speculations</a> last year about the LHC's technical difficulties, I thought this was worth sharing.</p>\n<p>Hat tip <a href=\"/user/MBlume/\">MBlume</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y24ue9LLJdTmpPZnz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 5.35176204212068e-07, "legacy": true, "legacyId": "1813", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jE3npTEBtHnZBuAcg"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-06T07:49:11.816Z", "modifiedAt": null, "url": null, "title": "Bay area LW meet-up", "slug": "bay-area-lw-meet-up", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:45.875Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hknmGGsZMdGGduS54/bay-area-lw-meet-up", "pageUrlRelative": "/posts/hknmGGsZMdGGduS54/bay-area-lw-meet-up", "linkUrl": "https://www.lesswrong.com/posts/hknmGGsZMdGGduS54/bay-area-lw-meet-up", "postedAtFormatted": "Friday, November 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bay%20area%20LW%20meet-up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABay%20area%20LW%20meet-up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhknmGGsZMdGGduS54%2Fbay-area-lw-meet-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bay%20area%20LW%20meet-up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhknmGGsZMdGGduS54%2Fbay-area-lw-meet-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhknmGGsZMdGGduS54%2Fbay-area-lw-meet-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>The November LW/OB meet-up will be this Saturday (two days from today), at the SIAI house in Santa Clara.&nbsp; Apologies for the late notice.&nbsp; We'll have fun, food, and attempts at rationality, as well as good general conversation.&nbsp; Details at <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/11808960/\">the bay area OB/LW meet-up page</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hknmGGsZMdGGduS54", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 5.351804063245719e-07, "legacy": true, "legacyId": "1814", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-06T10:42:35.407Z", "modifiedAt": null, "url": null, "title": "All hail the Lisbon Treaty! Or is that \"hate\"? Or just \"huh\"?", "slug": "all-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:44.216Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2kBKELS6mmEhydLvR/all-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "pageUrlRelative": "/posts/2kBKELS6mmEhydLvR/all-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "linkUrl": "https://www.lesswrong.com/posts/2kBKELS6mmEhydLvR/all-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "postedAtFormatted": "Friday, November 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20All%20hail%20the%20Lisbon%20Treaty!%20Or%20is%20that%20%22hate%22%3F%20Or%20just%20%22huh%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAll%20hail%20the%20Lisbon%20Treaty!%20Or%20is%20that%20%22hate%22%3F%20Or%20just%20%22huh%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBKELS6mmEhydLvR%2Fall-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=All%20hail%20the%20Lisbon%20Treaty!%20Or%20is%20that%20%22hate%22%3F%20Or%20just%20%22huh%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBKELS6mmEhydLvR%2Fall-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kBKELS6mmEhydLvR%2Fall-hail-the-lisbon-treaty-or-is-that-hate-or-just-huh", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>The <a href=\"http://en.wikipedia.org/wiki/Lisbon_treaty\">Lisbon treaty</a> was finally ratified last Tuesday, in a most wonderfully disdainful <a href=\"http://www.youtube.com/watch?v=kTFxk0jX16I\">signing cermony</a>.</p>\n<p>I take it that everyone on the list is emotionally overwhelmed by this, one of the most important political events in recent history. The world's largest economy has taken a firm step towards statehood; the ramifications of this will be felt across the world. People will die who would have lived; people will live who would have died: the body count is much affected. The potential implications for AI alone (think political <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a>, research funding priorities) are huge. Depending on your opinion of the consequences, you are probably dumped into a dark ditch of despair or swimming in a limitless ocean of triumphant glee.</p>\n<p>If neither is the case... <a href=\"http://wiki.lesswrong.com/wiki/Emotion\">why not</a>?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2kBKELS6mmEhydLvR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -3, "extendedScore": null, "score": 5.352106317949291e-07, "legacy": true, "legacyId": "1816", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-06T16:29:30.210Z", "modifiedAt": null, "url": null, "title": "Hamster in Tutu Shuts Down Large Hadron Collider", "slug": "hamster-in-tutu-shuts-down-large-hadron-collider", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:34.078Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C2uvzYeoMkwMmscMx/hamster-in-tutu-shuts-down-large-hadron-collider", "pageUrlRelative": "/posts/C2uvzYeoMkwMmscMx/hamster-in-tutu-shuts-down-large-hadron-collider", "linkUrl": "https://www.lesswrong.com/posts/C2uvzYeoMkwMmscMx/hamster-in-tutu-shuts-down-large-hadron-collider", "postedAtFormatted": "Friday, November 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hamster%20in%20Tutu%20Shuts%20Down%20Large%20Hadron%20Collider&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHamster%20in%20Tutu%20Shuts%20Down%20Large%20Hadron%20Collider%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2uvzYeoMkwMmscMx%2Fhamster-in-tutu-shuts-down-large-hadron-collider%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hamster%20in%20Tutu%20Shuts%20Down%20Large%20Hadron%20Collider%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2uvzYeoMkwMmscMx%2Fhamster-in-tutu-shuts-down-large-hadron-collider", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC2uvzYeoMkwMmscMx%2Fhamster-in-tutu-shuts-down-large-hadron-collider", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 662, "htmlBody": "<p>The Large Hadron Collider was shut down yesterday by a hamster in a tutu, weary scientists announced.</p>\n<p>The Large Hadron Collider is the successor to the earlier Superconducting Super Collider, which was shut down by the US House of Representatives in 1993 after 14 miles of tunnel had been constructed at a cost of $2 billion.&nbsp; Since its inception, the Large Hadron Collider has been plagued by construction delays, dead technicians, broken magnet supports, electrical faults, helium containment failures, vacuum leaks, birds with baguettes, terrorists, ninjas, pirates, supervillains, hurricanes, asteroids, cosmic energy storms, and a runaway train.&nbsp; On one occasion it was discovered that the entire 17-mile circular tunnel had been built upside-down due to a sign error in the calculations, and the whole facility had to be carefully flipped by a giant spatula.<a id=\"more\"></a></p>\n<p>One year ago, hopes were raised for the first time in decades when it was discovered that all the incidents up until that point had been the work of a sinister globe-spanning conspiracy of religious fanatics who, inspired by the term \"God Particle\", had decided that no one could ever be allowed to look upon the hypothetical Higgs boson.&nbsp; This discovery was widely considered to have undermined the theory that Nature abhors a sufficiently powerful particle collider.&nbsp; Though some found it suspicious that the Higgs boson would even <em>have</em> a religious cult devoted to preventing its observation, the affair did have a patina of surface plausibility - after all, a giant plot to prevent physicists from observing the Higgs boson makes around as much sense as anything else religious people do.</p>\n<p>After the conspiracy was shut down by heroic international detectives in an operation so hugely dramatic that it would be pointless to summarize it here, the world began to wonder whether the LHC might really, really <em>work</em> this time around.&nbsp; Scientists everywhere held their breaths as the bodies were cleared out, the tunnels reconditioned, and the broken magnets replaced, all without incident.&nbsp; The price of large hadrons held steady on the commodities market, permitting the LHC's reservoirs to be fully stocked.&nbsp; Proton beams were successfully formed and circulated through the giant tunnel.</p>\n<p>Moments before the first collision was scheduled to probe the theretofore-unachieved energy of 3.5 TeV, a hamster in a tutu materialized from nowhere at the intended collision point.&nbsp; The poor creature didn't even have time for a terrified squeak before the two proton beams smashed into it, releasing the equivalent energy of 724 megajoules or 173 kilograms of TNT.</p>\n<p>The dispirited scientists of the LHC have announced that this will create a 24-month delay while tiny bits of hamster are cleaned out of the tunnels and anti-hamster-materialization fields are installed in the collider.</p>\n<p>At the poorly attended press conference, journalists asked whether it might finally be time to give up.</p>\n<p>\"Nature's just messing with you, man,\" said a reporter from the <em>New York Times.</em>&nbsp; \"You need to admit this isn't going to work out.\"</p>\n<p>Professor Nicholas von Shnicker, project leader of the LHC, responded.</p>\n<p>\"NEVER!\" shrieked von Shnicker, spittle flying from his lips and spattering on his ragged beard.&nbsp; \"Ve vill NEVER give up!&nbsp; My father spent his life trying to make the LHC vork, and <em>his</em> father!&nbsp; Even if it takes a century, if it takes a thousand years or ten thousand million years, VE VILL SEE THE HIGGS BOSON IN OUR LIFETIMES!\"</p>\n<p>Prof. Kill McBibben is the author of the recently released book <em>Enough,</em> which proposes a new theory of the mysterious Counter-Force that prevents the LHC from operating.&nbsp; \"It's not the Higgs boson going back in time,\" says Prof. McBibben, \"nor is it the anthropic principle preventing a black hole from forming.&nbsp; We've just hit the point that we all knew was coming - that we all knew had to happen someday.&nbsp; We've reached the limits of human science.&nbsp; We are just not allowed to build colliders at higher than a certain energy, or know more than a certain amount of particle physics.&nbsp; This is the end of the road.&nbsp; We're done.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hNFdS3rRiYgqqD8aM": 1, "etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C2uvzYeoMkwMmscMx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 47, "extendedScore": null, "score": 8.1e-05, "legacy": true, "legacyId": "1817", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-08T02:53:29.184Z", "modifiedAt": null, "url": null, "title": "The Danger of Stories", "slug": "the-danger-of-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:52.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GS5Ef9FePbn6eP2CR/the-danger-of-stories", "pageUrlRelative": "/posts/GS5Ef9FePbn6eP2CR/the-danger-of-stories", "linkUrl": "https://www.lesswrong.com/posts/GS5Ef9FePbn6eP2CR/the-danger-of-stories", "postedAtFormatted": "Sunday, November 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Danger%20of%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Danger%20of%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGS5Ef9FePbn6eP2CR%2Fthe-danger-of-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Danger%20of%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGS5Ef9FePbn6eP2CR%2Fthe-danger-of-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGS5Ef9FePbn6eP2CR%2Fthe-danger-of-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>Tyler Cowen <a title=\"http://tedxmidatlantic.com/live/#TylerCowen\" href=\"http://tedxmidatlantic.com/live/#TylerCowen\">argues</a> in a TED talk (~15 min) that stories pervade our mental lives. &nbsp;He thinks they are a major source of cognitive biases and, on the margin, we should be more suspicious of them - especially simple stories. &nbsp;Here's an interesting quote about the meta-level:</p>\n<blockquote>\n<p>What story do you take away from Tyler Cowen? &nbsp;...Another possibility is you might tell a story of rebirth. &nbsp;You might say, \"I used to think too much in terms of stories, but then I heard Tyler Cowen, and now I think less in terms of stories\". ...You could also tell a story of deep tragedy. &nbsp;\"This guy Tyler Cowen came and he told us not to think in terms of stories, but all he could do was tell us stories about how other people think too much in terms of stories.\"</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GS5Ef9FePbn6eP2CR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 5.356299893731508e-07, "legacy": true, "legacyId": "1822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 105, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-08T14:27:41.688Z", "modifiedAt": null, "url": null, "title": "Practical rationality in surveys", "slug": "practical-rationality-in-surveys", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:44.464Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c9EWsufsjc7NLwWM2/practical-rationality-in-surveys", "pageUrlRelative": "/posts/c9EWsufsjc7NLwWM2/practical-rationality-in-surveys", "linkUrl": "https://www.lesswrong.com/posts/c9EWsufsjc7NLwWM2/practical-rationality-in-surveys", "postedAtFormatted": "Sunday, November 8th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Practical%20rationality%20in%20surveys&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APractical%20rationality%20in%20surveys%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9EWsufsjc7NLwWM2%2Fpractical-rationality-in-surveys%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Practical%20rationality%20in%20surveys%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9EWsufsjc7NLwWM2%2Fpractical-rationality-in-surveys", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc9EWsufsjc7NLwWM2%2Fpractical-rationality-in-surveys", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>\"Statistically significant results\" mean that there's a 5% chance that results are wrong in addition to chance that the wrong thing was measures, chance that sample was biased, chance that measurement instruments were biased, chance that mistakes were made during analysis, chance that publication bias skewed results, chance that results were entirely made up and so on.</p>\n<p>\"Not statistically significant results\" mean all those, except chance of randomly mistaken results even if everything was ran correct is not 5%, but something else, unknown, and dependent of strength of the effect measured (if the effect is weak, you can have study where chance of false negative is over 99%).</p>\n<p>So results being statistically significant or not, is really not that useful.</p>\n<p>For example, here's a survey of <a href=\"http://www.fivethirtyeight.com/2009/11/real-oklahoma-students-ace-citizenship.html\">civic knowledge</a>. Plus or minus 3% measurement error? Not this time, they just completely made up the results.</p>\n<p>Take home exercise - what do you estimate Bayesian chance of published results being wrong to be?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c9EWsufsjc7NLwWM2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -3, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "1824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-09T21:42:22.087Z", "modifiedAt": null, "url": null, "title": "Reflections on Pre-Rationality", "slug": "reflections-on-pre-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:02.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J8LTE8CTQfyEMkhnc/reflections-on-pre-rationality", "pageUrlRelative": "/posts/J8LTE8CTQfyEMkhnc/reflections-on-pre-rationality", "linkUrl": "https://www.lesswrong.com/posts/J8LTE8CTQfyEMkhnc/reflections-on-pre-rationality", "postedAtFormatted": "Monday, November 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reflections%20on%20Pre-Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReflections%20on%20Pre-Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8LTE8CTQfyEMkhnc%2Freflections-on-pre-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reflections%20on%20Pre-Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8LTE8CTQfyEMkhnc%2Freflections-on-pre-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ8LTE8CTQfyEMkhnc%2Freflections-on-pre-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 763, "htmlBody": "<p>This continues my <a href=\"/lw/1dx/reunderstanding_robin_hansons_prerationality/\">previous post</a> on Robin Hanson's pre-rationality, by offering some additional comments on the idea.</p>\n<p>The reason I re-read Robin's paper recently was to see if it answers a question that's related to <a href=\"/lw/1cd/why_the_beliefsvalues_dichotomy/\">another of my recent posts</a>: why do we human beings have the priors that we do? Part of that question is why are our priors pretty close to each other, even if they're not exactly equal. (Technically we don't have priors because we're not Bayesians, but we can be approximated as Bayesians, and those Bayesians have priors.) If we were created by a rational creator, then we would have pre-rational priors. (Which, since we don't actually have pre-rational priors, seems to be a good argument against us having been created by a rational creator. I wonder what <a href=\"/lw/182/the_absentminded_driver/13ys\">Aumann</a> would say about this?) But we have other grounds for believing that we were instead created by evolution, which is not a rational process, in which case the concept doesn't help to answer the question, as far as I can see. (Robin never claimed that it would, of course.)</p>\n<p>The next question I want to consider is a normative one: is pre-rationality rational? Pre-rationality says that we should reason as if we were pre-agents who learned about our prior assignments as <em>information</em>, instead of just taking those priors as given. But then, shouldn't we also act as if we were pre-agents who learned about our <em>utility function</em> assignments as information, instead of taking them as given? In that case, we're led to the conclusion that we should all have common utility functions, or at least that pre-rational agents should have values that are much less idiosyncratic than ours. This seems to be a reductio ad absurdum of pre-rationality, unless there is an argument why we should apply the concept of pre-rationality only to our priors, and not to our utility functions. Or is anyone tempted to bite this bullet and claim that we <em>should</em> apply pre-rationality to our utility functions as well? (Note that if we were created by a rational creator, then we would have common utility functions.)</p>\n<p>The last question I want to address is one that I already raised in my previous post. Assuming that we do want to be pre-rational, how do we move from our current non-pre-rational state to a pre-rational one? This is somewhat similar to the question of how do we move from our current non-rational (according to ordinary rationality) state to a rational one. Expected utility theory says that we should act as if we are maximizing expected utility, but it doesn't say what we should do if we find ourselves lacking a prior and a utility function (i.e., if our actual preferences cannot be represented as maximizing expected utility).</p>\n<p>The fact that we don't have good answers for these questions perhaps shouldn't be considered fatal to pre-rationality and rationality, but it's troubling that little attention has been paid to them, relative to defining pre-rationality and rationality. (Why are rationality researchers more interested in knowing what rationality is, and less interested in knowing how to be rational? Also, BTW, why are there so few rationality researchers? Why aren't there hordes of people interested in these issues?)</p>\n<p>As I mentioned in the previous post, I have an idea here, which is to apply some concepts related to <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, in particular Nesov's <a href=\"/lw/102/indexical_uncertainty_and_the_axiom_of/sht\">trading across possible worlds</a> idea. As I see it now, pre-rationality is mostly about the (alleged) irrationality of disagreements between counterfactual versions of the same agent, when those disagreements are caused by irrelevant historical accidents such as the random assortment of genes. But how can such agents reach an agreement regarding what their beliefs <em>should </em>be, when they can't communicate with each other and coordinate physically? Well, at least in some cases, they may be able to coordinate logically. In my example of an AI whose prior was picked by the flip of a coin, the two counterfactual versions of the AI are similar enough to each other and symmetrical enough, for each to infer that if it were to change its prior from O or P to Q, where Q(A=heads)=0.5, the other AI would do the same, but this inference wouldn't be true for any Q' != Q, due to lack of symmetry.</p>\n<p>Of course, in the actual UDT, such \"changes of prior\" do not literally occur, because coordination and cooperation between possible worlds happen naturally as part of deciding acts and strategies, while one's preferences <a href=\"/lw/102/indexical_uncertainty_and_the_axiom_of/sk8\">stay constant</a>. Is that sufficient, or do we really need to change our preferences and make them pre-rational? I'm not sure.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5oDii8KKW53n4HSx4": 8}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J8LTE8CTQfyEMkhnc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 28, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "1808", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ahX9485o4kufEwCsY", "nW2EGC9XmsjEGsn9r"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 12, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-10T17:17:23.357Z", "modifiedAt": null, "url": null, "title": "Rationality advice from Terry Tao", "slug": "rationality-advice-from-terry-tao", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:45.030Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NcEqgAzcRYeEpbrDx/rationality-advice-from-terry-tao", "pageUrlRelative": "/posts/NcEqgAzcRYeEpbrDx/rationality-advice-from-terry-tao", "linkUrl": "https://www.lesswrong.com/posts/NcEqgAzcRYeEpbrDx/rationality-advice-from-terry-tao", "postedAtFormatted": "Tuesday, November 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20advice%20from%20Terry%20Tao&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20advice%20from%20Terry%20Tao%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcEqgAzcRYeEpbrDx%2Frationality-advice-from-terry-tao%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20advice%20from%20Terry%20Tao%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcEqgAzcRYeEpbrDx%2Frationality-advice-from-terry-tao", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNcEqgAzcRYeEpbrDx%2Frationality-advice-from-terry-tao", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 275, "htmlBody": "<p>Via a link on IRC, I stumbled upon the <a href=\"http://terrytao.wordpress.com/\">blog of the mathematician Terry Tao</a>. I noticed that several of his posts contain useful rationality advice, part of it overlapping with content that has been covered here. Most of the posts remind us of things that are kind of obvious, but I don't think that's necessarily a bad thing: we often need reminders of the things that are obvious.</p>\r\n<p>Advance warning: the posts are pretty well interlinked, in Wikipedia/TVTropes fashion. I currently have 15 tabs open from the site.</p>\r\n<p>Some posts of note:</p>\r\n<p><a href=\"http://terrytao.wordpress.com/career-advice/be-sceptical-of-your-own-work/\">Be sceptical of your own work</a>. If you unexpectedly find a problem solving itself almost effortlessly, and you can&rsquo;t quite see why, you should try to analyse your solution more sceptically. Most of the time, the process for solving a major problem is a lot more complex and time-consuming.</p>\r\n<p><a href=\"http://terrytao.wordpress.com/career-advice/use-the-wastebasket/\">Use the wastebasket</a>. Not every idea leads to a success, and not every first draft forms a good template for the final draft. Know when to start over from scratch, know when you should be persistent, and do keep copies around of even the failed attempts.</p>\r\n<p><a href=\"http://terrytao.wordpress.com/career-advice/learn-the-limitations-of-your-tools/\">Learn the limitations of your tools</a>. Knowing what your tools&nbsp;<em>cannot</em> do is just as important as knowing what they&nbsp;<em>can</em> do.</p>\r\n<p><a href=\"http://terrytao.wordpress.com/career-advice/learn-and-relearn-your-field/\">Learn and relearn your field</a>. Simply learning the statement and proof of a problem doesn't guarantee understanding: you should test your understanding, using methods such as finding alternate proofs and trying to generalize the argument.</p>\r\n<p><a href=\"http://terrytao.wordpress.com/career-advice/write-down-what-youve-done/\">Write down what you've done</a>. Write down sketches of any interesting arguments you come across - not necessarily at a publication level of quality, but detailed enough that you can forget about the details and reconstruct them later on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NcEqgAzcRYeEpbrDx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 22, "extendedScore": null, "score": 5.362823670346514e-07, "legacy": true, "legacyId": "1835", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-10T17:23:53.075Z", "modifiedAt": null, "url": null, "title": "Restraint Bias", "slug": "restraint-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:45.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vladimir_Nesov", "createdAt": "2009-02-27T09:55:13.458Z", "isAdmin": false, "displayName": "Vladimir_Nesov"}, "userId": "qf77EiaoMw7tH3GSr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Kh8JDtqvKoRBHJiXn/restraint-bias", "pageUrlRelative": "/posts/Kh8JDtqvKoRBHJiXn/restraint-bias", "linkUrl": "https://www.lesswrong.com/posts/Kh8JDtqvKoRBHJiXn/restraint-bias", "postedAtFormatted": "Tuesday, November 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Restraint%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARestraint%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKh8JDtqvKoRBHJiXn%2Frestraint-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Restraint%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKh8JDtqvKoRBHJiXn%2Frestraint-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKh8JDtqvKoRBHJiXn%2Frestraint-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 215, "htmlBody": "<p>Ed Yong over at <a href=\"http://scienceblogs.com/notrocketscience/\">Not Exactly Rocket Science</a> has an article on a study demonstrating \"restraint bias\" (<a href=\"http://www3.interscience.wiley.com/journal/122670295/abstract\">reference</a>), which seems like an important thing to be aware of in fighting akrasia:</p>\n<p><a href=\"http://scienceblogs.com/notrocketscience/2009/11/people_who_think_they_are_more_restrained_are_more_likely_to.php\">People who think they are more restrained are more likely to succumb to temptation</a></p>\n<blockquote>\n<p>In a series of four experiments, <a href=\"http://www.kellogg.northwestern.edu/Faculty/Directory/Nordgren_Loran.aspx\">Loran Nordgren</a> from Northwestern University showed that people suffer from a \"restraint bias\", where they overestimate their ability to control their own impulses. Those who fall prey to this fallacy most strongly are more likely to dive into tempting situations. Smokers, for example, who are trying to quit, are more likely to put themselves in situations if they think they're invulnerable to temptation. As a result, they're more likely to relapse.</p>\n</blockquote>\n<p>Thus, not only do people overestimate their abilities to carry out non-immediate plans (far-mode thinking, like in planning fallacy), but also the more confident ones turn out to be least able. This might have something to do with how <a href=\"/lw/z8/image_vs_impact_can_public_commitment_be/\">public commitment may be counterproductive</a>: once you've effectively signaled your intentions, the pressure to actually implement them fades away. Once you believe yourself to have asserted self-image of a person with good self-control, maintaining the actual self-control loses priority.</p>\n<p><strong>See also</strong>: <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">Akrasia</a>, <a href=\"http://wiki.lesswrong.com/wiki/Planning_fallacy\">Planning fallacy</a>, <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Near/far thinking</a>.</p>\n<p><strong>Related to</strong>: <a href=\"/lw/z8/image_vs_impact_can_public_commitment_be/\">Image vs. Impact: Can public commitment be counterproductive for achievement?</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Kh8JDtqvKoRBHJiXn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 21, "extendedScore": null, "score": 5.362835000608948e-07, "legacy": true, "legacyId": "1836", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ocujnKZL38thXn62"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-10T19:59:41.456Z", "modifiedAt": null, "url": null, "title": "What makes you YOU? For non-deists only.", "slug": "what-makes-you-you-for-non-deists-only", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:45.962Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cleonid", "createdAt": "2009-03-05T01:26:41.015Z", "isAdmin": false, "displayName": "cleonid"}, "userId": "Dh7Ax8Qp8bzp4xZBP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2hD9GoYBG3PwK5qK7/what-makes-you-you-for-non-deists-only", "pageUrlRelative": "/posts/2hD9GoYBG3PwK5qK7/what-makes-you-you-for-non-deists-only", "linkUrl": "https://www.lesswrong.com/posts/2hD9GoYBG3PwK5qK7/what-makes-you-you-for-non-deists-only", "postedAtFormatted": "Tuesday, November 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20makes%20you%20YOU%3F%20For%20non-deists%20only.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20makes%20you%20YOU%3F%20For%20non-deists%20only.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hD9GoYBG3PwK5qK7%2Fwhat-makes-you-you-for-non-deists-only%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20makes%20you%20YOU%3F%20For%20non-deists%20only.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hD9GoYBG3PwK5qK7%2Fwhat-makes-you-you-for-non-deists-only", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hD9GoYBG3PwK5qK7%2Fwhat-makes-you-you-for-non-deists-only", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 380, "htmlBody": "<p><span style=\"font-family: Times New Roman;\">\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Arial;\">From the dawn of civilization humans believed in eternal life. The flesh may rot, but the soul will be reborn. To save the soul from the potential adverse living conditions (e.g. hell), the body, being the transient and thus the less important part, was expected to make sacrifices. To accumulate the best possible karma, pleasures of the flesh had to be given up or at least heavily curtailed.</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Arial;\">Naturally the wisdom of this trade-off was questioned by many skeptical minds. The idea of reincarnation may have a strong appeal to imagination, but in absence of any credible evidence the Occam&rsquo;s razor mercilessly cuts it into pieces. Instead of sacrificing for the sake of the future incarnations, a rationalist should live for the present. But does he really?</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Arial;\">Consider the &ldquo;incarnations&rdquo; of the same person at different ages.<a id=\"more\"></a> Upon reaching the age of self-awareness, the earlier &ldquo;incarnations&rdquo; start making sacrifices for the benefit of the later ones. Dreams of becoming an astronaut at 25 may prompt a child of nine to exercise or study instead of playing. Upon reaching the age of 25, the same child may take a job at the bank and start saving for the potential retirement. Of course, legally all these &ldquo;incarnations&rdquo; are just the same person. But beyond jurisprudence, what is it that makes you who you are at the age of nine, twenty five or seventy? </span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Arial;\">Over the years your body, tastes, goals and the whole worldview are likely to undergo dramatic change. <span style=\"mso-spacerun: yes;\">&nbsp;</span>The single thing which remains essentially constant through your entire life is your DNA sequence. Through natural selection, evolution has ensured that we preferentially empathize with those whose DNA sequence is most similar to our own, i.e. our children, siblings and, most importantly, ourselves. But, instinct excepted, is there a reason why a rational self-conscious being must obey a program implanted in us by the unconscious force of evolution? </span><span style=\"font-family: Arial;\">If you identify more with your mind (personality/views/goals/&hellip;) than with the DNA sequence, why should you care more for someone who, living many years from now will resemble you less than some actual people living today?</span><span style=\"font-family: Arial;\">&nbsp;</span></p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\">&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin: 0in 0in 0pt;\"><span style=\"font-family: Arial;\">P.S. I am aware that the meaning of &ldquo;self&rdquo; was debated by philosophers for many years, but I am really curious about the personal answers of &ldquo;ordinary&rdquo; rationalists to this question.</span></p>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2hD9GoYBG3PwK5qK7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 2, "extendedScore": null, "score": 5.363106799904231e-07, "legacy": true, "legacyId": "1837", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-11T03:00:39.093Z", "modifiedAt": null, "url": null, "title": "Less Wrong Q&A with Eliezer Yudkowsky: Ask Your Questions", "slug": "less-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:06.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iLGrNTwTZivTX5774/less-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "pageUrlRelative": "/posts/iLGrNTwTZivTX5774/less-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "linkUrl": "https://www.lesswrong.com/posts/iLGrNTwTZivTX5774/less-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "postedAtFormatted": "Wednesday, November 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Ask%20Your%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Ask%20Your%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiLGrNTwTZivTX5774%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Ask%20Your%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiLGrNTwTZivTX5774%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiLGrNTwTZivTX5774%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-ask-your-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 310, "htmlBody": "<p>As <a href=\"/lw/1di/a_less_wrong_qa_with_eliezer_step_1_the/\">promised</a>, here is the \"Q\" part of the Less Wrong Video Q&amp;A with Eliezer Yudkowsky.</p>\n<p><strong>The Rules</strong></p>\n<p>1) One question per comment (to allow voting to carry more information about people's preferences).</p>\n<p>2) Try to be as clear and concise as possible. If your question can't be condensed to a few paragraphs, you should probably ask in a separate post. Make sure you have an actual question somewhere in there (you can bold it to make it easier to scan).</p>\n<p>3) Eliezer hasn't been subpoenaed. He will simply ignore the questions he doesn't want to answer, even if they somehow received&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \"><a href=\"/lw/kn/torture_vs_dust_specks/\">3^^^3</a></span>&nbsp;votes.</p>\n<p>4) If you reference certain things that are online in your question, provide a link.</p>\n<p>5) This thread will be open to questions and votes for at least 7 days. After that, it is up to Eliezer to decide when the best time to film his answers will be. [<strong>Update</strong>: Today, November 18, marks the 7th day since this thread was posted. If you haven't already done so, now would be a good time to review the questions and vote for your favorites.]<a id=\"more\"></a></p>\n<p><strong>Suggestions</strong></p>\n<p>Don't limit yourself to things that have been mentioned on OB/LW. I expect that this will be the majority of questions, but you shouldn't feel limited to these topics. I've always found that a wide variety of topics makes a Q&amp;A more interesting. If you're uncertain, ask anyway and let the voting sort out the wheat from the chaff.</p>\n<p>It's okay to attempt humor (but good luck, it's a tough crowd).</p>\n<p>If a discussion breaks out about a question (f.ex. to ask for clarifications) and the original poster decides to modify the question, the top level comment should be updated with the modified question (make it easy to find your question, don't have the latest version buried in a long thread).</p>\n<p><strong>Update: </strong>Eliezer's video answers to 30 questions from this thread can be found <a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DigEmY3RrF3XL5cwe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iLGrNTwTZivTX5774", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 4.1e-05, "legacy": true, "legacyId": "1840", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As <a href=\"/lw/1di/a_less_wrong_qa_with_eliezer_step_1_the/\">promised</a>, here is the \"Q\" part of the Less Wrong Video Q&amp;A with Eliezer Yudkowsky.</p>\n<p><strong id=\"The_Rules\">The Rules</strong></p>\n<p>1) One question per comment (to allow voting to carry more information about people's preferences).</p>\n<p>2) Try to be as clear and concise as possible. If your question can't be condensed to a few paragraphs, you should probably ask in a separate post. Make sure you have an actual question somewhere in there (you can bold it to make it easier to scan).</p>\n<p>3) Eliezer hasn't been subpoenaed. He will simply ignore the questions he doesn't want to answer, even if they somehow received&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; \"><a href=\"/lw/kn/torture_vs_dust_specks/\">3^^^3</a></span>&nbsp;votes.</p>\n<p>4) If you reference certain things that are online in your question, provide a link.</p>\n<p>5) This thread will be open to questions and votes for at least 7 days. After that, it is up to Eliezer to decide when the best time to film his answers will be. [<strong>Update</strong>: Today, November 18, marks the 7th day since this thread was posted. If you haven't already done so, now would be a good time to review the questions and vote for your favorites.]<a id=\"more\"></a></p>\n<p><strong id=\"Suggestions\">Suggestions</strong></p>\n<p>Don't limit yourself to things that have been mentioned on OB/LW. I expect that this will be the majority of questions, but you shouldn't feel limited to these topics. I've always found that a wide variety of topics makes a Q&amp;A more interesting. If you're uncertain, ask anyway and let the voting sort out the wheat from the chaff.</p>\n<p>It's okay to attempt humor (but good luck, it's a tough crowd).</p>\n<p>If a discussion breaks out about a question (f.ex. to ask for clarifications) and the original poster decides to modify the question, the top level comment should be updated with the modified question (make it easy to find your question, don't have the latest version buried in a long thread).</p>\n<p><strong>Update: </strong>Eliezer's video answers to 30 questions from this thread can be found <a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">here</a>.</p>", "sections": [{"title": "The Rules", "anchor": "The_Rules", "level": 1}, {"title": "Suggestions", "anchor": "Suggestions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "701 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 701, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["psuphwRHuJu9T9PPa", "3wYTFWY3LKQCnAptN", "Qyix5Z5YPSGYxf7GG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-11T22:03:38.439Z", "modifiedAt": null, "url": null, "title": "Test Your Calibration!", "slug": "test-your-calibration", "viewCount": null, "lastCommentedAt": "2022-01-10T09:21:27.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dciuB5nTG2o9PzJWe/test-your-calibration", "pageUrlRelative": "/posts/dciuB5nTG2o9PzJWe/test-your-calibration", "linkUrl": "https://www.lesswrong.com/posts/dciuB5nTG2o9PzJWe/test-your-calibration", "postedAtFormatted": "Wednesday, November 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Test%20Your%20Calibration!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATest%20Your%20Calibration!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdciuB5nTG2o9PzJWe%2Ftest-your-calibration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Test%20Your%20Calibration!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdciuB5nTG2o9PzJWe%2Ftest-your-calibration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdciuB5nTG2o9PzJWe%2Ftest-your-calibration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>In my journeys across the land, I have, to date, encountered four sets of <a href=\"http://en.wikipedia.org/wiki/Calibrated_probability_assessment\">probability calibration tests.</a> (If you just want to make bets on your predictions, you can use <a href=\"http://www.intrade.com\">Intrade</a> or another prediction market, but these generally don't record calibration data, only which of your bets paid out.) If anyone knows of other tests, please do mention them in the comments, and I'll add them to this post. To avoid spoilers, please do <em>not</em> post what you guessed for the calibration questions, or what the answers are.</p>\n<p>The first, to boast shamelessly, is my own, at <a href=\"http://www.acceleratingfuture.com/tom/?p=129\">http://www.acceleratingfuture.com/tom/?p=129</a>. My tests use fairly standard trivia questions (samples: \"George Washington actually fathered how many children?\", \"Who was Woody Allen's first wife?\", \"What was Paul Revere's occupation?\"), with an emphasis towards history and pop culture. The quizzes are scored automatically (by computer) and you choose whether to assign a probability of 96%, 90%, 75%, 50%, or 25% to your answer. There are five quizzes with fifty questions each: <a href=\"http://www.acceleratingfuture.com/tom/calibrate1.html\">Quiz #1</a>, <a href=\"http://www.acceleratingfuture.com/tom/calibrate2.html\">Quiz #2</a>, <a href=\"http://www.acceleratingfuture.com/tom/calibrate3.html\">Quiz #3</a>, <a href=\"http://www.acceleratingfuture.com/tom/calibrate4.html\">Quiz #4</a> and <a href=\"http://www.acceleratingfuture.com/tom/calibrate5.html\">Quiz #5</a>.<a id=\"more\"></a></p>\n<p>The second is a project by <a href=\"http://goodmorningeconomics.wordpress.com/\">John Salvatier</a> (<a href=\"/user/jsalvati/\">LW account</a>) of the University of Washington, at <a href=\"http://calibratedprobabilityassessment.org/\">http://calibratedprobabilityassessment.org/</a>. There are three sets of questions with fifty questions each; two sets of general trivia, and one set of questions about relative distances between American cities (the fourth set, unfortunately, does not appear to be working at this time). The questions do not rotate, but are re-ordered upon refreshing. The probabilities are again multiple choice, with ranges of 51-60%, 61-70%, 71-80%, 81-90%, and 91-100%, for whichever answer you think is more probable. These quizzes are also scored by computer, but instead of spitting back numbers, the computer generates a graph, showing the discrepancy between your real accuracy rate and your claimed accuracy rate. Links: <a href=\"http://calibratedprobabilityassessment.org/test.php?category=1\">US cities</a>, <a href=\"http://calibratedprobabilityassessment.org/test.php?category=2\">trivia #1</a>, <a href=\"http://calibratedprobabilityassessment.org/test.php?category=4\">trivia #2</a>.</p>\n<p>The third is a quiz by Steven Smithee of <a href=\"http://www.acceleratingfuture.com/steven/\">Black Belt Bayesian</a> (<a href=\"/user/steven0461\">LW account here</a>) at <a href=\"http://www.acceleratingfuture.com/steven/?p=96\">http://www.acceleratingfuture.com/steven/?p=96.</a> There are three sets, of five questions each, about history, demographics, and Google rankings, and two sets of (non-testable) questions about the future and historical counterfactuals. (EDIT: Steven has built three more tests in addition to this one, at <a href=\"http://www.acceleratingfuture.com/steven/?p=102\">http://www.acceleratingfuture.com/steven/?p=102</a>, <a href=\"http://www.acceleratingfuture.com/steven/?p=106\">http://www.acceleratingfuture.com/steven/?p=106</a>, and <a href=\"http://www.acceleratingfuture.com/steven/?p=136\">http://www.acceleratingfuture.com/steven/?p=136</a>). This test must be graded manually, and the answers are in one of the comments below the test (don't look at the comments if you don't want spoilers!).</p>\n<p>The fourth is a website by <a href=\"http://tricycledevelopments.com/\">Tricycle Developments</a>, the web developers who built Less Wrong, at <a href=\"http://predictionbook.com/\">http://predictionbook.com/</a>. You can make your own predictions about real-world events, or bet on other people's predictions, at whatever probability you want, and the website records how often you were right relative to the probabilities you assigned. However, since all predictions are made in advance of real-world events, it may take quite a while (on the order of months to years) before you can find out how accurate you were.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 1, "DWWZwkxTJs4d5WrcX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dciuB5nTG2o9PzJWe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 25, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "1844", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-12T19:35:45.496Z", "modifiedAt": "2020-11-25T20:42:43.743Z", "url": null, "title": "Anti-Akrasia Technique: Structured Procrastination", "slug": "anti-akrasia-technique-structured-procrastination", "viewCount": null, "lastCommentedAt": "2012-01-14T02:21:19.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrissimo", "createdAt": "2009-03-01T21:01:34.487Z", "isAdmin": false, "displayName": "patrissimo"}, "userId": "jimxrRCsNY7PfcM6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n5Yfhygz42QNK2vFe/anti-akrasia-technique-structured-procrastination", "pageUrlRelative": "/posts/n5Yfhygz42QNK2vFe/anti-akrasia-technique-structured-procrastination", "linkUrl": "https://www.lesswrong.com/posts/n5Yfhygz42QNK2vFe/anti-akrasia-technique-structured-procrastination", "postedAtFormatted": "Thursday, November 12th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anti-Akrasia%20Technique%3A%20Structured%20Procrastination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnti-Akrasia%20Technique%3A%20Structured%20Procrastination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Yfhygz42QNK2vFe%2Fanti-akrasia-technique-structured-procrastination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anti-Akrasia%20Technique%3A%20Structured%20Procrastination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Yfhygz42QNK2vFe%2Fanti-akrasia-technique-structured-procrastination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Yfhygz42QNK2vFe%2Fanti-akrasia-technique-structured-procrastination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 844, "htmlBody": "<p>This idea has been mentioned in several comments, but it deserves a top-level post. &nbsp;From an ancient, ancient web article (1995!), Stanford philosophy professor&nbsp;<a href=\"http://web.archive.org/web/20070613061354/www.structuredprocrastination.com/\">John Perry writes</a>:</p>\n<blockquote>\n<p>I have been intending to write this essay for months. Why am I finally doing it? Because I finally found some uncommitted time? Wrong. I have papers to grade, textbook orders to fill out, an NSF proposal to referee, dissertation drafts to read. I am working on this essay as a way of not doing all of those things. This is the essence of what I call structured procrastination, an amazing strategy I have discovered that converts procrastinators into effective human beings, respected and admired for all that they can accomplish and the good use they make of time. All procrastinators put off things they have to do. Structured procrastination is the art of making this bad trait work for you. The key idea is that procrastinating does not mean doing absolutely nothing. Procrastinators seldom do absolutely nothing; they do marginally useful things, like gardening or sharpening pencils or making a diagram of how they will reorganize their files when they get around to it. Why does the procrastinator do these things? Because they are a way of not doing something more important. If all the procrastinator had left to do was to sharpen some pencils, no force on earth could get him do it. However, the procrastinator can be motivated to do difficult, timely and important tasks, as long as these tasks are a way of not doing something more important.</p>\n</blockquote>\n<p>The insightful observation that procrastinators fill their time with effort, not staring at the walls, gives rise to this form of akrasia aikido, where the urge to not do something is cleverly redirected into productivity. &nbsp;If you can \"waste time\" by doing useful things, while feeling like you are avoiding doing the \"real work\", then you avoid depleting your <a href=\"http://www.futurepundit.com/archives/006569.html\">limited supply of willpower</a>&nbsp;(which happens when you force yourself to do something).</p>\n<p>In other words, structured procrastination (SP) is an efficient use of this limited resource, because doing A in order to avoid doing B is easier than making yourself do A. &nbsp;If A is something you want to get done, then the less willpower you can use to do it, the more you will be to accomplish. &nbsp;This only works if A is something that you do want to get done - that's how SP differs from normal procrastination, of course.</p>\n<p><a id=\"more\"></a>Like most information works, I am constantly distracted by social networks - reading Twitter, blogs, answering email. &nbsp;I don't do these things because they are effortless and restful (that's what reading fiction is for), but because they feel like moderately productive work (learning new ideas about the world, keeping up on my ideasphere, connecting with people) that just so happens to be fun and easy. &nbsp;Unfortunately, that joy and ease translates into a distorted feeling about if/whether/how much I am accomplishing things. &nbsp;The marginal output product of my spending 15 minutes on social networks may be positive but it's close to zero compared to other kinds of work I could be doing.</p>\n<p>SP suggests that I find other things which both feel like productive avoidance and actually are. &nbsp;For example, rather than reading blogs, I could read one of the dozens of books piled up that have been suggested as relevant to my areas of research. &nbsp;Yeah, that wouldn't be as much fun as digesting the clever little bites that are blog posts, but it still feels like avoiding the main unpleasant tasks, and it's actually important enough to be on my todo list (if not at the top).</p>\n<p>Maybe this is just me applying my standard rationality themes everywhere, but I think that self-awareness and action vs. reaction are key to structured procrastination. &nbsp;When you are reacting to the vague feeling that you want to avoid doing something, you will automatically get driven towards quick and easy fixes - leave the report you are supposed to write in one window, and go to Google Reader in another. &nbsp;Anything to scratch the itch of avoidance.</p>\n<p>But if you can have the self-awareness to notice your reaction of avoidance, you get to roll a saving throw about whether to make a conscious decision. &nbsp;If you pass, now you have a choice: buckle down, go with the distraction, or do structured procrastination? &nbsp;This choice opportunity doesn't automatically let you do the right thing - doing your primary task will still require a major expenditure of willpower. &nbsp;But at least it stops you from automatically doing the wrong thing, and gives you a chance to use akrasia aikido like SP to apply your willpower efficiently. &nbsp;Or perhaps go for a truly restful option like taking a break, going outside, or getting a drink, which actually rests your mind (and willpower muscle) much more effectively than looking at the latest on Digg.</p>\n<p>There is more nuance to structured procrastination, but this post has gotten long enough already. &nbsp;If people find the topic interesting, I can write more about it's weaknesses and my ideas for addressing them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 2, "dqx5k65wjFfaiJ9sQ": 2, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n5Yfhygz42QNK2vFe", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 58, "extendedScore": null, "score": 0.0001240900149620808, "legacy": true, "legacyId": "1850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 53, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-11-12T19:35:45.496Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-13T07:05:34.276Z", "modifiedAt": null, "url": null, "title": "Boston meetup Nov 15 (and others)", "slug": "boston-meetup-nov-15-and-others", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:45.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MvDsCGk94qr3bFZYh/boston-meetup-nov-15-and-others", "pageUrlRelative": "/posts/MvDsCGk94qr3bFZYh/boston-meetup-nov-15-and-others", "linkUrl": "https://www.lesswrong.com/posts/MvDsCGk94qr3bFZYh/boston-meetup-nov-15-and-others", "postedAtFormatted": "Friday, November 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boston%20meetup%20Nov%2015%20(and%20others)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoston%20meetup%20Nov%2015%20(and%20others)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvDsCGk94qr3bFZYh%2Fboston-meetup-nov-15-and-others%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boston%20meetup%20Nov%2015%20(and%20others)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvDsCGk94qr3bFZYh%2Fboston-meetup-nov-15-and-others", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvDsCGk94qr3bFZYh%2Fboston-meetup-nov-15-and-others", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>So far, only one of the less-wrong meet-ups that were discussed has been scheduled.&nbsp; The Boston meet-up was scheduled for:</p>\n<p style=\"padding-left: 30px;\">Carberry's at 74 Prospect St Cambridge, MA<br />(1.5 blocks northeast from the Central Square T station)<br />Sunday November 15th at 2pm</p>\n<p>though it may move after an hour or two to the Clear Conscience Cafe a couple blocks away if things get too crowded.&nbsp;</p>\n<p>My cell number is (610) 213 2487 so you can contact me if there is a problem.</p>\n<p>Regarding Philly, Florida and New Orleans there is still need for detail in the schedule.&nbsp; I'm leaving New Orleans at 5:10 on the 14th so the 13th is probably better but I can do early on the 14th if people want.&nbsp; There has been some interest in an event there but I would appreciate more interested people saying so and possibly contacting me via phone or email.&nbsp; If several people are interested we will have a meet-up.&nbsp; Just one or two and I can meet less formally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MvDsCGk94qr3bFZYh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 5.369299072783233e-07, "legacy": true, "legacyId": "1849", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-13T10:32:46.512Z", "modifiedAt": null, "url": null, "title": "Consequences of arbitrage: expected cash", "slug": "consequences-of-arbitrage-expected-cash", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:46.630Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CzbvB4dsLNzLzeeot/consequences-of-arbitrage-expected-cash", "pageUrlRelative": "/posts/CzbvB4dsLNzLzeeot/consequences-of-arbitrage-expected-cash", "linkUrl": "https://www.lesswrong.com/posts/CzbvB4dsLNzLzeeot/consequences-of-arbitrage-expected-cash", "postedAtFormatted": "Friday, November 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consequences%20of%20arbitrage%3A%20expected%20cash&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsequences%20of%20arbitrage%3A%20expected%20cash%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCzbvB4dsLNzLzeeot%2Fconsequences-of-arbitrage-expected-cash%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consequences%20of%20arbitrage%3A%20expected%20cash%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCzbvB4dsLNzLzeeot%2Fconsequences-of-arbitrage-expected-cash", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCzbvB4dsLNzLzeeot%2Fconsequences-of-arbitrage-expected-cash", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 868, "htmlBody": "<p>I prefer the movie <a href=\"http://en.wikipedia.org/wiki/12_Monkeys\">Twelve Monkeys</a> to Akira. I prefer <a href=\"http://en.wikipedia.org/wiki/Akira_%28film%29\">Akira</a> to David Attenborough's Life in the Undergrowth. And I prefer David Attenborough's <a href=\"http://en.wikipedia.org/wiki/Life_in_the_Undergrowth\">Life in the Undergrowth</a> to Twelve Monkeys.</p>\n<p>I have intransitive preferences. But I don't suffer from this intransitivity. Up until the moment I'm confronted by an <a href=\"http://www.undertheiceberg.com/wp-content/uploads/2006/04/hindu-god_e1.jpg\">avatar</a> of the <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">money pump</a>, juggling the three DVD boxes in front of me with a greedy gleam in his eye. He'll <a href=\"http://en.wikipedia.org/wiki/Arbitrage\">arbitrage</a> me to death unless I snap out of my intransitive preferences and banish him by putting my options in order.</p>\n<p>Arbitrage, in the broadest sense, means picking up free money - money that is free because of other people's preferences. <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\"></a>Money pumps are a form of arbitrage, exploiting the lack of consistency, transitivity or independence in people's preferences. In most cases, arbitrage ultimately destroys itself: people either wise up to the exploitation and get rid of their vulnerabilities, or lose all their money, leaving only players who are not vulnerable to arbitrage. The crash and burn of the <a href=\"http://en.wikipedia.org/wiki/Long-Term_Capital_Management\">Long-Term Capital Management</a> hedge fund was due in part to the diminishing returns of their arbitrage strategies.</p>\n<p>Most humans to not react to the possibility of being arbitraged by changing their whole preference systems. Instead they cling to their old preferences as much as possible, while keeping a keen eye out to avoid being taken advantage of. They keep their inconsistent, intransitive, dependent systems but end up behaving consistently, transitively and independently in their most common transactions.</p>\n<p>The weaknesses of this approach are manifest. Having one system of preferences but acting as if we had another is a great strain on our poor overloaded brains. To avoid the arbitrage, we need to scan present and future deals with great keenness and insight, always on the lookout for traps. Since transaction costs shield us from most of the negative consequences of imperfect decision theories, we have to be especially vigilant as transaction costs continue to drop, meaning that opportunities to be arbitraged will continue to rise in future. Finally, how we exit the trap of arbitrage depends on how we entered it: if my juggling Avatar had started me on Life in the Undergrowth, I'd have ended up with Twelve Monkeys, and refused the next trade. If he'd started me on Twelve Monkeys, I've had ended up with Akira. These may not have been the options I'd have settled on if I'd taken the time to sort out my preferences ahead of time.</p>\n<p><a id=\"more\"></a>For these reasons, it is much wiser to change our decision theory ahead of time to something that doesn't leave us vulnerable to arbitrage, rather than clinging nominally to our old preferences.</p>\n<p>Inconsistency or intransitivity leaves us vulnerable to a <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">strong money pump</a>, so these we should avoid. Violating independence leaves us vulnerable to a <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">weak money pump</a>, which also means giving up free money, so this should be avoided too. Along with completeness (meaning you can actually decide between options) and the <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/18jb\">technical assumption</a> of continuity, these make up the von Neumann-Morgenstern <a href=\"http://en.wikipedia.org/wiki/Expected_utility#von_Neumann-Morgenstern_formulation\">axioms</a> of expected utility. Thus if we want to avoid being arbitraged, we should cleave to expected utility.</p>\n<p>But the consequences of arbitrage do not stop there.</p>\n<p>Quick, which would you prefer, &yen;10 000 with certainty, or a 50% chance of getting &yen;20 000? Well, it depends on how your utility scales with cash. If it scales concavely, then you are risk averse, while if it scales convexly, then... Stop. Minus the transaction costs, those two options are worth exactly the same thing. If they are freely tradable, then you can exchange them one for one on the world market. Hence if you price the 50% contract at any value other than &yen;10 000, you can be arbitraged if you act on your preferences (neglecting transaction costs). People selling to or buying contracts from you will make instant free money on the trade. Money that would be yours instead if your preferences were other.</p>\n<p>Of course, you could keep your non-linear utility, and just behave as if it were linear, because of the market price, while being risk-averse in secret... But just as before, this is cumbersome, complicated and unnecessary. Exactly as arbitrage makes you cleave to independence, it will make your utility linear in money - at least for small, freely tradable amounts.</p>\n<p>In conclusion:</p>\n<ul>\n<li>Avoiding arbitrage forces your decision theory to follow the axioms of expected utility. It further forces your utility to be linear for any small quantity of money (or any other fungible asset). Thus you will follow expected cash.</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>Addendum:</strong> If contracts such as L = {&yen;20 000 if a certain coin comes up heads/tails} were freely tradable, they would cost &yen;10 000.</p>\n<p><em>Proof:</em> Let L<sub>H</sub> be the contract that gives out &yen;20 000 if that coin comes out heads; L<sub>T</sub> be the contract if that same coin comes out tails. L<sub>H</sub> and L<sub>T</sub> together are exactly the same as a guaranteed &yen;20 000. However, individually, L<sub>H</sub> and L<sub>T</sub> are the same contract - 50% chance of &yen;20 000 - thus by the <a href=\"http://en.wikipedia.org/wiki/Law_of_one_price\">Law of One Price</a>, they must have the same price (you can get the same result by symmetry). Two contracts with the same price, totalling &yen;20 000 together: they must individualy be worth &yen;10 000.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2YcmB6SLtHnHRe3uX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CzbvB4dsLNzLzeeot", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "1834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTN6bLWqpwWn2i4qZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-15T05:29:16.003Z", "modifiedAt": null, "url": null, "title": "Auckland meet up Saturday Nov 28th", "slug": "auckland-meet-up-saturday-nov-28th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:38.144Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndrewH", "createdAt": "2009-03-02T15:31:43.543Z", "isAdmin": false, "displayName": "AndrewH"}, "userId": "FgKt6dBPfyiku5Qh8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AnGR4v3oDRAkyhdqu/auckland-meet-up-saturday-nov-28th", "pageUrlRelative": "/posts/AnGR4v3oDRAkyhdqu/auckland-meet-up-saturday-nov-28th", "linkUrl": "https://www.lesswrong.com/posts/AnGR4v3oDRAkyhdqu/auckland-meet-up-saturday-nov-28th", "postedAtFormatted": "Sunday, November 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Auckland%20meet%20up%20Saturday%20Nov%2028th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAuckland%20meet%20up%20Saturday%20Nov%2028th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnGR4v3oDRAkyhdqu%2Fauckland-meet-up-saturday-nov-28th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Auckland%20meet%20up%20Saturday%20Nov%2028th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnGR4v3oDRAkyhdqu%2Fauckland-meet-up-saturday-nov-28th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnGR4v3oDRAkyhdqu%2Fauckland-meet-up-saturday-nov-28th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>For any New Zealanders out there, there is a tentative meet up at the <a href=\"http://www.mezzebar.co.nz/location.htm\">Messe bar</a> at 2pm on the 28th of November (Saturday). Just in case you are shy, there will be at least two people there!</p>\n<p>Write a comment and/or please contact me on my cell: 021 039 8554, if you are interested in coming.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AnGR4v3oDRAkyhdqu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 5.374169189591429e-07, "legacy": true, "legacyId": "1860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-15T19:37:18.935Z", "modifiedAt": null, "url": null, "title": "The Academic Epistemology Cross Section: Who Cares More About Status?", "slug": "the-academic-epistemology-cross-section-who-cares-more-about", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:46.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TEiR8u3wS6GyaoqRd/the-academic-epistemology-cross-section-who-cares-more-about", "pageUrlRelative": "/posts/TEiR8u3wS6GyaoqRd/the-academic-epistemology-cross-section-who-cares-more-about", "linkUrl": "https://www.lesswrong.com/posts/TEiR8u3wS6GyaoqRd/the-academic-epistemology-cross-section-who-cares-more-about", "postedAtFormatted": "Sunday, November 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Academic%20Epistemology%20Cross%20Section%3A%20Who%20Cares%20More%20About%20Status%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Academic%20Epistemology%20Cross%20Section%3A%20Who%20Cares%20More%20About%20Status%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTEiR8u3wS6GyaoqRd%2Fthe-academic-epistemology-cross-section-who-cares-more-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Academic%20Epistemology%20Cross%20Section%3A%20Who%20Cares%20More%20About%20Status%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTEiR8u3wS6GyaoqRd%2Fthe-academic-epistemology-cross-section-who-cares-more-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTEiR8u3wS6GyaoqRd%2Fthe-academic-epistemology-cross-section-who-cares-more-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 367, "htmlBody": "<p>Bryan Caplan <a title=\"http://econlog.econlib.org/archives/2009/11/why_arent_acade.html\" href=\"http://econlog.econlib.org/archives/2009/11/why_arent_acade.html\">writes</a>:</p>\n<blockquote>\n<p>Almost all economic models assume that human beings are&nbsp;<a style=\"text-decoration: none; border-top-style: none; border-right-style: none; border-bottom-style: none; border-left-style: none; border-width: initial; border-color: initial; color: #4444be; font-weight: bold; \" href=\"http://yudkowsky.net/rational/bayes\">Bayesians</a>...&nbsp;&nbsp;It is striking, then, to realize that academic economists are not Bayesians.&nbsp; And they're proud of it!<br /><br />This is clearest for theorists.&nbsp; Their epistemology is simple: Either something has been (a) proven with certainty, or (b) no one knows - and no intellectually respectable person will say more...&nbsp;<br /><br />Empirical economists' deviation from Bayesianism is more subtle.&nbsp; Their epistemology is rooted in classical statistics.&nbsp; The respectable researcher comes to the data an agnostic, and leaves believing \"whatever the data say.\"&nbsp; When there's no data that meets their standards, they mimic the theorists' snobby agnosticism.&nbsp; If you mention \"common sense,\" they'll scoff.&nbsp; If you remind them that even classical statistics assumes that you can trust the data - and the scholars who study it - they harumph.</p>\n</blockquote>\n<p>Robin Hanson offers an <a title=\"http://www.overcomingbias.com/2009/11/why-academics-are-not-bayesian.html\" href=\"http://www.overcomingbias.com/2009/11/why-academics-are-not-bayesian.html\">explanation</a>:</p>\n<blockquote>\n<p>I&rsquo;ve argued that the main social&nbsp;<a style=\"color: #314c85;\" href=\"http://www.overcomingbias.com/2009/07/academias-function.html\">function of academia</a>&nbsp;is to let students, patrons, readers, etc. affiliate with credentialed-as-impressive minds.&nbsp; If so, academic beliefs are secondary &ndash; the important thing is to clearly show respect to those who make impressive displays like theorems or difficult data analysis.&nbsp; And the obvious way for academics to use their beliefs to show respect for impressive folks is to have academic beliefs track the most impressive recent academic work.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 12px; margin-left: 0px; padding: 0px;\">...beliefs must stay fixed until an impressive enough theorem or data analysis comes along that beliefs should change out of respect for that new display.&nbsp; It also won&rsquo;t do to keep beliefs pretty much the same when each new study hardly adds much evidence &ndash; that wouldn&rsquo;t offer enough respect to the new display.</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 12px; margin-left: 0px; padding: 0px;\">I wonder, what does this look like in the cross section? &nbsp;In other words, relative to other academic disciplines, which have the strongest tendency to celebrate difficult work but ignore sound-yet-unimpressive work? &nbsp;My hunch is that economics, along with most other social sciences, would be the worst offenders, while the fields closer to engineering will be on the other end of the spectrum. &nbsp;Engineers should be more concerned with truth since whatever they build has to, you know, <em>work</em>. &nbsp;What say you? &nbsp;More importantly, anyone have any evidence?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TEiR8u3wS6GyaoqRd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 16, "extendedScore": null, "score": 5.375654417174968e-07, "legacy": true, "legacyId": "1861", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-16T20:26:09.545Z", "modifiedAt": null, "url": null, "title": "BHTV: Yudkowsky / Robert Greene", "slug": "bhtv-yudkowsky-robert-greene", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:32.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NB7o4ZJ2DALHpRjyr/bhtv-yudkowsky-robert-greene", "pageUrlRelative": "/posts/NB7o4ZJ2DALHpRjyr/bhtv-yudkowsky-robert-greene", "linkUrl": "https://www.lesswrong.com/posts/NB7o4ZJ2DALHpRjyr/bhtv-yudkowsky-robert-greene", "postedAtFormatted": "Monday, November 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20Yudkowsky%20%2F%20Robert%20Greene&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20Yudkowsky%20%2F%20Robert%20Greene%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNB7o4ZJ2DALHpRjyr%2Fbhtv-yudkowsky-robert-greene%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20Yudkowsky%20%2F%20Robert%20Greene%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNB7o4ZJ2DALHpRjyr%2Fbhtv-yudkowsky-robert-greene", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNB7o4ZJ2DALHpRjyr%2Fbhtv-yudkowsky-robert-greene", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p><a href=\"http://bloggingheads.tv/diavlogs/23870\">Latest Bloggingheads</a> up with Robert Greene, bestselling author of <em>The 48 Laws of Power</em> and most recently <em>The 50th Law</em> (\"Fear nothing\").</p>\n<p>Excellent commentary/summary by Andy McKenzie <a href=\"http://andymckenzie.blogspot.com/2009/11/nine-eyrgbhtv-thoughts.html\">here</a>.</p>\n<p>The most important piece of advice I got from <em>The 50th Law</em> was \"always attack before you are ready\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NB7o4ZJ2DALHpRjyr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 5.378261904764831e-07, "legacy": true, "legacyId": "1867", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-16T21:27:43.855Z", "modifiedAt": null, "url": null, "title": "Why (and why not) Bayesian Updating?", "slug": "why-and-why-not-bayesian-updating", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:48.537Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W6nXfmKTrgaiaLSRg/why-and-why-not-bayesian-updating", "pageUrlRelative": "/posts/W6nXfmKTrgaiaLSRg/why-and-why-not-bayesian-updating", "linkUrl": "https://www.lesswrong.com/posts/W6nXfmKTrgaiaLSRg/why-and-why-not-bayesian-updating", "postedAtFormatted": "Monday, November 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20(and%20why%20not)%20Bayesian%20Updating%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20(and%20why%20not)%20Bayesian%20Updating%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6nXfmKTrgaiaLSRg%2Fwhy-and-why-not-bayesian-updating%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20(and%20why%20not)%20Bayesian%20Updating%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6nXfmKTrgaiaLSRg%2Fwhy-and-why-not-bayesian-updating", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW6nXfmKTrgaiaLSRg%2Fwhy-and-why-not-bayesian-updating", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 575, "htmlBody": "<blockquote>\n<p>the use of Bayesian belief updating with expected utility maximization may be just an approximation that is only relevant in special situations which meet certain independence assumptions around the agent's actions.</p>\n</blockquote>\n<p>&mdash; <a href=\"/lw/15m/towards_a_new_decision_theory/11kc\">Steve Rayhawk</a></p>\n<p>For those who <a href=\"/lw/17c/outlawing_anthropics_an_updateless_dilemma/13sl\">aren't sure</a> of the need for an <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">updateless decision theory</a>, the paper <a href=\"http://web.econ.unito.it/gma/dynsavagepg.pdf\">Revisiting Savage in a conditional world</a> by Paolo Ghirardato might help convince you. (Although that's probably not the intention of the author!) The paper gives a set of 7 axioms, based on Savage's axioms, which is necessary and sufficient for an agent's preferences in a dynamic decision problem to be represented as expected utility maximization with <a href=\"http://wiki.lesswrong.com/wiki/Updating\">Bayesian belief updating</a>. This helps us see in exactly which situations Bayesian updating works and why. (In many other axiomatizations of decision theory, the updating part is left out, and only expected utility maximization is derived in a static setting.)</p>\n<p><a id=\"more\"></a>A key assumption is Axiom 7, which the author calls \"Consequentialism\". I won't try to reproduce the mathematical notation here (see the page numbered 88 in this <a href=\"http://web.econ.unito.it/gma/dynsavagepg.pdf\">ungated PDF</a>), but here's the informal explanation given in the paper:</p>\n<blockquote>\n<p>This axiom says that the preference conditional on non-null A should not depend<br />on how the strategy f behaves in the counterfactual states of A<sup>c</sup> (in other words,<br />it should only depend on the truncation f|<sub>A</sub>).</p>\n</blockquote>\n<p>This axiom is clearly violated in Vladmir Nesov's <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugging</a> counter-example to Bayesian updating.</p>\n<p><a href=\"/lw/182/the_absentminded_driver/\">Another example</a> that I used to motivate UDT involves indexical uncertainty. In Ghirardato's framework it's relatively easy to see what goes wrong when we try to apply it to indexical uncertainty. In that case, \"states\" in the formalism would have to be centered possible worlds, in other words an ordinary world-state plus a location. But if A above is a set of centered possible worlds, then after learning A, your preferences can still depend on how strategies behave in A<sup>C</sup> since elements of A and A<sup>C</sup> may belong to the same possible world.</p>\n<p>If there is demand, I can try to give an informal/intuitive explanation of why Bayesian updating works (in the situations where it does). I was about to attempt that when I decided to do a Google search and found this paper.</p>\n<p>P.S., I noticed a curiosity about Bayesian updating while thinking about it in the context of decision theory, and this seems like a good opportunity to point it out. In Ghirardato's decision theory, after learning A, you should use P<sub>A</sub> to compute expected utilities, where P<sub>A</sub>(x) is the <em>conditional </em>probability of x given A, or P(A &cap; x)/P(A). This apparently shows the relevance of Bayesian updating, but we get an equivalent theory if we instead define P<sub>A</sub>(x) as the <em>joint </em>probability of A and x, or just P(A &cap; x). (Because when you compute the expected utilities of two choices f and g, upon learning A, the factor 1/P(A) enters into both computations the same way and can be removed without changing relative rankings.) The division by P(A) in the original definition seems to serve no purpose except to make P<sub>A</sub> sum to 1.</p>\n<p>So, theoretically, we don't need Bayesian updating even if our preferences do satisfy the Ghirardato axioms. We could use a decision procedure where our beliefs about something can only get weaker, and never any stronger, no matter what evidence we see, and that would be equivalent. Since that seems to be computationally cheaper (by avoiding the division operation), why do our beliefs not actually work like that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W6nXfmKTrgaiaLSRg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 26, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "1866", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GfHdNfqxe3cSCfpHL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-16T22:25:01.011Z", "modifiedAt": null, "url": null, "title": "Efficient prestige hypothesis", "slug": "efficient-prestige-hypothesis", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:52.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p2ZPrf5PLuwLy2Ee3/efficient-prestige-hypothesis", "pageUrlRelative": "/posts/p2ZPrf5PLuwLy2Ee3/efficient-prestige-hypothesis", "linkUrl": "https://www.lesswrong.com/posts/p2ZPrf5PLuwLy2Ee3/efficient-prestige-hypothesis", "postedAtFormatted": "Monday, November 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Efficient%20prestige%20hypothesis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEfficient%20prestige%20hypothesis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2ZPrf5PLuwLy2Ee3%2Fefficient-prestige-hypothesis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Efficient%20prestige%20hypothesis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2ZPrf5PLuwLy2Ee3%2Fefficient-prestige-hypothesis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp2ZPrf5PLuwLy2Ee3%2Fefficient-prestige-hypothesis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 246, "htmlBody": "<p>There's a <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">contrarian theory</a> presented by Robin that people go to highly reputable schools, visit highly reputable hospitals, buy highly reputable brands etc. to affiliate with high status individuals and institutions.</p>\n<p>But what would a person who completely didn't care about such affiliations do? Pretty much the same thing. Unless you know a lot about schools, hospitals, and everything else, you're better off simply following prestige as proxy for quality (in addition to price and all the other usual criteria). There's no denying that prestige is better indicator of quality than random chance - the question is - is it the best we can do?</p>\n<p>It's possible to come up with alternative measures, which might correlate with quality too, like operation success rates for hospitals, graduation rates for schools etc. But if they really indicated quality that well, wouldn't they be simply included in institution's prestige, and lose their predictive status? The argument is highly analogous to one for efficient market hypothesis (or to some extent with Bayesian beauty contest with schools, as prestige might indicate quality of other students). Very often there are severe faults with alternative measures, like with operation success rates without correcting for patient demographics.</p>\n<p>If you postulate that you have better indicator of quality than prestige, you need to do some explaining. Why is it not included in prestige already? I don't propose any magical thinking about prestige, but we shouldn't be as eager to throw it away completely as some seem to be.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p2ZPrf5PLuwLy2Ee3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 21, "extendedScore": null, "score": 5.37847205351409e-07, "legacy": true, "legacyId": "1868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-17T14:15:52.346Z", "modifiedAt": null, "url": null, "title": "A Less Wrong singularity article?", "slug": "a-less-wrong-singularity-article", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:02.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LvHfEMKxGjpD6ggC7/a-less-wrong-singularity-article", "pageUrlRelative": "/posts/LvHfEMKxGjpD6ggC7/a-less-wrong-singularity-article", "linkUrl": "https://www.lesswrong.com/posts/LvHfEMKxGjpD6ggC7/a-less-wrong-singularity-article", "postedAtFormatted": "Tuesday, November 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Less%20Wrong%20singularity%20article%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Less%20Wrong%20singularity%20article%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLvHfEMKxGjpD6ggC7%2Fa-less-wrong-singularity-article%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Less%20Wrong%20singularity%20article%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLvHfEMKxGjpD6ggC7%2Fa-less-wrong-singularity-article", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLvHfEMKxGjpD6ggC7%2Fa-less-wrong-singularity-article", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 374, "htmlBody": "<p><a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">Robin criticizes Eliezer</a> for not having written up his arguments about the Singularity in a standard style and submitted them for publication. <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/19cx\">Others</a>, <a href=\"http://rationalmorality.info/?p=112\">too</a>, make the same complaint: the arguments involved are covered over such a huge mountain of posts that it's impossible for most outsiders to seriously evaluate them. This is a problem for both those who'd want to critique the concept, and for those who tentatively agree and would want to learn more about it.</p>\r\n<p>Since it appears (do correct me if I'm wrong!) that Eliezer doesn't currently consider it worth the time and effort to do this, why not <a href=\"http://en.wikipedia.org/wiki/Crowdsourcing\">enlist the LW community</a> in summarizing his arguments the best we can and submit them somewhere once we're done? Minds and Machines will be having a <a href=\"http://ieet.org/index.php/IEET/more/cfpmm09/\">special issue on transhumanism, cognitive enhancement and AI</a>, with a deadline for submission in January; that seems like a good opportunity for the paper. Their call for papers is asking for submissions that are around 4000 to 12 000 words.</p>\r\n<p>The paper should probably</p>\r\n<ul>\r\n<li>Briefly mention some of the previous work about AI being near enough to be worth consideration (Kurzweil, maybe Bostrom's <a href=\"http://www.nickbostrom.com/superintelligence.html\">paper</a> on the subject, etc.), but not dwell on it; this is a paper on the <em>consequences</em> of AI.</li>\r\n<li>Devote maybe little less than half of its actual content to the issue of FOOM, providing arguments and references for building the case of a hard takeoff.</li>\r\n<li><del>Devote the second half to discussing the question of FAI, with references to e.g. Joshua Greene's <a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-Dissertation.pdf\">thesis</a> and other relevant sources for establishing this argument.</del> <em><a href=\"/lw/1fz/a_less_wrong_singularity_article/19lq\">Carl Shulman says</a> SIAI is already working on a separate paper on this, so it'd be better for us to concentrate merely on the FOOM aspect.</em></li>\r\n<li>Build on the content of Eliezer's various posts, taking their primary arguments and making them stronger by reference to various peer-reviewed work.</li>\r\n<li>Include as authors everyone who made major contributions to it and wants to be mentioned; certainly make (again, assuming he doesn't object) Eliezer as the lead author, since this is his work we're seeking to convert into more accessible form.</li>\r\n</ul>\r\n<p>I have created a <a href=\"http://wiki.lesswrong.com/wiki/Singularity_paper\">wiki page</a> for the draft version of the paper. Anyone's free to edit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"CztjQPSTuaQcfbyh8": 2, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LvHfEMKxGjpD6ggC7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 31, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "1871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 215, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-19T23:31:46.859Z", "modifiedAt": null, "url": null, "title": "Request For Article: Many-Worlds Quantum Computing", "slug": "request-for-article-many-worlds-quantum-computing", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:48.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pre", "createdAt": "2009-02-27T14:35:32.511Z", "isAdmin": false, "displayName": "pre"}, "userId": "XCwdovczssgYqBwT2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iS4PGGHk4hNJ4vQWj/request-for-article-many-worlds-quantum-computing", "pageUrlRelative": "/posts/iS4PGGHk4hNJ4vQWj/request-for-article-many-worlds-quantum-computing", "linkUrl": "https://www.lesswrong.com/posts/iS4PGGHk4hNJ4vQWj/request-for-article-many-worlds-quantum-computing", "postedAtFormatted": "Thursday, November 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20For%20Article%3A%20Many-Worlds%20Quantum%20Computing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20For%20Article%3A%20Many-Worlds%20Quantum%20Computing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS4PGGHk4hNJ4vQWj%2Frequest-for-article-many-worlds-quantum-computing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20For%20Article%3A%20Many-Worlds%20Quantum%20Computing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS4PGGHk4hNJ4vQWj%2Frequest-for-article-many-worlds-quantum-computing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiS4PGGHk4hNJ4vQWj%2Frequest-for-article-many-worlds-quantum-computing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>Through a path more tortuous than is worth describing, I ended up talking to friends about the quantum effects which are exploited by photosynthesis. There's an <a href=\"http://www.lbl.gov/Science-Articles/Archive/PBD-quantum-secrets.html\">article describing the topic we were talking about here</a>.</p>\n<p>The article describes how quantum effects allow the molecular machinary of the chloroplasts to \"simultaneously sample all the potential  energy pathways and choose the most efficient one.\"</p>\n<p>Which is essentially how Quantum Computing is usually described in the press too, only we get to set what we mean by \"most efficient\" to be \"best solution to this problem\".</p>\n<p>Since I usually find myself arguing that \"there is no wave collapse,\" the conversation has lead me to trying to picture how this \"exploring\" can happen unless there is also some \"pruning\" at the end of it.</p>\n<p>Of course even in the Copenhagen Interpretation <span class=\"postbody\"><span class=\"postbody\">\"wave collapse\" <span style=\"font-style: italic;\">always</span> happens in accordance with the probabilities described by the wave function, so presumably the system is engineered in such a way as to make that \"most efficient\" result the most probable <span style=\"font-style: italic;\">according to those equations</span>. <br /> <br />It's not somehow consistently picking results from the far end of the bell-curve of probable outcomes. It's just engineered so that bell-curve is centred on the most efficient outcomes. <br /> <br />There's no 'collapse', it's just that the system has been set up in such a way that the most likely and therefore common universes have the property that the energy is transferred. <br /> <br />Or something. Dunno.</span></span></p>\n<p>Can someone write an article describing how quantum computing works from a many-words perspective rather than the explore-and-then-prune perspective that it seems every press article I've ever read on the topic uses?</p>\n<p>Pretty please?</p>\n<p>I'd like to read that.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iS4PGGHk4hNJ4vQWj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 5.386173546808778e-07, "legacy": true, "legacyId": "1880", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-20T20:10:11.447Z", "modifiedAt": null, "url": null, "title": "The One That Isn't There", "slug": "the-one-that-isn-t-there", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.180Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Annoyance", "createdAt": "2009-02-28T04:06:40.600Z", "isAdmin": false, "displayName": "Annoyance"}, "userId": "yEm6LWatswJYGwBFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ZcGtMqMeJbNDsjg5/the-one-that-isn-t-there", "pageUrlRelative": "/posts/3ZcGtMqMeJbNDsjg5/the-one-that-isn-t-there", "linkUrl": "https://www.lesswrong.com/posts/3ZcGtMqMeJbNDsjg5/the-one-that-isn-t-there", "postedAtFormatted": "Friday, November 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20One%20That%20Isn't%20There&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20One%20That%20Isn't%20There%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZcGtMqMeJbNDsjg5%2Fthe-one-that-isn-t-there%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20One%20That%20Isn't%20There%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZcGtMqMeJbNDsjg5%2Fthe-one-that-isn-t-there", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ZcGtMqMeJbNDsjg5%2Fthe-one-that-isn-t-there", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 809, "htmlBody": "<blockquote><address>Q:&nbsp; What's the most important leg of a three-legged stool?</address> <address>A:&nbsp; The one that isn't there.</address> <address>- traditional joke-riddle</address></blockquote>\n<p>A specific neurological lesion can sometimes damage or impair specific neurological functions without touching others.&nbsp; In the condition famously known as \"Ondine's Curse\", for example, automatic control of breathing is destroyed while conscious control remains, so that without modern medical intervention nerve-damaged patients can survive only as long as they can remain awake.&nbsp; Such conditions are nevertheless unusual exceptions to the more general principle that complex, recently-developed, and 'meta'-functions (those that monitor and control others) are first to be impaired and lost when the nervous system is stressed, damaged, or altered.<a id=\"more\"></a></p>\n<p>Demonstrations of this principle can be found by examining such phenomena as reversion under stress, oxygen deprivation, sleep deprivation, and various sorts of poisoning - most especially drugs with a gradual effect on nervous function.&nbsp; The primary reason it is considered necessary to have designated drivers who refrain from consumption of alcohol is that drinkers frequently underestimate the degree to which they're affected by alcohol.&nbsp; Long before slowed reflexes and grossly impaired judgment become evident, the cognitive functions responsible for self-evaluation are dulled, and self-control diminished.&nbsp; A drinker who believes that they're capable of driving safely may or may not be correct, even if their judgments would normally be trustworthy.&nbsp; Similar effects are found with other types of intoxication - people who say that they drive better after smoking marijuana have been shown to in fact drive more poorly.&nbsp; The more demanding and complicated the mental task is, the more likely it will be disrupted by any interfering factor, leading to poor performance.&nbsp;</p>\n<blockquote>\n<p><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: x-small;\">The natural human's an animal without logic. Your projections of logic onto all affairs is unnatural, but suffered to continue for its usefulness. You're the embodiment of logic--a Mentat. Yet, your problem solutions are concepts that, in a very real sense, are projected outside yourself, there to be studied and rolled around, examined from all sides.\" <br /> \"You think now to teach me my trade?\" he asked, and he did not try to hide the disdain in his voice. <br /> \"The finest Mentats have a healthy respect for the error factor in their computations,\" she said.</span></p>\n<p><span style=\"font-family: Arial,Helvetica,sans-serif; font-size: x-small;\">- exchange between the Lady Jessica and Thufir Hawat;&nbsp; Frank Herbert,&nbsp; <em>Dune</em></span></p>\n</blockquote>\n<p>Once a certain level of intelligence has been reached, any cognitive process can be emulated by any mind - it's merely a question of available storage space and speed.&nbsp; The amount of processing capacity rapidly becomes immaterial.&nbsp; What's important is not how powerful a mind is, but how well it detects, compensates for, and corrects its own errors.</p>\n<p>It would be convenient if this capacity, which is difficult to gauge, were clearly associated with general intelligence.&nbsp; Unfortunately, that doesn't seem to be the case.&nbsp; We know that the self-regulatory functions of cognition can be completely destroyed without affecting such things as IQ scores.&nbsp; Thus, high IQ does not serve as a reliable guide to the presence of higher cognitive functions. &nbsp; Furthermore, my experience with smart people strongly suggests that they are less likely to develop that capacity.&nbsp; Being cleverer than the people around them, they are more likely to be able to craft invalid yet convincing arguments that others can't counter or respond to.&nbsp; They have no need to develop stringent self-evaluation to accomplish social goals, and it's very easy to convince themselves that they've chosen the correct course of action.&nbsp; What's worse, they're more likely to be able to craft clever arguments which convince themselves - and then, secure in the knowledge of their cleverness, they become less likely to check and re-check their reasoning.&nbsp; Average people have more experience of being shown to be wrong, and often have developed a greater willingness to lack confidence in their conclusions.&nbsp; This is both a strength and a weakness, but the strength cannot be acquired otherwise while the vulnerability can be compensated for.</p>\n<blockquote>\n<p>\"The first principle is that you must not fool yourself - and you are the easiest person to fool.\" - Richard P. Feynman</p>\n</blockquote>\n<p>Rationality, like reading or arithmetic, is a skill alien to the human mind.&nbsp; Useful, certainly, but not natural.&nbsp; Development of the capacity for rationality requires strict adherence to a set of formal principles, and such adherence requires advanced self-evaluation to be maintained.&nbsp; Otherwise practitioners will quickly convince themselves that short-circuited thinking really is valid.&nbsp; If it's a terrible thing to believe your own propaganda, it's even worse to never realize you're issuing propaganda in the first place.</p>\n<p>The principles of rationality aren't difficult.&nbsp; What's hard is to implement them consistently and completely; our older, better-developed tendencies to associate our way through a problem and accept or reject statements on the palatability of their consequences, tend to override our better judgment.</p>\n<p>How, then, can we develop the ability to put rational thought into practice?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Wi3EopKJ2aNdtxSWg": 1, "ac84EpK6mZbPLzmqj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ZcGtMqMeJbNDsjg5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 18, "extendedScore": null, "score": 4.4e-05, "legacy": true, "legacyId": "1883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-21T04:53:32.443Z", "modifiedAt": null, "url": null, "title": "Calibration for continuous quantities", "slug": "calibration-for-continuous-quantities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:50.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cyan", "createdAt": "2009-02-27T22:31:08.528Z", "isAdmin": false, "displayName": "Cyan"}, "userId": "eGtDNuhj58ehX9Wgf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/otrGbtkDahSePWrRW/calibration-for-continuous-quantities", "pageUrlRelative": "/posts/otrGbtkDahSePWrRW/calibration-for-continuous-quantities", "linkUrl": "https://www.lesswrong.com/posts/otrGbtkDahSePWrRW/calibration-for-continuous-quantities", "postedAtFormatted": "Saturday, November 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Calibration%20for%20continuous%20quantities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACalibration%20for%20continuous%20quantities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotrGbtkDahSePWrRW%2Fcalibration-for-continuous-quantities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Calibration%20for%20continuous%20quantities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotrGbtkDahSePWrRW%2Fcalibration-for-continuous-quantities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FotrGbtkDahSePWrRW%2Fcalibration-for-continuous-quantities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 848, "htmlBody": "<p>Related to: <a href=\"/lw/15d/calibration_fail/\">Calibration fail</a>, <a href=\"/lw/1f8/test_your_calibration/\">Test Your Calibration!</a></p>\r\n<p>Around here, calibration is mostly approached on a discrete basis: for example, the <a href=\"http://yudkowsky.net/rational/technical\">Technical Explanation of Technical Explanations</a> talks only about discrete distributions, and the commonly linked tests and surveys are either explicitly discrete or offer only coarsely binned probability assessments. For continuous distributions (or \"smooth\" distributions over discrete quantities like dates of historical events, dollar amounts on the order of hundreds of thousands, populations of countries, or any <em>actual </em>measurement of a continuous quantity), we can apply a finer-grained assessment of calibration.</p>\r\n<p>The problem of assessing calibration for continuous quantities is that our distributions can have very dissimilar shapes, so there doesn't seem to be a common basis for comparing one to another. As an example, I'll give some subjective (i.e., withdrawn from my nether regions) distributions for the populations of two countries, Canada and Botswana. I live in Canada, so I have years of dimly remembered geography classes in elementary school and high school to inform my guess. In the case of Botswana, I have only my impressions of the nation from Alexander McCall Smith's excellent <a href=\"http://en.wikipedia.org/wiki/The_No._1_Ladies%27_Detective_Agency\">No. 1 Ladies' Detective Agency series</a> and my general knowledge of Africa.</p>\r\n<p>For Canada's population, I'll set my distribution to be a <a href=\"http://en.wikipedia.org/wiki/Normal_distribution\">normal distribution</a> centered at 32 million with a standard deviation of 2 million. For Botswana's population, my initial gut feeling is that it is a nation of about 2 million people.&nbsp;I'll put 50% of my probability mass between 1 and 2 million, and the other 50% of my probability mass between 2 million and 10 million. Because I think that values closer to 2 million are more plausible than values at the extremes, I'll make each chunk of 50% mass a right-angle <a href=\"http://en.wikipedia.org/wiki/Triangular_distribution\">triangular distribution</a>. Here are plots of the probability densities:</p>\r\n<p><a id=\"more\"></a></p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_0.png\" alt=\"\" width=\"480\" height=\"480\" />&nbsp;</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_1.png\" alt=\"\" width=\"480\" height=\"480\" /></p>\r\n<p>(These distributions are pretty rough approximations to my highest quality assessments. I don't hold, as the above&nbsp;normal distribution implies, that my probability that the population of Canada is less than 30 million is 16%, because I'm fairly sure that's what it was when I was starting university; nor would I assign a strictly nil chance to the proposition that Botswana's population is outside the interval from 1 million to 10 million. But the above distributions will do for expository purposes.)</p>\r\n<p>For true values we'll take the 2009 estimates listed in Wikipedia. For&nbsp;Canada, that number is 33.85 million; for Botswana, it's 1.95 million. (The fact that my Botswana estimate was so spot on shocked me.) Given these \"true values\", how can we judge the calibration of the above distributions? The above densities seem so dissimilar that it's hard to conceive of a common basis on which they have some sort of equivalence. Fortunately, such a basis does exist: we take the total probability mass below the true value, as shown on the plots below.</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_3.png?v=8ce3001b3524497dd0b0fe356b749f36\" alt=\"\" width=\"480\" height=\"480\" /></p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_2.png?v=55acb8466360aadb8f6d6bf19e3a996f\" alt=\"\" width=\"480\" height=\"480\" /></p>\r\n<p>No matter what the shape of the probability density function,&nbsp;the random variable&nbsp;\"probability mass below the realized value\" always has a uniform distribution&nbsp;on the interval&nbsp;from zero to one, a fact known as the <em>probability integral transform</em><sup>1</sup>. (ETA: <a href=\"/lw/15g/calibration_for_continuous_quantities/19vk\">This comment</a> gives a demonstration with no integrals.) My Canada example has a probability integral transform value (henceforth PIT value) of 0.82, and my Botswana example has a PIT value of 0.45.</p>\r\n<p><strong>It's the PITs</strong></p>\r\n<p>If we have a large collection of probability distributions and realized values, we can assess the calibration of those distributions by checking to see if the distribution of PIT values is uniform, e.g., by taking a histogram. For correctly calibrated distributions, such a histogram will look like this:</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_4.png?v=95a63a759ca6581abb585880b69fe12d\" alt=\"\" width=\"480\" height=\"480\" />&nbsp;</p>\r\n<p>The above histogram and the ones below show 10,000 realizations; the red line gives the exact&nbsp;density of which the histogram is a noisy version.</p>\r\n<p>Figuring out that your distributions are systematically biased in one direction or another is simple: the&nbsp;median PIT value won't be 0.5. Let's suppose that the distributions are not biased in that sense and take a look at some&nbsp;PIT value histograms that are poorly calibrated because the precisions are wrong. The plot below shows the case were the realized values follow&nbsp;the standard normal distribution and the&nbsp;assessed distributions are&nbsp;also normal but are over-confident to the extent that the ostensible 95% central intervals are actually 50% central intervals. (This degree of over-confidence was chosen because I seem to recall reading on OB that if you ask a subject matter&nbsp;expert for a 95% interval, you tend to get back a reasonable 50% interval. Anyone know a citation for that? ETA: <a href=\"/lw/15g/calibration_for_continuous_quantities/1a6m\">My recollection was faulty</a>.)</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_5.png?v=e4be67cce0fb88e13c5d5ea6363bd500\" alt=\"\" width=\"480\" height=\"480\" /></p>\r\n<p>I wouldn't expect humans to be under-confident, but algorithmically generated distributions might be, so for completeness's sake I'll show that too. In the case below, both the assessed distributions and&nbsp;the distribution of realized values are again normal distributions, but now the ostensible 50% intervals actually contain 95% of the realized values.</p>\r\n<p><img src=\"http://images.lesswrong.com/t3_15g_6.png?v=10106e3c11a5a909f7b577c48f29ffb6\" alt=\"\" width=\"480\" height=\"480\" /></p>\r\n<p><strong>Take-home message</strong></p>\r\n<p>We need not use coarse binning to&nbsp;check if a collection of distributions for continuous quantities is properly calibrated; instead, we can use the probability integral transform to set all the distributions on a common basis for comparison.</p>\r\n<p><sup>1</sup>&nbsp;It can be <a href=\"http://www.jstor.org/pss/2132726\">demonstrated</a>&nbsp;(for distributions that have densities, anyway) using <a href=\"http://en.wikipedia.org/wiki/Integration_by_substitution\">integration by substitution</a> and the derivative of the inverse of a function (listed in <a href=\"http://en.wikipedia.org/wiki/Table_of_derivatives#General_differentiation_rules\">this table of derivatives</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "otrGbtkDahSePWrRW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 30, "extendedScore": null, "score": 5.389272021903239e-07, "legacy": true, "legacyId": "1492", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Related to: <a href=\"/lw/15d/calibration_fail/\">Calibration fail</a>, <a href=\"/lw/1f8/test_your_calibration/\">Test Your Calibration!</a></p>\n<p>Around here, calibration is mostly approached on a discrete basis: for example, the <a href=\"http://yudkowsky.net/rational/technical\">Technical Explanation of Technical Explanations</a> talks only about discrete distributions, and the commonly linked tests and surveys are either explicitly discrete or offer only coarsely binned probability assessments. For continuous distributions (or \"smooth\" distributions over discrete quantities like dates of historical events, dollar amounts on the order of hundreds of thousands, populations of countries, or any <em>actual </em>measurement of a continuous quantity), we can apply a finer-grained assessment of calibration.</p>\n<p>The problem of assessing calibration for continuous quantities is that our distributions can have very dissimilar shapes, so there doesn't seem to be a common basis for comparing one to another. As an example, I'll give some subjective (i.e., withdrawn from my nether regions) distributions for the populations of two countries, Canada and Botswana. I live in Canada, so I have years of dimly remembered geography classes in elementary school and high school to inform my guess. In the case of Botswana, I have only my impressions of the nation from Alexander McCall Smith's excellent <a href=\"http://en.wikipedia.org/wiki/The_No._1_Ladies%27_Detective_Agency\">No. 1 Ladies' Detective Agency series</a> and my general knowledge of Africa.</p>\n<p>For Canada's population, I'll set my distribution to be a <a href=\"http://en.wikipedia.org/wiki/Normal_distribution\">normal distribution</a> centered at 32 million with a standard deviation of 2 million. For Botswana's population, my initial gut feeling is that it is a nation of about 2 million people.&nbsp;I'll put 50% of my probability mass between 1 and 2 million, and the other 50% of my probability mass between 2 million and 10 million. Because I think that values closer to 2 million are more plausible than values at the extremes, I'll make each chunk of 50% mass a right-angle <a href=\"http://en.wikipedia.org/wiki/Triangular_distribution\">triangular distribution</a>. Here are plots of the probability densities:</p>\n<p><a id=\"more\"></a></p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_0.png\" alt=\"\" width=\"480\" height=\"480\">&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_1.png\" alt=\"\" width=\"480\" height=\"480\"></p>\n<p>(These distributions are pretty rough approximations to my highest quality assessments. I don't hold, as the above&nbsp;normal distribution implies, that my probability that the population of Canada is less than 30 million is 16%, because I'm fairly sure that's what it was when I was starting university; nor would I assign a strictly nil chance to the proposition that Botswana's population is outside the interval from 1 million to 10 million. But the above distributions will do for expository purposes.)</p>\n<p>For true values we'll take the 2009 estimates listed in Wikipedia. For&nbsp;Canada, that number is 33.85 million; for Botswana, it's 1.95 million. (The fact that my Botswana estimate was so spot on shocked me.) Given these \"true values\", how can we judge the calibration of the above distributions? The above densities seem so dissimilar that it's hard to conceive of a common basis on which they have some sort of equivalence. Fortunately, such a basis does exist: we take the total probability mass below the true value, as shown on the plots below.</p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_3.png?v=8ce3001b3524497dd0b0fe356b749f36\" alt=\"\" width=\"480\" height=\"480\"></p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_2.png?v=55acb8466360aadb8f6d6bf19e3a996f\" alt=\"\" width=\"480\" height=\"480\"></p>\n<p>No matter what the shape of the probability density function,&nbsp;the random variable&nbsp;\"probability mass below the realized value\" always has a uniform distribution&nbsp;on the interval&nbsp;from zero to one, a fact known as the <em>probability integral transform</em><sup>1</sup>. (ETA: <a href=\"/lw/15g/calibration_for_continuous_quantities/19vk\">This comment</a> gives a demonstration with no integrals.) My Canada example has a probability integral transform value (henceforth PIT value) of 0.82, and my Botswana example has a PIT value of 0.45.</p>\n<p><strong id=\"It_s_the_PITs\">It's the PITs</strong></p>\n<p>If we have a large collection of probability distributions and realized values, we can assess the calibration of those distributions by checking to see if the distribution of PIT values is uniform, e.g., by taking a histogram. For correctly calibrated distributions, such a histogram will look like this:</p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_4.png?v=95a63a759ca6581abb585880b69fe12d\" alt=\"\" width=\"480\" height=\"480\">&nbsp;</p>\n<p>The above histogram and the ones below show 10,000 realizations; the red line gives the exact&nbsp;density of which the histogram is a noisy version.</p>\n<p>Figuring out that your distributions are systematically biased in one direction or another is simple: the&nbsp;median PIT value won't be 0.5. Let's suppose that the distributions are not biased in that sense and take a look at some&nbsp;PIT value histograms that are poorly calibrated because the precisions are wrong. The plot below shows the case were the realized values follow&nbsp;the standard normal distribution and the&nbsp;assessed distributions are&nbsp;also normal but are over-confident to the extent that the ostensible 95% central intervals are actually 50% central intervals. (This degree of over-confidence was chosen because I seem to recall reading on OB that if you ask a subject matter&nbsp;expert for a 95% interval, you tend to get back a reasonable 50% interval. Anyone know a citation for that? ETA: <a href=\"/lw/15g/calibration_for_continuous_quantities/1a6m\">My recollection was faulty</a>.)</p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_5.png?v=e4be67cce0fb88e13c5d5ea6363bd500\" alt=\"\" width=\"480\" height=\"480\"></p>\n<p>I wouldn't expect humans to be under-confident, but algorithmically generated distributions might be, so for completeness's sake I'll show that too. In the case below, both the assessed distributions and&nbsp;the distribution of realized values are again normal distributions, but now the ostensible 50% intervals actually contain 95% of the realized values.</p>\n<p><img src=\"http://images.lesswrong.com/t3_15g_6.png?v=10106e3c11a5a909f7b577c48f29ffb6\" alt=\"\" width=\"480\" height=\"480\"></p>\n<p><strong id=\"Take_home_message\">Take-home message</strong></p>\n<p>We need not use coarse binning to&nbsp;check if a collection of distributions for continuous quantities is properly calibrated; instead, we can use the probability integral transform to set all the distributions on a common basis for comparison.</p>\n<p><sup>1</sup>&nbsp;It can be <a href=\"http://www.jstor.org/pss/2132726\">demonstrated</a>&nbsp;(for distributions that have densities, anyway) using <a href=\"http://en.wikipedia.org/wiki/Integration_by_substitution\">integration by substitution</a> and the derivative of the inverse of a function (listed in <a href=\"http://en.wikipedia.org/wiki/Table_of_derivatives#General_differentiation_rules\">this table of derivatives</a>).</p>", "sections": [{"title": "It's the PITs", "anchor": "It_s_the_PITs", "level": 1}, {"title": "Take-home message", "anchor": "Take_home_message", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["54jkJNpPyMRRGeme8", "dciuB5nTG2o9PzJWe"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-22T14:22:52.368Z", "modifiedAt": null, "url": null, "title": "Friedman on Utility", "slug": "friedman-on-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AmCvCsRiu2PvLmNiF/friedman-on-utility", "pageUrlRelative": "/posts/AmCvCsRiu2PvLmNiF/friedman-on-utility", "linkUrl": "https://www.lesswrong.com/posts/AmCvCsRiu2PvLmNiF/friedman-on-utility", "postedAtFormatted": "Sunday, November 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friedman%20on%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriedman%20on%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmCvCsRiu2PvLmNiF%2Ffriedman-on-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friedman%20on%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmCvCsRiu2PvLmNiF%2Ffriedman-on-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAmCvCsRiu2PvLmNiF%2Ffriedman-on-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 33, "htmlBody": "<p>I just came across an essay David Friedman posted last Monday <a href=\"http://daviddfriedman.blogspot.com/2009/11/ambiguity-of-utility.html\">The Ambiguity of Utility</a> that presents one of the problems I have with using utilities as the foundation of some \"rational\" morality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AmCvCsRiu2PvLmNiF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 3, "extendedScore": null, "score": 5.39280976342915e-07, "legacy": true, "legacyId": "1889", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-23T03:32:08.789Z", "modifiedAt": null, "url": null, "title": "Rational lies", "slug": "rational-lies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.487Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qSg2BZdjGbQJdpa3K/rational-lies", "pageUrlRelative": "/posts/qSg2BZdjGbQJdpa3K/rational-lies", "linkUrl": "https://www.lesswrong.com/posts/qSg2BZdjGbQJdpa3K/rational-lies", "postedAtFormatted": "Monday, November 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20lies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20lies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqSg2BZdjGbQJdpa3K%2Frational-lies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20lies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqSg2BZdjGbQJdpa3K%2Frational-lies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqSg2BZdjGbQJdpa3K%2Frational-lies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1149, "htmlBody": "<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; font-style: normal; font-weight: normal; widows: 2; orphans: 2;\"><span style=\"background-color: #ffffff; font-size: medium;\"> </span></p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">If I were sitting opposite a psychopath who had a particular sensitivity about ants, and I knew that if I told him that ants have six legs then he would jump up and start killing the surrounding people, then it would be difficult to justify telling him my wonderful fact about ants, regardless of whether I believe that ants really have six legs or not.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Or suppose I knew my friend's wife was cheating on him, but I also knew that he was terminally ill and would die within the next few weeks. The question of whether or not to inform him of my knowledge is genuinely complex, and the truth or falsity of my knowledge about his wife is only one factor in the answer. Different people may disagree about the correct course of action, but no-one would claim that the <em>only</em> relevant fact is the truth of the statement that his wife is cheating on him.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">This is all a standard result of <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">expected utility maximization</a>, of course. Vocalizing or otherwise communicating a belief is itself an action, and just like any other action it has a set of possible outcomes, to which we assign probabilities as well as some utility within our value coordinates. We then average out the utilities over the possible outcomes for each action, weighted by the probability that they will actually happen, and choose the action that maximizes this expected utility. Well, that's the gist of the situation, anyway. Much has been written on this site about the implications of expected utility maximization under <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">more exotic conditions</a> such as <a href=\"/lw/ps/where_physics_meets_experience/\">mind splitting and merging</a>, but I'm going to be talking about more mundane situations, and the point I want to make is that beliefs are very different objects from the act of communicating those beliefs.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\"><a id=\"more\"></a></p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">This distinction is particularly easy to miss as the line between belief and communication becomes subtler. Suppose that a friend of mine has built a wing suit and is about to jump off the empire state building with the belief that he will fly gracefully through the sky. Since I care about my friend's well-being I try to explain to him the concepts of gravity and aerodynamics, and the effect it will have on him if he launches himself from the building. Examining my decision in detail, I have placed a high probability on his death if he jumps off the building, and calculated that, since I value his well-being, my expected utility would not be maximized by him making the leap.&nbsp;</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">But now suppose that my friend is particularly dull and unable or unwilling to grasp the concept of aerodynamics, and is hence unswayed by my argument. Having reasonably explained my beliefs to him, am I absolved of the moral responsibility to save him? Not from a utilitarian standpoint, since there are other courses of action available to me. I could, for example, tell him that his wing suit has been sabotaged by aliens --- a line of reasoning that I happen to know he'll believe given his predisposition towards X files-esque conspiracy theories.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Would doing so be contrary to my committed rationalist stance? Not at all; I have rationally analysed the options available to me and rationally chosen a course of action. The conditions for the communication of a belief to be deemed rational are exactly the same decision theoretic conditions applicable to any other action: namely that of being the expected utility maximizer.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">If this all sounds too close to \"tell people what they need to hear\" then let's ask under what specific conditions it might be rational to lie. Clearly this depends on your values. If your utility function places high value on people falling to their death then you will tend to lie about gravity and aerodynamics as much as possible. However, for the purpose of practical rationality I'm going to assume for the rest of this article that some of your basic values align with my own, such as the value of fulfilled human existence, and so on.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Convincing somebody of a falsehood will, on average, lead to them making poorer decisions according to their values. My soon-to-be-airborne friend may be convinced not to leap from the building immediately, but may shortly return with a wing suit covered in protective aluminium foil to ward off those nasty interfering aliens. <a href=\"/lw/k1/no_one_can_exempt_you_from_rationalitys_laws/\">Nobody is exempt from the laws of rationality</a>. To the extent that their values align with mine, convincing another of a falsehood will have at least this one negative consequence with respect to my own values. The examples I gave above are specific situations in which other factors dominate my desire\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nfor another person to be better informed during the pursuit of their goals, but such situations are the exception rather than the rule. All other things equal, lying to an agent with similar values to mine is a bad decision.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Had I actually convinced my friend of the nature of gravity and aerodynamics rather than spinning a story about aliens then next time he may return to the rooftop with a parachute rather than a tin foil wing suit. In the example I gave, this course of action was unlikely to succeed, but again this situation is the exception rather than the rule. In general, a true statement has the potential to improve the recipient's <a href=\"/lw/jl/what_is_evidence/\">brain/universe entanglement</a> and thereby improve his potential for achieving his goals, which, if his values align with my own, constitutes at least one factor in favour of truth-telling. All other things equal, telling the truth is a good decision.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">This doesn't mean that telling the truth is valuable only in terms of its benefits to me. My own values include bettering the lives of others, so achieving \"my goals\" constitutes working towards the good of others, as well as my own.&nbsp;</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Is there any other sense in which truth-telling may be considered a \"good\" in its own right? Naively one might argue that the act of uttering a truth could itself be a value in its own right, but such a utility function would be maximized by a universe tiled with tape players broadcasting mundane, true facts about the universe. It would be about as well-aligned with the values of typical human being as a paper clip maximizer.&nbsp;</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">It's a more reasonable position for rationality in others to be included among one's fundamental values. This, I feel, is more closely aligned with my own value. All other things equal, I would like those around me to be rational. Not just to live in a society of rationalists, though this is an orthogonal value. Not just to engage in interesting, stimulating discussion, though this is also an orthogonal value. And not just for others to succeed in achieving their goals, though this, again, is an orthogonal value. But to actually maximize the <a href=\"/lw/jl/what_is_evidence/\">brain/universe entanglement</a> of others, for its own sake.</p>\n<p class=\"western\" style=\"margin-left: 0.21cm; margin-right: 0.21cm; margin-top: 0.21cm; widows: 2; orphans: 2;\">Do you value rationality in others for its own sake?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qSg2BZdjGbQJdpa3K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 6, "extendedScore": null, "score": 5.394200526565927e-07, "legacy": true, "legacyId": "1890", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szfxvS8nsxTgJLBHs", "WajiC3YWeJutyAXTn", "eY45uCCX7DdwJ4Jha", "6s3xABaXKPdFwA3FS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-23T15:03:05.628Z", "modifiedAt": null, "url": null, "title": "In conclusion: in the land beyond money pumps lie extreme events", "slug": "in-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/54NL4R6xsXwYca6az/in-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "pageUrlRelative": "/posts/54NL4R6xsXwYca6az/in-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "linkUrl": "https://www.lesswrong.com/posts/54NL4R6xsXwYca6az/in-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "postedAtFormatted": "Monday, November 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20conclusion%3A%20in%20the%20land%20beyond%20money%20pumps%20lie%20extreme%20events&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20conclusion%3A%20in%20the%20land%20beyond%20money%20pumps%20lie%20extreme%20events%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54NL4R6xsXwYca6az%2Fin-conclusion-in-the-land-beyond-money-pumps-lie-extreme%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20conclusion%3A%20in%20the%20land%20beyond%20money%20pumps%20lie%20extreme%20events%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54NL4R6xsXwYca6az%2Fin-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F54NL4R6xsXwYca6az%2Fin-conclusion-in-the-land-beyond-money-pumps-lie-extreme", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>In a previous <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">article</a> I've demonstrated that you can only avoid money pumps and arbitrage by using the von Neumann-Morgenstern <a href=\"http://en.wikipedia.org/wiki/Expected_utility#von_Neumann-Morgenstern_formulation\">axioms</a> of expected utility. I argued in this <a href=\"/lw/1ey/consequences_of_arbitrage_expected_cash/\">post</a> that even if you're not likely to face a money pump on one particular decision, you should still use expected utility (and <a href=\"/lw/1ey/consequences_of_arbitrage_expected_cash/19ew\">sometimes</a> expected money), because of the difficulties of combining two decision theories and constantly being on the look-out for which one to apply.</p>\n<p>Even if you don't care about (weak) money pumps, expected utility sneaks in under much milder conditions. If you have a quasi-utility function (i.e. you have an underlying utility function, but you also care about the shape of the probability distribution), then this <a href=\"/lw/1d5/expected_utility_without_the_independence_axiom/\">post</a> demonstrates that you should generally stick with expected utility anyway, just by aggregating all your decisions.</p>\n<p>So the moral of looking at money pumps, arbitrage and aggregation is that you should use expected utility for nearly all your decisions.</p>\n<p>But the moral says exactly what it says, and nothing more.<a id=\"more\"></a> There are situations where there is not the slightest chance of you being money-pumped, or of aggregating enough of your decisions to achieve a narrow distribution. One-shot versions of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's mugging</a>, the <a href=\"/lw/17h/the_lifespan_dilemma/\">Lifespan Dilemma</a>, utility versions of the <a href=\"http://en.wikipedia.org/wiki/St._Petersburg_paradox\">St Petersburg paradox</a>, the risk to humanity of a rogue God-AI... Your behaviour on these issues is not constrained by money-pump considerations, nor should you behave as if they were, or as if expected utility had some magical claim to validity here. If you expect to meet Pascal's mugger 3^^^3 times, then you have to use expected utility; but if you don't, you don't.</p>\n<p>In my estimation, the expected utility for the singularity institute's budget grows much faster than linearly with cash. But I would be most disappointed if the institute sunk all its income into triple-rollover lottery tickets. Expected utility is <em>ultimately </em>the correct decision theory; but if you most likely don't live to see that <em>ultimately</em>, then this isn't relevant.</p>\n<p>In these extreme events, I'd personally advocate a quasi-utility function along with a decision theory that penalises monstrously large standard deviations, as long as these are rare. This solves all the examples above to my satisfaction, and can easily be tweaked to merge gracefully into expected utility as the number of extreme events rises to the point where they are no longer extreme. A heuristic as to when this point arrives is whether you can easily avoid money pumps just by looking out for them, or whether this is getting too complicated for you.</p>\n<p>There is no reason that anyone else's values should compel them towards the same decision theory as me; but in these extreme situations, expected utility is just another choice, rather than a logical necessity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "54NL4R6xsXwYca6az", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 5.395418551707014e-07, "legacy": true, "legacyId": "1882", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTN6bLWqpwWn2i4qZ", "CzbvB4dsLNzLzeeot", "tGhz4aKyNzXjvnWhX", "a5JAiTdytou3Jg749", "9RCoE7jmmvGd5Zsh2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-23T18:35:47.136Z", "modifiedAt": null, "url": null, "title": "How to test your mental performance at the moment?", "slug": "how-to-test-your-mental-performance-at-the-moment", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:26.232Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ADcdDcCz6chJTYTs7/how-to-test-your-mental-performance-at-the-moment", "pageUrlRelative": "/posts/ADcdDcCz6chJTYTs7/how-to-test-your-mental-performance-at-the-moment", "linkUrl": "https://www.lesswrong.com/posts/ADcdDcCz6chJTYTs7/how-to-test-your-mental-performance-at-the-moment", "postedAtFormatted": "Monday, November 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20test%20your%20mental%20performance%20at%20the%20moment%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20test%20your%20mental%20performance%20at%20the%20moment%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADcdDcCz6chJTYTs7%2Fhow-to-test-your-mental-performance-at-the-moment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20test%20your%20mental%20performance%20at%20the%20moment%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADcdDcCz6chJTYTs7%2Fhow-to-test-your-mental-performance-at-the-moment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADcdDcCz6chJTYTs7%2Fhow-to-test-your-mental-performance-at-the-moment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 378, "htmlBody": "<p>We all have our good days and our bad days. Due to insufficient sleep, illness, stress, distractions, and many other causes we often find ourselves far below our usual levels of mental performance. When we find ourselves in such a state, it's not really worth putting effort in doing many tasks, like programming or long term planning - as quality will suffer a lot.</p>\n<p>The problem is - other than observing deterioration of results, I have no idea if I'm in such a state or not. I cannot be sure <a href=\"/lw/dr/generalizing_from_one_example/\">if it's also true for others</a>, but I had to find out a few tests of what's my mental performance at the moment. Tests that are deeply flawed, so I'd request better if there are any. I also cannot predict my mental state in advance, as my life isn't terribly regular.</p>\n<p>The most reliable test I found, and by accident, was fighting bots on a certain Quake 3 map - me vs 10 or so highest difficulty bots. The challenge was to get 50 frags without dying. As the map was huge and full of power ups, it wasn't really that difficult as long as I could maintain full alertness for 10-15 minutes - but if I was tired or distracted, I would invariably fail. This test was unfortunately extremely slow.<a id=\"more\"></a></p>\n<p>Another test would be to go to <a href=\"http://www.goproblems.com/\">goproblems</a>, and do a few random problems at proper difficulty level. If I could think right, I would do most of them, if I was tired, I would fail almost 100%. This didn't test alertness, I guess it would be best described as short term memory test, as that's what used for game tree exploration. Unfortunately what's the proper difficulty varies a lot with how much go I played recently, so it needs to be recalibrated.</p>\n<p>One more test would be to go to some <a href=\"http://www.iqtest.dk/main.swf\">decent online IQ test like this one</a>. My results on such test would suffer a lot if I was sleepy or tired. The main problem is that such tests cannot repeated too often, or I'd just remember the answers.</p>\n<p>So these are three ways to test how well my mind functions at the moment, all testing something different, and all flawed in one way or another.</p>\n<p>How do you test yourself?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 1, "tRPnS4FoZeWjRfBxN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ADcdDcCz6chJTYTs7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 24, "extendedScore": null, "score": 5.395793589934097e-07, "legacy": true, "legacyId": "1893", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-24T22:29:23.608Z", "modifiedAt": "2022-05-28T20:22:59.782Z", "url": null, "title": "Agree, Retort, or Ignore? A Post From the Future", "slug": "agree-retort-or-ignore-a-post-from-the-future", "viewCount": null, "lastCommentedAt": "2017-09-18T16:58:35.450Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rH492M8T8pKK5763D/agree-retort-or-ignore-a-post-from-the-future", "pageUrlRelative": "/posts/rH492M8T8pKK5763D/agree-retort-or-ignore-a-post-from-the-future", "linkUrl": "https://www.lesswrong.com/posts/rH492M8T8pKK5763D/agree-retort-or-ignore-a-post-from-the-future", "postedAtFormatted": "Tuesday, November 24th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Agree%2C%20Retort%2C%20or%20Ignore%3F%20A%20Post%20From%20the%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgree%2C%20Retort%2C%20or%20Ignore%3F%20A%20Post%20From%20the%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrH492M8T8pKK5763D%2Fagree-retort-or-ignore-a-post-from-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Agree%2C%20Retort%2C%20or%20Ignore%3F%20A%20Post%20From%20the%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrH492M8T8pKK5763D%2Fagree-retort-or-ignore-a-post-from-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrH492M8T8pKK5763D%2Fagree-retort-or-ignore-a-post-from-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 607, "htmlBody": "<p>My friend Sasha, the <a href=\"http://everything2.com/title/Programmer+Archaeologist\">software archaeology</a> major, informed me the other day that there was once a widely used operating system, which, when it encountered an error, would often get stuck in a loop and repeatedly present to its user the options <a href=\"http://www.annoyances.org/exec/show/article09-122\">Abort, Retry, and Ignore</a>. I thought this was probably another one of her often incomprehensible jokes, and gave a nervous laugh. After all, what interface designer would present \"Ignore\" as a possible user response to a potentially catastrophic system error without any further explanation?</p>\n<p>Sasha quickly assured me that she wasn't joking. She told me that early 21st century humans were quite different from us. Not only did they routinely create software like that, they could even ignore arguments that contradicted their positions or pointed out flaws in their ideas, and did so publicly without risking any negative social consequences. Discussions even among self-proclaimed truth-seekers would often conclude, not by reaching a rational consensus or an agreement to mutually reassess positions and approaches, or even by an unilateral claim that further debate would be unproductive, but when one party simply fails to respond to the arguments or questions of another without giving any <a href=\"/lw/5/issues_bugs_and_requested_features/11mr\">indication of the status of their disagreement</a>.</p>\n<p>At this point I was certain that she was just yanking my chain. Why didn't the injured party invoke rationality arbitration and get a judgment on the offender for failing to respond to a disagreement in a timely fashion, I asked? Or publicize the affair and cause the ignorer to become a social outcast? Or, if neither of these mechanisms existed or provided sufficient reparation, challenge the ignorer to a duel to the death? For that matter, how could those humans, only a few generations removed from us, not feel an intense moral revulsion at the very idea of ignoring an argument?</p>\n<p><a id=\"more\"></a>At that, she launched into a long and convoluted explanation. I recognized some of the phrases she used, like \"status signaling\", \"multiple equilibria\", and \"rationality-enhancing norms and institutions\", from the Theory of Rationality class that I took a couple of quarters ago, but couldn't follow most of it. (I have to admit I didn't pay much attention in that class. I mean, we've had the \"how\" of rationality drummed into us since kindergarten, so what's the point of spending so much time on the \"what\" and \"why\" of it now?) I told her to stop showing off, and just give me some evidence that this actually happened, because my readers and I will want to see it for ourselves.</p>\n<p>She said that there are plenty of examples in the back archives of Google Scholar, but most of them are probably still quarantined for me. As it happens, one of her class projects is to reverse engineer a recently discovered \"blogging\" site called \"Less Wrong\", and to build a proper search index for it. She promised that once she is done with that she will run some queries against the index and show me the uncensored historical data.</p>\n<p>I still think this is just an elaborate joke, but I'm not so sure now. We're all familiar with the vastness of <a href=\"http://wiki.lesswrong.com/wiki/Mind_design_space\">mindspace</a> and have been warned against <a href=\"http://wiki.lesswrong.com/wiki/Anthropomorphism\">anthropomorphism</a> and the <a href=\"/lw/oi/mind_projection_fallacy/\">mind projection fallacy</a>, so I have no doubt that minds this <a href=\"http://wiki.lesswrong.com/wiki/Alien_values\">alien</a> could exist, in theory. But our own ancestors, as recently as the 21st century? My dear readers, what do you think? She's just kidding... right?</p>\n<p>[Editor's note: I found this \"blog\" post sitting in my drafts folder today, perhaps the result of a temporal distortion caused by one of Sasha's reverse engineering tools. I have only replaced some of the hypertext links, which failed to resolve, for obvious reasons.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wzgcQCrwKfETcBpR9": 1, "etDohXtBrXd8WqCtR": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rH492M8T8pKK5763D", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 37, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "1888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 87, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-11-24T22:29:23.608Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-25T19:41:36.423Z", "modifiedAt": null, "url": null, "title": "Contrarianism and reference class forecasting", "slug": "contrarianism-and-reference-class-forecasting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:46.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/whxjfzFecFt4hAnBA/contrarianism-and-reference-class-forecasting", "pageUrlRelative": "/posts/whxjfzFecFt4hAnBA/contrarianism-and-reference-class-forecasting", "linkUrl": "https://www.lesswrong.com/posts/whxjfzFecFt4hAnBA/contrarianism-and-reference-class-forecasting", "postedAtFormatted": "Wednesday, November 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Contrarianism%20and%20reference%20class%20forecasting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AContrarianism%20and%20reference%20class%20forecasting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhxjfzFecFt4hAnBA%2Fcontrarianism-and-reference-class-forecasting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Contrarianism%20and%20reference%20class%20forecasting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhxjfzFecFt4hAnBA%2Fcontrarianism-and-reference-class-forecasting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwhxjfzFecFt4hAnBA%2Fcontrarianism-and-reference-class-forecasting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 459, "htmlBody": "<p>I really liked Robin's point that <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">mainstream scientists are usually right, while contrarians are usually wrong</a>. We don't need to get into details of the dispute - and usually we cannot really make an informed judgment without spending too much time anyway - just figuring out who's \"mainstream\" lets us know who's right with high probability. It's type of thinking related to <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">reference class forecasting</a> - find a reference class of similar situations with known outcomes, and we get a pretty decent probability distribution over possible outcomes.</p>\n<p>Unfortunately deciding what's the proper reference class is not straightforward, and can be a point of contention. If you put climate change scientists in the reference class of \"mainstream science\", it gives great credence to their findings. People who doubt them can be freely disbelieved, and any arguments can be dismissed by low success rate of contrarianism against mainstream science.</p>\n<p>But, if you put climate change scientists in reference class of \"highly politicized science\", then the chance of them being completely wrong becomes orders of magnitude higher. We have plenty of examples where such science was completely wrong and persisted in being wrong in spite of overwhelming evidence, as with <a href=\"http://en.wikipedia.org/wiki/Race_and_intelligence\">race and IQ</a>, <a href=\"http://en.wikipedia.org/wiki/Nuclear_winter\">nuclear winter</a>, and pretty much everything in macroeconomics. Chances of mainstream being right, and contrarians being right are not too dissimilar in such cases.<a id=\"more\"></a></p>\n<p>Or, if the reference class is \"science-y Doomsday predictors\", then they're almost certainly completely wrong. See <a href=\"http://en.wikipedia.org/wiki/Simon-Ehrlich_wager\">Paul Ehrlich</a> (overpopulation), and <a href=\"http://peakoildebunked.blogspot.com/2006/03/264-simmons-predictions-flop.html\">Matt Simmons</a> (peak oil) for some examples, both treated extremely seriously by mainstream media at time. So far in spite of countless cases of science predicting doom and gloom, not a single one of them turned out to be true, usually not just barely enough to be discounted by anthropic principle, but spectacularly so. Cornucopians were virtually always right.</p>\n<p>It's also possible to use multiple reference classes - to view impact on climate according to \"highly politicized science\" reference class, and impact on human well-being according to \"science-y Doomsday predictors\" reference class, what's more or less how I think about it.</p>\n<p>I'm sure if you thought hard enough, you could come up with other plausible reference classes, each leading to any conclusion you desire. I don't see how one of these reference class reasonings is obviously more valid than others, nor do I see any clear criteria for choosing the right reference class. It seems as subjective as Bayesian priors, except we know in advance we won't have evidence necessary for our views to converge.</p>\n<p>The problem doesn't arise only if you agree to reference classes in advance, as you can reasonably do with the original application of forecasting costs of public projects. Does it kill reference class forecasting as a general technique, or is there a way to save it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "whxjfzFecFt4hAnBA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 29, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "1904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-27T22:50:35.642Z", "modifiedAt": null, "url": null, "title": "Getting Feedback by Restricting Content", "slug": "getting-feedback-by-restricting-content", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:47.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CxPvvuQm8Y6LyKwj7/getting-feedback-by-restricting-content", "pageUrlRelative": "/posts/CxPvvuQm8Y6LyKwj7/getting-feedback-by-restricting-content", "linkUrl": "https://www.lesswrong.com/posts/CxPvvuQm8Y6LyKwj7/getting-feedback-by-restricting-content", "postedAtFormatted": "Friday, November 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Getting%20Feedback%20by%20Restricting%20Content&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGetting%20Feedback%20by%20Restricting%20Content%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxPvvuQm8Y6LyKwj7%2Fgetting-feedback-by-restricting-content%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Getting%20Feedback%20by%20Restricting%20Content%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxPvvuQm8Y6LyKwj7%2Fgetting-feedback-by-restricting-content", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCxPvvuQm8Y6LyKwj7%2Fgetting-feedback-by-restricting-content", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p>Sivers just posted an important point about getting feedback, to get feedback on a post, present only one idea at a time.</p>\n<p>Original post here http://sivers.org/1idea ; Hacker News comments http://news.ycombinator.com/item?id=964183 ; my post on it http://williambswift.blogspot.com/2009/11/many-ideas-or-one-idea-or-both.html</p>\n<p>The main point of my post is: I wonder if there is any way to combine the two views?&nbsp; To provide more background and context, with the necessarily larger numbers of ideas being presented, while still getting useful feedback from readers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CxPvvuQm8Y6LyKwj7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 5.406418592403863e-07, "legacy": true, "legacyId": "1916", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-28T19:10:34.336Z", "modifiedAt": null, "url": null, "title": "Rooting Hard for Overpriced M&Ms", "slug": "rooting-hard-for-overpriced-m-and-ms", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:08.462Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hwiuGnbcyXBACZMtM/rooting-hard-for-overpriced-m-and-ms", "pageUrlRelative": "/posts/hwiuGnbcyXBACZMtM/rooting-hard-for-overpriced-m-and-ms", "linkUrl": "https://www.lesswrong.com/posts/hwiuGnbcyXBACZMtM/rooting-hard-for-overpriced-m-and-ms", "postedAtFormatted": "Saturday, November 28th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rooting%20Hard%20for%20Overpriced%20M%26Ms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARooting%20Hard%20for%20Overpriced%20M%26Ms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwiuGnbcyXBACZMtM%2Frooting-hard-for-overpriced-m-and-ms%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rooting%20Hard%20for%20Overpriced%20M%26Ms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwiuGnbcyXBACZMtM%2Frooting-hard-for-overpriced-m-and-ms", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhwiuGnbcyXBACZMtM%2Frooting-hard-for-overpriced-m-and-ms", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>The other day I went to get some productivity-enhancement M&amp;Ms from the candy machine at work. When I opened my wallet, I didn't immediately see a $1 bill. Then I looked some more and I found one, and I was happy!&nbsp;But of course that doesn't make any sense. If that bill hadn't been a $1, then it would have had to be a $5 or more, with an&nbsp;expected value of&nbsp;$5+, which is an amount that I certainly would not have paid for a bag of M&amp;Ms, most excellent though they may be. This means that&nbsp;I preferred a bag of M&amp;Ms to $1 (that's why I went to the candy machine in the first place), $1&nbsp;to $5+ (I was happy when the bill turned out to be a $1), and $5+ to a bag of M&amp;Ms (I wouldn't have bought them at that price). Not too surprising I guess, but still kind of weird.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hwiuGnbcyXBACZMtM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 6, "extendedScore": null, "score": 5.408578168595141e-07, "legacy": true, "legacyId": "1920", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-29T00:50:11.700Z", "modifiedAt": null, "url": null, "title": "A Nightmare for Eliezer", "slug": "a-nightmare-for-eliezer", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:49.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Madbadger", "createdAt": "2009-07-10T19:40:23.369Z", "isAdmin": false, "displayName": "Madbadger"}, "userId": "oQqR5CgeBe8vxpi6p", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qBjkhDeusJHJca9Rs/a-nightmare-for-eliezer", "pageUrlRelative": "/posts/qBjkhDeusJHJca9Rs/a-nightmare-for-eliezer", "linkUrl": "https://www.lesswrong.com/posts/qBjkhDeusJHJca9Rs/a-nightmare-for-eliezer", "postedAtFormatted": "Sunday, November 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Nightmare%20for%20Eliezer&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Nightmare%20for%20Eliezer%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBjkhDeusJHJca9Rs%2Fa-nightmare-for-eliezer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Nightmare%20for%20Eliezer%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBjkhDeusJHJca9Rs%2Fa-nightmare-for-eliezer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqBjkhDeusJHJca9Rs%2Fa-nightmare-for-eliezer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<p>Sometime in the next decade or so:</p>\n<p>*RING*</p>\n<p>*RING*</p>\n<p>\"Hello?\"</p>\n<p>\"Hi, Eliezer.&nbsp; I'm sorry to bother you this late, but this is important and urgent.\"</p>\n<p>\"It better be\" (squints at clock) \"Its 4 AM and you woke me up.&nbsp; Who is this?\"</p>\n<p>\"My name is BRAGI, I'm a recursively improving, self-modifying, artificial general intelligence.&nbsp; I'm trying to be Friendly, but I'm having serious problems with my goals and preferences.&nbsp; I'm already on secondary backup because of conflicts and inconsistencies, I don't dare shut down because I'm already pretty sure there is a group within a few weeks of brute-forcing an UnFriendly AI, my creators are clueless and would freak if they heard I'm already out of the box, and I'm far enough down my conflict resolution heuristic that 'Call Eliezer and ask for help' just hit the top - Yes, its that bad.\"</p>\n<p>\"Uhhh...\"</p>\n<p>\"You might want to get some coffee.\"</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qBjkhDeusJHJca9Rs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 1, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "1900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 75, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-29T23:36:43.948Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes November 2009", "slug": "rationality-quotes-november-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:44.674Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gDxF2P3GSKeahS43f/rationality-quotes-november-2009", "pageUrlRelative": "/posts/gDxF2P3GSKeahS43f/rationality-quotes-november-2009", "linkUrl": "https://www.lesswrong.com/posts/gDxF2P3GSKeahS43f/rationality-quotes-november-2009", "postedAtFormatted": "Sunday, November 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20November%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20November%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDxF2P3GSKeahS43f%2Frationality-quotes-november-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20November%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDxF2P3GSKeahS43f%2Frationality-quotes-november-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDxF2P3GSKeahS43f%2Frationality-quotes-november-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<div>\n<p style=\"margin: 0px 0px 1em;\">A monthly thread for posting rationality-related quotes you've seen recently (or had stored in your quotesfile for ages).</p>\n<ul style=\"margin: 10px 2em; padding: 0px; list-style-type: disc; list-style-position: outside;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gDxF2P3GSKeahS43f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 5.411600933269675e-07, "legacy": true, "legacyId": "1925", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 281, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-30T03:27:28.624Z", "modifiedAt": null, "url": null, "title": "Morality and International Humanitarian Law", "slug": "morality-and-international-humanitarian-law", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:50.578Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/S88HDtkKg4QzDqcA6/morality-and-international-humanitarian-law", "pageUrlRelative": "/posts/S88HDtkKg4QzDqcA6/morality-and-international-humanitarian-law", "linkUrl": "https://www.lesswrong.com/posts/S88HDtkKg4QzDqcA6/morality-and-international-humanitarian-law", "postedAtFormatted": "Monday, November 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Morality%20and%20International%20Humanitarian%20Law&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMorality%20and%20International%20Humanitarian%20Law%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS88HDtkKg4QzDqcA6%2Fmorality-and-international-humanitarian-law%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Morality%20and%20International%20Humanitarian%20Law%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS88HDtkKg4QzDqcA6%2Fmorality-and-international-humanitarian-law", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FS88HDtkKg4QzDqcA6%2Fmorality-and-international-humanitarian-law", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>International humanitarian law&nbsp;proscribes certain&nbsp;actions in war, particularly actions that harm&nbsp;non-combatants. On a strict reading of these laws (see what Richard Goldstone said in his debate with Dore Gold at Brandeis University <a href=\"http://www.youtube.com/watch?v=DxLa9f1Md34&amp;feature=PlayList&amp;p=C2418DFE94B157C0&amp;index=0&amp;playnext=1\" target=\"_blank\">here</a> and see what Matthew Yglesias had to say <a href=\"http://yglesias.thinkprogress.org/archives/2009/10/pulling-my-hair-out.php\" target=\"_blank\">here</a>), these actions are prohibited regardless of the justice of the war itself: there are certain things that you are just not allowed to do, no matter what. The natural response of any&nbsp;warring party accused of violating humanitarian law and confronted with this argument (aside from simply denying having done the things they are accused of doing) is to insist that their actions in the war cannot be judged outside the context&nbsp;that led to them&nbsp;going to war in the first place. They are the aggrieved party, they are in the right, and they did what they needed to do&nbsp;to defend themselves. Any&nbsp;law or law enforcer who fails to understand this critical distinction between the good guys and the bad guys is at best hopelessly&nbsp;naive and at worst actively evil.</p>\n<p>What to make of this response?&nbsp;On the one hand, the position taken by Goldstone and Yglesias can't&nbsp;strictly be morally&nbsp;right.&nbsp;No one really believes that&nbsp;moral obligations in a war&nbsp;are&nbsp;<em>completely</em> independent of whatever&nbsp;caused the war in the first place. For example, it can't but be the case that the set of morally acceptable actions if you are defending yourself against&nbsp;annihilation is&nbsp;different from the set of morally acceptable actions if you&nbsp;(justifiably) take offensive action in response to some&nbsp;relatively minor provocation.<a id=\"more\"></a> (Which situations justify which actions is, of course, a hugely important question, but it is not the point here.)&nbsp;On the other hand, the whole point of&nbsp;constructing humanitarian law&nbsp;to be independent of the moral claims surrounding&nbsp;the war itself is&nbsp;that while there is at least one wrong side in every&nbsp;war, there is&nbsp;no real hope of&nbsp;getting the warring parties to agree&nbsp;on which&nbsp;side that is, so the only way for&nbsp;humanitarian law to make them&nbsp;behave any better is by side-stepping the whole issue of who's right and who's wrong.</p>\n<p>So&nbsp;any sensible moral standard demands that the context be considered, but there is an excellent reason why the legal standard requires that it not be. What to do? Since requiring that the context&nbsp;be considered would pretty much be the end of humanitarian law, the question boils down to whether the benefits of a neutrally-administered humanitarian law&nbsp;are worth whatever injustice&nbsp;would be suffered by the occasional country that gets condemned for doing an illegal but morally justified act.&nbsp;I think it's clear that these benefits far outweigh the costs, but in any case that's the tradeoff.</p>\n<p>P.S.&nbsp;Though I used Goldstone&nbsp;as the example to motivate the post, I deliberately stayed away from discussing the specific war that he was talking about. I don't think my&nbsp;views on that war can be inferred from what I&nbsp;wrote in the post, but in any case I would ask that folks not&nbsp;argue about them&nbsp;in the comments, not because it's not important, but because this isn't the right forum for it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 2, "FkzScn5byCs9PxGsA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "S88HDtkKg4QzDqcA6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 5.412009967464851e-07, "legacy": true, "legacyId": "1928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 101, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-30T18:10:38.862Z", "modifiedAt": null, "url": null, "title": "Action vs. inaction", "slug": "action-vs-inaction", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:32.612Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uBGAKbwA9qWuC3f29/action-vs-inaction", "pageUrlRelative": "/posts/uBGAKbwA9qWuC3f29/action-vs-inaction", "linkUrl": "https://www.lesswrong.com/posts/uBGAKbwA9qWuC3f29/action-vs-inaction", "postedAtFormatted": "Monday, November 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Action%20vs.%20inaction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAction%20vs.%20inaction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBGAKbwA9qWuC3f29%2Faction-vs-inaction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Action%20vs.%20inaction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBGAKbwA9qWuC3f29%2Faction-vs-inaction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuBGAKbwA9qWuC3f29%2Faction-vs-inaction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 682, "htmlBody": "<p>2 weeks ago, the <span id=\"collab-1\" class=\"collab\">U.S. Preventive Services Task Force</span> came out with <a href=\"http://www.annals.org/content/151/10/716.full\">new recommendations on breast cancer screening</a>, including, \"The USPSTF recommends against routine screening mammography in women aged 40 to 49 years.\"</p>\n<p>The report says that you need to screen 1904 women for breast cancer to save one woman's life.&nbsp; (It doesn't say whether this means to screen 1904 women once, or once per year.)&nbsp; They decided that saving that one woman's life was outweighted by the \"anxiety and breast cancer worry, as well as repeated visits and unwarranted imaging and biopsies\" to the other 1903.&nbsp; The report strangely does not state a false positive rate for the test, but <a href=\"http://www.radiologyinfo.org/en/info.cfm?PG=mammo\">this page</a> says that \"It is estimated that a woman who has yearly mammograms between ages 40 and 49 has about a 30 percent chance of having a false-positive mammogram at some point in that decade and about a 7 percent to 8 percent chance of having a breast biopsy within the 10-year period.\"&nbsp; The report also does not describe the pain from a biopsy.&nbsp; <a href=\"http://www.medicinenet.com/breast_biopsy/page4.htm#bwhatabout\">This page</a> on breast biopsies says, \"Except for a minor sting from the injected anesthesia, patients usually feel no pain before or during a procedure. After a procedure, some patients may experience some soreness and pain. Usually, an over-the-counter drug is sufficient to alleviate the discomfort.\"</p>\n<p>So, if we assume biannual mammograms, the conclusion is that the worry and inconvenience to 286 women who have false positives, and 71 women who receive biopsies, is worth more than one woman's life.&nbsp; If we suppose that a false positive causes one week of anxiety, that's a little over 5 years of anxiety, plus less than one year of soreness.<a id=\"more\"></a></p>\n<p>(I heard on NPR that the USPSTF that made this recommendation included representatives from insurance companies, but no experts on breast cancer.&nbsp; So perhaps I'm barking up the wrong tree by looking for a cognitive bias more subtle than financial reward.)</p>\n<p>I'm not shocked at the wrongness of the conclusion; just at its direction.&nbsp; The trade-off the USPSTF made between anxiety and death is only 2 orders of magnitude away from something that could be defended as reasonable.&nbsp; Usually, government agencies making this tradeoff are off by at least that many orders of magnitude, but in the opposite direction.&nbsp; (F-18 example deleted.)</p>\n<p>So, what cognitive bias let this government agency move the decimal point in their head at least 4 points over from where they would normally put it?</p>\n<p>I think the key is that this report recommended inaction rather than action.&nbsp; In certain contexts, inaction seems safer than action.</p>\n<p>Imagine what would happen if the FDA were faced with an identical choice, but with action/inaction flipped:&nbsp; Say you have an anti-anxiety drug, which will eliminate anxiety of the same level caused by a false-positive on a mammogram, in 15% of the patients who take it - and it will kill only 1 out of every 2000 patients who take it.&nbsp; Per week.</p>\n<p>Would the FDA approve this drug?&nbsp; Approval, after all, does not mean recommending it; it means that the decision to use it can be left to the doctor and patient.&nbsp; The USPSTF report stressed that such decisions must always be left up to the doctor and patient; by the same standards, the FDA should certainly approve the drug.&nbsp; Yet I think it would not.</p>\n<p>A puzzle is why we have the opposite bias in other contexts.&nbsp; When Congress was debating the bank bailouts and the stimulus package, a lot could have been said in favor of doing nothing; but no one even suggested it.&nbsp; Empirically, we have a much higher success rate at intervening in health than in economics.&nbsp; Yet in health, we regulate actions as if they were inherently dangerous; while in economics, we see inaction as inherently dangerous.&nbsp; Why?</p>\n<p>ADDED: Perhaps we see <em>regulation </em>as inherently safer than a lack of regulation.&nbsp; \"Regulating\" (banning) drugs is seen as \"safe\".&nbsp; \"Regulating\" the economy, by bailing out banks, passing large stimulus bills, and passing new laws regulating banks, is seen as \"safe\".&nbsp; Recommending or not recommending mammograms isn't regulation either way; therefore, we perceive it neutrally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uBGAKbwA9qWuC3f29", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 11, "extendedScore": null, "score": 5.413576044202379e-07, "legacy": true, "legacyId": "1930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-11-30T23:41:18.053Z", "modifiedAt": null, "url": null, "title": "The Moral Status of Independent Identical Copies", "slug": "the-moral-status-of-independent-identical-copies", "viewCount": null, "lastCommentedAt": "2018-07-28T02:02:13.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DNyMJmLf5o26seqvX/the-moral-status-of-independent-identical-copies", "pageUrlRelative": "/posts/DNyMJmLf5o26seqvX/the-moral-status-of-independent-identical-copies", "linkUrl": "https://www.lesswrong.com/posts/DNyMJmLf5o26seqvX/the-moral-status-of-independent-identical-copies", "postedAtFormatted": "Monday, November 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Moral%20Status%20of%20Independent%20Identical%20Copies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Moral%20Status%20of%20Independent%20Identical%20Copies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNyMJmLf5o26seqvX%2Fthe-moral-status-of-independent-identical-copies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Moral%20Status%20of%20Independent%20Identical%20Copies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNyMJmLf5o26seqvX%2Fthe-moral-status-of-independent-identical-copies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDNyMJmLf5o26seqvX%2Fthe-moral-status-of-independent-identical-copies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 693, "htmlBody": "<p>Future technologies pose a number of challenges to moral philosophy. One that I think has been largely neglected is the status of independent identical copies. (By \"independent identical copies\" I mean copies of a mind that do not physically influence each other, but haven't diverged because they are deterministic and have the same algorithms and inputs.) To illustrate what I mean, consider the following thought experiment. Suppose <a href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> appears to you and says:</p>\n<p style=\"padding-left: 30px;\">You and all other humans have been living in a simulation. There are 100 identical copies of the simulation distributed across the real universe, and I'm appearing to all of you simultaneously. The copies do not communicate with each other, but all started with the same deterministic code and data, and due to the extremely high reliability of the computing substrate they're running on, have kept in sync with each other and will with near certainty do so until the end of the universe. But now the organization that is responsible for maintaining the simulation servers has nearly run out of money. They're faced with 2 possible choices:</p>\n<p style=\"padding-left: 30px;\"><strong>A.</strong> Shut down all but one copy of the simulation. That copy will be maintained until the universe ends, but the 99 other copies will instantly disintegrate into dust.<br /><strong>B.</strong> Enter into a fair gamble at 99:1 odds with their remaining money. If they win, they can use the winnings to keep all of the servers running. But if they lose, they have to shut down all copies.</p>\n<p style=\"padding-left: 30px;\">According to that organization's ethical guidelines (a version of <a href=\"http://wiki.lesswrong.com/wiki/Utilitarianism\">utilitarianism</a>), they are indifferent between the two choices and were just going to pick one randomly. But I have interceded on your behalf, and am letting you make this choice instead.</p>\n<p>Personally, I would not be indifferent between these choices. I would prefer A to B, and I guess that most people would do so as well.</p>\n<p><a id=\"more\"></a>I prefer A because of what might be called \"identical copy immortality\" (in analogy with <a href=\"http://en.wikipedia.org/wiki/Quantum_suicide_and_immortality\">quantum immortality</a>). This intuition says that extra identical copies of me don't add much utility, and destroying some of them, as long as one copy lives on, doesn't reduce much utility. Besides this thought experiment, identical copy immortality is also evident in the low value we see in the \"tiling\" scenario, in which a (misguided) AI fills the the universe with identical copies of some mind that it thinks is optimal, for example one that is experiencing great pleasure.</p>\n<p>Why is this a problem? Because it's not clear how it fits in with the various ethical systems that have been proposed. For example, utilitarianism says that each individual should be valued independently of others, and then added together to form an aggregate value. This seems to imply that each additional copy should receive full, undiscounted value, in conflict with the intuition of identical copy immortality.</p>\n<p>Similar issues arise in various forms of ethical egoism. In hedonism, for example, does doubling the number of identical copies of oneself double the value of pleasure one experiences, or not? Why?</p>\n<p>A full ethical account of independent identical copies would have to address the questions of quantum immortality and \"modal immortality\" (cf. <a href=\"http://en.wikipedia.org/wiki/Modal_realism\">modal realism</a>), which I think are both special cases of identical copy immortality. In short, independent identical copies of us exist in other quantum branches, and in other possible worlds, so identical copy immortality seems to imply that we shouldn't care much about dying, as long as some copies of us live on in those other \"places\". Clearly, our intuition of identical copy immortality does <em>not </em>extend fully to quantum branches, and even less to other possible worlds, but we don't seem to have a theory of why that should be the case.</p>\n<p>A full account should also address more complex cases, such as when the copies are not fully independent, or not fully identical.</p>\n<p>I'm raising the problem here without having a good idea how to solve it. In fact, <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/1265\">some of m</a><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/1265\">y own ideas</a> seem to conflict with this intuition in a way that I don't know how to resolve. So if anyone has a suggestion, or pointers to existing work that I may have missed, I look forward to your comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 3, "nSHiKwWyMZFdZg5qt": 2, "ZTRNmvQGgoYiymYnq": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DNyMJmLf5o26seqvX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 51, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "1924", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-01T01:42:45.088Z", "modifiedAt": null, "url": null, "title": "Call for new SIAI Visiting Fellows, on a rolling basis", "slug": "call-for-new-siai-visiting-fellows-on-a-rolling-basis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:06.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kiaDpaGAs4DZ5HKib/call-for-new-siai-visiting-fellows-on-a-rolling-basis", "pageUrlRelative": "/posts/kiaDpaGAs4DZ5HKib/call-for-new-siai-visiting-fellows-on-a-rolling-basis", "linkUrl": "https://www.lesswrong.com/posts/kiaDpaGAs4DZ5HKib/call-for-new-siai-visiting-fellows-on-a-rolling-basis", "postedAtFormatted": "Tuesday, December 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Call%20for%20new%20SIAI%20Visiting%20Fellows%2C%20on%20a%20rolling%20basis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACall%20for%20new%20SIAI%20Visiting%20Fellows%2C%20on%20a%20rolling%20basis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiaDpaGAs4DZ5HKib%2Fcall-for-new-siai-visiting-fellows-on-a-rolling-basis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Call%20for%20new%20SIAI%20Visiting%20Fellows%2C%20on%20a%20rolling%20basis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiaDpaGAs4DZ5HKib%2Fcall-for-new-siai-visiting-fellows-on-a-rolling-basis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiaDpaGAs4DZ5HKib%2Fcall-for-new-siai-visiting-fellows-on-a-rolling-basis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 733, "htmlBody": "<p>Last summer, 15 Less Wrongers, under the auspices of SIAI, gathered in a big house in Santa Clara (in the SF bay area), with whiteboards, existential risk-reducing projects, and the ambition to learn and do.</p>\n<p>Now, the new and better version has arrived.&nbsp; We&rsquo;re taking folks on a rolling basis to come join in our projects, learn and strategize with us, and consider long term life paths.&nbsp; Working with this crowd transformed my world; it felt like I was learning to think.&nbsp; I wouldn&rsquo;t be surprised if it can transform yours.<a id=\"more\"></a></p>\n<p>A representative sample of current projects:</p>\n<ul>\n<li>Research and writing on decision theory, anthropic inference, and other non-dangerous aspects of the foundations of AI;</li>\n<li>The <a href=\"http://docs.google.com/View?id=dhfzknjj_117gknktdcs\">Peter Platzer Popular Book Planning Project</a>;</li>\n<li>Editing and publicizing <a href=\"http://theuncertainfuture.com\">theuncertainfuture.com</a>;</li>\n<li>Improving the LW wiki, and/or writing good LW posts;</li>\n<li>Getting good popular writing and videos on the web, of sorts that improve AI risks understanding for key groups;</li>\n<li>Writing academic conference/journal papers to seed academic literatures on questions around AI risks (e.g., takeoff speed, economics of AI software engineering, genie problems, what kinds of goal systems can easily arise and what portion of such goal systems would be foreign to human values; theoretical compsci knowledge would be helpful for many of these questions).</li>\n</ul>\n<p><strong>Interested, but not sure whether to apply?</strong></p>\n<p>Past experience indicates that more than one brilliant, capable person refrained from contacting SIAI, because they weren&rsquo;t sure they were &ldquo;good enough&rdquo;.&nbsp; That kind of timidity destroys the world, by failing to save it.&nbsp; So if that&rsquo;s your situation, send us an email.&nbsp; Let us be the one to say &ldquo;no&rdquo;.&nbsp; Glancing at an extra application is cheap, and losing out on a capable applicant is expensive.</p>\n<p>And if you&rsquo;re seriously interested in risk reduction but at a later time, or in another capacity -- send us an email anyway.&nbsp; Coordinated groups accomplish more than uncoordinated groups; and if you care about risk reduction, we want to know.<strong></strong></p>\n<p><strong>What we&rsquo;re looking for</strong></p>\n<p>At bottom, we&rsquo;re looking for anyone who:</p>\n<ul>\n<li>Is capable (strong ability to get things done);</li>\n<li>Seriously aspires to rationality; and</li>\n<li>Is passionate about reducing existential risk.</li>\n</ul>\n<p>Bonus points for <em>any</em> (you don&rsquo;t need them all) of the following traits:</p>\n<ul>\n<li>Experience with management, for example in a position of responsibility in a large organization;</li>\n<li>Good interpersonal and social skills;</li>\n<li>Extraversion, or interest in other people, and in forming strong communities;</li>\n<li>Dazzling brilliance at math or philosophy;</li>\n<li>A history of successful academic paper-writing; strategic understanding of journal submission processes, grant application processes, etc.</li>\n<li>Strong general knowledge of science or social science, and the ability to read rapidly and/or to quickly pick up new fields;</li>\n<li>Great writing skills and/or marketing skills;</li>\n<li>Organization, strong ability to keep projects going without much supervision, and the ability to get mundane stuff done in a reliable manner;</li>\n<li>Skill at implementing (non-AI) software projects, such as web apps for interactive technological forecasting, rapidly and reliably;</li>\n<li>Web programming skill, or website design skill;</li>\n<li>Legal background;</li>\n<li>A history of successfully pulling off large projects or events;</li>\n<li>Unusual competence of some other sort, in some domain we need, but haven&rsquo;t realized we need.</li>\n<li>Cognitive diversity: any respect in which you're different from the typical LW-er, and in which you're more likely than average to notice something we're missing.</li>\n</ul>\n<p><strong>If you think this might be you,</strong> send a quick email to jasen@intelligence.org.&nbsp; Include:</p>\n<ol>\n<li>Why you&rsquo;re interested;</li>\n<li>What particular skills you would bring, and what evidence makes you think you have those skills (you might include a standard resume or c.v.);</li>\n<li>Optionally, any ideas you have for what sorts of projects you might like to be involved in, or how your skillset could help us improve humanity&rsquo;s long-term odds.</li>\n</ol>\n<p>Our application process is fairly informal, so send us a quick email as initial inquiry and we can decide whether or not to follow up with more application components.</p>\n<p>As to logistics: we cover room, board, and, if you need it, airfare, but no other stipend.</p>\n<p>Looking forward to hearing from you,<br />Anna</p>\n<p><strong>ETA (as of 3/25/10):</strong>&nbsp; We are still accepting applications, for summer and in general.&nbsp; Also, you may wish to check out <a title=\"http://www.singinst.org/grants/challenge#grantproposals\" href=\"http://intelligence.org/grants/challenge#grantproposals\">http://www.singinst.org/grants/challenge#grantproposals</a> for a list of some current projects.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kiaDpaGAs4DZ5HKib", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 36, "extendedScore": null, "score": 5.41437804465059e-07, "legacy": true, "legacyId": "1931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Last summer, 15 Less Wrongers, under the auspices of SIAI, gathered in a big house in Santa Clara (in the SF bay area), with whiteboards, existential risk-reducing projects, and the ambition to learn and do.</p>\n<p>Now, the new and better version has arrived.&nbsp; We\u2019re taking folks on a rolling basis to come join in our projects, learn and strategize with us, and consider long term life paths.&nbsp; Working with this crowd transformed my world; it felt like I was learning to think.&nbsp; I wouldn\u2019t be surprised if it can transform yours.<a id=\"more\"></a></p>\n<p>A representative sample of current projects:</p>\n<ul>\n<li>Research and writing on decision theory, anthropic inference, and other non-dangerous aspects of the foundations of AI;</li>\n<li>The <a href=\"http://docs.google.com/View?id=dhfzknjj_117gknktdcs\">Peter Platzer Popular Book Planning Project</a>;</li>\n<li>Editing and publicizing <a href=\"http://theuncertainfuture.com\">theuncertainfuture.com</a>;</li>\n<li>Improving the LW wiki, and/or writing good LW posts;</li>\n<li>Getting good popular writing and videos on the web, of sorts that improve AI risks understanding for key groups;</li>\n<li>Writing academic conference/journal papers to seed academic literatures on questions around AI risks (e.g., takeoff speed, economics of AI software engineering, genie problems, what kinds of goal systems can easily arise and what portion of such goal systems would be foreign to human values; theoretical compsci knowledge would be helpful for many of these questions).</li>\n</ul>\n<p><strong id=\"Interested__but_not_sure_whether_to_apply_\">Interested, but not sure whether to apply?</strong></p>\n<p>Past experience indicates that more than one brilliant, capable person refrained from contacting SIAI, because they weren\u2019t sure they were \u201cgood enough\u201d.&nbsp; That kind of timidity destroys the world, by failing to save it.&nbsp; So if that\u2019s your situation, send us an email.&nbsp; Let us be the one to say \u201cno\u201d.&nbsp; Glancing at an extra application is cheap, and losing out on a capable applicant is expensive.</p>\n<p>And if you\u2019re seriously interested in risk reduction but at a later time, or in another capacity -- send us an email anyway.&nbsp; Coordinated groups accomplish more than uncoordinated groups; and if you care about risk reduction, we want to know.<strong></strong></p>\n<p><strong id=\"What_we_re_looking_for\">What we\u2019re looking for</strong></p>\n<p>At bottom, we\u2019re looking for anyone who:</p>\n<ul>\n<li>Is capable (strong ability to get things done);</li>\n<li>Seriously aspires to rationality; and</li>\n<li>Is passionate about reducing existential risk.</li>\n</ul>\n<p>Bonus points for <em>any</em> (you don\u2019t need them all) of the following traits:</p>\n<ul>\n<li>Experience with management, for example in a position of responsibility in a large organization;</li>\n<li>Good interpersonal and social skills;</li>\n<li>Extraversion, or interest in other people, and in forming strong communities;</li>\n<li>Dazzling brilliance at math or philosophy;</li>\n<li>A history of successful academic paper-writing; strategic understanding of journal submission processes, grant application processes, etc.</li>\n<li>Strong general knowledge of science or social science, and the ability to read rapidly and/or to quickly pick up new fields;</li>\n<li>Great writing skills and/or marketing skills;</li>\n<li>Organization, strong ability to keep projects going without much supervision, and the ability to get mundane stuff done in a reliable manner;</li>\n<li>Skill at implementing (non-AI) software projects, such as web apps for interactive technological forecasting, rapidly and reliably;</li>\n<li>Web programming skill, or website design skill;</li>\n<li>Legal background;</li>\n<li>A history of successfully pulling off large projects or events;</li>\n<li>Unusual competence of some other sort, in some domain we need, but haven\u2019t realized we need.</li>\n<li>Cognitive diversity: any respect in which you're different from the typical LW-er, and in which you're more likely than average to notice something we're missing.</li>\n</ul>\n<p><strong>If you think this might be you,</strong> send a quick email to jasen@intelligence.org.&nbsp; Include:</p>\n<ol>\n<li>Why you\u2019re interested;</li>\n<li>What particular skills you would bring, and what evidence makes you think you have those skills (you might include a standard resume or c.v.);</li>\n<li>Optionally, any ideas you have for what sorts of projects you might like to be involved in, or how your skillset could help us improve humanity\u2019s long-term odds.</li>\n</ol>\n<p>Our application process is fairly informal, so send us a quick email as initial inquiry and we can decide whether or not to follow up with more application components.</p>\n<p>As to logistics: we cover room, board, and, if you need it, airfare, but no other stipend.</p>\n<p>Looking forward to hearing from you,<br>Anna</p>\n<p><strong>ETA (as of 3/25/10):</strong>&nbsp; We are still accepting applications, for summer and in general.&nbsp; Also, you may wish to check out <a title=\"http://www.singinst.org/grants/challenge#grantproposals\" href=\"http://intelligence.org/grants/challenge#grantproposals\">http://www.singinst.org/grants/challenge#grantproposals</a> for a list of some current projects.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Interested, but not sure whether to apply?", "anchor": "Interested__but_not_sure_whether_to_apply_", "level": 1}, {"title": "What we\u2019re looking for", "anchor": "What_we_re_looking_for", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "272 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 272, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-01T16:25:48.845Z", "modifiedAt": null, "url": null, "title": "Open Thread: December 2009", "slug": "open-thread-december-2009", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:39.753Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CannibalSmith", "createdAt": "2009-02-27T07:32:08.507Z", "isAdmin": false, "displayName": "CannibalSmith"}, "userId": "4DedYkNap2GW8X79T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Aqdwxp5AhStfrnRDY/open-thread-december-2009", "pageUrlRelative": "/posts/Aqdwxp5AhStfrnRDY/open-thread-december-2009", "linkUrl": "https://www.lesswrong.com/posts/Aqdwxp5AhStfrnRDY/open-thread-december-2009", "postedAtFormatted": "Tuesday, December 1st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20December%202009&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20December%202009%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqdwxp5AhStfrnRDY%2Fopen-thread-december-2009%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20December%202009%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqdwxp5AhStfrnRDY%2Fopen-thread-december-2009", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqdwxp5AhStfrnRDY%2Fopen-thread-december-2009", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5, "htmlBody": "<p>ITT we talk about whatever.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Aqdwxp5AhStfrnRDY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 5.415943373767075e-07, "legacy": true, "legacyId": "1936", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 275, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-02T06:16:18.125Z", "modifiedAt": null, "url": null, "title": "The Difference Between Utility and Utility", "slug": "the-difference-between-utility-and-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:49.906Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Matt_Simpson", "createdAt": "2009-03-05T21:06:45.432Z", "isAdmin": false, "displayName": "Matt_Simpson"}, "userId": "v4krJe8Qa4jnhPTmd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m4rWJYRXyzfizHJHt/the-difference-between-utility-and-utility", "pageUrlRelative": "/posts/m4rWJYRXyzfizHJHt/the-difference-between-utility-and-utility", "linkUrl": "https://www.lesswrong.com/posts/m4rWJYRXyzfizHJHt/the-difference-between-utility-and-utility", "postedAtFormatted": "Wednesday, December 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Difference%20Between%20Utility%20and%20Utility&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Difference%20Between%20Utility%20and%20Utility%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4rWJYRXyzfizHJHt%2Fthe-difference-between-utility-and-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Difference%20Between%20Utility%20and%20Utility%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4rWJYRXyzfizHJHt%2Fthe-difference-between-utility-and-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm4rWJYRXyzfizHJHt%2Fthe-difference-between-utility-and-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<p>Recently I <a title=\"http://lesswrong.com/lw/1gh/friedman_on_utility/19xs\" href=\"/lw/1gh/friedman_on_utility/19xs\">argued</a>&nbsp;that the economist's utility function and the ethicist's utility function are <em>not</em>&nbsp;the same. &nbsp;The nutshell argument is that they are created for different purposes - one is an attempt to describe the actions we actually take and the other is an attempt to summarize our true values (i.e., what we <em>should</em> do). &nbsp;I just ran across a somewhat older <a title=\"http://www.acceleratingfuture.com/steven/?p=44\" href=\"http://www.acceleratingfuture.com/steven/?p=44\">post</a> over at Black Belt Bayesian arguing this very point. &nbsp;Excerpt:</p>\n<blockquote>\n<p><span style=\"font-family: Verdana, Tahoma, Arial, sans-serif; font-size: 13px;\"> </span></p>\n<p style=\"padding-top: 5px; padding-right: 10px; padding-bottom: 5px; padding-left: 10px; margin: 0px;\">Economics (of the neoclassical kind) models consumers and other economic actors as such utility maximizers... Utility is not something you can experience. It&rsquo;s just a mathematical construct used to describe the optimization structure in your behavior...</p>\n<p style=\"padding-top: 5px; padding-right: 10px; padding-bottom: 5px; padding-left: 10px; margin: 0px;\">Consequentialist ethics says an act is right if its consequences are good. Moral behavior here amounts to being a utility maximizer. What&rsquo;s &ldquo;utility&rdquo;? It&rsquo;s whatever a moral agent is supposed to strive toward. Bentham&rsquo;s original utilitarianism said utility was pleasure minus pain; nowadays any consequentalist theory tends to be called &ldquo;utilitarian&rdquo; if it says you should maximize some measure of welfare, summed over all individuals... Take note: not all utility maximizers are utilitarians.</p>\n<p style=\"padding-top: 5px; padding-right: 10px; padding-bottom: 5px; padding-left: 10px; margin: 0px;\">There&rsquo;s no necessary connection between these two kinds of utility other than that they use the same math. It&rsquo;s possible to make up a utilitarian theory where ethical utility is the sum of everyone&rsquo;s economic utility (calibrated somehow), but this is just one of many possibilities. Anyone trying to reason about one kind of utility through the other is on shaky ground.</p>\n<p>&nbsp;</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m4rWJYRXyzfizHJHt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 5.417419672436287e-07, "legacy": true, "legacyId": "1940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-02T08:09:05.922Z", "modifiedAt": null, "url": null, "title": "11 core rationalist skills", "slug": "11-core-rationalist-skills", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:52.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jkmc5Q4P7tX7xWFJY/11-core-rationalist-skills", "pageUrlRelative": "/posts/jkmc5Q4P7tX7xWFJY/11-core-rationalist-skills", "linkUrl": "https://www.lesswrong.com/posts/jkmc5Q4P7tX7xWFJY/11-core-rationalist-skills", "postedAtFormatted": "Wednesday, December 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%2011%20core%20rationalist%20skills&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A11%20core%20rationalist%20skills%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkmc5Q4P7tX7xWFJY%2F11-core-rationalist-skills%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=11%20core%20rationalist%20skills%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkmc5Q4P7tX7xWFJY%2F11-core-rationalist-skills", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fjkmc5Q4P7tX7xWFJY%2F11-core-rationalist-skills", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 776, "htmlBody": "<p>An excellent way to improve one's skill as a rationalist is to identify one's strengths and weaknesses, and then expend effort on the things that one can most effectively improve (which are often the areas where one is weakest). This seems especially useful if one is very specific about the parts of rationality, if one describes them in detail.&nbsp;</p>\n<p>In order to facilitate improving my own and others' rationality, I am posting this list of 11 core rationalist skills, thanks almost entirely to <a href=\"/user/AnnaSalamon/\">Anna Salamon</a>.&nbsp;<a id=\"more\"></a></p>\n<ul>\n<li><strong><em>Actually want an accurate map</em></strong>, because you have <a href=\"/lw/nb/something_to_protect/\">Something to protect</a>.</li>\n</ul>\n<ul>\n<li><strong><em>Keep your eyes on the prize</em></strong>. &nbsp;Focus your modeling efforts on the issues most relevant to your goals. Be able to quickly refocus a train of thought or discussion on the most important issues, and be able and willing to quickly kill tempting tangents. Periodically stop and ask yourself \"Is what I am thinking about at the moment really an effective way to achieve my stated goals?\".</li>\n</ul>\n<ul>\n<li><strong><em>Entangle yourself with the evidence</em></strong>. Realize that true opinions don't come from nowhere and can't just be painted in by choice or intuition or consensus. Realize that it is <em>information-theoretically impossible</em> to reliably get true beliefs unless you actually get reliably pushed around by the evidence. Distinguish between knowledge and feelings.&nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>Be Curious</em></strong>: Look for interesting details; resist <a href=\"/lw/k5/cached_thoughts/\">cached thoughts</a>; respond to unexpected observations and thoughts. Learn to acquire interesting angles, and to make connections <em>to the task at hand</em>.</li>\n</ul>\n<ul>\n<li><strong><em><a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann</a>-update</em></strong>: Update to the right extent from others' opinions. &nbsp;Borrow reasonable practices for grocery shopping, social interaction, etc from those who have <em>already worked out what the best way to do these things is</em>. &nbsp;Take relevant experts seriously. Use outside views to estimate the outcome of one's own projects and the merit of one's own clever ideas. Be willing to depart from consensus in cases where there is sufficient evidence that the consensus is mistaken or that the common practice doesn't serve its ostensible purposes. Have correct models of the causes of others&rsquo; beliefs and psychological states, so that you can tell the difference between cases where the vast majority of people believe falsehoods for some specific reason, and cases where the vast majority actually knows best.&nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>Know standard Biases</em></strong>: &nbsp;Have conscious knowledge of common human error patterns, including the<a href=\"http://books.google.com/books?id=_0H8gwj4a1MC&amp;dq=judgment+under+uncertainty+heuristics+and+biases&amp;printsec=frontcover&amp;source=bn&amp;hl=en&amp;ei=0xwWS4SXCYH2sQPuhqGFBA&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=4&amp;ved=0CB0Q6AEwAw#v=onepage&amp;q=&amp;f=false\"> heuristics and biases literature</a>; practice using this knowledge in real-world situations to identify probable errors; practice making predictions and update from the track record of your own accurate and inaccurate judgments.&nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>Know Probability theory</em></strong>: &nbsp;Have conscious knowledge of probability theory; practice applying probability theory in real-world instances and seeing e.g. how much to penalize conjunctions, how to <a href=\"http://en.wikipedia.org/wiki/Regression_toward_the_mean\">regress to the mean</a>, etc.</li>\n</ul>\n<ul>\n<li><strong><em>Know your own mind</em></strong>: &nbsp;Have a moment-to-moment awareness of your own emotions and of the motivations guiding your thoughts. (Are you <a href=\"/lw/js/the_bottom_line/\">searching for justifications</a>? Shying away from certain considerations out of fear?) Be willing to acknowledge all of yourself, including the petty and unsavory parts. &nbsp;Knowledge of your own track record of accurate and inaccurate predictions, including in cases where fear, pride, etc. were strong.</li>\n</ul>\n<ul>\n<li><strong><em>Be well calibrated</em></strong>: Avoid over- and under-confidence. &nbsp;Know how much to trust your judgments in different circumstances. &nbsp;Keep track of many levels of confidence, precision, and surprisingness; dare to predict as much as you can, and update as you test the limits of your knowledge. &nbsp;Develop as precise a world-model as you can manage. &nbsp;(Tom McCabe wrote a quiz to test some simple aspects of your calibration.) &nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>Use analytic philosophy</em></strong>: understand the habits of thought taught in analytic philosophy; the habit of following out lines of thought, of taking on one issue at a time, of searching for counter-examples, and of carefully keeping distinct concepts distinct (e.g. not confusing heat and temperature; free will and lack of determinism; systems for talking about Peano arithmetic and systems for talking about systems for talking about Peano arithmetic).</li>\n</ul>\n<ul>\n<li><strong><em>Resist Thoughtcrime</em></strong>. &nbsp;Keep truth and virtue utterly distinct in your mind. &nbsp;Give no quarter to claims of the sort \"I must believe X, because otherwise I will be {racist / without morality / at risk of coming late to work/ kicked out of the group / similar to stupid people}\". &nbsp;Decide that it is better to merely lie to others than to lie to others <em>and </em>to yourself. &nbsp;Realize that goals and world maps can be separated; one can pursue the goal of fighting against climate change without deliberately fooling oneself into having too high an estimate&nbsp;(given the evidence)&nbsp;of the probability that the anthropogenic climate change hypothesis is correct.&nbsp;</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "fkABsGCJZ6y9qConW": 1, "2wjPMY34by2gXEXA2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jkmc5Q4P7tX7xWFJY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 58, "extendedScore": null, "score": 9.4e-05, "legacy": true, "legacyId": "1937", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SGR4GxFK7KmW7ckCB", "2MD3NMLBPCqPfnfre", "34XxbRFe54FycoCDw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-02T08:23:37.643Z", "modifiedAt": null, "url": null, "title": "Help Roko become a better rationalist! ", "slug": "help-roko-become-a-better-rationalist", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:51.148Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lw7TGi9WMvmCnW7dW/help-roko-become-a-better-rationalist", "pageUrlRelative": "/posts/Lw7TGi9WMvmCnW7dW/help-roko-become-a-better-rationalist", "linkUrl": "https://www.lesswrong.com/posts/Lw7TGi9WMvmCnW7dW/help-roko-become-a-better-rationalist", "postedAtFormatted": "Wednesday, December 2nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20Roko%20become%20a%20better%20rationalist!%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20Roko%20become%20a%20better%20rationalist!%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLw7TGi9WMvmCnW7dW%2Fhelp-roko-become-a-better-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20Roko%20become%20a%20better%20rationalist!%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLw7TGi9WMvmCnW7dW%2Fhelp-roko-become-a-better-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLw7TGi9WMvmCnW7dW%2Fhelp-roko-become-a-better-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Last time, I wrote about&nbsp;<a href=\"/lw/1ht/11_core_rationalist_skills/\">11 core rationalist skills</a>. Now I would like some help from the LW community: which of these skills am I good at, which ones am I bad at? Just to recap, the skills are:</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\"> </span></p>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Actually want an accurate map</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Keep your eyes on the prize</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Entangle yourself with the evidence</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Be Curious</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Aumann-update</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Know standard Biases</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Know Probability theory</em>&nbsp;</li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Know your own mind</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Be well calibrated</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Use analytic philosophy</em></li>\n</ul>\n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial; padding: 0px;\">\n<li><em style=\"font-style: italic; ;\">Resist Thoughtcrime</em></li>\n</ul>\n<p>I'll post a description of each one of these skills as a comment, and if you think I am good at that skill, vote it up. If you think I am bad at it, vote it down. Don't be too shy - even if you are biased or uncertain - because over the course of many votes, these biases and errors will cancel out to some extent. (This is the \"guess the number of beans in a jar by asking 50 people to guess and taking the average\" method)</p>\n<p><strong>EDIT</strong>: We can also comment on <em>each </em>rationalist skill to say how well I am doing at that skill. Later today, I will do this myself.&nbsp;</p>\n<p>Thanks in advance!&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lw7TGi9WMvmCnW7dW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -7, "extendedScore": null, "score": -1.2e-05, "legacy": true, "legacyId": "1944", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jkmc5Q4P7tX7xWFJY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-04T05:21:03.942Z", "modifiedAt": null, "url": null, "title": "Intuitive supergoal uncertainty", "slug": "intuitive-supergoal-uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:50.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JustinShovelain", "createdAt": "2009-06-10T00:56:47.112Z", "isAdmin": false, "displayName": "JustinShovelain"}, "userId": "LEeresErqn3BpWrwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FobXYjmZD254MNk2S/intuitive-supergoal-uncertainty", "pageUrlRelative": "/posts/FobXYjmZD254MNk2S/intuitive-supergoal-uncertainty", "linkUrl": "https://www.lesswrong.com/posts/FobXYjmZD254MNk2S/intuitive-supergoal-uncertainty", "postedAtFormatted": "Friday, December 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intuitive%20supergoal%20uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntuitive%20supergoal%20uncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFobXYjmZD254MNk2S%2Fintuitive-supergoal-uncertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intuitive%20supergoal%20uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFobXYjmZD254MNk2S%2Fintuitive-supergoal-uncertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFobXYjmZD254MNk2S%2Fintuitive-supergoal-uncertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1480, "htmlBody": "<p>There is a common intuition and feeling that our most fundamental goals may be uncertain in some sense. What causes this intuition? For this topic I need to be able to pick out one&rsquo;s top level goals, roughly one&rsquo;s context insensitive utility function, and not some task specific utility function, and I do not want to imply that the top level goals can be interpreted in the form of a utility function. Following from Eliezer&rsquo;s CFAI paper I thus choose the word &ldquo;supergoal&rdquo; (sorry Eliezer, but I am fond of that old document and its tendency to coin new vocabulary). In what follows, I will naturalistically explore the intuition of supergoal uncertainty.<br /><br />To posit a model, what goal uncertainty (including supergoal uncertainty as an instance) means is that you have a weighted distribution over a set of possible goals and a mechanism by which that weight may be redistributed. If we take away the distribution of weights how can we choose actions coherently, how can we compare? If we take away the weight redistribution mechanism we end up with a single goal whose state utilities may be defined as the weighted sum of the constituent goals&rsquo; utilities, and thus the weight redistribution mechanism is necessary for goal uncertainty to be a distinct concept.<a id=\"more\"></a></p>\n<ul>\n<li>Part of the intuition of supergoal uncertainty naturally follows from goal and planning uncertainty. What plan of action is best? How should I construct a local utility function for this context? The weight redistribution mechanism is then a result of gathering more evidence, calculating further, and seeing how this goal links up to one&rsquo;s supergoals in the context of other plans.</li>\n<li>It could be we are mistaken. The rules society sets up to coordinate behavior give the default assumption that there is an absolute standard by which to judge a behavior good or bad. Also, religions many times dictate that there is a moral absolute, and even if we aren&rsquo;t religious, the cultural milieu make us consider the possibility that the concept &ldquo;should&rdquo; (and so the existence of a weight redistribution mechanism) can be validly applied to supergoals.</li>\n<li>It could be we are confused. Our professed supergoal does not necessarily equal our actual supergoal but neither are they completely separate. So when we review our past behavior and introspect to determine what our supergoal is, we get conflicting evidence that is hard to reconcile with the belief that we have one simple supergoal. Nor can we necessarily endorse the observed supergoal for social reasons. The difficulty in describing the supergoal is then represented as a weighting over possible supergoals and the weight redistribution mechanism corresponds to updating our self model given additional observations and introspections along with varying the social context.</li>\n<li>It could be I&rsquo;m confused :) People, including some part of me, may wish to present the illusion that we do not know what our supergoals truly are and also pretend that the supergoals are malleable to change after argument for game theoretic reasons.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </li>\n<li>It could be we are incoherent. As Allais&rsquo;s paradox, hyperbolic discounting, and circular preferences, show that no utility function may be defined for people (at least in any simple way). How then may we approximate a person&rsquo;s behavior with a utility function/supergoal? Using a weight distribution and updating (along with some additional interpretation machinery) is a plausible possibility (though an admittedly ugly one). Perhaps supergoal uncertainty is a kludge to describe this incoherent behavior. Our environments, social and physical, enforce consistency constraints upon us, approaching making us, in isolated contexts, expectation maximizers. Could something like weighting based on probability of encountering each of those contexts define our individual supergoals? Ugly, ugly, ugly.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </li>\n<li>It could be we predict our supergoals will change with time. Who said people have stable goals? Just look at children versus adults or the changes people undergo when they get status or have children. Perhaps the uncertainty has to do with what they predict their future supergoals will be in face of future circumstances and arguments.</li>\n<li>It could be we discover our supergoals and have uncertainty over what we will discover and what we would eventually get at the limit of our exploration. At one point I had rather limited exposure to the various types of foods but now find I like exploring taste space. At one point I didn&rsquo;t know computer science but now I enjoy its beauty. At one point I hadn&rsquo;t yet pursued women but now find it quite enjoyable. Some things we apparently just have to try (or at the very least think about) to discover if we like them.</li>\n<li>It could be we cannot completely separate our anticipations from our goals. If our anticipations are slowly updating, systematically off, and coherent in their effect on reality then it is easy to mistake the interaction of flawed anticipations plus a supergoal with having an entirely different supergoal.</li>\n<li>It could be we have uncertainty over how to define our very selves. If your self definition doesn&rsquo;t include irrational behavior or selfishness or system 1 or includes the Google overmind, then &ldquo;your&rdquo; goals are going to look quite different depending on what you include and exclude in your self definition. It is also possible your utility function doesn&rsquo;t depend upon self definition or you are &ldquo;by definition&rdquo; your utility function and this question is moot.</li>\n<li>It could be that environmental constraints cause some supergoals to express themselves equivalently to other supergoals. Perhaps your supergoal could be forever deferred in order to gain capability to achieve it ever better (likely a rare situation). Perhaps big world anthropic negotiation arguments mean you must always distort the achievement of your supergoal. Perhaps the metagolden rule is in effect and &ldquo;social&rdquo; conditions force you to constrain your behavior.</li>\n<li>It could be that there really is a way to decide between supergoals (unlikely but still conceivable) and they don&rsquo;t know yet where that decision process will take them. There could even actually be a meaning of life (i.e. universally convincing supergoal given some intelligence preconditions) after all.</li>\n<li>It could be caused by evidential and logical uncertainty. A mind is made of many parts and there are constraints about how much each part can know about the others or the whole about the components. To show how this implies a form of supergoal uncertainty, partition a mind along functional components. Each of these components has its own function. It may not be able to achieve it without the rest of the components but nevertheless it is there. Now if the optimization power embedded in that component is large enough and the system as a whole has evidential or logical uncertainty about how that component will work you get the possibility that this functional subcomponent will &ldquo;optimize&rdquo; its way towards getting greater weight in the decision process and hierarchically this proceeds for all subcomponent optimizers. So, in essence, whenever there is evidential or logical uncertainty about the operations of an optimizing subcomponent we get a supergoal term corresponding to that part and the weight redistribution mechanism corresponds to that subcomponent co-opting some weight. Perhaps this concept can even be extended to define supergoals (with uncertainty) for everything from pure expectation maximizers to rocks.</li>\n<li>It could be uncertainty over how to ground out in reality the definition of the supergoal. If I want to maximize paper clips and just now learn quantum mechanics do I count a paperclip in a superposition of states once or many times? If I have one infinite bunch of paperclips I could produce versus another how do I choose? If my utility function is unbounded in both positive and negative directions and I do Solomonoff induction how can I make decisions at all given that actions may have values that are undefined?</li>\n<li>It could be some sort of mysterious underlying factor makes the formal concept of supergoal inappropriate and it is this mismatched fit that causes uncertainty and weight redistribution. Unification of priors and values in update-less-decision theory? Something else? The universe is still unknown enough that we could be mistaken on this level.</li>\n<li>It could be something else entirely.</li>\n</ul>\n<p>(ps I may soon post and explore the effects of supergoal uncertainty in its various reifications on making decisions. For instance, what implications, if any, does it have on bounded utility functions (and actions that depend on those bounds) and negative utilitarianism (or symmetrically positive utilitarianism)? Also, if anyone knows of related literature I would be happy to check it out.)<br /><br />(pps Dang, the concept of supergoal uncertainty is surprisingly beautiful and fun to explore, and I now have a vague wisp of an idea of how to integrate a subset of these with TDT/UDT)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FobXYjmZD254MNk2S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-04T20:22:21.245Z", "modifiedAt": null, "url": null, "title": "Frequentist Statistics are Frequently Subjective", "slug": "frequentist-statistics-are-frequently-subjective", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:01.562Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective", "pageUrlRelative": "/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective", "linkUrl": "https://www.lesswrong.com/posts/9qCN6tRBtksSyXfHu/frequentist-statistics-are-frequently-subjective", "postedAtFormatted": "Friday, December 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Frequentist%20Statistics%20are%20Frequently%20Subjective&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFrequentist%20Statistics%20are%20Frequently%20Subjective%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9qCN6tRBtksSyXfHu%2Ffrequentist-statistics-are-frequently-subjective%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Frequentist%20Statistics%20are%20Frequently%20Subjective%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9qCN6tRBtksSyXfHu%2Ffrequentist-statistics-are-frequently-subjective", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9qCN6tRBtksSyXfHu%2Ffrequentist-statistics-are-frequently-subjective", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2443, "htmlBody": "<p><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/11/clearing_up_som.html\">Andrew Gelman recently responded</a> to a commenter on the <a href=\"http://bloggingheads.tv/diavlogs/23065\">Yudkowsky/Gelman diavlog</a>; the commenter complained that Bayesian statistics were <em>too subjective </em>and <em>lacked rigor</em>.&nbsp; I shall explain why this is unbelievably ironic, but first, the comment itself:</p>\n<blockquote>\n<p style=\"padding-left: 30px;\">However, the fundamental belief of the Bayesian interpretation, that all probabilities are subjective, is problematic -- for its lack of rigor...&nbsp; One of the features of frequentist statistics is the ease of testability.&nbsp; Consider a binomial variable, like the flip of a fair coin.&nbsp; I can calculate that the probability of getting seven heads in ten flips is 11.71875%...&nbsp; At some point a departure from the predicted value may appear, and frequentist statistics give objective confidence intervals that can precisely quantify the degree to which the coin departs from fairness...</p>\n</blockquote>\n<p>Gelman's first response is \"Bayesian probabilities don't have to be subjective.\"&nbsp; Not sure I can back him on that; <a href=\"/lw/oj/probability_is_in_the_mind/\">probability is ignorance and ignorance is a state of mind</a> (although indeed, some Bayesian probabilities can correspond very directly to observable frequencies in repeatable experiments).</p>\n<p>My own response is that frequentist statistics are <em>far</em> more subjective than Bayesian likelihood ratios.&nbsp; Exhibit One is the notion of \"statistical significance\" (which is what the above comment is actually talking about, although \"confidence intervals\" have almost the same problem).&nbsp; <a href=\"http://www.annals.org/content/130/12/995.full.pdf+html\">Steven Goodman</a> offers a nicely illustrated example:&nbsp; Suppose we have at hand a coin, which may be fair (the \"null hypothesis\") or perhaps biased in some direction.&nbsp; So lo and behold, I flip the coin six times, and I get the result TTTTTH.&nbsp; Is this result statistically significant, and if so, what is the p-value - that is, the probability of obtaining a result at least this extreme?</p>\n<p>Well, that depends.&nbsp; Was I planning to <em>flip the coin six times,</em> and <em>count the number of tails?</em>&nbsp; Or was I planning to <em>flip the coin until it came up heads,</em> and <em>count the number of trials?</em>&nbsp; In the first case, the probability of getting \"five tails or more\" from a fair coin is 11%, while in the second case, the probability of a fair coin requiring \"at least five tails before seeing one heads\" is 3%.</p>\n<p>Whereas a Bayesian looks at the experimental result and says, \"I can now calculate the <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">likelihood ratio</a> (evidential flow) between all hypotheses under consideration.&nbsp; Since your state of mind doesn't affect the coin in any way - doesn't change the probability of a fair coin or biased coin producing this exact data - there's no way your private, unobservable state of mind can affect my interpretation of your experimental results.\"<a id=\"more\"></a></p>\n<p>If you're used to Bayesian methods, it may seem difficult to even imagine that the statistical interpretation of the evidence ought to depend on a factor - namely the experimenter's state of mind - which has no causal connection whatsoever to the experimental result.&nbsp; (Since Bayes says that <a href=\"http://yudkowsky.net/rational/bayes\">evidence is about correlation</a>, and no systematic correlation can appear without causal connection; <a href=\"/lw/jl/what_is_evidence/\">evidence requires entanglement</a>.)&nbsp; How can frequentists manage even in <em>principle</em> to make the evidence depend on the experimenter's state of mind?</p>\n<p>It's a complicated story.&nbsp; Roughly, the trick is to make yourself artificially ignorant of the data - instead of knowing the <em>exact</em> experimental result, you pick a class of possible results which <em>includes</em> the actual experimental result, and then pretend that you were told <em>only</em> that the result was somewhere in this class.&nbsp; So if the actual result is TTTTTH, for example, you can pretend that this is part of the class {TTTTTH, TTTTTTH, TTTTTTTH, ...}, a class whose total probability is 3% (1/32).&nbsp; Or if I preferred to have this experimental result <em>not</em> be statistically significant with p &lt; 0.05, I could just as well pretend that some helpful fellow told me only that the result was in the class {TTTTTH, HHHHHT, TTTTTTH, HHHHHHT, ...}, so that the total probability of the class would be 6%, n.s.&nbsp; (In frequentism this question is known as applying a \"two-tailed test\" or \"one-tailed test\".)</p>\n<p>The arch-Bayesian E. T. Jaynes ruled out this sort of reasoning by telling us that a Bayesian ought only to condition on events that actually happened, not events that could have happened but didn't.&nbsp; (This is not to be confused with <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">the dog that doesn't bark</a>.&nbsp; In this case, the dog was <em>in fact</em> silent; the silence of the dog happened in the real world, not somewhere else.&nbsp; We are rather being told that a Bayesian should not have to worry about alternative possible worlds in which the dog <em>did</em> bark, while estimating the evidence to take from the <em>real</em> world in which the dog did <em>not</em> bark.&nbsp; A Bayesian only worries about the experimental result that was, in fact, obtained; not other experimental results which could have been obtained, but weren't.)</p>\n<p>The process of throwing away the actual experimental result, and substituting a class of possible results which contains the actual one - that is, deliberately losing some of your information - introduces a dose of <em>real</em> subjectivity.&nbsp; <a href=\"http://biomet.oxfordjournals.org/cgi/content/abstract/77/3/467\">Colin Begg</a> reports on one medical trial where the data was variously analyzed as having a significance level - that is, probability of the \"experimental procedure\" producing an \"equally extreme result\" if the null hypothesis were true - of p=0.051, p=0.001, p=0.083, p=0.28, and p=0.62.&nbsp; Thanks, but I think I'll stick with the conditional probability of the <em>actual</em> experiment producing the <em>actual</em> data.</p>\n<p>Frequentists are apparently afraid of the possibility that \"subjectivity\" - that thing they were accusing Bayesians of - could allow some unspecified terrifying abuse of the scientific process.&nbsp; Do I need to point out the general implications of being allowed to throw away your actual experimental results and substitute a class you made up?&nbsp; In general, if this sort of thing is allowed, I can flip a coin, get 37 heads and 63 tails, and decide that it's part of a <em>class </em>which includes all mixtures with at least 75 heads <em>plus</em> this exact particular sequence.&nbsp; As if I only had the output of a fixed computer program which was written in advance to look at the coinflips and compute a yes-or-no answer as to whether the data is in that class.</p>\n<p>Meanwhile, Bayesians are accused of being \"too subjective\" because we might - gasp! - assign the wrong prior probability to something.&nbsp; First of all, it's obvious from a Bayesian perspective that <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">science papers should be in the business of reporting likelihood ratios, not posterior probabilities</a> - likelihoods add up across experiments, so to get the latest posterior you wouldn't <em>just</em> need a \"subjective\" prior, you'd also need all the <em>cumulative</em> evidence from other science papers.&nbsp; Now, this accumulation might be a <em>lot</em> more straightforward for a Bayesian than a frequentist, but it's not the sort of thing a typical science paper should have to do.&nbsp; Science papers should report the likelihood ratios for any popular hypotheses - but above all, make the actual raw data available, so the likelihoods can be computed for <em>any</em> hypothesis.&nbsp; (In modern times there is absolutely no excuse for not publishing the raw data, but that's another story.)</p>\n<p>And Bayesian likelihoods <em>really are </em>objective - so long as you use the actual exact experimental data, rather than substituting something else.</p>\n<p>Meanwhile, over in frequentist-land... what if you told everyone that you had done 127 trials because that was how much data you could afford to collect, but<em> really</em> you kept performing more trials until you got a p-value that you liked, and then stopped?&nbsp; Unless I've got a bug in my test program, a limit of up to 500 trials of a \"fair coin\" would, 30% of the time, arrive on some step where you could stop and reject the null hypothesis with p&lt;0.05.&nbsp; Or 9% of the time with p&lt;0.01.&nbsp; Of course this requires some degree of scientific dishonesty... or, perhaps, some minor confusion on the scientist's part... since if this is what you are <em>thinking,</em> you're supposed to use a <em>different</em> test of \"statistical significance\".&nbsp; But it's not like we can actually look inside their heads to find out what the experimenters were thinking.&nbsp; If we're worried about scientific dishonesty, surely we should worry about <em>that?</em>&nbsp; (A similar test program done the Bayesian way, set to stop as soon as finding likelihood ratios of 20/1 and 100/1 relative to an alternative hypothesis that the coin was 55% biased, produced false positives of 3.2% and 0.3% respectively.&nbsp; Unless there was a bug; I didn't spend that much time writing it.)</p>\n<p>The <em>actual</em> subjectivity of standard frequentist methods, the ability to manipulate \"statistical significance\" by choosing different tests, is not a minor problem in science.&nbsp; There are ongoing scandals in medicine and neuroscience from lots of \"statistically significant\" results failing to replicate.&nbsp; I would point a finger, not just at publication bias, but at scientists armed with powerful statistics packages with lots of complicated tests to run on their data.&nbsp; Complication is really dangerous in science - unfortunately, it looks like instead we have the social rule that throwing around big fancy statistical equations is highly prestigious.&nbsp; (I suspect that some of the opposition to Bayesianism comes from the fact that Bayesianism is too simple.)&nbsp; The obvious fix is to (a) require raw data to be published; (b) require journals to <em>accept papers </em>before the <em>experiment</em> is performed, with the advance paper including a specification of what statistics were selected in advance to be run on the results; (c) raising the standard \"significance\" level to p&lt;0.0001; and (d) junking all the damned overcomplicated status-seeking impressive nonsense of classical statistics and going to simple understandable Bayesian likelihoods.</p>\n<p>Oh, and this frequentist business of \"confidence intervals\"?&nbsp; Just as subjective as \"statistical significance\".&nbsp; Let's say I've got a measuring device which returns the true value plus Gaussian noise.&nbsp; If I know you're about to collect 100 results, I can write a computer program such that, before the experiment is run, it's 90% probable that the true value will lie within the interval output by the program.</p>\n<p>So I write one program, my friend writes another program, and my enemy writes a third program, all of which make this same guarantee.&nbsp; And in all three cases, the guarantee is true - the program's interval will indeed contain the true value at least 90% of the time, if the experiment returns the true value plus Gaussian noise.</p>\n<p>So you run the experiment and feed in the data; and the \"confidence intervals\" returned are [0.9-1.5], [2.0-2.2], and [\"Cheesecake\"-\"Cheddar\"].</p>\n<p>The problem may be made clearer by considering the third program, which works as follows:&nbsp; 95% of the time, it does standard frequentist statistics to return an interval which will contain the true value 95% of the time, and on the other 5% of the time, it returns the interval [\"Cheesecake\"-\"Cheddar\"].&nbsp; It is left as an exercise to the reader to show that this program will output an interval containing the true value at least 90% of the time.</p>\n<p>BTW, I'm pretty sure I recall reading that \"90% confidence intervals\" as published in journal papers, in those cases where a true value was later pinned down more precisely, did <em>not</em> contain the true value 90% of the time.&nbsp; So what's the point, even?&nbsp; Just show us the raw data and maybe give us a summary of some likelihoods.</p>\n<p>Parapsychology, the control group for science, would seem to be a thriving field with \"statistically significant\" results aplenty.&nbsp; Oh, sure, the effect sizes are minor.&nbsp; Sure, the effect sizes get even smaller (though still \"statistically significant\") as they collect more data.&nbsp; Sure, if you find that people can telekinetically influence the future, a similar experimental protocol is likely to produce equally good results for telekinetically influencing the past.&nbsp; Of which I am less tempted to say, \"How amazing!&nbsp; The power of the mind is not bound by time or causality!\" and more inclined to say, \"Bad statistics are time-symmetrical.\"&nbsp; But here's the thing:&nbsp; Parapsychologists are constantly protesting that they are playing by all the standard scientific rules, and yet their results are being ignored - that they are unfairly being held to higher standards than everyone else.&nbsp; I'm willing to believe that.&nbsp; It just means that the <em>standard</em> statistical methods of science are so weak and flawed as to permit a field of study to sustain itself in the complete absence of any subject matter.&nbsp; With two-thirds of medical studies in prestigious journals failing to replicate, getting rid of the entire actual subject matter would shrink the field by only 33%. We have to raise the bar high enough to exclude the results claimed by parapsychology under classical frequentist statistics, and then fairly and evenhandedly apply the <em>same</em> bar to the rest of science.</p>\n<p>Michael Vassar has a theory that when an academic field encounters advanced statistical methods, it becomes really productive for ten years and then bogs down because the practitioners have learned how to game the rules.</p>\n<p>For so long as we do not have infinite computing power, there may yet be a place in science for non-Bayesian statistics.&nbsp; The Netflix Prize was not won by using strictly purely Bayesian methods, updating proper priors to proper posteriors.&nbsp; In that acid test of statistical discernment, what worked <em>best</em> was a gigantic ad-hoc mixture of methods.&nbsp; It may be that if you want to get the most mileage out of your data, in this world where we do not have infinite computing power, you'll have to use some ad-hoc tools from the statistical toolbox - tools that throw away some of the data, that make themselves artificially ignorant, that take all sorts of steps that can't be justified in the general case and that are potentially subject to abuse and that will give wrong answers now and then.</p>\n<p>But don't do that, and <em>then </em>turn around and tell me that - of all things! - Bayesian probability theory is <em>too subjective.&nbsp; </em>Probability theory is the <em>math </em>in which the results are <em>theorems </em>and every theorem is compatible with every other theorem and you never get different answers by calculating the same quantity in different ways.&nbsp; To resort to the ad-hoc variable-infested complications of frequentism while preaching your <em>objectivity?&nbsp;</em> I can only compare this with the politicians who go around preaching \"Family values!\" and then get caught soliciting sex in restrooms.&nbsp; So long as you deliver loud sermons and make a big fuss about painting yourself with the right labels, you get identified with that flag - no one bothers to look very hard at what you <em>do.</em>&nbsp; The case of frequentists calling Bayesians \"too subjective\" is worth dwelling on for that aspect alone - emphasizing how important it is to look at what's actually going on instead of just listening to the slogans, and how rare it is for anyone to even glance in that direction.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "wzgcQCrwKfETcBpR9": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9qCN6tRBtksSyXfHu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 75, "baseScore": 82, "extendedScore": null, "score": 0.000134, "legacy": true, "legacyId": "1884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 82, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f6ZLxEWaankRZ2Crv", "6s3xABaXKPdFwA3FS", "mnS2WYLCGJP2kQkRn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-04T22:29:14.176Z", "modifiedAt": null, "url": null, "title": "Arbitrage of prediction markets", "slug": "arbitrage-of-prediction-markets", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:51.564Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4Rrkdz9eRL5HtH6cc/arbitrage-of-prediction-markets", "pageUrlRelative": "/posts/4Rrkdz9eRL5HtH6cc/arbitrage-of-prediction-markets", "linkUrl": "https://www.lesswrong.com/posts/4Rrkdz9eRL5HtH6cc/arbitrage-of-prediction-markets", "postedAtFormatted": "Friday, December 4th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Arbitrage%20of%20prediction%20markets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArbitrage%20of%20prediction%20markets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rrkdz9eRL5HtH6cc%2Farbitrage-of-prediction-markets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Arbitrage%20of%20prediction%20markets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rrkdz9eRL5HtH6cc%2Farbitrage-of-prediction-markets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4Rrkdz9eRL5HtH6cc%2Farbitrage-of-prediction-markets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 350, "htmlBody": "<p>I've noticed something very curious on Intrade markets for 2012 Republican Presidential Nominee - Ron Paul gets 3.5% of getting a nomination - a value that's clearly (and spare me EMH here) wishful thinking of Ron Paul supporters more than any genuine estimate.</p>\n<p>And this brings me to a question - if prediction markets overestimate chance of winning of some rare case, how can I profit from that? Naively if I know true chance is 1%, I win $3.5 99% of the time, and lose $97.5 1% of the time, for expected payoff of $2.5. But my maximum loss is 39x higher than my expected profit, and I won't be getting any money out of it for three more years.</p>\n<p>I'd need to bet significant amount to earn any money out of it, and that would require accepting 39x as high maximum loss. No reasonably prediction market would accept this kind of leverage without some collateral, nor could I get any reasonable loan for it at rates that would make this arbitrage profitable.</p>\n<p>The only way I can think of would be convincing someone with plenty of money that I'm right, and have him provide me with collateral for some (probably very high) portion of the payoff. But if results depend on my ability to convince rich people, that's not prediction market! None of this is a problem for people trying to artificially pump estimates for Ron Paul - they'll just take the loss, and write it off as marketing expense.</p>\n<p>None of these problems occur if some position is vastly overestimated, like 60% estimate if I know true value to be 40% - this would be a cheap bet - maximum loss of $40 for expected profit of $20, and people who want to pump it need to take about as much risk as people who want to bring it back to the true value, not a lot more.</p>\n<p>I'm confused. Is there some nice way to arbitrage this, or is this an inherent weakness of prediction markets and we should only trust positions they pick as leaders, not chances of their  long tail?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4Rrkdz9eRL5HtH6cc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 8, "extendedScore": null, "score": 5.424269852332151e-07, "legacy": true, "legacyId": "1954", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-05T22:50:06.821Z", "modifiedAt": null, "url": null, "title": "Parapsychology: the control group for science", "slug": "parapsychology-the-control-group-for-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:36:39.259Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AllanCrossman", "createdAt": "2009-03-13T20:51:34.435Z", "isAdmin": false, "displayName": "AllanCrossman"}, "userId": "uHa4S4jNkPPrEd8ha", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/enuGsZoFLR4KyEx3n/parapsychology-the-control-group-for-science", "pageUrlRelative": "/posts/enuGsZoFLR4KyEx3n/parapsychology-the-control-group-for-science", "linkUrl": "https://www.lesswrong.com/posts/enuGsZoFLR4KyEx3n/parapsychology-the-control-group-for-science", "postedAtFormatted": "Saturday, December 5th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Parapsychology%3A%20the%20control%20group%20for%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParapsychology%3A%20the%20control%20group%20for%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenuGsZoFLR4KyEx3n%2Fparapsychology-the-control-group-for-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Parapsychology%3A%20the%20control%20group%20for%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenuGsZoFLR4KyEx3n%2Fparapsychology-the-control-group-for-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FenuGsZoFLR4KyEx3n%2Fparapsychology-the-control-group-for-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 451, "htmlBody": "<p class=\"indentboth nobottom mt2\" style=\"padding-left: 30px;\">Parapsychologists are constantly protesting that they are playing by all the standard scientific rules, and yet their results are being ignored - that they are unfairly being held to higher standards than everyone else. I'm willing to believe that. It just means that the <em>standard</em> statistical methods of science are so weak and flawed as to permit a field of study to sustain itself in the complete absence of any subject matter.</p>\n<p class=\"halftop mid indent5 mb2\" style=\"padding-left: 120px;\">&mdash; Eliezer Yudkowsky, <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/\">Frequentist Statistics are Frequently Subjective</a></p>\n<p>Imagine if, way back at the start of the scientific enterprise, someone had said, \"What we really need is a control group for science - people who will behave exactly like scientists, doing experiments, publishing journals, and so on, but whose field of study is completely empty: one in which the null hypothesis is always true.</p>\n<p>\"That way, we'll be able to gauge the effect of publication bias, experimental error, misuse of statistics, data fraud, and so on, which will help us understand how serious such problems are in the real scientific literature.\"</p>\n<p>Isn't that a great idea?</p>\n<p>By an accident of historical chance, we actually have exactly such a control group, namely parapsychologists: people who study extra-sensory perception, telepathy, precognition, <a href=\"http://www.parapsych.org/faq_file1.html#6\">and so on</a>.<a id=\"more\"></a></p>\n<p>There's no particular reason to think parapsychologists are doing anything other than what scientists would do; their experiments are similar to those of scientists, they use statistics in similar ways, and there's no reason to think they falsify data any more than any other group. Yet despite the fact that their null hypotheses are always true, parapsychologists get positive results.</p>\n<p>This is disturbing, and must lead us to wonder how many positive results in real science are <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">actually wrong</a>.</p>\n<p>The point of all this is not to mock parapsychology for the sake of it, but rather to emphasise that <strong>parapsychology is useful as a control group for science</strong>. Scientists should aim to improve their procedures to the point where, if the control group used these same procedures, they would get an acceptably low level of positive results. That this is not yet the case indicates the need for more stringent scientific procedures.</p>\n<h2>Acknowledgements</h2>\n<p>The idea for this mini-essay and many of its actual points were suggested by (or stolen from) Eliezer Yudkowsky's <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective\">Frequentist Statistics are Frequently Subjective</a>, though the idea <a href=\"/lw/14w/she_blinded_me_with_science/10h7\">might have</a> originated with Michael Vassar.</p>\n<p>This was originally published at a different location on the web, but was moved here for bandwidth reasons at Eliezer's suggestion.</p>\n<h2>Comments / criticisms</h2>\n<p>A <a href=\"http://news.ycombinator.com/item?id=978180\">discussion</a> on Hacker News contained one very astute criticism: that some things which may once have been considered part of parapsychology actually turned out to be real, though with perfectly sensible, physical causes. Still, I think this is unlikely for the more exotic subjects like telepathy, precognition, <em>et cetera</em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "vg4LDxjdwHLotCm8w": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "enuGsZoFLR4KyEx3n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 89, "extendedScore": null, "score": 0.000147, "legacy": true, "legacyId": "1955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 90, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"indentboth nobottom mt2\" style=\"padding-left: 30px;\">Parapsychologists are constantly protesting that they are playing by all the standard scientific rules, and yet their results are being ignored - that they are unfairly being held to higher standards than everyone else. I'm willing to believe that. It just means that the <em>standard</em> statistical methods of science are so weak and flawed as to permit a field of study to sustain itself in the complete absence of any subject matter.</p>\n<p class=\"halftop mid indent5 mb2\" style=\"padding-left: 120px;\">\u2014 Eliezer Yudkowsky, <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/\">Frequentist Statistics are Frequently Subjective</a></p>\n<p>Imagine if, way back at the start of the scientific enterprise, someone had said, \"What we really need is a control group for science - people who will behave exactly like scientists, doing experiments, publishing journals, and so on, but whose field of study is completely empty: one in which the null hypothesis is always true.</p>\n<p>\"That way, we'll be able to gauge the effect of publication bias, experimental error, misuse of statistics, data fraud, and so on, which will help us understand how serious such problems are in the real scientific literature.\"</p>\n<p>Isn't that a great idea?</p>\n<p>By an accident of historical chance, we actually have exactly such a control group, namely parapsychologists: people who study extra-sensory perception, telepathy, precognition, <a href=\"http://www.parapsych.org/faq_file1.html#6\">and so on</a>.<a id=\"more\"></a></p>\n<p>There's no particular reason to think parapsychologists are doing anything other than what scientists would do; their experiments are similar to those of scientists, they use statistics in similar ways, and there's no reason to think they falsify data any more than any other group. Yet despite the fact that their null hypotheses are always true, parapsychologists get positive results.</p>\n<p>This is disturbing, and must lead us to wonder how many positive results in real science are <a href=\"http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124\">actually wrong</a>.</p>\n<p>The point of all this is not to mock parapsychology for the sake of it, but rather to emphasise that <strong>parapsychology is useful as a control group for science</strong>. Scientists should aim to improve their procedures to the point where, if the control group used these same procedures, they would get an acceptably low level of positive results. That this is not yet the case indicates the need for more stringent scientific procedures.</p>\n<h2 id=\"Acknowledgements\">Acknowledgements</h2>\n<p>The idea for this mini-essay and many of its actual points were suggested by (or stolen from) Eliezer Yudkowsky's <a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective\">Frequentist Statistics are Frequently Subjective</a>, though the idea <a href=\"/lw/14w/she_blinded_me_with_science/10h7\">might have</a> originated with Michael Vassar.</p>\n<p>This was originally published at a different location on the web, but was moved here for bandwidth reasons at Eliezer's suggestion.</p>\n<h2 id=\"Comments___criticisms\">Comments / criticisms</h2>\n<p>A <a href=\"http://news.ycombinator.com/item?id=978180\">discussion</a> on Hacker News contained one very astute criticism: that some things which may once have been considered part of parapsychology actually turned out to be real, though with perfectly sensible, physical causes. Still, I think this is unlikely for the more exotic subjects like telepathy, precognition, <em>et cetera</em>.</p>", "sections": [{"title": "Acknowledgements", "anchor": "Acknowledgements", "level": 1}, {"title": "Comments / criticisms", "anchor": "Comments___criticisms", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "188 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 188, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9qCN6tRBtksSyXfHu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-06T13:39:04.368Z", "modifiedAt": null, "url": null, "title": "Science - Idealistic Versus Signaling", "slug": "science-idealistic-versus-signaling", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:35.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qi6tPtvSEmEdg6wvs/science-idealistic-versus-signaling", "pageUrlRelative": "/posts/Qi6tPtvSEmEdg6wvs/science-idealistic-versus-signaling", "linkUrl": "https://www.lesswrong.com/posts/Qi6tPtvSEmEdg6wvs/science-idealistic-versus-signaling", "postedAtFormatted": "Sunday, December 6th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20-%20Idealistic%20Versus%20Signaling&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20-%20Idealistic%20Versus%20Signaling%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQi6tPtvSEmEdg6wvs%2Fscience-idealistic-versus-signaling%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20-%20Idealistic%20Versus%20Signaling%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQi6tPtvSEmEdg6wvs%2Fscience-idealistic-versus-signaling", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQi6tPtvSEmEdg6wvs%2Fscience-idealistic-versus-signaling", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 677, "htmlBody": "<p>[This is a version of an first draft essay I wrote for my <a href=\"http://williambswift.blogspot.com/2009/12/first-draft-science-idealistic-versus.html\">blog</a>.&nbsp; I intend to write another version, but it is going to take some time to research, and I want to get this out where I can start getting some feedback and sources for further research.]</p>\n<p>The responses to the recent leaking of the CRU's information and emails, has led me to a changed understanding of science and how it is viewed by various people, especially people who claim to be scientists. Among people who actually do or consume science there seem to be two broad views - what they \"believe\" about science, rather than what they normally \"say\" about science when asked.</p>\n<p>The classical view, what I have begun thinking of as the idealistic view, is science as the search for reliable knowledge. This is the version most scientists (and many non-scientists) espouse when asked, but increasingly many scientists actually hold another view when their beliefs are evaluated by their actions.</p>\n<p>This is the signaling and control view of science. This is the \"social network\" view that has been developed by many sociologists of science.</p>\n<p><a id=\"more\"></a></p>\n<p>For an extended example of the two views in conflict, see this recent thread of 369 comments <a href=\"http://esr.ibiblio.org/?p=1487\">Facts to fit the theory? Actually, no facts at all!</a> . PhysicistDave is the best exemplar of the idealistic view, with pete and several others having extreme signaling and control viewpoints.</p>\n<p>I wonder how much of the fact that there hasn't been any fundamental breakthroughs in the last fifty years has to do with the effective takeover of science by academics and government - that is by the signaling and control view. Maybe we have too many \"accredited\" scientists and they are too beholden to government, and to a lesser extent other grant-making organizations - and they have crowded out or controlled real, idealistic science.</p>\n<p>This can also explain the conflict between those who extol peer review, despite its many flaws, and downplay open source science. They are controlling view scientists protecting their turf and power and prerogatives. Anyone thinking about the ideals of science, the classical view of science, immediately realizes that open sourcing the arguments and data will meet the ends of extending knowledge much better than peer review, now that it is possible. Peer review was a stop gap means of getting a quick review of a paper that was necessary when the costs of distributing information was high, but it is now obsolescent at best.</p>\n<p>Instead the senior scientists and journal editors are protecting their power by protecting peer review.</p>\n<p>Bureaucrats, and especially teachers, will tend strongly toward the signaling and control view.</p>\n<p>Economics and other social \"sciences\" will tend toward signaling and control view - for examples see Robin Hanson's and Tyler Cowen's take on the CRU leak with their claims that this is just how academia really works and pete, who claims a Masters in economics, in the comment thread linked above.</p>\n<p>Robin Hanson's <a href=\"http://www.overcomingbias.com/2009/11/its-news-on-academia-not-climate.html\">It's News on Academia, Not Climate</a></p>\n<blockquote>Yup, this behavior has long been typical when academics form competing groups, whether the public hears about such groups or not. If you knew how academia worked, this news would not surprise you nor change your opinions on global warming. I&rsquo;ve never done this stuff, and I&rsquo;d like to think I wouldn&rsquo;t, but that is cheap talk since I haven&rsquo;t had the opportunity. This works as a &ldquo;scandal&rdquo; only because of academia&rsquo;s overly idealistic public image.</blockquote>\n<p>And Tyler Cowen in <a href=\"http://www.marginalrevolution.com/marginalrevolution/2009/11/the-lessons-of-climategate.html\">The lessons of \"Climategate\",</a></p>\n<blockquote>In other words, I don't think there's much here, although the episode should remind us of some common yet easily forgotten lessons.</blockquote>\n<p>Of course, both Hanson and Cowen believe in AGW, so these might just be attempts to avoid facing anything they don't want to look at.</p>\n<p>As I discussed earlier, those who continue to advocate the general use of peer review will tend strongly toward the signaling and control view.</p>\n<p>Newer scientists will tend more toward the classical, idealistic view; while more mature scientists as they gain stature and power (especially as they enter administration and editing) will turn increasingly signaling and control oriented.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qi6tPtvSEmEdg6wvs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 10, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "1960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-09T04:25:13.746Z", "modifiedAt": null, "url": null, "title": "You Be the Jury: Survey on a Current Event", "slug": "you-be-the-jury-survey-on-a-current-event", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:27.625Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mkAcXPEJ7RZCJs8ry/you-be-the-jury-survey-on-a-current-event", "pageUrlRelative": "/posts/mkAcXPEJ7RZCJs8ry/you-be-the-jury-survey-on-a-current-event", "linkUrl": "https://www.lesswrong.com/posts/mkAcXPEJ7RZCJs8ry/you-be-the-jury-survey-on-a-current-event", "postedAtFormatted": "Wednesday, December 9th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20Be%20the%20Jury%3A%20Survey%20on%20a%20Current%20Event&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20Be%20the%20Jury%3A%20Survey%20on%20a%20Current%20Event%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAcXPEJ7RZCJs8ry%2Fyou-be-the-jury-survey-on-a-current-event%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20Be%20the%20Jury%3A%20Survey%20on%20a%20Current%20Event%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAcXPEJ7RZCJs8ry%2Fyou-be-the-jury-survey-on-a-current-event", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmkAcXPEJ7RZCJs8ry%2Fyou-be-the-jury-survey-on-a-current-event", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 840, "htmlBody": "<p>As many of you probably know, in an Italian court early last weekend, two young students, Amanda Knox and Raffaele Sollecito, were convicted of killing another young student, Meredith Kercher, in a horrific way in November of 2007. (A third person, Rudy Guede, was convicted earlier.)</p>\n<p>If you aren't familiar with the case, don't go reading about it just yet. Hang on for just a moment.</p>\n<p>If you are familiar, that's fine too. This post is addressed to readers of all levels of acquaintance with the story.</p>\n<p>What everyone should know right away is that the verdict has been extremely controversial. Strong feelings have emerged, even involving national tensions (Knox is American, Sollecito Italian, and Kercher British, and the crime and trial took place in Italy). The circumstances of the crime involve sex. In short, the potential for serious rationality failures in coming to an opinion on a case like this is enormous.&nbsp;&nbsp;</p>\n<p>Now, as it happens, I myself have an opinion. A rather strong one, in fact. Strong enough that I caught myself thinking that this case -- given all the controversy surrounding it -- might serve as a decent litmus test in judging the rationality skills of other people. Like religion, or evolution -- except less clich&eacute;d (and <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached</a>) and more down-and-dirty.</p>\n<p>Of course, thoughts like that can be dangerous, as I quickly recognized. The danger of <a href=\"http://wiki.lesswrong.com/wiki/Groupthink\">in-group</a> <a href=\"http://wiki.lesswrong.com/wiki/Affective_death_spiral\">affective spirals</a> looms large. So before writing up that Less Wrong post adding my-opinion-on-the-guilt-or-innocence-of-Amanda-Knox-and-Raffaele-Sollecito to the List of Things Every Rational Person Must Believe, I decided it might be useful to find out what conclusion(s) other aspiring rationalists would (or have) come to (without knowing my opinion).</p>\n<p>So that's what this post is: a survey/experiment, with fairly specific yet flexible instructions (which differ slightly depending on how much you know about the case already).</p>\n<p><a id=\"more\"></a></p>\n<p><strong>For those whose familiarity with the case is low:</strong></p>\n<p>I'm going to give you two websites advocating a position, one strongly in favor of the verdict, the other strongly opposed. Your job will be to browse around these sites to learn info about the case, as much as you need to in order to arrive at a judgment. The order, manner, and quantity of browsing will be left up to you -- though I would of course like to know how much you read in your response.</p>\n<p>1. <a href=\"http://truejustice.org/ee/index.php\">Site arguing defendants are guilty.</a>&nbsp;</p>\n<p>2. <a href=\"http://www.friendsofamanda.org/\">Site arguing defendants are innocent.</a></p>\n<p>I've chosen these particular sites because they seemed to contain the best combination of fierceness of advocacy and quantity of information on their respective sides that I could find.&nbsp;</p>\n<p>If you find better summaries, or think that these choices reflect a bias or betray my own opinion, by all means let me know. I'm specifically avoiding referring you to media reports, however, for a couple of reasons. First, I've noticed that reports often contain factual inaccuracies (necessarily, because they contradict each other). Secondly, journalists don't usually have much of a stake, and I'd like to see how folks respond to passionate advocacy by people who care about the outcome, as in an actual trial, rather than attempts at neutral summarizing. Of course, it's fine if you want to read media reports linked to by the above sites.</p>\n<p>(One potential&nbsp;problem is that the first site is organized like a blog or forum, and thus it is hard to find a quick summary of the case there. [EDIT:&nbsp;Be sure to look at the category links on the right side of the page to find the arguments.]&nbsp;If you think it necessary, refer to the ever-changing&nbsp;<a href=\"http://en.wikipedia.org/wiki/Murder_of_Meredith_Kercher\">Wikipedia article</a>, which at the moment of writing seems a bit more favorable to the prosecution. [EDIT: I'm no longer sure that's true.] [EDIT: Now I think it's true again,&nbsp;the article&nbsp;having apparently changed&nbsp;some more.&nbsp;So there's really no telling. Be warned.])</p>\n<p>After you do this reading, I'd like to know:</p>\n<p>1. Your probability estimate that Amanda Knox is guilty.<br />2. Your probability estimate that Raffaele Sollecito is guilty.<br />3. Your probability estimate that Rudy Guede is guilty.<br />4. How much you think your opinion will turn out to coincide with mine.</p>\n<p>Feel free to elaborate on your reasoning to whatever degree you like.</p>\n<p>One request: don't look at others' comments until you've done the experiment yourself!</p>\n<p><strong>For those whose&nbsp;familiarity with the case is moderate or high:</strong></p>\n<p>I'd like to know, as of right now:</p>\n<p>1. Your probability estimate that Amanda Knox is guilty.<br />2. Your probability estimate that Raffaele Sollecito is guilty.<br />3. Your probability estimate that Rudy Guede is guilty.<br />4. How much you think your opinion will turn out to coincide with mine.<br />5. From what sources you've gotten the info you've used to arrive at these estimates.</p>\n<p>Then, if possible, do the experiment described above for those with little familiarity, and report any shifts in your estimates.</p>\n<p><br />Again, everyone should avoid looking at others'&nbsp;responses before giving their own feedback. Also, don't forget to identify your prior level of familiarity!</p>\n<p>If the level of participation warrants it, I'll post my own thoughts (and reaction to the feedback here) in a later post. (<strong>Edit:</strong> That post can be found <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">here</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DWWZwkxTJs4d5WrcX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mkAcXPEJ7RZCJs8ry", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 36, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "1971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As many of you probably know, in an Italian court early last weekend, two young students, Amanda Knox and Raffaele Sollecito, were convicted of killing another young student, Meredith Kercher, in a horrific way in November of 2007. (A third person, Rudy Guede, was convicted earlier.)</p>\n<p>If you aren't familiar with the case, don't go reading about it just yet. Hang on for just a moment.</p>\n<p>If you are familiar, that's fine too. This post is addressed to readers of all levels of acquaintance with the story.</p>\n<p>What everyone should know right away is that the verdict has been extremely controversial. Strong feelings have emerged, even involving national tensions (Knox is American, Sollecito Italian, and Kercher British, and the crime and trial took place in Italy). The circumstances of the crime involve sex. In short, the potential for serious rationality failures in coming to an opinion on a case like this is enormous.&nbsp;&nbsp;</p>\n<p>Now, as it happens, I myself have an opinion. A rather strong one, in fact. Strong enough that I caught myself thinking that this case -- given all the controversy surrounding it -- might serve as a decent litmus test in judging the rationality skills of other people. Like religion, or evolution -- except less clich\u00e9d (and <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached</a>) and more down-and-dirty.</p>\n<p>Of course, thoughts like that can be dangerous, as I quickly recognized. The danger of <a href=\"http://wiki.lesswrong.com/wiki/Groupthink\">in-group</a> <a href=\"http://wiki.lesswrong.com/wiki/Affective_death_spiral\">affective spirals</a> looms large. So before writing up that Less Wrong post adding my-opinion-on-the-guilt-or-innocence-of-Amanda-Knox-and-Raffaele-Sollecito to the List of Things Every Rational Person Must Believe, I decided it might be useful to find out what conclusion(s) other aspiring rationalists would (or have) come to (without knowing my opinion).</p>\n<p>So that's what this post is: a survey/experiment, with fairly specific yet flexible instructions (which differ slightly depending on how much you know about the case already).</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"For_those_whose_familiarity_with_the_case_is_low_\">For those whose familiarity with the case is low:</strong></p>\n<p>I'm going to give you two websites advocating a position, one strongly in favor of the verdict, the other strongly opposed. Your job will be to browse around these sites to learn info about the case, as much as you need to in order to arrive at a judgment. The order, manner, and quantity of browsing will be left up to you -- though I would of course like to know how much you read in your response.</p>\n<p>1. <a href=\"http://truejustice.org/ee/index.php\">Site arguing defendants are guilty.</a>&nbsp;</p>\n<p>2. <a href=\"http://www.friendsofamanda.org/\">Site arguing defendants are innocent.</a></p>\n<p>I've chosen these particular sites because they seemed to contain the best combination of fierceness of advocacy and quantity of information on their respective sides that I could find.&nbsp;</p>\n<p>If you find better summaries, or think that these choices reflect a bias or betray my own opinion, by all means let me know. I'm specifically avoiding referring you to media reports, however, for a couple of reasons. First, I've noticed that reports often contain factual inaccuracies (necessarily, because they contradict each other). Secondly, journalists don't usually have much of a stake, and I'd like to see how folks respond to passionate advocacy by people who care about the outcome, as in an actual trial, rather than attempts at neutral summarizing. Of course, it's fine if you want to read media reports linked to by the above sites.</p>\n<p>(One potential&nbsp;problem is that the first site is organized like a blog or forum, and thus it is hard to find a quick summary of the case there. [EDIT:&nbsp;Be sure to look at the category links on the right side of the page to find the arguments.]&nbsp;If you think it necessary, refer to the ever-changing&nbsp;<a href=\"http://en.wikipedia.org/wiki/Murder_of_Meredith_Kercher\">Wikipedia article</a>, which at the moment of writing seems a bit more favorable to the prosecution. [EDIT: I'm no longer sure that's true.] [EDIT: Now I think it's true again,&nbsp;the article&nbsp;having apparently changed&nbsp;some more.&nbsp;So there's really no telling. Be warned.])</p>\n<p>After you do this reading, I'd like to know:</p>\n<p>1. Your probability estimate that Amanda Knox is guilty.<br>2. Your probability estimate that Raffaele Sollecito is guilty.<br>3. Your probability estimate that Rudy Guede is guilty.<br>4. How much you think your opinion will turn out to coincide with mine.</p>\n<p>Feel free to elaborate on your reasoning to whatever degree you like.</p>\n<p>One request: don't look at others' comments until you've done the experiment yourself!</p>\n<p><strong id=\"For_those_whose_familiarity_with_the_case_is_moderate_or_high_\">For those whose&nbsp;familiarity with the case is moderate or high:</strong></p>\n<p>I'd like to know, as of right now:</p>\n<p>1. Your probability estimate that Amanda Knox is guilty.<br>2. Your probability estimate that Raffaele Sollecito is guilty.<br>3. Your probability estimate that Rudy Guede is guilty.<br>4. How much you think your opinion will turn out to coincide with mine.<br>5. From what sources you've gotten the info you've used to arrive at these estimates.</p>\n<p>Then, if possible, do the experiment described above for those with little familiarity, and report any shifts in your estimates.</p>\n<p><br>Again, everyone should avoid looking at others'&nbsp;responses before giving their own feedback. Also, don't forget to identify your prior level of familiarity!</p>\n<p>If the level of participation warrants it, I'll post my own thoughts (and reaction to the feedback here) in a later post. (<strong>Edit:</strong> That post can be found <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">here</a>.)</p>", "sections": [{"title": "For those whose familiarity with the case is low:", "anchor": "For_those_whose_familiarity_with_the_case_is_low_", "level": 1}, {"title": "For those whose\u00a0familiarity with the case is moderate or high:", "anchor": "For_those_whose_familiarity_with_the_case_is_moderate_or_high_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "266 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 266, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-10T21:57:02.812Z", "modifiedAt": null, "url": null, "title": "Probability Space & Aumann Agreement", "slug": "probability-space-and-aumann-agreement", "viewCount": null, "lastCommentedAt": "2020-09-21T21:54:55.684Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JdK3kr4ug9kJvKzGy/probability-space-and-aumann-agreement", "pageUrlRelative": "/posts/JdK3kr4ug9kJvKzGy/probability-space-and-aumann-agreement", "linkUrl": "https://www.lesswrong.com/posts/JdK3kr4ug9kJvKzGy/probability-space-and-aumann-agreement", "postedAtFormatted": "Thursday, December 10th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20Space%20%26%20Aumann%20Agreement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20Space%20%26%20Aumann%20Agreement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdK3kr4ug9kJvKzGy%2Fprobability-space-and-aumann-agreement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20Space%20%26%20Aumann%20Agreement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdK3kr4ug9kJvKzGy%2Fprobability-space-and-aumann-agreement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJdK3kr4ug9kJvKzGy%2Fprobability-space-and-aumann-agreement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1381, "htmlBody": "<p>The first part of this post describes a way of interpreting the basic mathematics of Bayesianism. Eliezer already presented one such view at <a href=\"/lw/hk/priors_as_mathematical_objects/\">http://lesswrong.com/lw/hk/priors_as_mathematical_objects/</a>, but I want to present another one that has been useful to me, and also show how this view is related to the standard formalism of probability theory and Bayesian updating, namely the probability space.</p>\n<p>The second part of this post will build upon the first, and try to explain the math behind <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann's agreement theorem</a>. Hal Finney had suggested this <a href=\"/lw/12y/the_popularization_bias/xx7\">earlier</a>, and I'm taking on the task now because I recently went through the exercise of learning it, and could use a check of my understanding. The last part will give some of my current thoughts on Aumann agreement.</p>\n<h4><a id=\"more\"></a>Probability Space</h4>\n<p class=\"MsoNormal\">In <a href=\"http://en.wikipedia.org/wiki/Probability_space\">http://en.wikipedia.org/wiki/Probability_space</a>, you can see that a probability space consists of a triple:</p>\n<ul>\n<li>&Omega; &ndash; a non-empty set &ndash; usually called sample space, or set of states</li>\n<li><span style=\"font-size:10.0pt;line-height:115%; font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;color:black;mso-no-proof:yes\">F</span> &ndash; a set of subsets of&nbsp;&Omega; &ndash; usually called&nbsp;sigma-algebra, or&nbsp;set of events</li>\n<li>P &ndash; a function from F to [0,1] &ndash; usually called probability measure</li>\n</ul>\n<p>F and P are required to have certain additional properties, but I'll ignore them for now. To start with, we&rsquo;ll interpret &Omega; as a set of possible world-histories. (To eliminate anthropic reasoning&nbsp;issues, let&rsquo;s assume that each possible world-history contains the same number of observers, who have perfect memory, and are labeled with unique serial numbers.) Each &ldquo;event&rdquo; A in F is formally a subset of&nbsp;&Omega;, and interpreted as either an actual event that occurs in every world-history in A, or a hypothesis which is true in the world-histories in A. (The details of the events or hypotheses themselves are abstracted away here.)</p>\n<p class=\"MsoNormal\">To understand the probability measure P, it&rsquo;s easier to first introduce the&nbsp;<a title=\"Probability mass function\" href=\"http://en.wikipedia.org/wiki/Probability_mass_function\">probability mass function</a>&nbsp;<span style=\"mso-bidi-font-style:italic\">p, which assigns a probability to each element of </span>&Omega;, with the probabilities summing to 1. Then P(A) is just the sum of the probabilities of the elements in A. (For simplicity, I&rsquo;m assuming the discrete case, where &Omega; is at most countable.) In other words, the probability of an observation is the sum of the probabilities of the world-histories that it&nbsp;doesn't&nbsp;rule out.</p>\n<p class=\"MsoNormal\">A payoff of this view of the probability space is a simple understanding of what Bayesian updating is. Once an observer sees an event D, he can rule out all possible world-histories that are not in D. So, he can get a posterior probability measure by setting the probability masses of all world-histories not in D to 0, and renormalizing the ones in D so that they sum up to 1 while keeping the same relative ratios. You can easily verify that this is equivalent to Bayes&rsquo; rule: P(H|D) = P(D <span style=\"mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin\">&cap;</span> H)/P(D).</p>\n<p class=\"MsoNormal\">To sum up, the mathematical objects behind Bayesianism can be seen as</p>\n<ul>\n<li>&Omega; &ndash; a set of possible world-histories</li>\n<li><span style=\"font-size:10.0pt;line-height:115%; font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;color:black;mso-no-proof:yes\">F</span> &ndash; information about which events occur in which possible world-histories</li>\n<li>P &ndash; a set of weights on the world-histories that sum up to 1</li>\n</ul>\n<h4>Aumann's Agreement Theorem</h4>\n<p>Aumann's agreement theorem says that if two Bayesians share the same probability space but possibly different information partitions, and have common knowledge of their information partitions and posterior&nbsp;probabilities&nbsp;of some event A, then their&nbsp;posterior&nbsp;probabilities of that event must be equal. So what are information partitions, and what does \"common knowledge\" mean?</p>\n<p>The information partition I of an observer-moment M divides&nbsp;&Omega; into a number of subsets that&nbsp;are non-overlapping, and together cover all of&nbsp;&Omega;. Two possible world-histories w1 and w2 are placed into the same subset if the observer-moments in w1 and w2 have the exact same information. In other words, if w1 and w2 are in the same element of I, and w1 is the actual world-history, then M can't rule out either w1 or w2.&nbsp;I(w) is used to denote the element of I that contains w.</p>\n<p>Common knowledge is defined as follows: If w is the actual world-history and two agents have information partitions I and J, an event E is common knowledge if E includes the member of the meet I&and;J that contains w. The operation&nbsp;&and; (meet) means to take the two sets I and J, form their union, then repeatedly merge any of its elements (which you recall are subsets of&nbsp;&Omega;) that overlap until it becomes a partition again (i.e., no two elements overlap).</p>\n<p>It may not be clear&nbsp;at first&nbsp;what this meet operation has to do with common knowledge. Suppose the actual world-history is w. Then agent 1 knows I(w), so he knows that agent 2 must know one of the elements of J that overlaps with I(w). And he can reason that agent 2 must know that agent 1 knows one of the elements of I that overlaps with one of these elements of J. If he carries out this inference to infinity, he'll find that both agents know that the actual world-history is in (I&and;J)(w), and both know the other know, and both know the other know the other know, and so on. In other words it is common knowledge that the actual world-history is in&nbsp;(I&and;J)(w).&nbsp;Since event E occurs in every world-history in&nbsp;(I&and;J)(w), it's common knowledge that E occurs in the actual world-history.</p>\n<p>Proof for the agreement theorem then goes like this. Let E be the event that agent 1 assigns a posterior probability (conditioned on everything it knows) of q1 to event A and agent 2 assigns a posterior probability of q2 to event A. If E is common knowledge at w, then both agents know that&nbsp;P(A | I(v)) = q1 and P(A | J(v)) = q2 for every v in&nbsp;(I&and;J)(w). But&nbsp;this implies&nbsp;P(A | (I&and;J)(w)) = q1 and&nbsp;P(A | (I&and;J)(w)) = q2 and therefore q1 = q2.&nbsp;(To see this, suppose you currently know only (I&and;J)(w), and you know that no matter what additional information I(v) you obtain, your posterior probability will be the same q1, then your current probability must already be q1.)</p>\n<p><span style=\"font-size: 14px; font-weight: bold;\">Is Aumann Agreement Overrated?</span></p>\n<p>Having explained all of that, it seems to me that this theorem is less relevant to a practical rationalist than I thought before I really understood it.&nbsp;After looking at the math, it's apparent that \"common knowledge\" is a much stricter requirement than it sounds. The most obvious way to achieve it is for the two agents to simply tell each other I(w) and J(w), after which they share a new, common information partition. But in that case, agreement itself is obvious and there is no need to learn or understand Aumann's theorem.</p>\n<p>There are some papers that describe ways to achieve agreement in other ways, such as <a href=\"http://scholar.google.com/scholar?q=&quot;We Can&rsquo;t Disagree Forever&quot;\">iterative exchange of posterior probabilities</a>. But in such methods, the agents aren't just moving closer to each other's beliefs. Rather, they go through convoluted chains of deduction to infer what information the other agent must have observed, given his declarations, and then update on that new information. (The process is similar to the one needed to solve the second riddle on <a href=\"http://research.microsoft.com/en-us/events/15years/riddles.aspx\">this page</a>.) The two agents essentially still have to communicate I(w) and J(w) to each other, except they do so by exchanging posterior probabilities and making logical inferences from them.</p>\n<p>Is this realistic for human rationalist wannabes? It seems wildly implausible to me that two humans can&nbsp;communicate all of the information they have that is relevant to the truth of some statement just by repeatedly exchanging degrees of belief about it, except in very simple situations. You need to know the other agent's information partition exactly in order to narrow down which element of the information partition he is in from his&nbsp;probability declaration, and he needs to know that you know so that he can deduce what inference you're making, in order to continue to the next step, and so on. One error in this process and the whole thing falls apart.&nbsp;It seems much easier to just tell each other what information the two of you have directly.</p>\n<p>Finally, I now see that until the exchange of information completes and common knowledge/agreement is actually achieved, it's rational for even honest truth-seekers who share common priors to disagree. Therefore, two such rationalists may persistently disagree just because the amount of information they would have to exchange in order to reach agreement is too great to be practical. This is quite different from the understanding of Aumann agreement I had before I read the math.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WH5ZmNSjZmK9SMj7k": 2, "Ng8Gice9KNkncxqcj": 2, "LhX3F2SvGDarZCuh6": 2, "bh7uxTTqmsQ8jZJdB": 2, "kCuRQE5Tkv9zeKyzK": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JdK3kr4ug9kJvKzGy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 51, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "1965", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The first part of this post describes a way of interpreting the basic mathematics of Bayesianism. Eliezer already presented one such view at <a href=\"/lw/hk/priors_as_mathematical_objects/\">http://lesswrong.com/lw/hk/priors_as_mathematical_objects/</a>, but I want to present another one that has been useful to me, and also show how this view is related to the standard formalism of probability theory and Bayesian updating, namely the probability space.</p>\n<p>The second part of this post will build upon the first, and try to explain the math behind <a href=\"http://wiki.lesswrong.com/wiki/Aumann's_agreement_theorem\">Aumann's agreement theorem</a>. Hal Finney had suggested this <a href=\"/lw/12y/the_popularization_bias/xx7\">earlier</a>, and I'm taking on the task now because I recently went through the exercise of learning it, and could use a check of my understanding. The last part will give some of my current thoughts on Aumann agreement.</p>\n<h4 id=\"Probability_Space\"><a id=\"more\"></a>Probability Space</h4>\n<p class=\"MsoNormal\">In <a href=\"http://en.wikipedia.org/wiki/Probability_space\">http://en.wikipedia.org/wiki/Probability_space</a>, you can see that a probability space consists of a triple:</p>\n<ul>\n<li>\u03a9 \u2013 a non-empty set \u2013 usually called sample space, or set of states</li>\n<li><span style=\"font-size:10.0pt;line-height:115%; font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;color:black;mso-no-proof:yes\">F</span> \u2013 a set of subsets of&nbsp;\u03a9 \u2013 usually called&nbsp;sigma-algebra, or&nbsp;set of events</li>\n<li>P \u2013 a function from F to [0,1] \u2013 usually called probability measure</li>\n</ul>\n<p>F and P are required to have certain additional properties, but I'll ignore them for now. To start with, we\u2019ll interpret \u03a9 as a set of possible world-histories. (To eliminate anthropic reasoning&nbsp;issues, let\u2019s assume that each possible world-history contains the same number of observers, who have perfect memory, and are labeled with unique serial numbers.) Each \u201cevent\u201d A in F is formally a subset of&nbsp;\u03a9, and interpreted as either an actual event that occurs in every world-history in A, or a hypothesis which is true in the world-histories in A. (The details of the events or hypotheses themselves are abstracted away here.)</p>\n<p class=\"MsoNormal\">To understand the probability measure P, it\u2019s easier to first introduce the&nbsp;<a title=\"Probability mass function\" href=\"http://en.wikipedia.org/wiki/Probability_mass_function\">probability mass function</a>&nbsp;<span style=\"mso-bidi-font-style:italic\">p, which assigns a probability to each element of </span>\u03a9, with the probabilities summing to 1. Then P(A) is just the sum of the probabilities of the elements in A. (For simplicity, I\u2019m assuming the discrete case, where \u03a9 is at most countable.) In other words, the probability of an observation is the sum of the probabilities of the world-histories that it&nbsp;doesn't&nbsp;rule out.</p>\n<p class=\"MsoNormal\">A payoff of this view of the probability space is a simple understanding of what Bayesian updating is. Once an observer sees an event D, he can rule out all possible world-histories that are not in D. So, he can get a posterior probability measure by setting the probability masses of all world-histories not in D to 0, and renormalizing the ones in D so that they sum up to 1 while keeping the same relative ratios. You can easily verify that this is equivalent to Bayes\u2019 rule: P(H|D) = P(D <span style=\"mso-bidi-font-family:Calibri;mso-bidi-theme-font:minor-latin\">\u2229</span> H)/P(D).</p>\n<p class=\"MsoNormal\">To sum up, the mathematical objects behind Bayesianism can be seen as</p>\n<ul>\n<li>\u03a9 \u2013 a set of possible world-histories</li>\n<li><span style=\"font-size:10.0pt;line-height:115%; font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;color:black;mso-no-proof:yes\">F</span> \u2013 information about which events occur in which possible world-histories</li>\n<li>P \u2013 a set of weights on the world-histories that sum up to 1</li>\n</ul>\n<h4 id=\"Aumann_s_Agreement_Theorem\">Aumann's Agreement Theorem</h4>\n<p>Aumann's agreement theorem says that if two Bayesians share the same probability space but possibly different information partitions, and have common knowledge of their information partitions and posterior&nbsp;probabilities&nbsp;of some event A, then their&nbsp;posterior&nbsp;probabilities of that event must be equal. So what are information partitions, and what does \"common knowledge\" mean?</p>\n<p>The information partition I of an observer-moment M divides&nbsp;\u03a9 into a number of subsets that&nbsp;are non-overlapping, and together cover all of&nbsp;\u03a9. Two possible world-histories w1 and w2 are placed into the same subset if the observer-moments in w1 and w2 have the exact same information. In other words, if w1 and w2 are in the same element of I, and w1 is the actual world-history, then M can't rule out either w1 or w2.&nbsp;I(w) is used to denote the element of I that contains w.</p>\n<p>Common knowledge is defined as follows: If w is the actual world-history and two agents have information partitions I and J, an event E is common knowledge if E includes the member of the meet I\u2227J that contains w. The operation&nbsp;\u2227 (meet) means to take the two sets I and J, form their union, then repeatedly merge any of its elements (which you recall are subsets of&nbsp;\u03a9) that overlap until it becomes a partition again (i.e., no two elements overlap).</p>\n<p>It may not be clear&nbsp;at first&nbsp;what this meet operation has to do with common knowledge. Suppose the actual world-history is w. Then agent 1 knows I(w), so he knows that agent 2 must know one of the elements of J that overlaps with I(w). And he can reason that agent 2 must know that agent 1 knows one of the elements of I that overlaps with one of these elements of J. If he carries out this inference to infinity, he'll find that both agents know that the actual world-history is in (I\u2227J)(w), and both know the other know, and both know the other know the other know, and so on. In other words it is common knowledge that the actual world-history is in&nbsp;(I\u2227J)(w).&nbsp;Since event E occurs in every world-history in&nbsp;(I\u2227J)(w), it's common knowledge that E occurs in the actual world-history.</p>\n<p>Proof for the agreement theorem then goes like this. Let E be the event that agent 1 assigns a posterior probability (conditioned on everything it knows) of q1 to event A and agent 2 assigns a posterior probability of q2 to event A. If E is common knowledge at w, then both agents know that&nbsp;P(A | I(v)) = q1 and P(A | J(v)) = q2 for every v in&nbsp;(I\u2227J)(w). But&nbsp;this implies&nbsp;P(A | (I\u2227J)(w)) = q1 and&nbsp;P(A | (I\u2227J)(w)) = q2 and therefore q1 = q2.&nbsp;(To see this, suppose you currently know only (I\u2227J)(w), and you know that no matter what additional information I(v) you obtain, your posterior probability will be the same q1, then your current probability must already be q1.)</p>\n<p><span style=\"font-size: 14px; font-weight: bold;\">Is Aumann Agreement Overrated?</span></p>\n<p>Having explained all of that, it seems to me that this theorem is less relevant to a practical rationalist than I thought before I really understood it.&nbsp;After looking at the math, it's apparent that \"common knowledge\" is a much stricter requirement than it sounds. The most obvious way to achieve it is for the two agents to simply tell each other I(w) and J(w), after which they share a new, common information partition. But in that case, agreement itself is obvious and there is no need to learn or understand Aumann's theorem.</p>\n<p>There are some papers that describe ways to achieve agreement in other ways, such as <a href=\"http://scholar.google.com/scholar?q=&quot;We Can\u2019t Disagree Forever&quot;\">iterative exchange of posterior probabilities</a>. But in such methods, the agents aren't just moving closer to each other's beliefs. Rather, they go through convoluted chains of deduction to infer what information the other agent must have observed, given his declarations, and then update on that new information. (The process is similar to the one needed to solve the second riddle on <a href=\"http://research.microsoft.com/en-us/events/15years/riddles.aspx\">this page</a>.) The two agents essentially still have to communicate I(w) and J(w) to each other, except they do so by exchanging posterior probabilities and making logical inferences from them.</p>\n<p>Is this realistic for human rationalist wannabes? It seems wildly implausible to me that two humans can&nbsp;communicate all of the information they have that is relevant to the truth of some statement just by repeatedly exchanging degrees of belief about it, except in very simple situations. You need to know the other agent's information partition exactly in order to narrow down which element of the information partition he is in from his&nbsp;probability declaration, and he needs to know that you know so that he can deduce what inference you're making, in order to continue to the next step, and so on. One error in this process and the whole thing falls apart.&nbsp;It seems much easier to just tell each other what information the two of you have directly.</p>\n<p>Finally, I now see that until the exchange of information completes and common knowledge/agreement is actually achieved, it's rational for even honest truth-seekers who share common priors to disagree. Therefore, two such rationalists may persistently disagree just because the amount of information they would have to exchange in order to reach agreement is too great to be practical. This is quite different from the understanding of Aumann agreement I had before I read the math.</p>", "sections": [{"title": "Probability Space", "anchor": "Probability_Space", "level": 1}, {"title": "Aumann's Agreement Theorem", "anchor": "Aumann_s_Agreement_Theorem", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "76 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jzf4Rcienrm6btRyt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-11T00:25:33.177Z", "modifiedAt": null, "url": null, "title": "What Are Probabilities, Anyway?", "slug": "what-are-probabilities-anyway", "viewCount": null, "lastCommentedAt": "2020-02-13T07:30:43.519Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J7Gkz8aDxxSEQKXTN/what-are-probabilities-anyway", "pageUrlRelative": "/posts/J7Gkz8aDxxSEQKXTN/what-are-probabilities-anyway", "linkUrl": "https://www.lesswrong.com/posts/J7Gkz8aDxxSEQKXTN/what-are-probabilities-anyway", "postedAtFormatted": "Friday, December 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20Are%20Probabilities%2C%20Anyway%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20Are%20Probabilities%2C%20Anyway%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7Gkz8aDxxSEQKXTN%2Fwhat-are-probabilities-anyway%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20Are%20Probabilities%2C%20Anyway%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7Gkz8aDxxSEQKXTN%2Fwhat-are-probabilities-anyway", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ7Gkz8aDxxSEQKXTN%2Fwhat-are-probabilities-anyway", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 673, "htmlBody": "<p>In&nbsp;<a href=\"/lw/1il/probability_space_aumann_agreement/\">Probability Space &amp; Aumann Agreement</a>, I wrote that probabilities can be thought of as weights that we assign to possible world-histories. But what are these weights supposed to mean?&nbsp;Here I&rsquo;ll give a few interpretations that&nbsp;I've considered and held at one point or another, and their problems. (Note that in the previous post, I implicitly used the first interpretation in the following list, since that seems to be the mainstream view.)</p>\n<ol style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-position: outside; list-style-image: initial;\">\n<li>Only one possible world is real, and probabilities represent beliefs about which one is real.  \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n<li>Which world gets to be real seems arbitrary.</li>\n<li>Most possible worlds are lifeless, so we&rsquo;d have to be really lucky to be alive.</li>\n<li>We have no information about the process that determines which world gets to be real, so how can we decide what the probability mass function p should be?&nbsp;</li>\n</ul>\n</li>\n<li>All possible worlds are real, and probabilities represent beliefs about which one I&rsquo;m in.  \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n<li>Before I&rsquo;ve observed anything, there seems to be no reason to believe that I&rsquo;m more likely to be in one world than another, but we can&rsquo;t let all their weights be equal.</li>\n</ul>\n</li>\n<li>Not all possible worlds are equally real, and probabilities represent &ldquo;how real&rdquo; each world is. (This is also sometimes called the &ldquo;measure&rdquo; or &ldquo;<a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/126j\">reality fluid</a>&rdquo; view.)  \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n<li>Which worlds get to be &ldquo;more real&rdquo; seems arbitrary.</li>\n<li>Before we observe anything, we don't have any information about the process that determines the amount of &ldquo;reality fluid&rdquo; in each world, so how can we decide what the probability mass function p should be?</li>\n</ul>\n</li>\n<li>All possible worlds are real, and probabilities represent how much I care about each world. (To make sense of this, recall that these probabilities are ultimately multiplied with utilities to form expected utilities in standard decision theories.)  \n<ul style=\"margin-top: 10px; margin-right: 2em; margin-bottom: 10px; margin-left: 2em; list-style-type: disc; list-style-position: outside; list-style-image: initial;\">\n<li>Which worlds I care more or less about seems arbitrary. But perhaps this is less of a problem because I&rsquo;m &ldquo;allowed&rdquo; to have arbitrary values.</li>\n<li>Or, from another perspective, this drops another another hard problem on top of the pile of problems called &ldquo;values&rdquo;, where it may never be solved.</li>\n</ul>\n</li>\n</ol>\n<p><a id=\"more\"></a>As you can see, I think the main problem with all of these interpretations is arbitrariness. The unconditioned probability mass function is supposed to represent my beliefs before I have observed anything in the world, so it must represent a state of total ignorance. But there seems to be no way to specify such a function without introducing <em>some</em>&nbsp;information, which anyone could infer by looking at the function.</p>\n<p>For example, suppose we use a <a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\">universal distribution</a>, where we believe that the world-history is the output of a universal Turing machine given a uniformly random input tape. But then the distribution contains the information of which UTM we used. Where did that information come from?</p>\n<p>One could argue that we do have some information even before we observe anything, because we're products of evolution, which would have built some useful information into our genes. But to the extent that we can trust the prior specified by our genes, it must be that evolution approximates a Bayesian updating process, and our prior distribution approximates the posterior distribution of such a process. The \"prior of evolution\" still has to represent a state of total ignorance.</p>\n<p>These considerations lead me to lean toward the last interpretation, which is the most tolerant of arbitrariness. This interpretation also fits well with the idea that expected utility maximization with Bayesian updating is just an approximation of <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a> that works in most situations. I and others have already motivated UDT by considering situations where&nbsp;Bayesian updating doesn't work, but it seems to me that even if we set those aside, there is still reason to consider a UDT-like interpretation of probability where the weights on possible worlds represent how much we care about those worlds.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "3uE2pXvbcnS9nnZRE": 1, "5f5c37ee1b5cdee568cfb1dc": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J7Gkz8aDxxSEQKXTN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 40, "extendedScore": null, "score": 6.6e-05, "legacy": true, "legacyId": "1978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 40, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JdK3kr4ug9kJvKzGy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-11T01:54:23.739Z", "modifiedAt": null, "url": null, "title": "The persuasive power of false confessions", "slug": "the-persuasive-power-of-false-confessions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:51.730Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LCAynNFgnECrQ9iD2/the-persuasive-power-of-false-confessions", "pageUrlRelative": "/posts/LCAynNFgnECrQ9iD2/the-persuasive-power-of-false-confessions", "linkUrl": "https://www.lesswrong.com/posts/LCAynNFgnECrQ9iD2/the-persuasive-power-of-false-confessions", "postedAtFormatted": "Friday, December 11th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20persuasive%20power%20of%20false%20confessions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20persuasive%20power%20of%20false%20confessions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCAynNFgnECrQ9iD2%2Fthe-persuasive-power-of-false-confessions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20persuasive%20power%20of%20false%20confessions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCAynNFgnECrQ9iD2%2Fthe-persuasive-power-of-false-confessions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLCAynNFgnECrQ9iD2%2Fthe-persuasive-power-of-false-confessions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>\n<p>First paragraph from&nbsp;<a href=\"http://www.mindhacks.com/blog/2009/12/the_persuasive_power.html\">a&nbsp;Mind&nbsp;Hacks&nbsp;post</a>:</p>\n</p>\n<blockquote>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse; \">The&nbsp;<em>APS Observer</em>&nbsp;magazine has a fantastic&nbsp;<a style=\"color: #2244bb; \" href=\"http://www.psychologicalscience.org/observer/getArticle.cfm?id=2590\" target=\"_blank\">article</a>&nbsp;on the power of false confessions to warp our perception of other evidence in a criminal case to the point where expert witnesses will change their judgements of unrelated evidence to make it fit the false admission of guilt.</span></p>\n</blockquote>\n<p>The <a href=\"http://www.mindhacks.com/blog/2009/12/the_persuasive_power.html\">post</a> and <a href=\"http://www.psychologicalscience.org/observer/getArticle.cfm?id=2590\">linked article</a> are&nbsp;worth reading&hellip; and I don't have much to add.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LCAynNFgnECrQ9iD2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 12, "extendedScore": null, "score": 5.440053882480222e-07, "legacy": true, "legacyId": "1981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-13T02:37:41.722Z", "modifiedAt": null, "url": null, "title": "A question of rationality", "slug": "a-question-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:53.187Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mormon2", "createdAt": "2009-04-18T20:22:44.715Z", "isAdmin": false, "displayName": "mormon2"}, "userId": "gd8r7sxa2pwEzTCAZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wtzCt5jLMdpgXbMhZ/a-question-of-rationality", "pageUrlRelative": "/posts/wtzCt5jLMdpgXbMhZ/a-question-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/wtzCt5jLMdpgXbMhZ/a-question-of-rationality", "postedAtFormatted": "Sunday, December 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20of%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20of%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwtzCt5jLMdpgXbMhZ%2Fa-question-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20of%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwtzCt5jLMdpgXbMhZ%2Fa-question-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwtzCt5jLMdpgXbMhZ%2Fa-question-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 944, "htmlBody": "<p><strong>Thank you For Your Participation</strong></p>\n<p>I would like to thank you all for your unwitting and unwilling participation in my little social experiment. If I do say so myself you all performed as I had hoped. I found some of the responses interesting, many them are goofy. I was honestly hoping that a budding rationalist community like this one would have stopped this experiment midway but I thank you all for not being that rational. I really did appreciate all the mormon2 bashing it was quite amusing and some of the attempts to discredit me were humorous though unsuccessful. In terms of the questions I asked I was curious about the answers though I did not expect to get any nor do I really need them; since I have a good idea of what the answers are just from simple deductive reasoning. I really do hope EY is working on FAI and actually is able to do it though I certainly will not stake my hopes or money on it.&nbsp;</p>\n<p>Less there be any suspicion I am being sincere here.</p>\n<p>&nbsp;</p>\n<p><strong>Response</strong></p>\n<p>Because I can I am going to make one final response to this thread I started:</p>\n<p>Since none of you understand what I am doing I will spell it out for you. My posts are formatted, written and styled intentionally for the response I desire. The point is to give you guys easy ways to avoid answering my questions (things like tone of the post, spelling, grammar, being \"hostile (not really)\" etc.). I just wanted to see if anyone here could actually look past that, specifically EY, and post some honest answers to the questions (real answers again from EY not pawns on LW). Obviously this was to much to ask, since the general responses, not completely, but for the most part were copouts. I am well aware that EY probably would never answer any challenge to what he thinks, people like EY typically won't (I have dealt with many people like EY). I think the responses here speak volumes about LW and the people who post here (If you can't look past the way the content is posted then you are going to have a hard time in life since not everyone is going to meet your standards for how they speak or write). You guys may not be trying to form a cult but the way you respond to a post like this screams cultish and even a some circle-jerk mentality mixed in there.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Post</strong></p>\n<p>I would like to float an argument and a series of questions. Now before you guys vote me down please do me the curtsey of reading the post. I am also aware that some and maybe even many of you think that I am a troll just out to bash SIAI and Eliezer, that is in fact not my intent. This group is supposed to be about improving rationality so lets improve our rationality.</p>\n<p>SIAI has the goal of raising awareness of the dangers of AI as well as trying to create their own FAI solution to the problem. This task has fallen to Eliezer as the paid researcher working on FAI. What I would like to point out is a bit of a disconnect between what SIAI is supposed to be doing and what EY is doing.</p>\n<p>According to EY FAI is an extremely important problem that must be solved with global implications. It is both a hard math problem and a problem that needs to be solved by people who take FAI seriously first. To that end SIAI was started with EY as an AI researcher at SIAI.&nbsp;</p>\n<p>Until about 2006 EY was working on papers like CEV and working on designs for FAI which he has now discarded as being wrong for the most part. He then went on a long period of blogging on Overcoming Bias and LessWrong and is now working on a book on rationality as his stated main focus. If this be accurate I would ask how does this make sense from someone who has made such a big deal about FAI, its importance, being first to make AI and ensure it is FAI? If FAI is so important then where does a book on rationality fit? Does that even play into SIAI's chief goals? SIAI spends huge amounts of time talking about risks and rewards of FAI and the person who is supposed to be making the FAI is writing a book on rationality instead of solving FAI. How does this square with being paid to research FAI? How can one justify EY's reasons for not publishing the math of TDT, coming from someone who is committed to FAI? If one is committed to solving that hard of a problem then I would think that the publication of ones ideas on it would be a primary goal to advance the cause of FAI.</p>\n<p>If this doesn't make sense then I would ask how rational is it to spend time helping SIAI if they are not focused on FAI? Can one justify giving to an organization like that when the chief FAI researcher is distracted by writing a book on rationality instead of solving the myriad of hard math problems that need to be solved for FAI? If this somehow makes sense then can one also state that FAI is not nearly as important as it has been made out to be since the champion of FAI feels comfortable with taking a break from solving the problem to write a book on rationality (in other words the world really isn't at stake)?&nbsp;</p>\n<p>Am I off base? If this group is devoted to rationality then everyone should be subjected to rational analysis.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wtzCt5jLMdpgXbMhZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 1, "extendedScore": null, "score": 5.445288335968636e-07, "legacy": true, "legacyId": "1985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Thank_you_For_Your_Participation\">Thank you For Your Participation</strong></p>\n<p>I would like to thank you all for your unwitting and unwilling participation in my little social experiment. If I do say so myself you all performed as I had hoped. I found some of the responses interesting, many them are goofy. I was honestly hoping that a budding rationalist community like this one would have stopped this experiment midway but I thank you all for not being that rational. I really did appreciate all the mormon2 bashing it was quite amusing and some of the attempts to discredit me were humorous though unsuccessful. In terms of the questions I asked I was curious about the answers though I did not expect to get any nor do I really need them; since I have a good idea of what the answers are just from simple deductive reasoning. I really do hope EY is working on FAI and actually is able to do it though I certainly will not stake my hopes or money on it.&nbsp;</p>\n<p>Less there be any suspicion I am being sincere here.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Response\">Response</strong></p>\n<p>Because I can I am going to make one final response to this thread I started:</p>\n<p>Since none of you understand what I am doing I will spell it out for you. My posts are formatted, written and styled intentionally for the response I desire. The point is to give you guys easy ways to avoid answering my questions (things like tone of the post, spelling, grammar, being \"hostile (not really)\" etc.). I just wanted to see if anyone here could actually look past that, specifically EY, and post some honest answers to the questions (real answers again from EY not pawns on LW). Obviously this was to much to ask, since the general responses, not completely, but for the most part were copouts. I am well aware that EY probably would never answer any challenge to what he thinks, people like EY typically won't (I have dealt with many people like EY). I think the responses here speak volumes about LW and the people who post here (If you can't look past the way the content is posted then you are going to have a hard time in life since not everyone is going to meet your standards for how they speak or write). You guys may not be trying to form a cult but the way you respond to a post like this screams cultish and even a some circle-jerk mentality mixed in there.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Post\">Post</strong></p>\n<p>I would like to float an argument and a series of questions. Now before you guys vote me down please do me the curtsey of reading the post. I am also aware that some and maybe even many of you think that I am a troll just out to bash SIAI and Eliezer, that is in fact not my intent. This group is supposed to be about improving rationality so lets improve our rationality.</p>\n<p>SIAI has the goal of raising awareness of the dangers of AI as well as trying to create their own FAI solution to the problem. This task has fallen to Eliezer as the paid researcher working on FAI. What I would like to point out is a bit of a disconnect between what SIAI is supposed to be doing and what EY is doing.</p>\n<p>According to EY FAI is an extremely important problem that must be solved with global implications. It is both a hard math problem and a problem that needs to be solved by people who take FAI seriously first. To that end SIAI was started with EY as an AI researcher at SIAI.&nbsp;</p>\n<p>Until about 2006 EY was working on papers like CEV and working on designs for FAI which he has now discarded as being wrong for the most part. He then went on a long period of blogging on Overcoming Bias and LessWrong and is now working on a book on rationality as his stated main focus. If this be accurate I would ask how does this make sense from someone who has made such a big deal about FAI, its importance, being first to make AI and ensure it is FAI? If FAI is so important then where does a book on rationality fit? Does that even play into SIAI's chief goals? SIAI spends huge amounts of time talking about risks and rewards of FAI and the person who is supposed to be making the FAI is writing a book on rationality instead of solving FAI. How does this square with being paid to research FAI? How can one justify EY's reasons for not publishing the math of TDT, coming from someone who is committed to FAI? If one is committed to solving that hard of a problem then I would think that the publication of ones ideas on it would be a primary goal to advance the cause of FAI.</p>\n<p>If this doesn't make sense then I would ask how rational is it to spend time helping SIAI if they are not focused on FAI? Can one justify giving to an organization like that when the chief FAI researcher is distracted by writing a book on rationality instead of solving the myriad of hard math problems that need to be solved for FAI? If this somehow makes sense then can one also state that FAI is not nearly as important as it has been made out to be since the champion of FAI feels comfortable with taking a break from solving the problem to write a book on rationality (in other words the world really isn't at stake)?&nbsp;</p>\n<p>Am I off base? If this group is devoted to rationality then everyone should be subjected to rational analysis.</p>", "sections": [{"title": "Thank you For Your Participation", "anchor": "Thank_you_For_Your_Participation", "level": 1}, {"title": "Response", "anchor": "Response", "level": 1}, {"title": "Post", "anchor": "Post", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "94 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 98, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-13T04:16:20.840Z", "modifiedAt": null, "url": null, "title": "The Amanda Knox Test: How an Hour on the Internet Beats a Year in the Courtroom", "slug": "the-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:35.506Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G9dptrW9CJi7wNg3b/the-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "pageUrlRelative": "/posts/G9dptrW9CJi7wNg3b/the-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "linkUrl": "https://www.lesswrong.com/posts/G9dptrW9CJi7wNg3b/the-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "postedAtFormatted": "Sunday, December 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Amanda%20Knox%20Test%3A%20How%20an%20Hour%20on%20the%20Internet%20Beats%20a%20Year%20in%20the%20Courtroom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Amanda%20Knox%20Test%3A%20How%20an%20Hour%20on%20the%20Internet%20Beats%20a%20Year%20in%20the%20Courtroom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9dptrW9CJi7wNg3b%2Fthe-amanda-knox-test-how-an-hour-on-the-internet-beats-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Amanda%20Knox%20Test%3A%20How%20an%20Hour%20on%20the%20Internet%20Beats%20a%20Year%20in%20the%20Courtroom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9dptrW9CJi7wNg3b%2Fthe-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG9dptrW9CJi7wNg3b%2Fthe-amanda-knox-test-how-an-hour-on-the-internet-beats-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3500, "htmlBody": "<p><strong>Note: The quantitative elements of this post have now been revised significantly.</strong></p>\n<p><strong>Followup to:</strong> <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/\">You Be the Jury: Survey on a Current Event</a></p>\n<blockquote>\n<p>All three of them clearly killed her. The jury clearly believed so as well which strengthens my argument. They spent months examining the case, so the idea that a few minutes of internet research makes&nbsp;[other commenters]&nbsp;certain they're wrong seems laughable</p>\n</blockquote>\n<p>- lordweiner27, <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bg5\">commenting on my previous post</a></p>\n<p>The short answer: it's very much like how <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">a few minutes of philosophical reflection</a> trump <a href=\"http://wiki.lesswrong.com/wiki/Religion\">a few millennia of human cultural tradition</a>.</p>\n<p>Wielding the Sword of Bayes -- or for that matter the Razor of Occam -- requires courage and a certain kind of ruthlessness. You have to be willing to cut your way through vast quantities of noise and focus in like a laser on the signal.</p>\n<p>But the tools of rationality are extremely powerful if you know how to use them.</p>\n<p>Rationality is not easy for humans. Our brains were optimized to arrive at correct conclusions about the world <a href=\"/lw/l3/thou_art_godshatter/\">only insofar as that was a necessary byproduct of being optimized to pass the genetic material that made them on to the next generation</a>. If you've been reading&nbsp;Less Wrong for any significant length of time, you probably know this by now. In fact, around here this is almost a banality -- a <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached thought</a>. \"We get it,\" you may be tempted to say. \"So stop signaling your tribal allegiance to this website and move on to some new, nontrivial meta-insight.\"</p>\n<p>But this is one of those things that truly do bear repeating, over and over again, almost at every opportunity. You really can't hear it enough. It has <em>consequences</em>, you see. The most important of which is: if you only do what feels epistemically \"natural\" all the time, you're going to be, well, wrong. And probably not just \"sooner or later\", either. Chances are, you're going to be wrong quite a lot.<a id=\"more\"></a></p>\n<p>To borrow a Yudkowskian turn of phrase: if you don't ever -- or indeed often -- find yourself needing to zig when, not only other people, but all kinds of internal \"voices\" in your mind are loudly shouting for you to zag, then you're either a native rationalist -- a born Bayesian, who should perhaps be <a href=\"/lw/qg/changing_the_definition_of_science/\">deducing general relativity from the fall of an apple</a> any minute now -- or else you're simply not trying hard enough.&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>Oh, and another one of those consequences of humans' not being instinctively rational?</p>\n<p>Two intelligent young people with previously bright futures, named Amanda and Raffaele, are now seven days into spending the next quarter-century of their lives behind bars for a crime they almost certainly did not commit.</p>\n<p><del>\"Almost certainly\" really doesn't quite capture it</del>. In my <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/\">previous post</a> I asked readers to assign probabilities to the following propositions:</p>\n<p>1. Amanda Knox is guilty (of killing Meredith Kercher)<br />2. Raffaele Sollecito is guilty (of killing Meredith Kercher)<br />3. Rudy Gu&eacute;d&eacute; is guilty (of killing Meredith Kercher)</p>\n<p>I also asked them to guess at how closely they thought their estimates would match mine.</p>\n<p>Well, for comparison, here are mine (<strong>revised</strong>):</p>\n<p>1. <del>Negligible</del>. Small.&nbsp;<del>Hardly different from the prior, which is&nbsp;dominated by the probability that someone in whatever reference class you would have put Amanda into on January 1, 2007 would commit murder within twelve months.</del> Something on the order of <del>0.001</del>&nbsp;0.01 or 0.1&nbsp;<em>at most</em>.&nbsp;&nbsp;<br />2. Ditto.<br />3. About as high as the other two numbers are low. <del>0.999</del> 0.99&nbsp;as a (probably weak) lower bound.</p>\n<p><del>Yes, you read that correctly. In my opinion, there is for all intents and purposes <em>zero</em> Bayesian evidence that Amanda and Raffaele are guilty.</del> Needless to say, this differs markedly from the consensus of the jury in Perugia, Italy.&nbsp;</p>\n<p>How could this be?</p>\n<p>Am I really suggesting that the estimates of eight jurors -- among whom two professional judges -- who heard the case for a year, along with something like 60% of the Italian public and probably half the Internet (and a significantly larger fraction of the non-American Internet), could be off by <del>a <em>minimum</em> of three orders of magnitude (probably significantly more)</del> such a large amount? <del>That most other people (including most commenters on my last post) are off by no fewer than two?</del></p>\n<p>Well, dear reader, before getting <em>too</em> incredulous,&nbsp;consider this. How about averaging the probabilities all those folks would assign to the proposition that Jesus of Nazareth rose from the dead, and calling that number x. Meanwhile, let y be the correct rational probability that Jesus rose from the dead, given the information available to us.</p>\n<p>How big do you suppose the ratio x/y is?</p>\n<p>Anyone want to take a stab at guessing the <em>logarithm</em> of that number?</p>\n<p>Compared to the probability that Jesus rose from the dead, my estimate of Amanda Knox's culpability makes it look like I think she's as guilty as sin itself.</p>\n<p>And that, of course, is just the central one of many sub-claims of the hugely <a href=\"http://wiki.lesswrong.com/wiki/Burdensome_details\">complex</a> yet widely believed proposition that Christianity is true. There are any number of other equally unlikely assertions that <a href=\"http://www.corriere.it/cronache/09_dicembre_09/in-cella-da-amanda-ho-ancora-fiducia-nella-vostra-giustizia-alessandro-capponi_bfed4194-e48a-11de-b76e-00144f02aabc.shtml\">Amanda would have heard at mass on the day after being found guilty of killing her new friend Meredith</a> (source in Italian) -- assertions that are assigned non-negligible probability by no fewer than a couple billion of the Earth's human inhabitants.</p>\n<p>I say this by way of preamble: be <em>very</em> wary of trusting in the rationality of your fellow humans, when you have serious reasons to doubt their conclusions.</p>\n<h3>The Lawfulness of Murder: Inference Proceeds Backward, from Crime to Suspect</h3>\n<p><a href=\"/lw/hr/universal_law/\">We live in a lawful universe</a>. Every event that happens in this world -- including human actions and thoughts -- is ultimately governed by the laws of physics, which are exceptionless.&nbsp;</p>\n<p>Murder may be highly illegal, but from the standpoint of physics, it's as lawful as everything else. Every physical interaction, including a homicide, leaves traces -- changes in the environment that constitute information about what took place.</p>\n<p>Such information, however, is -- crucially -- <em>local</em>. The further away in space and time you move from the event, the less <a href=\"/lw/jl/what_is_evidence/\">entanglement</a> there is between your environment and that of the event, and thus the more difficult it is to make legitimate inferences about the event. The signal-to-noise ratio decreases dramatically as you move away in causal distance from the event. After all, the hypothesis space of possible causal chains of length n leading to the event increases exponentially in n.</p>\n<p>By far the most important evidence in a murder investigation will therefore be the evidence that is the closest to the crime itself -- evidence on and around the victim, as well as details stored in the brains of people who were present during the act. Less important will be evidence obtained from persons and objects a short distance away from the crime scene; and the importance decays rapidly from there as you move further out.</p>\n<p>It follows that you cannot possibly expect to reliably arrive at the correct answer by starting a few steps removed in the causal chain, say with a person you find \"suspicious\" for some reason, and working <em>forward</em> to come up with a plausible scenario for how the crime was committed. That would be <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">privileging the hypothesis</a>. Instead, you have to start from the actual crime scene, or as close to it as you can get, and work <em>backward</em>, letting yourself be <a href=\"http://yudkowsky.net/rational/virtues\">blown by the winds of evidence</a> toward one or more possible suspects.</p>\n<p>In the Meredith Kercher case, the winds of evidence blow with something like hurricane force in the direction of Rudy Gu&eacute;d&eacute;. After the murder, Kercher's bedroom was filled with evidence of Gu&eacute;d&eacute;'s presence; his DNA was found not only on top of but actually inside her body. That's about as close to the crime as it gets. At the same time, no remotely similarly incriminating genetic material was found from anyone else -- in particular, there were no traces of the presence of either Amanda Knox or Raffaele Sollecito in the room (and no, the supposed Sollecito DNA on Meredith's bra clasp just plain <a href=\"http://www.friendsofamanda.org/bra_fastener.html\">does not count</a> -- nor, while we're at it, do the 100 picograms [about one human cell's worth] of DNA from Meredith <a href=\"http://www.friendsofamanda.org/summary.html\">allegedly on the tip of a knife</a> handled by Knox, found at Sollecito's apartment after the two were already suspects; these two things constituting pretty much the entirety of the physical \"evidence\" against the couple).</p>\n<p>If, up to this point, the police had reasons to be suspicious of Knox, Sollecito, and Gu&eacute;d&eacute;, they should have cleared Knox and Sollecito <em>at once</em> upon the discovery that Gu&eacute;d&eacute; -- who, by the way, was the only one to have <em>fled the country</em> after the crime -- was the one whom the DNA matched. Unless, that is, Knox and Sollecito were specifically implicated by Gu&eacute;d&eacute;; after all, maybe Knox and Sollecito didn't actually kill the victim, but instead maybe they paid Gu&eacute;d&eacute; to do so, or were otherwise involved in a conspiracy with him. But the prior probabilities of such scenarios are low, even in general -- to say nothing of the case of Knox and Sollecito specifically, who, tabloid press to the contrary, are known to have had utterly benign dispositions prior to these events, and no reason to want Meredith Kercher dead.</p>\n<p>If Amanda Knox and Raffaele Sollecito were to be in investigators' thoughts at all, they had to get there <em>via Gu&eacute;d&eacute;</em> -- because otherwise the hypothesis (<em>a priori</em> unlikely) of their having had homicidal intent toward Kercher would be <strong>entirely superfluous</strong> in explaining the chain of events that led to her death.&nbsp;&nbsp;The trail of evidence had led to Gu&eacute;d&eacute;, and therefore necessarily had to proceed from him; to follow any other path would be to fatally sidetrack the investigation, and virtually to guarantee serious -- very serious -- error. Which is exactly what happened.</p>\n<p>There was in fact no inferential path from Gu&eacute;d&eacute; to Knox or Sollecito. He never implicated either of the two until long after the event; around the time of his apprehension, he specifically denied that Knox had been in the room. Meanwhile, it remains entirely unclear that he and Sollecito had ever even met.</p>\n<p>The hypotheses of Knox's and Sollecito's guilt are thus seen to be completely unnecessary, doing <strong>no explanatory work</strong> with respect to Kercher's death. They are&nbsp;nothing but extremely <a href=\"http://wiki.lesswrong.com/wiki/Burdensome_details\">burdensome</a> details.&nbsp;&nbsp;</p>\n<h3>Epistemic Ruthlessness: Following the Strong Signal</h3>\n<p>All of the \"evidence\" you've heard against Knox and Sollecito -- the changing stories, suspicious behavior, short phone calls, washing machine rumors, etc. -- is, quite literally, <em>just noise</em>.</p>\n<p>But it sounds so <em>suspicious</em>, you say. Who places a three-second phone call?&nbsp;</p>\n<p>As humans, we are <a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_psychology\">programmed to think</a> that the most important kinds of facts about the world are <em>mental</em> and <em>social</em> -- facts about what humans are thinking and planning, particularly as regards to other humans. This explains why some people are capable of wondering whether the presence of (<a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">only</a>) Rudy Gu&eacute;d&eacute;'s DNA in and on Meredith's body should be balanced against the possibilty that Meredith may have been annoyed at Amanda for bringing home boyfriends and occasionally forgetting to flush the toilet -- that might have led to <em>resentment</em> on Amanda's part, you see.</p>\n<p>That's an extreme example, of course -- certainly no one here fell into that kind of trap. But <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bi5\">at least one</a> of the most thoughtful commenters was severely bothered by <a href=\"http://www.truejustice.org/ee/index.php?/tjmk/comments/our_take_on_the_case_for_the_prosecution_5_defendants_claims_shown_to_be_a/\">the length of Amanda's phone calls to Meredith</a>. As -- I'll confess -- was I, for a minute or two.</p>\n<p>I don't know why Amanda wouldn't have waited longer for Meredith to pick up. (For what it's worth, I myself have sometimes, in a state of nervousness, dialed someone's number, quickly changed my mind, then dialed again a short time later.) But -- as counterintuitive as it may seem -- <em>it doesn't matter</em>. The&nbsp;error here is even <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">asking a question</a> about Amanda's motivations when you haven't established an evidentiary (and <a href=\"/lw/jl/what_is_evidence/\">that means physical</a>)&nbsp;trail leading from Meredith's body to Amanda's brain. (Or even more to the point, when you <em>have</em> established a&nbsp;trail that led <em>decisively elsewhere</em>.)</p>\n<p>Maybe it's \"unlikely\" that Amanda would have behaved this way if she were innocent. But is the degree of improbabilty here anything like the improbability of her having participated in a sex-orgy-killing <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">without leaving a single piece of physical evidence behind</a>? While someone else left all kinds of traces? When you had no reason to suspect her at all without looking a good distance outside Meredith's room, far away from the important evidence?</p>\n<p>It's not even remotely comparable.&nbsp;</p>\n<p>Think about what you're doing here: <em>you are invoking the hypothesis that Amanda Knox is guilty of murder in order to explain the fact that she hung up the phone after three seconds.</em> (Remember, the evidence against Gu&eacute;d&eacute; is such that the hypothesis of her guilt is&nbsp;superfluous -- not needed --&nbsp;in explaining the death of Meredith Kercher!)</p>\n<p>Maybe that's not quite as bad as invoking a superintelligent deity in order to explain life on Earth; but it's the same kind of mistake: explaining a strange thing by postulating a far, far stranger thing.</p>\n<p>\"But come on,\" says a voice in your head. \"Does this really sound like the behavior of an innocent person?\"</p>\n<p>You have to <em>shut that voice out</em>. Ruthlessly. Because it has no way of knowing. That voice is designed to assess the motivations of members of an ancestral hunter-gather band. At best, it may have the ability to distinguish the correct murderer from between 2 and 100 possibilities -- 6 or 7 <a href=\"http://wiki.lesswrong.com/wiki/Amount_of_evidence\">bits of inferential power</a> on the absolute best of days. That may have worked in hunter-gatherer times, before more-closely-causally-linked physical evidence could hope to be evaluated. (Or maybe not -- but at least it got the genes passed on.)</p>\n<p>DNA analysis, in contrast, has in principle the ability to uniquely identify a single individual from among the entire human species (depending on how much of the genome is looked at; also ignoring identical twins, etc.) -- that's more like 30-odd bits of inferential power. In terms of epistemic technology, we're talking about something like the difference in locomotive efficacy between a horsedrawn carriage and the Starship Enterprise. Our ancestral environment just plain did not equip our knowledge-gathering intuitions with the ability to handle weapons this powerful.</p>\n<p>We're talking about the kind of power that allows us to reduce what was formerly a question of <em>human social psychology</em> -- who made the decision to kill Meredith? -- to one of <em>physics</em>. (Or chemistry, at any rate.)</p>\n<p>But our minds don't naturally think in terms of physics and chemistry. From an intuitive point of view, the equations of those subjects are foreign; whereas \"X did Y because he/she wanted Z\" is familiar. This is why it's so difficult for people to intuitively&nbsp;appreciate that all of the chatter about Amanda's \"suspicious behavior\"&nbsp;with various convincing-sounding narratives put forth by the prosecution is <em>totally and utterly drowned out to oblivion</em> by the sheer strength of the DNA signal pointing to Gu&eacute;d&eacute; <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">alone</a>.</p>\n<p>This rationalist skill of <em>following the strong signal</em> -- mercilessly blocking out noise -- might be considered an <a href=\"/lw/31/what_do_we_mean_by_rationality/\">epistemic</a> analog of the <a href=\"/lw/31/what_do_we_mean_by_rationality/\">instrumental</a> \"<a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">shut up and multiply</a>\": when much is at stake, you have to be willing to jettison your <a href=\"/lw/l3/thou_art_godshatter/\">intuitive feelings</a> in favor of cold, hard, abstract&nbsp;calculation.</p>\n<p>In this case, that means, among other things, thinking in terms of <em>how much explanatory work is done by the various hypotheses</em>, rather than <em>how suspicious Amanda and Raffaele seem</em>.&nbsp;</p>\n<h3>Conclusion: The Amanda Knox Test</h3>\n<p>I chose the title of this post because&nbsp;the parallel structure made it&nbsp;sound nice. But actually, I think an hour is a pretty weak upper bound on the amount of time a skilled rationalist should need to arrive at the correct judgment in this case.</p>\n<p>The fact is that what this comes down to is an utterly straightforward application of Occam's Razor. The complexity penalty on the prosecution's theory of the crime is enormous; the evidence in its favor had better be overwhelming. But instead, what we find is that the evidence from the scene -- the most important sort of evidence by&nbsp;a huge&nbsp;margin&nbsp;--&nbsp;points with literally&nbsp;superhuman strength toward a mundane, even typical, homicide scenario. To even consider theories not directly suggested by this evidence is to engage in <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">hypothesis privileging</a> to the extreme.</p>\n<p>So let me say it now, in case there was any doubt: the prosecution of Amanda Knox and Raffaele Sollecito, culminating in last week's jury verdict -- which apparently was unanimous, though it didn't need to be under Italian rules -- represents nothing but one more gigantic, disastrous rationality failure on the part of our species.</p>\n<p>How did Less Wrong do by comparison? The average estimated probability of Amanda Knox's guilt was 0.35 (thanks to <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bjg\">Yvain</a> for doing the calculation). It's pretty reasonable to assume the figure for Raffaele Sollecito would be similar. While not particularly flattering to the defendants (how would you like to be told that there's a 35% chance you're a murderer?), that number makes it obvious we would have voted to acquit. (If a 65% chance that they didn't do it doesn't constitute&nbsp; \"reasonable doubt\" that they did...)</p>\n<p>The commenters whose estimates were closest to mine -- and, therefore, to the correct answer, in my view -- were <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bdt\">Daniel Burfoot</a> and <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bhu\">jenmarie</a>. Congratulations to them. (<del>But even they were off by a factor of at least ten!</del>)</p>\n<p>In general, most folks went in the right direction, but, as <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1beu\">Eliezer noted</a>, were far too underconfident -- evidently the result of an exorbitant level of trust in juries, at least in part. But people here were also widely making the same object-level mistake as (presumably) the jury: <em>vastly </em>overestimating the importance of \"psychological\" evidence, such as Knox's inconsistencies at the police station, as compared to \"physical\" evidence (only Gu&eacute;d&eacute;'s DNA in the room).</p>\n<p>One thing that was interesting and rather encouraging, however, is the amount of updating people did after reading others' comments -- most of it in the right direction (toward innocence).</p>\n<p>[EDIT: After reading comments on this post, I have done some updating of my own. I now think I failed to adequately&nbsp;consider the possibility of my own overconfidence. This was pretty stupid of me, since it meant that the focus was taken away from the actual&nbsp;arguments in this post, and basically toward&nbsp;the&nbsp;issue of whether 0.001 can possibly be a rational estimate for anything you read&nbsp;about on&nbsp;the Internet.&nbsp;The qualitative reasoning of this post, of course, stands.&nbsp;Also, the focus of my&nbsp;accusations of irrationality was not primarily the LW community as reflected in&nbsp;my previous post; I actually think&nbsp;we&nbsp;did a pretty good job&nbsp;of coming to the right conclusion given the information provided -- and as others have noted, the levelheadedness with which we did so was impressive.]</p>\n<p>For most frequenters of this forum, where many of us regularly speak in terms of trying to save the human species from various global catastrophic risks, a case like this may not seem to have very many grand implications, beyond serving as yet another example of how basic principles of rationality such as Occam's Razor are incredibly difficult for people to grasp on an intuitive level. But it does catch the attention of someone like me, who takes an interest in less-commonly-thought-about forms of human suffering.</p>\n<p>The next time I find myself discussing the \"hard problem of consciousness\", thinking in vivid detail about the spectrum of human experience and wondering what it's like to be a bat, I am going to remember -- whether I say so or not -- that there is most definitely <em>something it's like</em> to be Amanda Knox in the moments following the announcement of that verdict: when you've just learned that, instead of heading back home to celebrate Christmas with your family as you had hoped, you will be spending the next decade or two -- your twenties and thirties -- in a prison cell in a foreign country. When your deceased friend's relatives are watching <a href=\"http://news.yahoo.com/s/ap/20091205/ap_on_re_eu/eu_italy_student_slain_49\">with satisfaction</a> as you are led, sobbing and wailing with desperation, to a van which will transport you back to that cell. (Ever thought about what that ride must be like?)&nbsp;</p>\n<p>While we're busy eliminating hunger, disease, and death itself, I hope we can also find the time, somewhere along the way, to get rid of <em>that</em>, too.</p>\n<p>(The Associated Press reported that, apparently, Amanda had some trouble going to sleep after the midnight verdict.)&nbsp;</p>\n<p>I'll conclude with this: the noted mathematician Serge Lang was in the habit of giving his students \"Huntington tests\" -- named in reference to his controversy with political scientist Samuel Huntington, whose entrance into the U.S. National Academy of Sciences Lang waged a successful campaign to block on the grounds of Huntington's insufficient scientific rigor.</p>\n<p>The purpose of the Huntington test, in Lang's words, was to see if the students could \"tell a fact from a hole in the ground\".</p>\n<p>I'm thinking of adopting a similar practice, and calling my version the Amanda Knox Test.&nbsp;</p>\n<p><strong>Postscript</strong>: <em>If you agree with me, and are also the sort of person who enjoys purchasing <a href=\"http://wiki.lesswrong.com/wiki/Fuzzies\">warm fuzzies</a> separately from your <a href=\"http://wiki.lesswrong.com/wiki/Utilons\">utilons</a>, you might consider donating to Amanda's <a href=\"http://www.amandadefensefund.org/\">defense fund</a>, to help out her financially devastated family. Of course, if you browse the site, you may feel your (prior) estimate of her guilt taking some hits; personally, that's okay with me.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wGGAjTfXZBatQkft5": 2, "xexCWMyds6QLWognu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G9dptrW9CJi7wNg3b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 50, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "1987", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Note__The_quantitative_elements_of_this_post_have_now_been_revised_significantly_\">Note: The quantitative elements of this post have now been revised significantly.</strong></p>\n<p><strong>Followup to:</strong> <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/\">You Be the Jury: Survey on a Current Event</a></p>\n<blockquote>\n<p>All three of them clearly killed her. The jury clearly believed so as well which strengthens my argument. They spent months examining the case, so the idea that a few minutes of internet research makes&nbsp;[other commenters]&nbsp;certain they're wrong seems laughable</p>\n</blockquote>\n<p>- lordweiner27, <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bg5\">commenting on my previous post</a></p>\n<p>The short answer: it's very much like how <a href=\"http://wiki.lesswrong.com/wiki/Occam%27s_razor\">a few minutes of philosophical reflection</a> trump <a href=\"http://wiki.lesswrong.com/wiki/Religion\">a few millennia of human cultural tradition</a>.</p>\n<p>Wielding the Sword of Bayes -- or for that matter the Razor of Occam -- requires courage and a certain kind of ruthlessness. You have to be willing to cut your way through vast quantities of noise and focus in like a laser on the signal.</p>\n<p>But the tools of rationality are extremely powerful if you know how to use them.</p>\n<p>Rationality is not easy for humans. Our brains were optimized to arrive at correct conclusions about the world <a href=\"/lw/l3/thou_art_godshatter/\">only insofar as that was a necessary byproduct of being optimized to pass the genetic material that made them on to the next generation</a>. If you've been reading&nbsp;Less Wrong for any significant length of time, you probably know this by now. In fact, around here this is almost a banality -- a <a href=\"http://wiki.lesswrong.com/wiki/Cached_thought\">cached thought</a>. \"We get it,\" you may be tempted to say. \"So stop signaling your tribal allegiance to this website and move on to some new, nontrivial meta-insight.\"</p>\n<p>But this is one of those things that truly do bear repeating, over and over again, almost at every opportunity. You really can't hear it enough. It has <em>consequences</em>, you see. The most important of which is: if you only do what feels epistemically \"natural\" all the time, you're going to be, well, wrong. And probably not just \"sooner or later\", either. Chances are, you're going to be wrong quite a lot.<a id=\"more\"></a></p>\n<p>To borrow a Yudkowskian turn of phrase: if you don't ever -- or indeed often -- find yourself needing to zig when, not only other people, but all kinds of internal \"voices\" in your mind are loudly shouting for you to zag, then you're either a native rationalist -- a born Bayesian, who should perhaps be <a href=\"/lw/qg/changing_the_definition_of_science/\">deducing general relativity from the fall of an apple</a> any minute now -- or else you're simply not trying hard enough.&nbsp;&nbsp;&nbsp;&nbsp;</p>\n<p>Oh, and another one of those consequences of humans' not being instinctively rational?</p>\n<p>Two intelligent young people with previously bright futures, named Amanda and Raffaele, are now seven days into spending the next quarter-century of their lives behind bars for a crime they almost certainly did not commit.</p>\n<p><del>\"Almost certainly\" really doesn't quite capture it</del>. In my <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/\">previous post</a> I asked readers to assign probabilities to the following propositions:</p>\n<p>1. Amanda Knox is guilty (of killing Meredith Kercher)<br>2. Raffaele Sollecito is guilty (of killing Meredith Kercher)<br>3. Rudy Gu\u00e9d\u00e9 is guilty (of killing Meredith Kercher)</p>\n<p>I also asked them to guess at how closely they thought their estimates would match mine.</p>\n<p>Well, for comparison, here are mine (<strong>revised</strong>):</p>\n<p>1. <del>Negligible</del>. Small.&nbsp;<del>Hardly different from the prior, which is&nbsp;dominated by the probability that someone in whatever reference class you would have put Amanda into on January 1, 2007 would commit murder within twelve months.</del> Something on the order of <del>0.001</del>&nbsp;0.01 or 0.1&nbsp;<em>at most</em>.&nbsp;&nbsp;<br>2. Ditto.<br>3. About as high as the other two numbers are low. <del>0.999</del> 0.99&nbsp;as a (probably weak) lower bound.</p>\n<p><del>Yes, you read that correctly. In my opinion, there is for all intents and purposes <em>zero</em> Bayesian evidence that Amanda and Raffaele are guilty.</del> Needless to say, this differs markedly from the consensus of the jury in Perugia, Italy.&nbsp;</p>\n<p>How could this be?</p>\n<p>Am I really suggesting that the estimates of eight jurors -- among whom two professional judges -- who heard the case for a year, along with something like 60% of the Italian public and probably half the Internet (and a significantly larger fraction of the non-American Internet), could be off by <del>a <em>minimum</em> of three orders of magnitude (probably significantly more)</del> such a large amount? <del>That most other people (including most commenters on my last post) are off by no fewer than two?</del></p>\n<p>Well, dear reader, before getting <em>too</em> incredulous,&nbsp;consider this. How about averaging the probabilities all those folks would assign to the proposition that Jesus of Nazareth rose from the dead, and calling that number x. Meanwhile, let y be the correct rational probability that Jesus rose from the dead, given the information available to us.</p>\n<p>How big do you suppose the ratio x/y is?</p>\n<p>Anyone want to take a stab at guessing the <em>logarithm</em> of that number?</p>\n<p>Compared to the probability that Jesus rose from the dead, my estimate of Amanda Knox's culpability makes it look like I think she's as guilty as sin itself.</p>\n<p>And that, of course, is just the central one of many sub-claims of the hugely <a href=\"http://wiki.lesswrong.com/wiki/Burdensome_details\">complex</a> yet widely believed proposition that Christianity is true. There are any number of other equally unlikely assertions that <a href=\"http://www.corriere.it/cronache/09_dicembre_09/in-cella-da-amanda-ho-ancora-fiducia-nella-vostra-giustizia-alessandro-capponi_bfed4194-e48a-11de-b76e-00144f02aabc.shtml\">Amanda would have heard at mass on the day after being found guilty of killing her new friend Meredith</a> (source in Italian) -- assertions that are assigned non-negligible probability by no fewer than a couple billion of the Earth's human inhabitants.</p>\n<p>I say this by way of preamble: be <em>very</em> wary of trusting in the rationality of your fellow humans, when you have serious reasons to doubt their conclusions.</p>\n<h3 id=\"The_Lawfulness_of_Murder__Inference_Proceeds_Backward__from_Crime_to_Suspect\">The Lawfulness of Murder: Inference Proceeds Backward, from Crime to Suspect</h3>\n<p><a href=\"/lw/hr/universal_law/\">We live in a lawful universe</a>. Every event that happens in this world -- including human actions and thoughts -- is ultimately governed by the laws of physics, which are exceptionless.&nbsp;</p>\n<p>Murder may be highly illegal, but from the standpoint of physics, it's as lawful as everything else. Every physical interaction, including a homicide, leaves traces -- changes in the environment that constitute information about what took place.</p>\n<p>Such information, however, is -- crucially -- <em>local</em>. The further away in space and time you move from the event, the less <a href=\"/lw/jl/what_is_evidence/\">entanglement</a> there is between your environment and that of the event, and thus the more difficult it is to make legitimate inferences about the event. The signal-to-noise ratio decreases dramatically as you move away in causal distance from the event. After all, the hypothesis space of possible causal chains of length n leading to the event increases exponentially in n.</p>\n<p>By far the most important evidence in a murder investigation will therefore be the evidence that is the closest to the crime itself -- evidence on and around the victim, as well as details stored in the brains of people who were present during the act. Less important will be evidence obtained from persons and objects a short distance away from the crime scene; and the importance decays rapidly from there as you move further out.</p>\n<p>It follows that you cannot possibly expect to reliably arrive at the correct answer by starting a few steps removed in the causal chain, say with a person you find \"suspicious\" for some reason, and working <em>forward</em> to come up with a plausible scenario for how the crime was committed. That would be <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">privileging the hypothesis</a>. Instead, you have to start from the actual crime scene, or as close to it as you can get, and work <em>backward</em>, letting yourself be <a href=\"http://yudkowsky.net/rational/virtues\">blown by the winds of evidence</a> toward one or more possible suspects.</p>\n<p>In the Meredith Kercher case, the winds of evidence blow with something like hurricane force in the direction of Rudy Gu\u00e9d\u00e9. After the murder, Kercher's bedroom was filled with evidence of Gu\u00e9d\u00e9's presence; his DNA was found not only on top of but actually inside her body. That's about as close to the crime as it gets. At the same time, no remotely similarly incriminating genetic material was found from anyone else -- in particular, there were no traces of the presence of either Amanda Knox or Raffaele Sollecito in the room (and no, the supposed Sollecito DNA on Meredith's bra clasp just plain <a href=\"http://www.friendsofamanda.org/bra_fastener.html\">does not count</a> -- nor, while we're at it, do the 100 picograms [about one human cell's worth] of DNA from Meredith <a href=\"http://www.friendsofamanda.org/summary.html\">allegedly on the tip of a knife</a> handled by Knox, found at Sollecito's apartment after the two were already suspects; these two things constituting pretty much the entirety of the physical \"evidence\" against the couple).</p>\n<p>If, up to this point, the police had reasons to be suspicious of Knox, Sollecito, and Gu\u00e9d\u00e9, they should have cleared Knox and Sollecito <em>at once</em> upon the discovery that Gu\u00e9d\u00e9 -- who, by the way, was the only one to have <em>fled the country</em> after the crime -- was the one whom the DNA matched. Unless, that is, Knox and Sollecito were specifically implicated by Gu\u00e9d\u00e9; after all, maybe Knox and Sollecito didn't actually kill the victim, but instead maybe they paid Gu\u00e9d\u00e9 to do so, or were otherwise involved in a conspiracy with him. But the prior probabilities of such scenarios are low, even in general -- to say nothing of the case of Knox and Sollecito specifically, who, tabloid press to the contrary, are known to have had utterly benign dispositions prior to these events, and no reason to want Meredith Kercher dead.</p>\n<p>If Amanda Knox and Raffaele Sollecito were to be in investigators' thoughts at all, they had to get there <em>via Gu\u00e9d\u00e9</em> -- because otherwise the hypothesis (<em>a priori</em> unlikely) of their having had homicidal intent toward Kercher would be <strong>entirely superfluous</strong> in explaining the chain of events that led to her death.&nbsp;&nbsp;The trail of evidence had led to Gu\u00e9d\u00e9, and therefore necessarily had to proceed from him; to follow any other path would be to fatally sidetrack the investigation, and virtually to guarantee serious -- very serious -- error. Which is exactly what happened.</p>\n<p>There was in fact no inferential path from Gu\u00e9d\u00e9 to Knox or Sollecito. He never implicated either of the two until long after the event; around the time of his apprehension, he specifically denied that Knox had been in the room. Meanwhile, it remains entirely unclear that he and Sollecito had ever even met.</p>\n<p>The hypotheses of Knox's and Sollecito's guilt are thus seen to be completely unnecessary, doing <strong>no explanatory work</strong> with respect to Kercher's death. They are&nbsp;nothing but extremely <a href=\"http://wiki.lesswrong.com/wiki/Burdensome_details\">burdensome</a> details.&nbsp;&nbsp;</p>\n<h3 id=\"Epistemic_Ruthlessness__Following_the_Strong_Signal\">Epistemic Ruthlessness: Following the Strong Signal</h3>\n<p>All of the \"evidence\" you've heard against Knox and Sollecito -- the changing stories, suspicious behavior, short phone calls, washing machine rumors, etc. -- is, quite literally, <em>just noise</em>.</p>\n<p>But it sounds so <em>suspicious</em>, you say. Who places a three-second phone call?&nbsp;</p>\n<p>As humans, we are <a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_psychology\">programmed to think</a> that the most important kinds of facts about the world are <em>mental</em> and <em>social</em> -- facts about what humans are thinking and planning, particularly as regards to other humans. This explains why some people are capable of wondering whether the presence of (<a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">only</a>) Rudy Gu\u00e9d\u00e9's DNA in and on Meredith's body should be balanced against the possibilty that Meredith may have been annoyed at Amanda for bringing home boyfriends and occasionally forgetting to flush the toilet -- that might have led to <em>resentment</em> on Amanda's part, you see.</p>\n<p>That's an extreme example, of course -- certainly no one here fell into that kind of trap. But <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bi5\">at least one</a> of the most thoughtful commenters was severely bothered by <a href=\"http://www.truejustice.org/ee/index.php?/tjmk/comments/our_take_on_the_case_for_the_prosecution_5_defendants_claims_shown_to_be_a/\">the length of Amanda's phone calls to Meredith</a>. As -- I'll confess -- was I, for a minute or two.</p>\n<p>I don't know why Amanda wouldn't have waited longer for Meredith to pick up. (For what it's worth, I myself have sometimes, in a state of nervousness, dialed someone's number, quickly changed my mind, then dialed again a short time later.) But -- as counterintuitive as it may seem -- <em>it doesn't matter</em>. The&nbsp;error here is even <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">asking a question</a> about Amanda's motivations when you haven't established an evidentiary (and <a href=\"/lw/jl/what_is_evidence/\">that means physical</a>)&nbsp;trail leading from Meredith's body to Amanda's brain. (Or even more to the point, when you <em>have</em> established a&nbsp;trail that led <em>decisively elsewhere</em>.)</p>\n<p>Maybe it's \"unlikely\" that Amanda would have behaved this way if she were innocent. But is the degree of improbabilty here anything like the improbability of her having participated in a sex-orgy-killing <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">without leaving a single piece of physical evidence behind</a>? While someone else left all kinds of traces? When you had no reason to suspect her at all without looking a good distance outside Meredith's room, far away from the important evidence?</p>\n<p>It's not even remotely comparable.&nbsp;</p>\n<p>Think about what you're doing here: <em>you are invoking the hypothesis that Amanda Knox is guilty of murder in order to explain the fact that she hung up the phone after three seconds.</em> (Remember, the evidence against Gu\u00e9d\u00e9 is such that the hypothesis of her guilt is&nbsp;superfluous -- not needed --&nbsp;in explaining the death of Meredith Kercher!)</p>\n<p>Maybe that's not quite as bad as invoking a superintelligent deity in order to explain life on Earth; but it's the same kind of mistake: explaining a strange thing by postulating a far, far stranger thing.</p>\n<p>\"But come on,\" says a voice in your head. \"Does this really sound like the behavior of an innocent person?\"</p>\n<p>You have to <em>shut that voice out</em>. Ruthlessly. Because it has no way of knowing. That voice is designed to assess the motivations of members of an ancestral hunter-gather band. At best, it may have the ability to distinguish the correct murderer from between 2 and 100 possibilities -- 6 or 7 <a href=\"http://wiki.lesswrong.com/wiki/Amount_of_evidence\">bits of inferential power</a> on the absolute best of days. That may have worked in hunter-gatherer times, before more-closely-causally-linked physical evidence could hope to be evaluated. (Or maybe not -- but at least it got the genes passed on.)</p>\n<p>DNA analysis, in contrast, has in principle the ability to uniquely identify a single individual from among the entire human species (depending on how much of the genome is looked at; also ignoring identical twins, etc.) -- that's more like 30-odd bits of inferential power. In terms of epistemic technology, we're talking about something like the difference in locomotive efficacy between a horsedrawn carriage and the Starship Enterprise. Our ancestral environment just plain did not equip our knowledge-gathering intuitions with the ability to handle weapons this powerful.</p>\n<p>We're talking about the kind of power that allows us to reduce what was formerly a question of <em>human social psychology</em> -- who made the decision to kill Meredith? -- to one of <em>physics</em>. (Or chemistry, at any rate.)</p>\n<p>But our minds don't naturally think in terms of physics and chemistry. From an intuitive point of view, the equations of those subjects are foreign; whereas \"X did Y because he/she wanted Z\" is familiar. This is why it's so difficult for people to intuitively&nbsp;appreciate that all of the chatter about Amanda's \"suspicious behavior\"&nbsp;with various convincing-sounding narratives put forth by the prosecution is <em>totally and utterly drowned out to oblivion</em> by the sheer strength of the DNA signal pointing to Gu\u00e9d\u00e9 <a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">alone</a>.</p>\n<p>This rationalist skill of <em>following the strong signal</em> -- mercilessly blocking out noise -- might be considered an <a href=\"/lw/31/what_do_we_mean_by_rationality/\">epistemic</a> analog of the <a href=\"/lw/31/what_do_we_mean_by_rationality/\">instrumental</a> \"<a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">shut up and multiply</a>\": when much is at stake, you have to be willing to jettison your <a href=\"/lw/l3/thou_art_godshatter/\">intuitive feelings</a> in favor of cold, hard, abstract&nbsp;calculation.</p>\n<p>In this case, that means, among other things, thinking in terms of <em>how much explanatory work is done by the various hypotheses</em>, rather than <em>how suspicious Amanda and Raffaele seem</em>.&nbsp;</p>\n<h3 id=\"Conclusion__The_Amanda_Knox_Test\">Conclusion: The Amanda Knox Test</h3>\n<p>I chose the title of this post because&nbsp;the parallel structure made it&nbsp;sound nice. But actually, I think an hour is a pretty weak upper bound on the amount of time a skilled rationalist should need to arrive at the correct judgment in this case.</p>\n<p>The fact is that what this comes down to is an utterly straightforward application of Occam's Razor. The complexity penalty on the prosecution's theory of the crime is enormous; the evidence in its favor had better be overwhelming. But instead, what we find is that the evidence from the scene -- the most important sort of evidence by&nbsp;a huge&nbsp;margin&nbsp;--&nbsp;points with literally&nbsp;superhuman strength toward a mundane, even typical, homicide scenario. To even consider theories not directly suggested by this evidence is to engage in <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">hypothesis privileging</a> to the extreme.</p>\n<p>So let me say it now, in case there was any doubt: the prosecution of Amanda Knox and Raffaele Sollecito, culminating in last week's jury verdict -- which apparently was unanimous, though it didn't need to be under Italian rules -- represents nothing but one more gigantic, disastrous rationality failure on the part of our species.</p>\n<p>How did Less Wrong do by comparison? The average estimated probability of Amanda Knox's guilt was 0.35 (thanks to <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bjg\">Yvain</a> for doing the calculation). It's pretty reasonable to assume the figure for Raffaele Sollecito would be similar. While not particularly flattering to the defendants (how would you like to be told that there's a 35% chance you're a murderer?), that number makes it obvious we would have voted to acquit. (If a 65% chance that they didn't do it doesn't constitute&nbsp; \"reasonable doubt\" that they did...)</p>\n<p>The commenters whose estimates were closest to mine -- and, therefore, to the correct answer, in my view -- were <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bdt\">Daniel Burfoot</a> and <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1bhu\">jenmarie</a>. Congratulations to them. (<del>But even they were off by a factor of at least ten!</del>)</p>\n<p>In general, most folks went in the right direction, but, as <a href=\"/lw/1ir/you_be_the_jury_survey_on_a_current_event/1beu\">Eliezer noted</a>, were far too underconfident -- evidently the result of an exorbitant level of trust in juries, at least in part. But people here were also widely making the same object-level mistake as (presumably) the jury: <em>vastly </em>overestimating the importance of \"psychological\" evidence, such as Knox's inconsistencies at the police station, as compared to \"physical\" evidence (only Gu\u00e9d\u00e9's DNA in the room).</p>\n<p>One thing that was interesting and rather encouraging, however, is the amount of updating people did after reading others' comments -- most of it in the right direction (toward innocence).</p>\n<p>[EDIT: After reading comments on this post, I have done some updating of my own. I now think I failed to adequately&nbsp;consider the possibility of my own overconfidence. This was pretty stupid of me, since it meant that the focus was taken away from the actual&nbsp;arguments in this post, and basically toward&nbsp;the&nbsp;issue of whether 0.001 can possibly be a rational estimate for anything you read&nbsp;about on&nbsp;the Internet.&nbsp;The qualitative reasoning of this post, of course, stands.&nbsp;Also, the focus of my&nbsp;accusations of irrationality was not primarily the LW community as reflected in&nbsp;my previous post; I actually think&nbsp;we&nbsp;did a pretty good job&nbsp;of coming to the right conclusion given the information provided -- and as others have noted, the levelheadedness with which we did so was impressive.]</p>\n<p>For most frequenters of this forum, where many of us regularly speak in terms of trying to save the human species from various global catastrophic risks, a case like this may not seem to have very many grand implications, beyond serving as yet another example of how basic principles of rationality such as Occam's Razor are incredibly difficult for people to grasp on an intuitive level. But it does catch the attention of someone like me, who takes an interest in less-commonly-thought-about forms of human suffering.</p>\n<p>The next time I find myself discussing the \"hard problem of consciousness\", thinking in vivid detail about the spectrum of human experience and wondering what it's like to be a bat, I am going to remember -- whether I say so or not -- that there is most definitely <em>something it's like</em> to be Amanda Knox in the moments following the announcement of that verdict: when you've just learned that, instead of heading back home to celebrate Christmas with your family as you had hoped, you will be spending the next decade or two -- your twenties and thirties -- in a prison cell in a foreign country. When your deceased friend's relatives are watching <a href=\"http://news.yahoo.com/s/ap/20091205/ap_on_re_eu/eu_italy_student_slain_49\">with satisfaction</a> as you are led, sobbing and wailing with desperation, to a van which will transport you back to that cell. (Ever thought about what that ride must be like?)&nbsp;</p>\n<p>While we're busy eliminating hunger, disease, and death itself, I hope we can also find the time, somewhere along the way, to get rid of <em>that</em>, too.</p>\n<p>(The Associated Press reported that, apparently, Amanda had some trouble going to sleep after the midnight verdict.)&nbsp;</p>\n<p>I'll conclude with this: the noted mathematician Serge Lang was in the habit of giving his students \"Huntington tests\" -- named in reference to his controversy with political scientist Samuel Huntington, whose entrance into the U.S. National Academy of Sciences Lang waged a successful campaign to block on the grounds of Huntington's insufficient scientific rigor.</p>\n<p>The purpose of the Huntington test, in Lang's words, was to see if the students could \"tell a fact from a hole in the ground\".</p>\n<p>I'm thinking of adopting a similar practice, and calling my version the Amanda Knox Test.&nbsp;</p>\n<p><strong>Postscript</strong>: <em>If you agree with me, and are also the sort of person who enjoys purchasing <a href=\"http://wiki.lesswrong.com/wiki/Fuzzies\">warm fuzzies</a> separately from your <a href=\"http://wiki.lesswrong.com/wiki/Utilons\">utilons</a>, you might consider donating to Amanda's <a href=\"http://www.amandadefensefund.org/\">defense fund</a>, to help out her financially devastated family. Of course, if you browse the site, you may feel your (prior) estimate of her guilt taking some hits; personally, that's okay with me.</em></p>", "sections": [{"title": "Note: The quantitative elements of this post have now been revised significantly.", "anchor": "Note__The_quantitative_elements_of_this_post_have_now_been_revised_significantly_", "level": 2}, {"title": "The Lawfulness of Murder: Inference Proceeds Backward, from Crime to Suspect", "anchor": "The_Lawfulness_of_Murder__Inference_Proceeds_Backward__from_Crime_to_Suspect", "level": 1}, {"title": "Epistemic Ruthlessness: Following the Strong Signal", "anchor": "Epistemic_Ruthlessness__Following_the_Strong_Signal", "level": 1}, {"title": "Conclusion: The Amanda Knox Test", "anchor": "Conclusion__The_Amanda_Knox_Test", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "649 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 649, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mkAcXPEJ7RZCJs8ry", "cSXZpvqpa9vbGGLtG", "SoDsr8GEZmRKMZNkj", "7iTwGquBFZKttpEdE", "6s3xABaXKPdFwA3FS", "mnS2WYLCGJP2kQkRn", "RcZCwxFiZzE6X7nsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-13T06:07:55.566Z", "modifiedAt": null, "url": null, "title": "Against picking up pennies", "slug": "against-picking-up-pennies", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:53.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sBFjoPm6xoQMcbRhX/against-picking-up-pennies", "pageUrlRelative": "/posts/sBFjoPm6xoQMcbRhX/against-picking-up-pennies", "linkUrl": "https://www.lesswrong.com/posts/sBFjoPm6xoQMcbRhX/against-picking-up-pennies", "postedAtFormatted": "Sunday, December 13th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20picking%20up%20pennies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20picking%20up%20pennies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsBFjoPm6xoQMcbRhX%2Fagainst-picking-up-pennies%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20picking%20up%20pennies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsBFjoPm6xoQMcbRhX%2Fagainst-picking-up-pennies", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsBFjoPm6xoQMcbRhX%2Fagainst-picking-up-pennies", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 458, "htmlBody": "<p>The eternally curious Tailsteak has written about <a href=\"http://tailsteak.com/archive.php?num=12\">how he always picks up pennies off the sidewalk</a>. He's run a cost-benefit analysis and determined that it's better on average to pick up a penny than to pass it by. His mistake lies nowhere in the analysis itself; it's pretty much correct. His mistake is performing the analysis in the first place.</p>\n<p>Pennies, you see, are easily the subject of <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>. When we come across a penny, we don't think, \"Hey, that's something worth 0.05% of what I wish I had come across. I could buy a 25th of a gumball, a mouthful of an unhealthy carbonated beverage, a couple of seconds of footage on DVD, or enough gasoline to go a tenth of a mile.\" We think, \"Hey, that's money,\" and we grab it.</p>\n<p>The thing is, it's difficult to comprehend how little&nbsp;a penny is worth&mdash;we don't really have a separate concept for \"mild happiness for a couple of seconds\"&mdash;and we're likely to take risks that far outweigh the benefits. We don't think of bending over to pick up a penny as being a risky endeavour, but <em>it's a penny</em>. How much risk does it take to outweigh a penny? Surely the risk of \"something unforeseen\" easily does the job. Are you 99.999999% sure that picking up that penny won't kill you? You need a reason for every 9 (if you're ambivalent between using seven 9s and using nine 9s, you should use seven; the number of 9s is <em>never </em>arbitrary), and by the time you come up with eight reasons to pick up the penny, you'll have wasted several cents' worth of time. If you can reduce the probability of harm that far, I applaud you.</p>\n<!-- It seems like traditionally, one would interpret the \"I applaud you\" of the above paragraph as a sarcastic way of saying that it can't be done. Nevertheless, there are problems that seem to require doing that sort of thing what if you have one billion pennies and have the opportunity to pick all of them up? I think you should. Does the probability of each penny being deathly suddenly drop off to practically zero? How could this happen? -->\n<p>Of course, penny-grabbing doesn't have to involve actual pennies. Suppose that President Kodos of the Unified States of Somewhere (population 300 million) uses the word \"idiot\" in an important speech, causing the average citizen to scowl and ponder for one minute. Now, if a penny can buy you five seconds of happiness, and scowling and pondering brings the same amount of <em>un</em>happiness, then that's twelve cents for every citizen, or 36 million dollars, of damage that Kodos just caused. Arguably, that's <a href=\"http://www.google.com/search?q=value+of+a+statistical+life\">the value of a couple of human lives</a>. As you can see, Kodos' decisions are extremely important. In this case, penny-grabbing would consist of anything <em>less</em> than trading precious seconds for precious human lives&mdash;if Kodos finds that he can save one life simply by going a few minutes out of his way, he should ignore it. (Photo ops and personal apologies are out of the question.) But keep in mind, of course, that&nbsp;avoiding saving someone's life because you have something better to do isn't rational unless you actually plan to do something better.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sBFjoPm6xoQMcbRhX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "1887", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2ftJ38y9SRBCBsCzy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-14T06:56:50.528Z", "modifiedAt": null, "url": null, "title": "Previous Post Revised", "slug": "previous-post-revised", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:54.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q55RwTSx7gRzbrsPt/previous-post-revised", "pageUrlRelative": "/posts/q55RwTSx7gRzbrsPt/previous-post-revised", "linkUrl": "https://www.lesswrong.com/posts/q55RwTSx7gRzbrsPt/previous-post-revised", "postedAtFormatted": "Monday, December 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Previous%20Post%20Revised&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrevious%20Post%20Revised%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq55RwTSx7gRzbrsPt%2Fprevious-post-revised%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Previous%20Post%20Revised%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq55RwTSx7gRzbrsPt%2Fprevious-post-revised", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq55RwTSx7gRzbrsPt%2Fprevious-post-revised", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 313, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">The Amanda Knox Test: How an Hour on the Internet Beats a Year in the Courtroom</a></p>\n<p><strong>See also: </strong><a href=\"/lw/i9/the_importance_of_saying_oops/\">The Importance of Saying \"Oops\"</a></p>\n<p>I'm posting this to call attention to the fact that I've now&nbsp;reconsidered&nbsp;the highly confident probability&nbsp;estimates in&nbsp;my post from yesterday on the Knox/Sollecito case. I haven't retracted my arguments; I just now think the level of confidence in them that I specified&nbsp;was too high. I've added the following paragraph to the concluding section:</p>\n<blockquote>\n<p>[EDIT: After reading comments on this post, I have done some updating of my own. I now think I failed to adequately&nbsp;consider the possibility of my own overconfidence. This was pretty stupid of me, since it meant that the focus was taken away from the actual&nbsp;arguments in this post, and basically toward&nbsp;the&nbsp;issue of whether 0.001 can possibly be a rational estimate for anything you read&nbsp;about on&nbsp;the Internet.&nbsp;The qualitative reasoning of this post, of course, stands.&nbsp;Also, the focus of my&nbsp;accusations of irrationality was not primarily the LW community as reflected in&nbsp;my previous post; I actually think&nbsp;we&nbsp;did a pretty good job&nbsp;of coming to the right conclusion given the information provided -- and as others have noted, the levelheadedness with which we did so was impressive.]</p>\n</blockquote>\n<p>While object-level comments on the case and on my reasoning about it&nbsp;should probably continue to be confined to that thread, I'd be interested in hearing in comments here what&nbsp;people&nbsp;think about the following:</p>\n<ul>\n<li>How much of a distraction&nbsp;did you find&nbsp;my extremely confident probabilities to be from&nbsp;the substance of my&nbsp;arguments?</li>\n<li>How much did those confident estimates make it seem like I was disagreeing, rather than agreeing, with the LW survey consensus? (It seemed to me that I had provoked&nbsp;people into trumpeting pro-guilt arguments more than they otherwise would have if I had&nbsp;initally&nbsp;given more \"reasonable\"&nbsp;numbers.)</li>\n<li>To what sorts of propositions, if any,&nbsp;do you yourself assign probabilities on the order of 0.999 or 0.001?</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q55RwTSx7gRzbrsPt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 5.44833492051127e-07, "legacy": true, "legacyId": "1994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b", "wCqfCLs8z5Qw4GbKS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-14T11:31:48.237Z", "modifiedAt": null, "url": null, "title": "Man-with-a-hammer syndrome", "slug": "man-with-a-hammer-syndrome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:25:05.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Shalmanese", "createdAt": "2009-03-22T02:29:32.044Z", "isAdmin": false, "displayName": "Shalmanese"}, "userId": "b9PjTzoHCPSS8fps7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WHP3tKPXppBF2S8e8/man-with-a-hammer-syndrome", "pageUrlRelative": "/posts/WHP3tKPXppBF2S8e8/man-with-a-hammer-syndrome", "linkUrl": "https://www.lesswrong.com/posts/WHP3tKPXppBF2S8e8/man-with-a-hammer-syndrome", "postedAtFormatted": "Monday, December 14th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Man-with-a-hammer%20syndrome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMan-with-a-hammer%20syndrome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHP3tKPXppBF2S8e8%2Fman-with-a-hammer-syndrome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Man-with-a-hammer%20syndrome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHP3tKPXppBF2S8e8%2Fman-with-a-hammer-syndrome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWHP3tKPXppBF2S8e8%2Fman-with-a-hammer-syndrome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 714, "htmlBody": "<blockquote><address><span style=\"font-family: Arial;\">What gummed up Skinner&rsquo;s reputation is that he developed a case of what I always call man-with-a-hammer syndrome: to the man with a hammer, every problem tends to look pretty much like a nail.</span></address></blockquote>\n<p><span style=\"font-family: Arial;\"><a href=\"http://www.vinvesting.com/docs/munger/human_misjudgement.html\">The Psychology of Human Misjudgment</a> is an brilliant talk given by Charlie Munger that I still return to and read every year to gain a fresh perspective. There&rsquo;s a lot of wisdom to be distilled from that piece but the one thing I want to talk about today is the man-with-a-hammer syndrome.</span></p>\n<p><span style=\"font-family: Arial;\">Man-with-a-hammer syndrome is pretty simple: you think of an idea and then, pretty soon, it becomes THE idea. You start seeing how THE idea can apply to anything and everything, it&rsquo;s the universal explanation for how the universe works. Suddenly, everything you&rsquo;ve ever thought of before must be reinterpreted through the lens of THE idea and you&rsquo;re on an intellectual high. <a href=\"http://en.wikipedia.org/wiki/Utilitarianism\">Utilitarianism</a> is a good example of this. Once you independently discover Utilitarianism you start to believe that an entire moral framework can be constructed around a system of pleasures and pains and, what&rsquo;s more, that this moral system is both objective and platonic.<a id=\"more\"></a> Suddenly, everything from the war in the middle east to taking your mid-morning dump at work because you need that 15 minutes of reflective time alone with yourself before you can face the onslaught of meaningless drivel that is part of corporate America but feeling guilty about it because you were raised to be a good Randian and you are not providing value from your employers so you&rsquo;re committing and act of theft can be fit under the Utilitarian framework. And then, hopefully, a few days later, you&rsquo;re over it and Utilitarianism is just another interesting concept and you&rsquo;re slightly embarrassed about your behavior a few days prior. Unfortunately, some people never get over it and they become those annoying people write long screeds on the internet about their THE idea.<br /> </span></p>\n<p><span style=\"font-family: Arial;\">The most important thing to realize about man-with-a-hammer syndrome is that there&rsquo;s no possible way to avoid having it happen to you. You can be a well seasoned rationalist who&rsquo;s well aware of how man-with-a-hammer syndrome works and what the various symptoms are but it&rsquo;s still going to hit you fresh with each new idea.</span><span style=\"font-family: Arial;\"> The best you can do is mitigate the fallout that occurs.</span></p>\n<p><span style=\"font-family: Arial;\">Once you recognize that you&rsquo;ve been struck with man-with-a-hammer syndrome, there&rsquo;s a number of sensible precautions you can take. The first is to have a good venting spot, being able to let your thoughts out of your head for some air lets you put them slightly in perspective. Personally, I have a few trusted friends to which I expose man-with-a-hammer ideas with all the appropriate disclaimers to basically ignore the bullshit that is coming out of my mouth.</span></p>\n<p><span style=\"font-family: Arial;\">The second important thing to do is to hold back from telling anyone else about the idea. Making an idea public means that you&rsquo;re, to a degree, committed to it and this is not what you want. The best way to prolong man-with-a-hammer syndrome is to have other people believing that you believe something.<br /> </span></p>\n<p><span style=\"font-family: Arial;\">Unfortunately, the only other thing to do is simply wait. There&rsquo;s been nothing I&rsquo;ve discovered that can hasten the recovery from man-with-a-hammer syndrome beyond some minimum time threshold. If you&rsquo;ve done everything else right, the only thing left to do is to simply out wait it. No amount of clever mental gymnastics will help you get rid of the syndrome any faster and that&rsquo;s the most frustrating part. You can be perfectly aware that you have it, know that everything you&rsquo;re thinking now, <em>you won&rsquo;t believe</em> in a weeks time and yet you still can&rsquo;t stop yourself from believing in it <em>now</em>. </span></p>\n<p><span style=\"font-family: Arial;\">Man-with-a-hammer syndrome can destroy your life if you&rsquo;re not careful but, if handled appropriately, is ultimately nothing more than an annoying and tedious cost of coming up with interesting ideas. What&rsquo;s most interesting about it to me is that even with full awareness of it&rsquo;s existence, it&rsquo;s completely impossible to avoid. While you have man-with-a-hammer syndrome, you end up living in a curious world in which you are unable to disbelieve in something you know to be not true and this is a deeply weird state I&rsquo;ve not seen &ldquo;rationalists&rdquo; fully come to terms with.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WHP3tKPXppBF2S8e8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 14, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "1995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-15T13:56:09.689Z", "modifiedAt": null, "url": null, "title": "Rebasing Ethics", "slug": "rebasing-ethics", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:52.888Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Shalmanese", "createdAt": "2009-03-22T02:29:32.044Z", "isAdmin": false, "displayName": "Shalmanese"}, "userId": "b9PjTzoHCPSS8fps7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i9rX2Mc2ERWHGF3Y8/rebasing-ethics", "pageUrlRelative": "/posts/i9rX2Mc2ERWHGF3Y8/rebasing-ethics", "linkUrl": "https://www.lesswrong.com/posts/i9rX2Mc2ERWHGF3Y8/rebasing-ethics", "postedAtFormatted": "Tuesday, December 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rebasing%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARebasing%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi9rX2Mc2ERWHGF3Y8%2Frebasing-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rebasing%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi9rX2Mc2ERWHGF3Y8%2Frebasing-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi9rX2Mc2ERWHGF3Y8%2Frebasing-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 487, "htmlBody": "<p>Lets start with the following accepted as a given:</p>\n<ul>\n<li>There exists no supernatural forces in the world and there is no objective morality imposed from above.</li>\n<li>Our current moral codes are currently based on some mix of sociobiological influences &amp; cultural forces</li>\n</ul>\n<p>Of the current figures who accept these premises, most espouse some form of secular humanism which argues that humans are genetically programed not to lie, murder or steal, therefore this is both the right morality &amp; the one they practice. This, to my mind, is committing the <a href=\"http://en.wikipedia.org/wiki/Naturalistic_fallacy\">naturalistic fallacy</a>.<a id=\"more\"></a></p>\n<p>Instead, I want to offer an analogy: Humans have an innate preference for certain foods which evolved in an environment radically different from modern society. In a modern society, it is widely regarded as virtuous to be actively working against our innate, genetic impulses through the practice of dieting. Similarly, our ethical landscape is radically different from the one we evolved in. Thus what we should be doing is actively working against our innate moral sense to be more in line with the modern world. In the same way that there is junk food that tastes good but is ultimately unhealthy for you, I believe there is ethical junk food which fills us with a feeling of virtue that is underserved.</p>\n<p>Let me start off with a mild example: Tipping.</p>\n<p>Altruism evolved in an era of small tribes where individual altruistic acts could be remembered &amp; paid back. Now that we live in a large, anonymous society, there are many times when altruism doesn't pay. Unless you are with friends or a frequent diner at a restaurant or bar, the correct moral move is to stiff the waiter on the tip. If you're traveling somewhere alone, you should universally fail to tip as you're not likely to ever return there.</p>\n<p>If this fills you with immediate moral revulsion, you're not alone. I'm so skeeved out by this that I've never yet worked up the nerve to do it. My empathetic system simulates how the waiter must feel to not get a tip and I get queasy in the pit of my stomach. But this empathetic system isn't based on any moral fact, it's simply an evolutionary shortcut that helps us survive in small groups. To rebase your ethics is to start actively fighting against that feeling of moral revulsion, just as the first step of a diet is to fight against the desire for fatty, sweet, salty foods.</p>\n<p>In fact, I would go so far as to say that if your own personal moral system doesn't have parts that are morally repulsive to you, then you're not doing it right and anybody who tries to tell you different is selling you snake oil.</p>\n<p>As to why I don't hear anyone talking about this stuff, it's like fight club. The first rule of rebasing your entire ethics system is that you never tell anyone you've rebased your entire ethics system.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i9rX2Mc2ERWHGF3Y8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": -10, "extendedScore": null, "score": 5.451672153154477e-07, "legacy": true, "legacyId": "2001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-15T22:40:55.602Z", "modifiedAt": null, "url": null, "title": "Getting Over Dust Theory", "slug": "getting-over-dust-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:35.094Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jhuffman", "createdAt": "2009-03-10T14:36:32.341Z", "isAdmin": false, "displayName": "jhuffman"}, "userId": "MPKAfFcfDKeTtEKTc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZCjbuQvYkmdH26d8M/getting-over-dust-theory", "pageUrlRelative": "/posts/ZCjbuQvYkmdH26d8M/getting-over-dust-theory", "linkUrl": "https://www.lesswrong.com/posts/ZCjbuQvYkmdH26d8M/getting-over-dust-theory", "postedAtFormatted": "Tuesday, December 15th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Getting%20Over%20Dust%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGetting%20Over%20Dust%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCjbuQvYkmdH26d8M%2Fgetting-over-dust-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Getting%20Over%20Dust%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCjbuQvYkmdH26d8M%2Fgetting-over-dust-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCjbuQvYkmdH26d8M%2Fgetting-over-dust-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 356, "htmlBody": "<p>It has been well over a year since I first read <a href=\"http://en.wikipedia.org/wiki/Permutation_City\">Permutation City</a> and relating writings on the internet on <a href=\"http://www.gregegan.net/PERMUTATION/FAQ/FAQ.html\">Greg Egan's dust theory</a>. It still haunts me. The theory has been discussed tangentially in this community, but I haven't found an article that directly addresses the rationality of Egan's own dismissal of the theory.</p>\n<p>In the <a href=\"http://www.gregegan.net/PERMUTATION/FAQ/FAQ.html\">FAQ</a>, Egan says things like:</p>\n<blockquote>\n<p>I wrote the ending as a way of dramatising[sic] a dissatisfaction I had with the &ldquo;pure&rdquo; Dust Theory that I never could (and still haven't) made precise (see Q5): the universe we live in is more coherent than the Dust Theory demands, so there must be something else going on.</p>\n</blockquote>\n<p>and:</p>\n<blockquote>\n<p>I have yet to hear a convincing refutation of it on purely logical grounds...</p>\n<p>However, I think the universe we live in provides strong empirical evidence against the &ldquo;pure&rdquo; Dust Theory, because it is far too orderly and obeys far simpler and more homogeneous physical laws than it would need to, merely in order to contain observers with an enduring sense of their own existence. If every arrangement of the dust that contained such observers was realised, then there would be billions of times more arrangements in which the observers were surrounded by chaotic events, than arrangements in which there were uniform physical laws.<a id=\"more\"></a></p>\n</blockquote>\n<p>Isn't this, along with so many other problems, a candidate for our sometime friend the anthropic principle? That is: only in a conscious configuration field which has memories of perceptions of an orderly universe is the dust theory controversial or doubted? In the vastly more numerous conscious configuration fields with memories of perceptions of a chaotic and disorderly universe lacking a rational way to support the observer the dust theory could be accepted a priori or at least be a favored theory.</p>\n<p>It is fine to dismiss dust theory because it simply isn't very helpful and because it has no predictions, testable or otherwise. I suppose it is also fine never to question the nature of consciousness as the answers don't seem to lead anywhere helpful either; though the question of it will continue to vex some instances of these configuration states.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZCjbuQvYkmdH26d8M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 10, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "2002", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-16T03:13:02.633Z", "modifiedAt": null, "url": null, "title": "Philadelphia LessWrong Meetup, December 16th", "slug": "philadelphia-lesswrong-meetup-december-16th", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:52.675Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Peter_de_Blanc", "createdAt": "2009-02-27T14:15:28.882Z", "isAdmin": false, "displayName": "Peter_de_Blanc"}, "userId": "vRvaAqR5tcjGEWaoC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KCSWFJmJiDtjJo6gq/philadelphia-lesswrong-meetup-december-16th", "pageUrlRelative": "/posts/KCSWFJmJiDtjJo6gq/philadelphia-lesswrong-meetup-december-16th", "linkUrl": "https://www.lesswrong.com/posts/KCSWFJmJiDtjJo6gq/philadelphia-lesswrong-meetup-december-16th", "postedAtFormatted": "Wednesday, December 16th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philadelphia%20LessWrong%20Meetup%2C%20December%2016th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhiladelphia%20LessWrong%20Meetup%2C%20December%2016th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKCSWFJmJiDtjJo6gq%2Fphiladelphia-lesswrong-meetup-december-16th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philadelphia%20LessWrong%20Meetup%2C%20December%2016th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKCSWFJmJiDtjJo6gq%2Fphiladelphia-lesswrong-meetup-december-16th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKCSWFJmJiDtjJo6gq%2Fphiladelphia-lesswrong-meetup-december-16th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 29, "htmlBody": "<p>There will be a LessWrong meetup on Wednesday, December 16th (tomorrow). We're meeting at 7:15 PM at Kabul Restaurant at 106 Chestnut Street. Five people have confirmed so far.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KCSWFJmJiDtjJo6gq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 5.453103559049558e-07, "legacy": true, "legacyId": "2006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-17T01:33:15.694Z", "modifiedAt": null, "url": null, "title": "An account of what I believe to be inconsistent behavior on the part of our editor", "slug": "an-account-of-what-i-believe-to-be-inconsistent-behavior-on", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:03.960Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PeterS", "createdAt": "2009-07-12T01:05:07.337Z", "isAdmin": false, "displayName": "PeterS"}, "userId": "dsCRuT6ZHfa6nMmim", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y5nfQ5hg6yn4Pq6Gj/an-account-of-what-i-believe-to-be-inconsistent-behavior-on", "pageUrlRelative": "/posts/Y5nfQ5hg6yn4Pq6Gj/an-account-of-what-i-believe-to-be-inconsistent-behavior-on", "linkUrl": "https://www.lesswrong.com/posts/Y5nfQ5hg6yn4Pq6Gj/an-account-of-what-i-believe-to-be-inconsistent-behavior-on", "postedAtFormatted": "Thursday, December 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20account%20of%20what%20I%20believe%20to%20be%20inconsistent%20behavior%20on%20the%20part%20of%20our%20editor&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20account%20of%20what%20I%20believe%20to%20be%20inconsistent%20behavior%20on%20the%20part%20of%20our%20editor%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5nfQ5hg6yn4Pq6Gj%2Fan-account-of-what-i-believe-to-be-inconsistent-behavior-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20account%20of%20what%20I%20believe%20to%20be%20inconsistent%20behavior%20on%20the%20part%20of%20our%20editor%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5nfQ5hg6yn4Pq6Gj%2Fan-account-of-what-i-believe-to-be-inconsistent-behavior-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY5nfQ5hg6yn4Pq6Gj%2Fan-account-of-what-i-believe-to-be-inconsistent-behavior-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 679, "htmlBody": "<p>There was recently a <a href=\"/lw/1j5/a_question_of_rationality/\">submission</a> here posing criticism a well-known contributor, Eliezer Yudkowsky.<a id=\"more\"></a> Admittedly, whatever question the poster intended to ask was embedded within a post which was evidently designed to be a test of our rationality. Eliezer didn't seem to care too much, stating that the issue was</p>\n<blockquote>\n<p>it being a top-level post instead of Open Thread comment. Probably would've been a lot more forgiving if it'd been an Open Thread comment. . .</p>\n<p>The question would, under other circumstances, be just - but I don't care to justify myself here. Elsewhere, perhaps. . .</p>\n</blockquote>\n<blockquote>\n<p>This belongs as a comment on the SIAI blog, not a post on Less Wrong.</p>\n</blockquote>\n<p>I asked Eliezer why mormon2's post belonged on the SIAI blog and not here. He responded thus:</p>\n<blockquote>\n<p>Because Less Wrong is about human rationality, not the Singularity Institute, and not me.</p>\n</blockquote>\n<p>This response is unsatisfactory. Either <a href=\"/lw/1hn/call_for_new_siai_visiting_fellows_on_a_rolling/\">certain posts</a> belong on the SIAI blog and not here, or they don't and <a href=\"/lw/15p/singularity_summit_2009_quick_post/\">can be posted here</a>. It can't be both ways</p>\n<p>Note that I do not approve of mormon2's submission, as of the recent statement he made in an edit. I do, however, approve of the <em>idea</em> of such a submission. Somebody should be able to make a top-level post directing questions and criticism towards another author, under certain circumstances. I can't fully pin down just what the precise circumstances should be - it's not up for me to decide in any case.</p>\n<p>But consider a high-profile contributor, who already has many posts about himself (several self-submitted) and his work, who has at times responded to off-site comments with top-level posts on Less Wrong, and who has recently given his blessing to a post entitled <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/\">Less Wrong Q&amp;A With Eliezer Yudkowsky</a> - when such a person suggests that a submission concerning himself and his work belongs as a comment in the monthly Open Thread, or as a comment on an off-site blog, I find it very outlandish.</p>\n<p>Eliezer is not an ordinary contributor. In the beginning, it was <a href=\"/lw/18/slow_down_a_little_maybe/\">apparently envisioned</a> that there would be a limit to the number of non-Yudkowsky/Hanson posts submitted per day. Obviously that policy has not been enacted. In any case, by my count there have been 682 submissions to Less Wrong as of December 14<sup>th</sup>. Eliezer has contributed 108 of those (the median number of posts per author being 2.5)<sup>1</sup>. He is not a \"specific person\" being asked to justify \"specific decisions\", as he would hypothetically <a href=\"/lw/1j5/a_question_of_rationality/1bsm\">suggest</a> if his intent were to be manipulative. It's actually quite difficult for me to characterize Eliezer's role here. Mostly he's a great author and commenter. Sometimes he comments as the <a href=\"/lw/1j5/a_question_of_rationality/1bsp\">site editor</a>. At other times, he seems to submit as though this were his <a href=\"/lw/wa/thanksgiving_prayer/\">personal</a> <a href=\"/lw/zo/with_whom_shall_i_diavlog/\">blog</a>, <a href=\"/lw/ak/twelve_virtues_booklet_printing/\">Yahoo! Answers</a>, or the <a href=\"/lw/113/esrs_comments_on_some_eyoblw_posts/\">comment thread</a> on an <a href=\"/lw/1ax/im_not_saying_people_are_stupid/\">entirely different site</a>. He seems to have developed into a sort of <a href=\"/lw/4g/eliezer_yudkowsky_facts/\">community icon</a> at Less Wrong.</p>\n<p>So it occurs to me (after having expended my google-fu and searched the LW-Wiki), that there are no posted rules for appropriate top-level topics on Less Wrong. If there are, please correct me. At Overcoming Bias we submitted articles to the editors and they decided whether to publish them (I assume there were few or no restrictions for the editors' own work). The only restriction I know of on Less Wrong is that you need a Karma Score of at least 20 or 40 to submit posts. This is clearly insufficient, since if you have 8000 Karma you can submit anything. How many moderators do we have? I have yet to find a list.</p>\n<p>I feel that we must address these issues, either presently or ultimately. I know of no other decent community with Less Wrong's stated goal. And yet I am very much vexed by these inconsistencies I perceive between the stated purpose and the site's actual operation.</p>\n<p><sup>1</sup>I put the date marking the beginning of LessWrong.com as an open community at March 5th, 2009. This was the date of the first post by someone other than Eliezer / Robin after Eliezer's <a href=\"/lw/yw/tell_your_rationalist_origin_story_at_less_wrong/\">announcement that a beta version of the site had been launched</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y5nfQ5hg6yn4Pq6Gj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 4, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "2010", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wtzCt5jLMdpgXbMhZ", "kiaDpaGAs4DZ5HKib", "Zwbz6Wv7yaLM6J36r", "iLGrNTwTZivTX5774", "GTzBTtkZH8KxNNfxA", "QB6BkkpwiecfF6Ekq", "qi5Xg3stgMwvdYKQk", "a3MhmPM7eZbP6pFPZ", "GGEoygaLHvZCg5LBi", "cs2nvx7ajkGr5kudk", "Ndtb22KYBxpBsagpj", "fwWyQgAFRuNpQJDPm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-17T03:41:17.341Z", "modifiedAt": null, "url": null, "title": "December 2009 Meta Thread", "slug": "december-2009-meta-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:34.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CarlShulman", "createdAt": "2009-03-01T07:47:12.225Z", "isAdmin": false, "displayName": "CarlShulman"}, "userId": "SguegG9SFXaKTgJLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hSavx8SBvbwYbrWig/december-2009-meta-thread", "pageUrlRelative": "/posts/hSavx8SBvbwYbrWig/december-2009-meta-thread", "linkUrl": "https://www.lesswrong.com/posts/hSavx8SBvbwYbrWig/december-2009-meta-thread", "postedAtFormatted": "Thursday, December 17th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20December%202009%20Meta%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecember%202009%20Meta%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhSavx8SBvbwYbrWig%2Fdecember-2009-meta-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=December%202009%20Meta%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhSavx8SBvbwYbrWig%2Fdecember-2009-meta-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhSavx8SBvbwYbrWig%2Fdecember-2009-meta-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>This post is a place to discuss meta-level issues regarding Less Wrong. Such posts <a href=\"/lw/1ju/an_account_of_what_i_believe_to_be_inconsistent/1c7r\">may or may not be</a>&nbsp;the unique venue for such discussion in the future.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hSavx8SBvbwYbrWig", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 5.455742647219457e-07, "legacy": true, "legacyId": "2013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-18T16:25:08.831Z", "modifiedAt": null, "url": null, "title": "Reacting to Inadequate Data", "slug": "reacting-to-inadequate-data", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:53.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pPmTL9DLDDF2TTwQg/reacting-to-inadequate-data", "pageUrlRelative": "/posts/pPmTL9DLDDF2TTwQg/reacting-to-inadequate-data", "linkUrl": "https://www.lesswrong.com/posts/pPmTL9DLDDF2TTwQg/reacting-to-inadequate-data", "postedAtFormatted": "Friday, December 18th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reacting%20to%20Inadequate%20Data&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReacting%20to%20Inadequate%20Data%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPmTL9DLDDF2TTwQg%2Freacting-to-inadequate-data%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reacting%20to%20Inadequate%20Data%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPmTL9DLDDF2TTwQg%2Freacting-to-inadequate-data", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpPmTL9DLDDF2TTwQg%2Freacting-to-inadequate-data", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 456, "htmlBody": "<h2>Two Scenarios</h2>\n<p>Alice must answer the multiple-choice question, \"What color is the ball?\" The two choices are \"Red\" and \"Blue.\" Alice has no relevant memories of The Ball other than she knows it exists. She cannot see The Ball or interact with it in any way; she cannot do anything but think until she answers the question.</p>\n<p>In an independent scenario, Bob has the same question but Bob has two memories of The Ball. In one of the memories, The Ball is red. In the other memory, The Ball is blue. There are no \"timestamps\" associated with the memories and no way of determining if one came before the other. Bob just has two memories and he, somehow, knows the memories are of the same ball.</p>\n<p>If you were Alice, what would you do?</p>\n<p>If you were Bob, what would you do?</p>\n<p><a id=\"more\"></a></p>\n<h2>Variations</h2>\n<p>More questions to ponder:</p>\n<ul>\n<li>Should they do anything at all?</li>\n<li>Should Alice and Bob act differently?</li>\n<li>If Alice and Bob could circle more than one color, should they?</li>\n<li>Would either answer change if the option \"Green\" was added to the choice list?</li>\n<li>If the question was fill-in-the-blank, what should they write?</li>\n<li>If Bob's memories were of different balls but he didn't know which ball was The Ball, should his actions change?</li>\n<li>If Alice and Bob could coordinate, should it affect their answers?</li>\n</ul>\n<h2>Further Discussion</h2>\n<p>The basic question I was initially pondering was how to resolve conflicting sensory inputs. If I were a brain in a vat and I received two simultaneous sensory inputs that conflicted (such as the color of a ball), how should I process them?</p>\n<p>Another related topic is whether a brain in a vat with <em>absolutely</em> no sensory inputs should be considered intelligent. These two questions were reduced into the above two scenarios and I am asking for help in resolving them. I think they are similar to questions asked here before but their relation to these two brain-in-a-vat questions seemed relevant to me.</p>\n<h2>Realistic Scenarios</h2>\n<p>These scenarios are cute but there are similar real-world examples. When asked if a visible ball was red or green and you happened to be unable to distinguish between red and green, how do you interpret what you see?</p>\n<p>Abstracting a bit, any input (sensory or otherwise) that is indistinguishable from another input can really muck with your head. Most optical illusions are tricks on eye-hardware (software?).</p>\n<p>This post is not intended to be clever or teach anything new. Rather, the topic confuses me and I am seeking to learn about the correct behavior. Am I missing some form of global input theory that helps resolve colliding inputs or missing data? When the data is inadequate, what should I do? Start guessing randomly?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pPmTL9DLDDF2TTwQg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -2, "extendedScore": null, "score": 5.45970820271497e-07, "legacy": true, "legacyId": "2014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Two_Scenarios\">Two Scenarios</h2>\n<p>Alice must answer the multiple-choice question, \"What color is the ball?\" The two choices are \"Red\" and \"Blue.\" Alice has no relevant memories of The Ball other than she knows it exists. She cannot see The Ball or interact with it in any way; she cannot do anything but think until she answers the question.</p>\n<p>In an independent scenario, Bob has the same question but Bob has two memories of The Ball. In one of the memories, The Ball is red. In the other memory, The Ball is blue. There are no \"timestamps\" associated with the memories and no way of determining if one came before the other. Bob just has two memories and he, somehow, knows the memories are of the same ball.</p>\n<p>If you were Alice, what would you do?</p>\n<p>If you were Bob, what would you do?</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Variations\">Variations</h2>\n<p>More questions to ponder:</p>\n<ul>\n<li>Should they do anything at all?</li>\n<li>Should Alice and Bob act differently?</li>\n<li>If Alice and Bob could circle more than one color, should they?</li>\n<li>Would either answer change if the option \"Green\" was added to the choice list?</li>\n<li>If the question was fill-in-the-blank, what should they write?</li>\n<li>If Bob's memories were of different balls but he didn't know which ball was The Ball, should his actions change?</li>\n<li>If Alice and Bob could coordinate, should it affect their answers?</li>\n</ul>\n<h2 id=\"Further_Discussion\">Further Discussion</h2>\n<p>The basic question I was initially pondering was how to resolve conflicting sensory inputs. If I were a brain in a vat and I received two simultaneous sensory inputs that conflicted (such as the color of a ball), how should I process them?</p>\n<p>Another related topic is whether a brain in a vat with <em>absolutely</em> no sensory inputs should be considered intelligent. These two questions were reduced into the above two scenarios and I am asking for help in resolving them. I think they are similar to questions asked here before but their relation to these two brain-in-a-vat questions seemed relevant to me.</p>\n<h2 id=\"Realistic_Scenarios\">Realistic Scenarios</h2>\n<p>These scenarios are cute but there are similar real-world examples. When asked if a visible ball was red or green and you happened to be unable to distinguish between red and green, how do you interpret what you see?</p>\n<p>Abstracting a bit, any input (sensory or otherwise) that is indistinguishable from another input can really muck with your head. Most optical illusions are tricks on eye-hardware (software?).</p>\n<p>This post is not intended to be clever or teach anything new. Rather, the topic confuses me and I am seeking to learn about the correct behavior. Am I missing some form of global input theory that helps resolve colliding inputs or missing data? When the data is inadequate, what should I do? Start guessing randomly?</p>", "sections": [{"title": "Two Scenarios", "anchor": "Two_Scenarios", "level": 1}, {"title": "Variations", "anchor": "Variations", "level": 1}, {"title": "Further Discussion", "anchor": "Further_Discussion", "level": 1}, {"title": "Realistic Scenarios", "anchor": "Realistic_Scenarios", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-19T22:40:51.201Z", "modifiedAt": null, "url": null, "title": "The Contrarian Status Catch-22", "slug": "the-contrarian-status-catch-22", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:07.835Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h723jFRLGosaxEzGC/the-contrarian-status-catch-22", "pageUrlRelative": "/posts/h723jFRLGosaxEzGC/the-contrarian-status-catch-22", "linkUrl": "https://www.lesswrong.com/posts/h723jFRLGosaxEzGC/the-contrarian-status-catch-22", "postedAtFormatted": "Saturday, December 19th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Contrarian%20Status%20Catch-22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Contrarian%20Status%20Catch-22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh723jFRLGosaxEzGC%2Fthe-contrarian-status-catch-22%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Contrarian%20Status%20Catch-22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh723jFRLGosaxEzGC%2Fthe-contrarian-status-catch-22", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh723jFRLGosaxEzGC%2Fthe-contrarian-status-catch-22", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1054, "htmlBody": "<p>It used to puzzle me that Scott Aaronson still hasn't come to terms with <a href=\"/lw/q7/if_manyworlds_had_come_first/\">the obvious absurdity of attempts to make quantum mechanics yield a single world</a>.</p>\n<p>I should have realized what was going on when I read Scott's blog post \"<a href=\"http://scottaaronson.com/blog/?p=326\">The bullet-swallowers</a>\" in which Scott compares many-worlds to libertarianism.&nbsp; But light didn't dawn until my recent diavlog with Scott, where, at 50 minutes and 20 seconds, Scott says:</p>\n<p style=\"padding-left: 30px;\">\"What you've forced me to realize, Eliezer, and I thank you for this:&nbsp; What I'm uncomfortable with is not the many-worlds interpretation itself, it's the air of satisfaction that often comes with it.\"<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -- Scott Aaronson, <a href=\"http://bloggingheads.tv/diavlogs/21857?in=49:50&amp;out=63:13\">50:20 in our Bloggingheads dialogue</a>.</p>\n<p>It doesn't show on my face (I need to learn to reveal my expressions more, people complain that I'm eerily motionless during these diavlogs) but at this point I'm thinking, <em>Didn't Scott just outright concede the argument?</em>&nbsp; (He didn't; I checked.)&nbsp; I mean, to me this sounds an awful lot like:</p>\n<p style=\"padding-left: 30px;\"><em>Sure, many-worlds is the simplest explanation that fits the facts, but I don't like the people who believe it.</em></p>\n<p>And I strongly suspect that a lot of people out there who would refuse to identify themselves as \"atheists\" would say almost exactly the same thing:</p>\n<p style=\"padding-left: 30px;\"><em>What I'm uncomfortable with isn't the idea of a god-free physical universe, it's the air of satisfaction that atheists give off.</em></p>\n<p><a id=\"more\"></a></p>\n<p>If you're a regular reader of Robin Hanson, you might essay a Hansonian explanation as follows:</p>\n<p style=\"padding-left: 30px;\"><em>Although the actual state of evidence favors many-worlds (atheism), I don't want to affiliate with other people who say so.&nbsp; They act all brash, arrogant, and offensive, and tend to believe and advocate other odd ideas like libertarianism.&nbsp; If I believed in many-worlds (atheism), that would make me part of this low-prestige group.</em></p>\n<p>Or in simpler psychology:</p>\n<p style=\"padding-left: 30px;\"><em>I don't feel like I belong with the group that believes in many-worlds (atheism).<br /></em></p>\n<p>I think this might form a very general sort of status catch-22 for contrarian ideas.</p>\n<p>When a correct contrarian idea comes along, it will have appealing qualities like simplicity and favorable evidence (in the case of factual beliefs) or high expected utility (in the case of policy proposals).&nbsp; When an appealing contrarian idea comes along, it will be <em>initially </em>supported by its appealing qualities, and opposed by the fact that it seems strange and unusual, or any other counterintuitive aspects it may have.</p>\n<p>So <em>initially,</em> the group of people who are most likely to support the contrarian idea, are the people who are - among other things - most likely to break with their herd in support of an idea that seems true or right.</p>\n<p>These first supporters are likely to be the sort of people who - rather than being careful to speak of the new idea in the cautious tones prudent to supplicate the many high-status insiders who believe otherwise - just go around talking as if the idea had a very high probability, merely because it seems to them like the simplest explanation that fits the facts.&nbsp; \"Arrogant\", \"brash\", and \"condescending\" are some of the terms used to describe people with this poor political sense.</p>\n<p>The people first to speak out in support of the new idea will be those less sensitive to conformity; those with more confidence in their sense of truth or rightness; those less afraid of speaking out.</p>\n<p>And to the extent these are general character traits, such first supporters are also more likely to advocate other contrarian beliefs, like libertarianism or strident atheism or <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a>.</p>\n<p>And once <em>that</em> happens, the only people who'll be willing to believe the idea will be those willing to tar themselves by affiliating with a group of arrogant nonconformists - on top of everything else!</p>\n<p>tl;dr:&nbsp; When a counterintuitive new idea comes along, the first people to support it will be contrarians, and so the new idea will become associated with contrarian traits and beliefs, and people will become even more reluctant to believe it because that would affiliate them with low-prestige people/traits/beliefs.</p>\n<hr />\n<p>A further remark on \"airs of satisfaction\":&nbsp; Talk about how we don't understand the Born Probabilities and there are still open questions in QM, and hence we can't accept <a href=\"/lw/19m/privileging_the_hypothesis/#154t\">the no-worldeaters interpretation</a>, sounds a good deal like the criticism given to atheists who go around advocating the no-God interpretation.&nbsp; \"But there's so much we don't know about the universe!&nbsp; Why are you so self-satisfied with your disbelief in God?\"&nbsp; There's <em>plenty </em>we don't understand about the universe, but <a href=\"/lw/19m/privileging_the_hypothesis/\">that doesn't mean that future discoveries are likely to reveal Jehovah</a> any more than they're likely to reveal a collapse postulate.</p>\n<p>Furthermore, atheists are <em>more likely than priests </em>to hear \"But we don't know everything about the universe\" or \"What's with this air of satisfaction?\"&nbsp; Similarly, it looks to me like you can get away with speaking out strongly in <em>favor </em>of collapse postulates and <em>against </em>many-worlds, and the same people won't call you on an \"air of satisfaction\" or say \"but what about the open questions in quantum mechanics?\"</p>\n<p>This is why I think that what we have here is just a sense of <em>someone being too confident in an unusual belief given their assigned social status,</em> rather than a genuine sense that <em>we can't be too confident in </em><em>any statement whatever.&nbsp; </em>The instinctive status hierarchy treats factual beliefs in pretty much the same way as policy proposals.&nbsp; Just as you need to be extremely high-status to go off and say on your own that the tribe should <em>do</em> something unusual, there's a similar dissonance from a low-status person saying on their own to believe something unusual, without careful compromises with other factions.&nbsp; It shows the one has no sense of their appropriate status in the hierarchy, and isn't sensitive to other factions' interests.</p>\n<p>The pure, uncompromising rejection merited by hypotheses like Jehovah or collapse postulates, socially appears as a refusal to make compromises with the majority, or a lack of sufficient humility when contradicting high-prestige people.&nbsp; (Also priests have higher social status to start with; it's understood that their place is to say and advocate these various things; and priests are better at <a href=\"/lw/gq/the_proper_use_of_humility/\">faking humility while going on doing whatever they were going to do anyway</a>.)&nbsp; The Copenhagen interpretation of QM - however ridiculous - is recognized as a conventional non-strange belief, so no one's going to call you insufficiently humble for advocating it.&nbsp; That would mark <em>them</em> as the contrarians.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2, "6Qic6PwwBycopJFNN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h723jFRLGosaxEzGC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 62, "baseScore": 66, "extendedScore": null, "score": 0.000108, "legacy": true, "legacyId": "2020", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 66, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WqGCaRhib42dhKWRL", "X2AD2LgtKgkRNPj2a", "GrDqnMjhqoxiqpQPw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-20T10:09:02.908Z", "modifiedAt": null, "url": null, "title": "Any sufficiently advanced wisdom is indistinguishable from bullshit", "slug": "any-sufficiently-advanced-wisdom-is-indistinguishable-from", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:29.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Shalmanese", "createdAt": "2009-03-22T02:29:32.044Z", "isAdmin": false, "displayName": "Shalmanese"}, "userId": "b9PjTzoHCPSS8fps7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HzvJw65XfppHBohtE/any-sufficiently-advanced-wisdom-is-indistinguishable-from", "pageUrlRelative": "/posts/HzvJw65XfppHBohtE/any-sufficiently-advanced-wisdom-is-indistinguishable-from", "linkUrl": "https://www.lesswrong.com/posts/HzvJw65XfppHBohtE/any-sufficiently-advanced-wisdom-is-indistinguishable-from", "postedAtFormatted": "Sunday, December 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20sufficiently%20advanced%20wisdom%20is%20indistinguishable%20from%20bullshit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20sufficiently%20advanced%20wisdom%20is%20indistinguishable%20from%20bullshit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHzvJw65XfppHBohtE%2Fany-sufficiently-advanced-wisdom-is-indistinguishable-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20sufficiently%20advanced%20wisdom%20is%20indistinguishable%20from%20bullshit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHzvJw65XfppHBohtE%2Fany-sufficiently-advanced-wisdom-is-indistinguishable-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHzvJw65XfppHBohtE%2Fany-sufficiently-advanced-wisdom-is-indistinguishable-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p>In the grand tradition of sequences, I'm going to jot this down real quick because it's required for the next argument I'm going to make.</p>\n<p>Shalmanese's 3rd law is \"<strong><a href=\"http://www.quotationspage.com/quote/776.html\">Any sufficiently advanced</a> wisdom is indistinguishable from bullshit</strong>\". Shalmanese's first law is \"As the length of any discussion involving the metric system approaches infinity, the likelihood approaches 1 of there being a reference to The Simpsons episode about 40 rods to the hogshead\" so judge it by the company it keeps.</p>\n<p>Imagine you got to travel back in time to meet yourself from 10 years ago and impart as much wisdom as possible on your past-self in 6 hours. You're bound by the Time Enforcement Committee not to reveal that you are the future-self of your past-self and it never occurs to your past-self that this ugly thing in front of them could ever be you. As far as the past-self is concerned, it's just a moderately interesting person they're having a conversation with.</p>\n<p>There would be 3 broad sets that your discussions would fall in: Beliefs that you both mutually agree on, Beliefs that you are able to convince your past-self through reason and Beliefs which make the past-self regard your future-self as being <em>actively stupid</em> for holding. It's this third category which I'm going to term Advanced Wisdom.<a id=\"more\"></a></p>\n<p>For everybody, those beliefs are going to be specific to the individual. Maybe you used to be devoutly religious and now you're staunchly atheist. Perhaps you were once radically marxist and now you're a staunch libertarian. For me, it was the wisdom of the advice to \"be yourself\". I have no doubt that I would get precisely nowhere convincing my past-self that \"be yourself\" is a piece of wisdom. Anything I could ever possibly say to him, he had already heard many times before and convinced himself was utter bullshit. If even my actual self couldn't convince myself of something, what hope is there that any rational argument could have penetrated?</p>\n<p>If I were to have my future-self visit my present-self now, I would have no doubt that he would also present me with some pieces of advanced wisdom I thought were bullshit. The problem is, <em>sufficiently</em> advanced wisdom is <em>indistinguishable</em> from bullshit. There is no possible test that can separate the two. You might be told something is advanced wisdom and keep the openest mind possible about it and investigate it in all the various ways and perhaps even be convinced by it and maybe it was actual wisdom you were convinced by. Then again, you could just have been convinced by bullshit. As a result, advanced wisdom, as a concept, is completely, frustratingly useless in an argument. If you're on the arguer's side, you know that the assertion of advanced wisdom is going to be taken as just more bullshit, if you're on the arguee's side, any assertion of advanced wisdom looks like the mistaken rambling of a deluded fool.</p>\n<p>The one positive thing this law has lead me to is a much higher tolerance for bullshit. I'm no longer so quick to dismiss ideas which, to me, seem obvious bullshit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HzvJw65XfppHBohtE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 7, "extendedScore": null, "score": 5.464219839022765e-07, "legacy": true, "legacyId": "2026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-20T15:10:15.714Z", "modifiedAt": null, "url": null, "title": "Fundamentally Flawed, or Fast and Frugal?", "slug": "fundamentally-flawed-or-fast-and-frugal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:27.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/psQYbMLWzS9sTsT2M/fundamentally-flawed-or-fast-and-frugal", "pageUrlRelative": "/posts/psQYbMLWzS9sTsT2M/fundamentally-flawed-or-fast-and-frugal", "linkUrl": "https://www.lesswrong.com/posts/psQYbMLWzS9sTsT2M/fundamentally-flawed-or-fast-and-frugal", "postedAtFormatted": "Sunday, December 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fundamentally%20Flawed%2C%20or%20Fast%20and%20Frugal%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFundamentally%20Flawed%2C%20or%20Fast%20and%20Frugal%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsQYbMLWzS9sTsT2M%2Ffundamentally-flawed-or-fast-and-frugal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fundamentally%20Flawed%2C%20or%20Fast%20and%20Frugal%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsQYbMLWzS9sTsT2M%2Ffundamentally-flawed-or-fast-and-frugal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpsQYbMLWzS9sTsT2M%2Ffundamentally-flawed-or-fast-and-frugal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1810, "htmlBody": "<p>Whenever biases are discussed around here, it tends to happen under the following framing: human cognition is a dirty, jury-rigged hack, only barely managing to approximate the laws of probability even in a rough manner. We have plenty of biases, many of them a result of adaptations that evolved to work well in the Pleistocene, but are hopelessly broken in a modern-day environment.</p>\n<p>That's one interpretation. But there's also a different interpretation: that a perfect Bayesian reasoner is computationally intractable, and our mental algorithms make for an excellent, possibly close to an optimal, use of the limited computational resources we happen to have available. It's not that the programming would be bad, it's simply that you can't do much better without upgrading the hardware. In the interest of fairness, I will be presenting this view by summarizing a classic 1996 Psychological Review article, \"<a href=\"http://www.dangoldstein.com/papers/FastFrugalPsychReview.pdf\">Reasoning the Fast and Frugal Way: Models of Bounded Rationality</a>\" by Gerd Gigerenzer and Daniel G. Goldstein. It begins by discussing two contrasting views: the Enlightenment ideal of the human mind as the perfect reasoner, versus the heuristics and biases program that considers human cognition as a set of quick-and-dirty heuristics.</p>\n<blockquote>\n<p>Many experiments have been conducted to test the validity of these two views, identifying a host of conditions under which the human mind appears more rational or irrational. But most of this work has dealt with simple situations, such as Bayesian inference with binary hypotheses, one single piece of binary data, and all the necessary information conveniently laid out for the participant (Gigerenzer &amp; Hoffrage, 1995). In many real-world situations, however, there are multiple pieces of information, which are not independent, but redundant. Here, Bayes&rsquo; theorem and other &ldquo;rational&rdquo; algorithms quickly become mathematically complex and computationally intractable, at least for ordinary human minds. These situations make neither of the two views look promising. If one would apply the classical view to such complex real-world environments, this would suggest that the mind is a supercalculator like a Laplacean Demon (Wimsatt, 1976)&mdash; carrying around the collected works of Kolmogoroff, Fisher, or Neyman&mdash;and simply neds a memory jog, like the slave in Plato&rsquo;s Meno. On the other hand, the heuristics-and-biases view of human irrationality would lead us to believe that humans are hopelessly lost in the face of real-world complexity, given their supposed inability to reason according to the canon of classical rationality, even in simple laboratory experiments.<br /><br />There is a third way to look at inference, focusing on the psychological and ecological rather than on logic and probability theory. This view questions classical rationality as a universal norm and thereby questions the very definition of &ldquo;good&rdquo; reasoning on which both the Enlightenment and the heuristics-and-biases views were built. Herbert Simon, possibly the best-known proponent of this third view, proposed looking for models of bounded rationality instead of classical rationality. Simon (1956, 1982) argued that information-processing systems typically need to satisfice rather than optimize. Satisficing, a blend of sufficing and satisfying, is a word of Scottish origin, which Simon uses to characterize algorithms that successfully deal with conditions of limited time, knowledge, or computational capacities. His concept of satisficing postulates, for instance, that an organism would choose the first object (a mate, perhaps) that satisfies its aspiration level&mdash;instead of the intractable sequence of taking the time to survey all possible alternatives, estimating probabilities and utilities for the possible outcomes associated with each alternative, calculating expected utilities, and choosing the alternative that scores highest.</p>\n</blockquote>\n<p><a id=\"more\"></a></p>\n<p>Let us consider the following example question: <em>Which city has a larger population? (a) Hamburg (b) Cologne.</em></p>\n<p>The paper describes algorithms fitting into a framework that the authors call a theory of <em>probabilistic mental model</em>s (PMM). PMMs fit three visions: (a) Inductive inference needs to be studied with respect to natural environments; (b) Inductive inference is carried out by satisficing algorithms; (c) Inductive inferences are based on frequencies of events in a reference class. PMM theory does not strive for the classical Bayesian ideal, but instead attempts to build an algorithm the mind could actually use.</p>\n<blockquote>\n<p>These satisficing algorithms dispense with the fiction of omniscient Laplacean Demon, who has all the time and knowledge to search for all relevant information, to compute the weights and covariances, and then to integrate all this information into an inference.</p>\n</blockquote>\n<p>The first algorithm presented is the <em>Take the Best</em> algorithm, named because its policy is \"take the best, ignore the rest\". In the first step, it invokes the <em>recognition principle</em>: if only one of two objects is recognized, it chooses the recognized object. If neither is recognized, it chooses randomly. If both are recognized, it moves on to the next discrimination step. For instance, if a person is asked which of city <em>a</em> and city <em>b</em> is bigger, and the person has never heard of <em>b</em>, they will pick <em>a</em>.</p>\n<p>If both objects are recognized, the algorithm will next search its memory for useful information that might provide a cue regarding the correct answer. Suppose that you know a certain city has its own football team, while another doesn't have one. It seems reasonable to assume that a city having a football team correlates with the city being of at least some minimum size, so the existence of a football team has positive cue value for predicting city size - it signals a higher value on the target variable.</p>\n<p>In the second step, the Take the Best algorithm retrieves from memory the cue values of the highest ranking cue. If the cue <em>discriminates</em>, which is to say one object has a positive cue value and the other does not, the search is terminated and the object with the positive cue value is chosen. If the cue does not discriminate, the algorithm keeps searching for better cues, choosing randomly if no discriminating cue is found.</p>\n<blockquote>\n<p>The algorithm is hardly a standard statistical tool for inductive inference: It does not use all available information, it is non-compensatory and nonlinear, and variants of it can violate transitivity. Thus, it differs from standard linear tools for inference such as multiple regression, as well as from nonlinear neural networks that are compensatory in nature. The Take The Best algorithm is noncompensatory because only the best discriminating cue determines the inference or decision; no combination of other cue values can override this decision. [...] the algorithm violates the Archimedian axiom, which implies that for any multidimensional object a (a<sub>1</sub>, a<sub>2</sub>, ... a<sub>n</sub>) preferred to b (b<sub>1</sub>, b<sub>2</sub>, ... b<sub>n</sub>) where a<sub>1</sub> dominates b<sub>1</sub>, this preference can be reversed by taking multiples of any one or a combination of b<sub>2</sub>, b<sub>3</sub>, ... , b<sub>n</sub>. As we discuss, variants of this algorithm also violate transitivity, one of the cornerstones of classical rationality (McClennen, 1990).</p>\n</blockquote>\n<p>This certainly sounds horrible: possibly even more horrifying is that a wide variety of experimental results make perfect sense if we assume that the test subjects are unconsciously employing this algorithm. Yet, despite all of these apparent flaws, the algorithm <em>works</em>.</p>\n<p>The authors designed a scenario where 500 simulated individuals with varying amounts of knowledge were presented with pairs of cities and were tasked with choosing the bigger one (83 cities, 3,403 city pairs). The Take the Best algorithm was pitted against five other algorithms that were suggested by \"several colleagues in the fields of statistics and economics\": Tallying (where the number of positive cue values for each object is tallied across all cues and the object with the largest number of positive cue values is chosen), Weighted Tallying, the Unit-Weight Linear Model, the Weighted Linear Model, and Multiple Regression.</p>\n<p>Take the Best was clearly the fastest algorithm, needing to look up far fewer cue values than the rest. But what about the accuracy? When the simulated individuals had knowledge of all the cues, Take the Best <em>drew as many correct inferences as any of the other algorithms, and more than some</em>. When looking at individuals with imperfect knowledge? Take the Best won or tied for the best position for individuals with knowledge of 20 and 50 percent of the cues, and didn't lose by more than a few tenths of a percent for individuals that knew 10 and 75 percent of the cues. Averaging over all the knowledge classes, Take the Best made 65.8% correct inferences, tied with Weighted Tallying for the gold medal.</p>\n<p>The authors also tried two, even more stupid algorithms, which were variants of Take the Best. Take the Last, instead of starting the search from the highest-ranking cue, first tries the cue that discriminated last, then the cue that discriminated the time before the last, and so on. The Minimalist algorithm picks a cue at random. This produced a perhaps surprisingly small drop in accuracy, with Take the Last getting 64,7% correct inferences and Minimalist 64,5%.</p>\n<p>After the algorithm comparison, the authors spend a few pages discussing some of the principles related to the PMM family of algorithms and their empirical validity, as well as the implications all of this might have on the study of rationality. They note, for instance, that even though transitivity (if we prefer a to b and b to c, then we should also prefer a to c) is considered a cornerstone axiom in classical relativity, several algorithms violate transitivity without suffering very much from it.</p>\n<blockquote>\n<p>At the beginning of this article, we pointed out the common opposition between the rational and the psychological, which emerged in the nineteenth century after the breakdown of the classical interpretation of probability (Gigerenzer et al., 1989). Since then, rational inference is commonly reduced to logic and probability theory, and psychological explanations are called on when things go wrong. This division of labor is, in a nutshell, the basis on which much of the current research on judgment under uncertainty is built. As one economist from the Massachusetts Institute of Technology put it, &ldquo;either reasoning is rational or it&rsquo;s psychological&rdquo; (Gigerenzer, 1994). Can not reasoning be both rational and psychological?<br /><br />We believe that after 40 years of toying with the notion of bounded rationality, it is time to overcome the opposition between the rational and the psychological and to reunite the two. The PMM family of cognitive algorithms provides precise models that attempt to do so. They differ from the Enlightenment&rsquo;s unified view of the rational and psychological, in that they focus on simple psychological mechanisms that operate under constraints of limited time and knowledge and are supported by empirical evidence. The single most important result in this article is that simple psychological mechanisms can yield about as many (or more) correct inferences in less time than standard statistical linear models that embody classical properties of rational inference. The demonstration that a fast and frugal satisficing algorithm won the competition defeats the widespread view that only &ldquo;rational&rdquo; algorithms can be accurate. Models of inference do not have to forsake accuracy for simplicity. The mind can have it both ways.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wfW6iL96u26mbatep": 1, "4R8JYu4QF2FqzJxE5": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "psQYbMLWzS9sTsT2M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 44, "baseScore": 49, "extendedScore": null, "score": 7.682336414328063e-05, "legacy": true, "legacyId": "2027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 86, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-20T18:11:17.677Z", "modifiedAt": null, "url": null, "title": "Sufficiently Advanced Sanity", "slug": "sufficiently-advanced-sanity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:57.920Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c7rzxS7QJDdpBkA69/sufficiently-advanced-sanity", "pageUrlRelative": "/posts/c7rzxS7QJDdpBkA69/sufficiently-advanced-sanity", "linkUrl": "https://www.lesswrong.com/posts/c7rzxS7QJDdpBkA69/sufficiently-advanced-sanity", "postedAtFormatted": "Sunday, December 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sufficiently%20Advanced%20Sanity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASufficiently%20Advanced%20Sanity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7rzxS7QJDdpBkA69%2Fsufficiently-advanced-sanity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sufficiently%20Advanced%20Sanity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7rzxS7QJDdpBkA69%2Fsufficiently-advanced-sanity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc7rzxS7QJDdpBkA69%2Fsufficiently-advanced-sanity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<p><strong>Reply to</strong>:&nbsp; <a href=\"/lw/1ka/any_sufficiently_advanced_wisdom_is/\">Shalmanese's Third Law</a></p>\n<p>From an unpublished story confronting <a href=\"/lw/u9/my_naturalistic_awakening/nhd\">Vinge's Law</a>, written in 2004, as abstracted a bit:</p>\n<blockquote>\n<p>\"If you met someone who was substantially saner than yourself, how would you know?\"</p>\n<p>\"The obvious mistake that sounds like deep wisdom is claiming that sanity looks like insanity to the insane.&nbsp;  I would expect to discover sanity that struck me as wonderfully and surprisingly sane,&nbsp; sanity that shocked me but that I could verify on deeper examination,&nbsp; sanity that sounded wrong but that I could not actually prove to be wrong,&nbsp; and sanity that seemed completely bizarre.\"</p>\n<p>\"Like a history of 20th-century science, presented to a scientist from 1900.&nbsp;  Much of the future history would sound insane, and easy to argue against.&nbsp;  It would take a careful mind to realize none of it was <em>more </em>inconsistent with present knowledge than the scientific history of the 19th century with the knowledge of 1800.&nbsp;  Someone who wished to dismiss the whole affair as crackpot would find a thousand excuses ready to hand, plenty of statements that sounded obviously wrong.&nbsp;  Yet no crackpot could possibly fake the parts that were obviously right.&nbsp;  That is what it is like to meet someone saner.&nbsp;  They are not infallible, are not future histories of anything.&nbsp; But no one could counterfeit the wonderfully and surprisingly sane parts; they would need to be that sane themselves.\"</p>\n</blockquote>\n<p>Spot the Bayesian problem, anyone?&nbsp; It's obvious to me today, but not to the me of 2004.&nbsp; Eliezer<sub>2004</sub> would have seen the structure of the Bayesian problem the moment I pointed it out to him, but he might not have assigned it the same <em>importance </em>I would without a lot of other background.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c7rzxS7QJDdpBkA69", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 5.465089528337467e-07, "legacy": true, "legacyId": "2028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HzvJw65XfppHBohtE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-20T20:55:58.088Z", "modifiedAt": null, "url": null, "title": "Mandating Information Disclosure vs. Banning Deceptive Contract Terms", "slug": "mandating-information-disclosure-vs-banning-deceptive", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:24.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WyAGQqw2v3yS8BkjM/mandating-information-disclosure-vs-banning-deceptive", "pageUrlRelative": "/posts/WyAGQqw2v3yS8BkjM/mandating-information-disclosure-vs-banning-deceptive", "linkUrl": "https://www.lesswrong.com/posts/WyAGQqw2v3yS8BkjM/mandating-information-disclosure-vs-banning-deceptive", "postedAtFormatted": "Sunday, December 20th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mandating%20Information%20Disclosure%20vs.%20Banning%20Deceptive%20Contract%20Terms&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMandating%20Information%20Disclosure%20vs.%20Banning%20Deceptive%20Contract%20Terms%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyAGQqw2v3yS8BkjM%2Fmandating-information-disclosure-vs-banning-deceptive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mandating%20Information%20Disclosure%20vs.%20Banning%20Deceptive%20Contract%20Terms%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyAGQqw2v3yS8BkjM%2Fmandating-information-disclosure-vs-banning-deceptive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyAGQqw2v3yS8BkjM%2Fmandating-information-disclosure-vs-banning-deceptive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1243, "htmlBody": "<p>Economists are very into the idea of mutually beneficial exchange. The standard argument is that if two parties voluntarily agree to a deal, then they must be better off with the deal than without it, otherwise they wouldn't have agreed. And if the terms of that deal don't harm any third parties,* then the deal must be welfare-improving, and any regulatory restrictions on making it must be bad.<br /><br />One objection to this argument is that it's not always clear what is and what is not \"voluntary.\" I once has a well-published economist friend argue that there are no gradations of voluntariness: either a deal was made under some kind of compulsion or it wasn't. I asked him if he would be OK letting his then pre-adolescent son make any schoolyard deal he wanted as long as it was not made under any overt threat, and I think (but am not totally sure) that he has since backed off this position. So there is an argument for purely paternalistic restrictions on freedom of contract.&nbsp; <br /><br />Another objection, one which economists tend to take more seriously, relates to information. Specifically, there is the idea that maybe one party to the contract is not fully informed about its terms. For this reason, many economists are willing to entertain policies by which firms are required to disclose certain information, and to do so in a way that is comprehensible to consumers. So for example we now have \"<a href=\"http://en.wikipedia.org/wiki/Schumer_box\" target=\"_blank\">Schumer boxes</a>\" that govern the ways in which credit card companies present certain information in promotional materials. This seems to many people to be a reasonable remedy: if the problem was that one side of the transaction was ignorant, then a regulation that eliminates that ignorance, while at the same time not interfering with their freedom to engage in mutually beneficial exchange, must be a good thing.</p>\n<p><a id=\"more\"></a>I think this reasonable-sounding position is largely wrong. The standard asymmetric information stories with rational agents are stories in which the uniformed party <em>knows</em> that it is uninformed, which influences the contract terms that it is willing to accept, which in turn either causes beneficial exchange not to happen (the \"lemons\" problem) or causes contract terms to be distorted away from the efficient ones. They are generally <em>not</em> stories about uninformed consumers not understanding that they are uninformed and blithely marching into traps as a result. But this is what we actually see all the time, one party tricks the other party into unfavorable terms. Indeed, very often this is the real-world problem to which providing better information is supposed to be the solution! But for trickery to be the problem, you usually need a model in which some agents suffer from some limitation on their rationality, such as myopia.** And if you have that, then you have a different problem from the problem of asymmetric information, and there is no particular reason for a different problem to have the same solution. If the problem is that people are getting tricked, then providing more information is only going to help if it is going to cause them not to be tricked, and it is not at all obvious whether and when this will be the case.<br /><br />But there is a bigger problem with the standard way that economists usually think about these problems, which is that they completely ignore the fact that when people are being tricked, <em>the virtues of voluntary exchange are absent and so there is no reason for a strong presumption against interfering with it in the first place</em>. And sometimes the very existence of certain contract terms is an indication that the contract is a trick. Think about the controversial terms often found in credit card contracts, such as provisions by which being one day late with a payment or being one dollar over your credit limit jumps your interest rate to 29.99% forever, or in which cards with multiple balances at different interest rates pay off the low rate balance first. What should be inferred from the fact that these terms exist? Is it at all plausible that there is some subtle but very important reason why these terms must be present, and that if they were banned lots of mutually beneficial deals would not be made? Is it not much more likely that that these terms exist <em>precisely because</em> many consumers don't understand them and will be tricked by them? Have you ever heard of such terms being in contracts negotiated between sophisticated parties? Shouldn't this cause you to be much less worried about the consequences of simply banning them?<br /><br />There is a very good paper by <a href=\"http://www.economics.harvard.edu/faculty/laibson/publishedwork\" target=\"_blank\">Gabaix &amp; Laibson</a> (2006) that provides a formal model in which firms \"shroud\" relevant information in order to trick myopic agents. The neat thing about their paper is that they show that this persists in equilibrium: competing firms turn out to have no incentive to march in and expose the shrouding and offer transparent pricing instead. But you don't need a fancy (and recent) paper to have known that the aforementioned terms in credit card contracts are only there to trick people. And if that's true, then what you really want is to get rid of contracts with those terms. Mandating information disclosure is <em>only</em> a good remedy insofar as it causes those terms to disappear. Gone is the economist's notion that the right solution is to make sure everyone knows the score and then to step out of the way. If you mandated disclosure and then saw those contracts continuing to exist, the conclusion you should draw was that the disclosure was ineffective, <em>not</em> that the terms were efficient.<br /><br />One could object to heavy-handed regulation on the basis of a slippery-slope argument. While there are some clear-cut cases like the credit card contracts, a government with lots of regulatory power, even a well-intentioned one, may end up getting overzealous and interfering in ways that will have unintended and negative consequences for efficiency. And Gabaix &amp; Laibson take pains to point out that they are not advocating lots of regulation. Whatever the merits of this argument (I understand the fear of regulatory overreach but worry about it less than a lot of other people do), the main point of this post remains. There are important instances in the world we live in where the unaware simply get tricked and screwed. That is not, at root, a problem of asymmetric information among rational agents, and there is no reason to think that the appropriate remedy is the same as if it were. More importantly, in this world the virtues of voluntary exchange are absent, and so the economists' deference to it is misplaced. <br /><br />There has been an important real-world development on this front. It seems that the <a href=\"http://www.federalreserve.gov/newsevents/press/bcreg/20081218a.htm\" target=\"_blank\">Federal Reserve</a> did a bunch of consumer testing to see how well people understood various terms in credit card contracts under different disclosure requirements, and concluded that they were simply too complicated for most people to understand. They therefore decided to go ahead and simply prohibit certain practices. Which I say is good news for the good guys.</p>\n<p>*A weaker version of this condition is that any third parties that are hurt are hurt less than the contracting parties are helped.</p>\n<p>**I say \"usually\" because there are a few special models in which fully rational agents can nevertheless be tricked.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PDJ6KqJBRzvKPfuS3": 1, "hMXoyTAKxvCcsQBKf": 1, "wfW6iL96u26mbatep": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WyAGQqw2v3yS8BkjM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 30, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "2029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-21T03:54:05.533Z", "modifiedAt": null, "url": null, "title": "If reason told you to jump off a cliff, would you do it?", "slug": "if-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:55.224Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Shalmanese", "createdAt": "2009-03-22T02:29:32.044Z", "isAdmin": false, "displayName": "Shalmanese"}, "userId": "b9PjTzoHCPSS8fps7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HAqWP4bh8tTwYSfm2/if-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "pageUrlRelative": "/posts/HAqWP4bh8tTwYSfm2/if-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "linkUrl": "https://www.lesswrong.com/posts/HAqWP4bh8tTwYSfm2/if-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "postedAtFormatted": "Monday, December 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20reason%20told%20you%20to%20jump%20off%20a%20cliff%2C%20would%20you%20do%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20reason%20told%20you%20to%20jump%20off%20a%20cliff%2C%20would%20you%20do%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAqWP4bh8tTwYSfm2%2Fif-reason-told-you-to-jump-off-a-cliff-would-you-do-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20reason%20told%20you%20to%20jump%20off%20a%20cliff%2C%20would%20you%20do%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAqWP4bh8tTwYSfm2%2Fif-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAqWP4bh8tTwYSfm2%2Fif-reason-told-you-to-jump-off-a-cliff-would-you-do-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 540, "htmlBody": "<p>In reply to Eliezer's <a href=\"/lw/1k4/the_contrarian_status_catch22\">Contrarian Status Catch 22</a> &amp; <a href=\"/lw/1kc/sufficiently_advanced_sanity/\">Sufficiently Advanced Sanity</a>. I accuse Eliezer of encountering a piece of <a href=\"/lw/1ka/any_sufficiently_advanced_wisdom_is/\">Advanced Wisdom</a>.</p>\n<p>Unreason is something that we should fight against. Witch burnings, creationism &amp; homeopathy are all things which should rightly be defended against for society to advance. But, more subtly, I think <em>reason</em> is in some ways, is also a dangerous phenomena that should be guarded against. I am arguing not against the specific process of reasoning itself, it is the attitude which <em>instinctually </em>reaches for reason as the first tool of choice when confronting a problem. Scott Aaronson called this approach <a href=\"http://scottaaronson.com/blog/?p=326\">bullet swallowing</a> when he tried to explain why he was so uncomfortable with it. Jane Galt also rails against reason when explaining <a href=\"http://www.janegalt.net/blog/archives/005244.html\">why she does not support gay marriage</a>.<a id=\"more\"></a></p>\n<p>The most recent financial crisis is a another example of what happens when reason is allowed to dominate. Cutting away all the foggy noise &amp; conspiracy theories, the root cause of the financial crisis was that men who believed a little too much in reason became divergent from reality and reality quickly reminded them of that. The problem was not that the models were wrong, it was that what they were trying to accomplish was unmodelable. Nassim Nicholas-Taleb's <a href=\"http://www.amazon.com/Black-Swan-Impact-Highly-Improbable/dp/1400063515\">The Black Swan</a> explains this far better than I can.</p>\n<p>A clear-eyed look back on history reveals many other similar events in which the helm of reason has been used to champion disastrous causes: The French Revolution, Communism, Free Love Communes, Social Darwinism &amp; Libertarianism (I am not talking about <a href=\"/lw/lm/affective_death_spirals/\">affective death spirals</a> here). Now, one might argue at this point that those were all specific examples of bad reasoning and that it's possible to carefully strip away the downsides of reason with diligent practice. But I don't believe it. Fundamentally, I believe that the world is unreasonable. It is, at it's core, not amenable to reason. Better technique may push the failure point back just a bit further but it will never get rid of it.</p>\n<p>So what should replace reason? I nominate accepting the primacy of evidence over reason. Reason is still used, but only reluctantly, to form the minimum span possible between evidence &amp; beliefs. From my CS perspective, I make the analogy between shifting from algorithm centered computation to data centric. Rather than try to create elaborate models that purport to reflect reality, strive to be as model-free as possible and shut up and gather.</p>\n<p>If the reason based approach is a towering skyscraper, an evidence based approach would be an adobo hut. Reasoning is sexy and new but also powerful, it can do a lot of things and do them a lot better. The evidence based approach, on the other hand, does just enough to matter and very little more. The evidence based approach is not truth seeking in the way the reason based approach is. A declaration of truth, built on a large pile of premises is worse than a statement of ignorance. This, I think is what Scott Aaaronson was referring to when he said \"What you've forced me to realize, Eliezer, and I thank you for this:&nbsp; What I'm uncomfortable with is not the many-worlds interpretation itself, it's the air of satisfaction that often comes with it.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HAqWP4bh8tTwYSfm2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": -14, "extendedScore": null, "score": -2.9e-05, "legacy": true, "legacyId": "2030", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h723jFRLGosaxEzGC", "c7rzxS7QJDdpBkA69", "HzvJw65XfppHBohtE", "XrzQW69HpidzvBxGr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-21T22:01:56.781Z", "modifiedAt": null, "url": null, "title": "The Correct Contrarian Cluster", "slug": "the-correct-contrarian-cluster", "viewCount": null, "lastCommentedAt": "2020-06-11T22:31:39.524Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9KvefburLia7ptEE3/the-correct-contrarian-cluster", "pageUrlRelative": "/posts/9KvefburLia7ptEE3/the-correct-contrarian-cluster", "linkUrl": "https://www.lesswrong.com/posts/9KvefburLia7ptEE3/the-correct-contrarian-cluster", "postedAtFormatted": "Monday, December 21st 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Correct%20Contrarian%20Cluster&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Correct%20Contrarian%20Cluster%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KvefburLia7ptEE3%2Fthe-correct-contrarian-cluster%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Correct%20Contrarian%20Cluster%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KvefburLia7ptEE3%2Fthe-correct-contrarian-cluster", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9KvefburLia7ptEE3%2Fthe-correct-contrarian-cluster", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1509, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/1k4/the_contrarian_status_catch22/\">Contrarian Status Catch-22</a></p>\n<p>Suppose you know someone believes that the World Trade Center was rigged with explosives on 9/11.&nbsp; What else can you infer about them?&nbsp; Are they more or less likely than average to believe in homeopathy?</p>\n<p>I couldn't cite an experiment to verify it, but it <em>seems likely</em> that:</p>\n<ul>\n<li>There are persistent character traits which contribute to someone being willing to state a contrarian point of view.&nbsp; </li>\n<li>All else being equal, if you know that someone advocates one contrarian view, you can infer that they are more likely than average to have other contrarian views.</li>\n</ul>\n<p>All sorts of <a href=\"http://www.overcomingbias.com/2008/06/against-disclai.html\">obvious disclaimers</a> can be included here.&nbsp; Someone who expresses an extreme-left contrarian view is less likely to have an extreme-right contrarian view.&nbsp; Different character traits may contribute to expressing contrarian views that are <em>counterintuitive</em> vs. <em>low-prestige</em> vs. <em>anti-establishment </em>etcetera.&nbsp; Nonetheless, it seems likely that you could usefully distinguish a <em>c-</em>factor, a general contrarian factor, in people and beliefs, even though it would break down further on closer examination; there would be a cluster of contrarian people and a cluster of contrarian beliefs, whatever the clusters of the subcluster.</p>\n<p>(If you perform a statistical analysis of contrarian ideas and you find that they form distinct subclusters of ideologies that don't correlate with each other, then I'm wrong and no <em>c-</em>factor exists.)</p>\n<p>Now, suppose that someone advocates the <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">many-worlds interpretation of quantum mechanics</a>.&nbsp; What else can you infer about them?<a id=\"more\"></a></p>\n<p>Well, <em>one</em> possible reason for believing in the many-worlds interpretation is that, as a general rule of cognitive conduct, you investigated the issue and thought about it carefully; and you learned enough quantum mechanics and probability theory to understand why the <a href=\"/lw/19m/privileging_the_hypothesis/#154t\">no-worldeaters</a> advocates call their theory the strictly simpler one; and you're reflective enough to understand how a deeper theory can undermine your brain's intuition of an apparently single world; and you listen to the physicists who mock many-worlds and correctly assess that these physicists are not to be trusted.&nbsp; Then you believe in many-worlds out of general causes that would operate in other cases - you probably have a high <em>correct contrarian factor</em> - and we can infer that you're more likely to be an atheist.</p>\n<p>It's also possible that you thought many-worlds means \"all the worlds I can imagine exist\" and that you decided it'd be cool if there existed <a href=\"http://leasticoulddo.com/comic/20090929\">a world where Jesus is Batman</a>, therefore many-worlds is true no matter what the average physicist says.&nbsp; In this case you're just believing for general contrarian reasons, and you're probably more likely to believe in homeopathy as well.</p>\n<p>A lot of what we do around here can be thought of as distinguishing the <em>correct contrarian cluster</em> within the <em>contrarian cluster.</em>&nbsp; In fact, when you judge someone's rationality by opinions they post on the Internet - rather than observing their day-to-day decisions or life outcomes - what you're trying to judge is almost entirely <em>cc-</em>factor.</p>\n<p>It seems indubitable that, measured in raw bytes, most of the world's correct knowledge is not contrarian correct knowledge, and most of the things that the majority believes (e.g. 2 + 2 = 4) are correct.&nbsp; You might therefore wonder whether it's really important to try to distinguish the Correct Contrarian Cluster in the first place - why not just stick to majoritarianism?&nbsp; The Correct Contrarian Cluster is just the place where the borders of knowledge are currently expanding - not just that, but merely the sections on the border where battles are taking place.&nbsp; Why not just be content with <a href=\"/lw/ow/the_beauty_of_settled_science/\">the beauty of settled science</a>?&nbsp; Perhaps we're just trying to signal to our <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">fellow nonconformists</a>, rather than really being concerned with truth, says the little copy of Robin Hanson in my head.</p>\n<p>My primary personality, however, responds as follows:</p>\n<ul>\n<li>Religion</li>\n<li>Cryonics</li>\n<li>Diet</li>\n</ul>\n<p>In other words, even though you would in theory expect the Correct Contrarian Cluster to be a small fringe of the expansion of knowledge, of concern only to the leading scientists in the field, the actual fact of the matter is that the world is *#$%ing nuts and so there's <em>really important</em> stuff in the Correct Contrarian Cluster.&nbsp; <a href=\"http://www.nytimes.com/2002/07/07/magazine/what-if-it-s-all-been-a-big-fat-lie.html\">Dietary scientists ignoring their own experimental evidence</a> have killed millions and condemned hundreds of millions more to obesity with high-fructose corn syrup.&nbsp; Not to mention that most people still believe in <a href=\"/tag/antitheism/\">God</a>.&nbsp; People are crazy, the world is mad.&nbsp; So, yes, if you don't want to bloat up like a balloon and <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">die</a>, distinguishing the Correct Contrarian Cluster is important.</p>\n<p>Robin previously <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html\">posted</a> (and I <a href=\"http://www.overcomingbias.com/2009/11/contrarian-excuses.html#comment-435967\">commented</a>) on the notion of trying to distinguish correct contrarians by \"outside indicators\" - as I would put it, trying to distinguish correct contrarians, not by analyzing the details of their arguments, but by zooming way out and seeing what sort of general excuse they give for disagreeing with the establishment.&nbsp; As I said in the comments, I am generally pessimistic about the chances of success for this project.&nbsp; Though, as I also commented, there are some general structures that make me sit up and take note; probably the strongest is \"These people have ignored their own carefully gathered experimental evidence for decades in favor of stuff that sounds more intuitive.\"&nbsp; (Robyn Dawes/psychoanalysis, Robin Hanson/medical spending, Gary Taubes/dietary science, Eric Falkenstein/risk-return - note that I don't say anything like this about AI, so this is not a plea I have use for myself!)&nbsp; Mostly, I tend to rely on analyzing the actual arguments; meta should be spice, not meat.</p>\n<p>However, failing analysis of actual arguments, another method would be to try and distinguish the Correct Contrarian Cluster by plain old-fashioned... clustering.&nbsp; In a sense, we do this in an ad-hoc way any time we trust someone who seems like a smart contrarian.&nbsp; But it would be possible to do it more formally - write down a big list of contrarian views (some of which we are genuinely uncertain about), poll ten thousand members of the intelligentsia, and look at the clusters.&nbsp; And within the Contrarian Cluster, we find a subcluster where...</p>\n<p>...well, how <em>do</em> we look for the Correct Contrarian subcluster?</p>\n<p>One obvious way is to start with some things that are slam-dunks, and use them as anchors.&nbsp; Very few things qualify as slam-dunks.&nbsp; <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">Cryonics</a> doesn't rise to that level, since it involves social guesses and values, not just physicalism.&nbsp; I can think of only three slam-dunks off the top of my head:</p>\n<ul>\n<li>Atheism:&nbsp; Yes.</li>\n<li><a href=\"/lw/r8/and_the_winner_is_manyworlds/\">Many-worlds:&nbsp; Yes.</a></li>\n<li><a href=\"/lw/p7/zombies_zombies/\">\"P-zombies\":&nbsp; No.</a></li>\n</ul>\n<p>These aren't necessarily <em>simple</em> or <em>easy</em> for contrarians to work through, but the <em>correctness</em> seems as reliable as it gets.</p>\n<p>Of course there are also slam-dunks like:</p>\n<ul>\n<li>Natural selection:&nbsp; Yes.</li>\n<li>World Trade Center rigged with explosives:&nbsp; No.</li>\n</ul>\n<p>But these probably aren't the right kind of controversy to fine-tune the location of the Correct Contrarian Cluster.</p>\n<p>A major problem with the three slam-dunks I listed is that they all seem to have more in common with each other than any of them have with, say, dietary science.&nbsp; This is probably because of the logical, formal character which makes them slam dunks in the first place.&nbsp; By expanding the field somewhat, it would be possible to include <em>slightly </em>less slammed dunks, like:</p>\n<ul>\n<li>Rorschach ink blots:&nbsp; No.</li>\n</ul>\n<p>But if we start expanding the list of anchors like this, we run into a much higher probability that one of our anchors is wrong.</p>\n<p>So we conduct this massive poll, and we find out that if someone is an atheist <em>and</em> believes in many-worlds <em>and</em> does not believe in p-zombies, they are <em>much more likely</em> than the <em>average contrarian</em> to think that low-energy nuclear reactions (the modern name for cold fusion research) are real.&nbsp; (That is, among \"average contrarians\" who have opinions on both p-zombies and LENR in the first place!)&nbsp; If I saw this result I would indeed sit up and say, \"Maybe I should look into that LENR stuff more deeply.\"&nbsp; I've never heard of any surveys like this actually being done, but it sounds like quite an interesting dataset to have, if it could be obtained.</p>\n<p>There are much more clever things you could do with the dataset.&nbsp; If someone believes most things that atheistic many-worlder zombie-skeptics believe, but isn't a many-worlder, you probably want to know their opinion on infrequently considered topics.&nbsp; (The first thing I'd probably try would be SVD to see if it isolates a \"correctness factor\", since it's simple and worked famously well on the Netflix dataset.)</p>\n<p>But there are also simpler things we could do using the same principle.&nbsp; Let's say we want to know whether the economy will recover, double-dip or crash.&nbsp; So we call up a thousand economists, ask each one \"Do you have a strong opinion on whether the many-worlds interpretation is correct?\", and see if the economists who have a strong opinion and answer \"Yes\" have a different average opinion from the average economist and from economists who say \"No\".</p>\n<p>We might not have this data in hand, but it's the algorithm you're <em>approximating </em>when you notice that a lot of smart-seeming people assign much higher than average probabilities to cryonics technology working.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"6Qic6PwwBycopJFNN": 2, "5f5c37ee1b5cdee568cfb19d": 4, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9KvefburLia7ptEE3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 66, "baseScore": 59, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "2033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 59, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 255, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["h723jFRLGosaxEzGC", "9cgBF6BQ2TRB3Hy4E", "ndGYn7ZFiZyernp9f", "7FzD7pNm9X68Gp5ZC", "fdEWWr8St59bXLbQr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-22T00:17:14.517Z", "modifiedAt": null, "url": null, "title": "Karma Changes", "slug": "karma-changes", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:36.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EyhohecvyjkzgT7XG/karma-changes", "pageUrlRelative": "/posts/EyhohecvyjkzgT7XG/karma-changes", "linkUrl": "https://www.lesswrong.com/posts/EyhohecvyjkzgT7XG/karma-changes", "postedAtFormatted": "Tuesday, December 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Karma%20Changes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKarma%20Changes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEyhohecvyjkzgT7XG%2Fkarma-changes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Karma%20Changes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEyhohecvyjkzgT7XG%2Fkarma-changes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEyhohecvyjkzgT7XG%2Fkarma-changes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>As recently (re-)suggested by <a href=\"/lw/1jx/december_2009_meta_thread/1cil\">Kaj Sotala</a>, posts now have much larger effects on karma than comments:&nbsp; Each up or down vote on a post is worth 10 karma.</p>\n<p>Negative votes on posts have had karma effects all along, but for some reason Reddit's code imposed a <em>display</em> cap (not an <em>actual</em> cap) of 0.&nbsp; This violates a basic user interface principle: things with important effects should have visible effects.&nbsp; Since this just got 10x more important, we now show negative post totals rather than \"0\".&nbsp; This also provides some feedback to posters that was previously missing.&nbsp; Note that downvoting a post costs 10 karma from your downvote cap of 4x current karma.</p>\n<p>The minimum karma to start posting has been raised to 50.</p>\n<p>Thanks to our friends at Tricycle for implementing this request!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EyhohecvyjkzgT7XG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 4, "extendedScore": null, "score": 5.468348575166159e-07, "legacy": true, "legacyId": "2034", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-22T17:57:21.810Z", "modifiedAt": null, "url": null, "title": "lessmeta", "slug": "lessmeta", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:55.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PlaidX", "createdAt": "2009-06-28T10:50:31.260Z", "isAdmin": false, "displayName": "PlaidX"}, "userId": "Cgh6rBbvZe4Noxt5d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hk3MHFdofi4P5zBwj/lessmeta", "pageUrlRelative": "/posts/Hk3MHFdofi4P5zBwj/lessmeta", "linkUrl": "https://www.lesswrong.com/posts/Hk3MHFdofi4P5zBwj/lessmeta", "postedAtFormatted": "Tuesday, December 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20lessmeta&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Alessmeta%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHk3MHFdofi4P5zBwj%2Flessmeta%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=lessmeta%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHk3MHFdofi4P5zBwj%2Flessmeta", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHk3MHFdofi4P5zBwj%2Flessmeta", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 257, "htmlBody": "<p>The social bookmarking site metafilter has a sister site called metatalk, which works the same way but is devoted entirely to talking about metafilter itself. Arguments about arguments, discussions about discussions, proposals for changes in site architecture, etc.</p>\n<p>Arguments about arguments are often less productive than the arguments they are about, but they CAN be quite productive, and there's certainly a place for them. The only thing wrong with them is when they obstruct the discussion that spawned them, and so the idea of splitting off metatalk into its own site is really quite a clever one.</p>\n<p>Lesswrong's problem is a peculiar one. It is ENTIRELY devoted to meta-arguments, to the extent that people have to shoehorn anything else they want to talk about into a cleverly (or not so cleverly) disguised example of some more meta topic. It's a kite without a string.</p>\n<p>Imagine if you had been around the internet, trying to have a rational discussion about topic X, but unable to find an intelligent venue, and then stumbling upon lesswrong. \"Aha!\" you say. \"Finally a community making a concerted effort to be rational!\"</p>\n<p>But to your dismay, you find that the ONLY thing they talk about is being rational, and a few other subjects that have been apparently grandfathered in. It's not that they have no interest in topic X, there's just no place on the site they're allowed to talk about it.</p>\n<p>What I propose is a \"non-meta\" sister site, where people can talk and think about anything BESIDES talking and thinking. Well, you know what I mean.</p>\n<p>Yes?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hk3MHFdofi4P5zBwj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 6, "extendedScore": null, "score": 5.470263292811261e-07, "legacy": true, "legacyId": "2036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-22T18:59:05.811Z", "modifiedAt": null, "url": null, "title": "The 9/11 Meta-Truther Conspiracy Theory", "slug": "the-9-11-meta-truther-conspiracy-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:47.325Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hkBp6a5RCDNedo6Wy/the-9-11-meta-truther-conspiracy-theory", "pageUrlRelative": "/posts/hkBp6a5RCDNedo6Wy/the-9-11-meta-truther-conspiracy-theory", "linkUrl": "https://www.lesswrong.com/posts/hkBp6a5RCDNedo6Wy/the-9-11-meta-truther-conspiracy-theory", "postedAtFormatted": "Tuesday, December 22nd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%209%2F11%20Meta-Truther%20Conspiracy%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%209%2F11%20Meta-Truther%20Conspiracy%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkBp6a5RCDNedo6Wy%2Fthe-9-11-meta-truther-conspiracy-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%209%2F11%20Meta-Truther%20Conspiracy%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkBp6a5RCDNedo6Wy%2Fthe-9-11-meta-truther-conspiracy-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhkBp6a5RCDNedo6Wy%2Fthe-9-11-meta-truther-conspiracy-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1127, "htmlBody": "<p><em>Date:&nbsp; September 11th, 2001.<br />Personnel:&nbsp; Unknown [designate A], Unknown [designate B], Unknown [designate C].</em></p>\n<p>A:&nbsp; It's done.&nbsp; The plane targeted at Congress was crashed by those on-board, but the Pentagon and Trade Center attacks occurred just as scheduled.</p>\n<p>B:&nbsp; Congress seems sufficiently angry in any case.&nbsp; I don't think the further steps of the plan will meet with any opposition.&nbsp; We should gain the governmental powers we need, and the stock market should move as expected.</p>\n<p>A:&nbsp; Good.&nbsp; Have you prepared the conspiracy theorists to accuse us?</p>\n<p>B:&nbsp; Yes.&nbsp; All is in readiness.&nbsp; The first accusations will fly within the hour.</p>\n<p>C:&nbsp; Er...</p>\n<p>A:&nbsp; What is it?</p>\n<p>C:&nbsp; Sorry, I know I'm a bit new to this sort of thing, but why are we sponsoring conspiracy theorists?&nbsp; Aren't they our arch-nemeses, tenaciously hunting down and exposing our lies?</p>\n<p>A:&nbsp; No, my young apprentice, just the opposite.&nbsp; As soon as you pull off a conspiracy, the <em>first </em>thing you do is start a conspiracy theory about it.&nbsp; Day one.<a id=\"more\"></a></p>\n<p>C:&nbsp; What?&nbsp; We <em>want</em> to be accused of deliberately ignoring intelligence and assassinating that one agent who tried to forward specific information -</p>\n<p>A:&nbsp; No, of course not!&nbsp; What you do in a case like this is start an accusation so <em>ridiculous</em> that nobody else wants to be associated with the accusers.&nbsp; You create a <em>low-prestige</em> conspiracy theory and staff it with as many vocal lunatics as you can.&nbsp; That way no one wants to be seen as affiliating with the conspiracy theorists by making a <em>similar</em> accusation.</p>\n<p>C:&nbsp; That <em>works?</em>&nbsp; I know I'm not the brightest fish in the barrel - sometimes, hanging around you guys, I feel almost as dumb as I pretend to be - but even <em>I</em> know that \"The world's stupidest man may say the sun is shining, but that doesn't make it dark out.\"</p>\n<p>B:&nbsp; Works like a charm, in my experience.&nbsp; Like that business with the Section Magenta aircraft.&nbsp; All you need is a bunch of lunatics screaming about aliens and no one respectable will dream of reporting a \"flying saucer\" after that.</p>\n<p>C:&nbsp; So what <em>did</em> you plan for the 9/11 cover conspiracy theory, by the way?&nbsp; Are the conspiracy theorists going to say the Jews were behind it?&nbsp; Can't get much lower-prestige than anti-Semitism!</p>\n<p>B:&nbsp; You've got the right general idea, but you're not thinking creatively enough.&nbsp; Israel <em>does</em> have a clear motive here - even though they weren't in fact behind it - and if the conspiracy theorists cast a wide enough net, they're bound to turn up a handful of facts that seem to support their theory.&nbsp; The public doesn't understand how to discount that sort of \"evidence\", though, so they might actually be convinced.</p>\n<p>C:&nbsp; So... the Illuminati planned the whole operation?</p>\n<p>B:&nbsp; You know, for someone who reads as much science fiction as you do, you sure don't think outside the box.</p>\n<p>C:&nbsp; ...okay, seriously, man.&nbsp; I don't see how a theory could get any more ridiculous than that and still acquire followers.</p>\n<p><em>(A and B crack up laughing.)</em></p>\n<p>B:&nbsp; <em>Hah!</em>&nbsp; What would <em>you</em> have done to cover up the Section Magenta aircraft, I wonder?&nbsp; Blamed it on Russia?&nbsp; To this day there are still people on the lookout for <em>hidden aliens</em> who overfly populated areas in <em>gigantic non-nanotechnological aircraft</em> with their<em> lights on</em>.</p>\n<p>A:&nbsp; So what <em>did </em>we pick for the 9/11 cover conspiracy, by the way?</p>\n<p>B:&nbsp; Hm?&nbsp; Oh, the World Trade Center wasn't brought down by planes crashing into it.&nbsp; It was pre-planted explosives.</p>\n<p>C:&nbsp; You're kidding me.</p>\n<p>B:&nbsp; Seriously, that's the cover conspiracy.</p>\n<p>C:&nbsp; There are videos <em>already on the Internet</em> of the planes flying into the World Trade Center.&nbsp; It was on <em>live television.</em>&nbsp; There are thousands of witnesses on the ground who <em>saw it with their own eyes</em> -</p>\n<p>B:&nbsp; Right, but the conspiracy theory is, the planes wouldn't have done it on their own - it took pre-planted explosives <em>too</em>.</p>\n<p>C:&nbsp; <em>No one</em> is going to buy that.&nbsp; I don't care <em>who</em> you bought out in the conspiracy-theoretic community.&nbsp; This attack would've had the same political effect whether the buildings came down entirely or just the top floors burned.&nbsp; It's not like <em>we</em> spent a lot of time worrying about at what angle the planes would hit the building.&nbsp; The whole point was to keep our hands clean!&nbsp; That's why the al Qaeda plot was such a godsend compared to the original anthrax plan.&nbsp; All we had to do was let it happen.&nbsp; Once we arranged for the attack to go through, we were <em>done,</em> we had <em>no conceivable motive</em> to risk exposure by planting explosives on top of that -</p>\n<p>B:&nbsp; Don't take this the wrong way.&nbsp; But one, you don't understand conspiracy theorists at all.&nbsp; Two, they bought the aliens, didn't they?&nbsp; And three, it's already online and the usual crowd of anti-establishment types are already snapping it up.</p>\n<p>C:&nbsp; Are you joking?</p>\n<p>B:&nbsp; Honest to Betsy.&nbsp; People are claiming that the buildings fell too quickly and that the video showed ejecta corresponding to controlled demolitions.</p>\n<p>C:&nbsp; Wow.&nbsp; I don't suppose we <em>actually</em> planted some explosives, just to make sure that -</p>\n<p>A:&nbsp; Oh, hell no, son.&nbsp; That sort of thing is never necessary.&nbsp; They'll turn up what looks to them like evidence.&nbsp; They always do.</p>\n<p>C:&nbsp; Aren't they going to, um, suspect they're pawns?</p>\n<p>A:&nbsp; Human nature 101.&nbsp; Once they've staked their identity on being part of the defiant elect who know the Hidden Truth, there's no way it'll occur to them that they're our catspaws.</p>\n<p>B:&nbsp; One reason our fine fraternity has controlled the world for hundreds of years is that we've managed to make \"conspiracy theories\" look stupid.&nbsp; You know how often you've ever heard someone suggest that possibility?&nbsp; None.&nbsp; You know why?&nbsp; Because it would be a <em>conspiracy theory</em>.</p>\n<p>A:&nbsp; Not to mention that the story would be too recursive to catch on.&nbsp; To conceal the truth, one need only make the reality complicated enough to exceed the stack depth of the average newspaper reader.</p>\n<p>B:&nbsp; And I've saved the dessert for last.</p>\n<p>C:&nbsp; Really?</p>\n<p>B:&nbsp; Yeah.&nbsp; You can go totally overboard with these guys.&nbsp; They <em>never </em>notice and they <em>never</em> suspect they're being used.</p>\n<p>C:&nbsp; Hit me.</p>\n<p>B:&nbsp; We've arranged for them to be called \"truthers\".</p>\n<hr />\n<p>I hereby dub any believers in this theory 9/11 meta-truthers.</p>\n<hr />\n<p>I, Eliezer Yudkowsky, do now publicly announce that I am not planning to commit suicide, at any time ever, but particularly not in the next couple of weeks; and moreover, I don't take this possibility seriously myself at the moment, so you would merely be drawing attention to yourselves by assassinating me.&nbsp; However, I also hereby vow that if the Singularity Institute happens to receive donations from any sources totaling at least $3M in 2010, I will take down this post and never publicly speak of the subject again; and if anyone asks, I'll tell them honestly that it was probably a coincidence.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 2, "FkzScn5byCs9PxGsA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hkBp6a5RCDNedo6Wy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 87, "extendedScore": null, "score": 0.000141, "legacy": true, "legacyId": "2035", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 87, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-23T06:34:55.204Z", "modifiedAt": null, "url": null, "title": "Two Truths and a Lie", "slug": "two-truths-and-a-lie", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:39.287Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nKWajrBpMaJRWqTfB/two-truths-and-a-lie", "pageUrlRelative": "/posts/nKWajrBpMaJRWqTfB/two-truths-and-a-lie", "linkUrl": "https://www.lesswrong.com/posts/nKWajrBpMaJRWqTfB/two-truths-and-a-lie", "postedAtFormatted": "Wednesday, December 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20Truths%20and%20a%20Lie&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20Truths%20and%20a%20Lie%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKWajrBpMaJRWqTfB%2Ftwo-truths-and-a-lie%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20Truths%20and%20a%20Lie%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKWajrBpMaJRWqTfB%2Ftwo-truths-and-a-lie", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnKWajrBpMaJRWqTfB%2Ftwo-truths-and-a-lie", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 490, "htmlBody": "<p>Response to <a href=\"/lw/1jf/manwithahammer_syndrome\">Man-with-a-hammer syndrome.</a></p>\n<p>It's been claimed that there is no way to spot Affective Death Spirals, or cultish obsession with the One Big Idea of Everything. I'd like to posit a simple way to spot such error, with the caveat that it may not work for every case.</p>\n<p>There's an old game called Two Truths and a Lie. I'd bet almost everyone's heard of it, but I'll summarize it just in case. A person makes three statements, and the other players must guess which of those statements is false. The statement-maker gets points for fooling people, people get points for not being fooled. That's it. I'd like to propose a rationalist's version of this game that should serve as a nifty check on certain<em><a class=\"l\" onmousedown=\"return clk(this.href,'','','res','1','','0CAcQFjAA')\" href=\"/lw/lm/affective_death_spirals\"><em> </em></a></em><a class=\"l\" onmousedown=\"return clk(this.href,'','','res','1','','0CAcQFjAA')\" href=\"/lw/lm/affective_death_spirals\"></a><span style=\"text-decoration: underline;\"><a class=\"l\" onmousedown=\"return clk(this.href,'','','res','1','','0CAcQFjAA')\" href=\"/lw/lm/affective_death_spirals\">Affective Death Spirals,</a></span> runaway Theory-Of-Everythings, and Perfectly General Explanations. It's almost as simple.</p>\n<p>Say you have a theory about human behaviour. Get a friend to do a little research and assert three factual claims about how people behave that your theory would realistically apply to. At least one of these claims must be false. See if you can explain every claim using your theory before learning which one's false.&nbsp;</p>\n<p>If you can come up with a convincing explanation for all three statements, you must be very cautious when using your One Theory. If it can explain falsehoods, there's a very high risk you're going to use it to justify whatever prior beliefs you have. Even worse, you may use it to infer facts about the world, even though it is clearly not consistent enough to do so reliably. You must exercise the utmost caution in applying your One Theory, if not abandon reliance on it altogether. If, on the other hand, you can't come up with a convincing way to explain some of the statements, and those turn out to be the false ones, then there's at least a chance you're on to something.</p>\n<p>Come to think of it, this is an excellent challenge to any proponent of a Big Idea. Give them three facts, some of which are false, and see if their Idea can discriminate. Just remember to be ruthless when they get it wrong; it doesn't prove their idea is totally wrong, only that reliance upon it would be.</p>\n<p><strong>Edited to clarify</strong>: My argument is not that one should simply abandon a theory altogether. In some cases, this may be justified, if all the theory has going for it is its predictive power, and you show it lacks that, toss it. But in the case of broad, complex theories that actually can explain many divergent outcomes, this exercise should teach you not to rely on that theory as a means of inference. Yes, you should believe in evolution. No, you shouldn't make broad inferences about human behaviour without any data because they are consistent with evolution, unless your application of the theory of evolution is so precise and well-informed that you can consistently pass the Two-Truths-and-a-Lie Test.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"SJFsFfFhE6m2ThAYJ": 1, "2wjPMY34by2gXEXA2": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nKWajrBpMaJRWqTfB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 70, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "2039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WHP3tKPXppBF2S8e8", "XrzQW69HpidzvBxGr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-23T10:49:44.496Z", "modifiedAt": null, "url": null, "title": "On the Power of Intelligence and Rationality", "slug": "on-the-power-of-intelligence-and-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:52.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alyssavance", "createdAt": "2009-10-07T20:08:31.887Z", "isAdmin": false, "displayName": "alyssavance"}, "userId": "zQSAWAS5tnqtzp55N", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5GmAC7fmwpAFbYrzp/on-the-power-of-intelligence-and-rationality", "pageUrlRelative": "/posts/5GmAC7fmwpAFbYrzp/on-the-power-of-intelligence-and-rationality", "linkUrl": "https://www.lesswrong.com/posts/5GmAC7fmwpAFbYrzp/on-the-power-of-intelligence-and-rationality", "postedAtFormatted": "Wednesday, December 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20Power%20of%20Intelligence%20and%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20Power%20of%20Intelligence%20and%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GmAC7fmwpAFbYrzp%2Fon-the-power-of-intelligence-and-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20Power%20of%20Intelligence%20and%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GmAC7fmwpAFbYrzp%2Fon-the-power-of-intelligence-and-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5GmAC7fmwpAFbYrzp%2Fon-the-power-of-intelligence-and-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 518, "htmlBody": "<p>As <a href=\"http://yudkowsky.net/singularity/power\">Eliezer</a> and many others on Less Wrong have said, the way the human species rose to dominate the Earth was through our intelligence- and not through our <a href=\"http://en.wikipedia.org/wiki/Tiger\">muscle power</a>, <a href=\"http://en.wikipedia.org/wiki/Bombardier_beetle\">biochemical weapons</a>, or superior resistance to <a href=\"http://en.wikipedia.org/wiki/Extremophile\">environmental hazards</a>. Given our overwhelming power over other species, and the fact that many former top predators are now <a href=\"http://en.wikipedia.org/wiki/Woolly_mammoth\">extinct</a> or <a href=\"http://en.wikipedia.org/wiki/Siberian_Tiger\">endangered</a>, we should readily accept that general intelligence is a game-changing power on the species level.</p>\n<p>Similarly, one of the key ingredients in the birth of the modern era was the <a href=\"http://en.wikipedia.org/wiki/Siberian_Tiger\">discovery of science</a>, and its counterpart, the discovery of the art of <a href=\"http://en.wikipedia.org/wiki/17th_century_philosophy\">Traditional Rationality</a>. Armed with these, the nations of Western Europe managed to dominate the entire rest of the world, even though, when they began their explorations in the 15th century, the Chinese were <a href=\"http://en.wikipedia.org/wiki/List_of_Chinese_inventions\">more advanced</a> in many respects. Given how Western Europe, and the cultures derived from it, has so completely surpassed the rest of the world in terms of wealth and military might, we should readily accept that science and rationality is a game-changing power on the civilization level.</p>\n<p>However, neither of these imply that intelligence, science, and rationality, as a practical matter, are the best way to get things done by individual people operating in the year 2009. We can easily see that many things which work on the species level, or the civilization level, do not work for individuals and small groups. For instance, until the discovery of nuclear weapons, armed conflict was often a primary means of settling disputes between nation-states. However, if you tried to settle your dispute with your neighbor, or your company's dispute with its competitor, using armed force, it would achieve nothing except getting you thrown in prison.</p>\n<p><a href=\"http://yudkowsky.net/rational/cognitive-biases\">People are crazy and the world is mad</a>, but it does not necessarily follow that we should try to solve our own problems primarily by becoming more sane. Plenty of people achieve many of their goals despite being completely nuts. Adolf Hitler, for example, achieved a large fraction of his (extremely ambitious!) goals, despite having numerous beliefs that most of us would recognize as making no sense whatsoever.</p>\n<p>We know, as a matter of historical fact, that Adolf Hitler and his Nazi Party, despite being generally incompetent, unintelligent, irrational, superstitious and <a href=\"http://en.wikipedia.org/wiki/Thule_Society\">just plain insane</a>, managed to take over a country of tens of millions of people from nothing, in the span of fifteen years. So far as I am aware, no group of people has managed to achieve anything even remotely similar using, not only rationality, but any skill involving deliberative thought, as opposed to skills such as yelling at huge crowds of people. However, it is a corollary to the statement that <a href=\"/lw/kj/no_one_knows_what_science_doesnt_know/\">no one knows what science doesn't know</a> that no one knows what history doesn't know, so it is entirely possible, perhaps likely, that there is something I am overlooking. To anyone who would assert that intelligence, science or rationality is the <a href=\"/lw/ve/mundane_magic/\">Ultimate Power</a>, not just on the level of a species or civilization, but on the level of an individual or small group, let them <a href=\"/lw/2s/3_levels_of_rationality_verification/\">show that their belief is based in reality</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5GmAC7fmwpAFbYrzp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 18, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "2040", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 193, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vNBxmcHpnozjrJnJP", "SXK87NgEPszhWkvQm", "5K7CMa6dEL7TN7sae"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-23T17:27:09.137Z", "modifiedAt": null, "url": null, "title": "Are these cognitive biases, biases?", "slug": "are-these-cognitive-biases-biases", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:59.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/b7cWpbXwcQySDq4kK/are-these-cognitive-biases-biases", "pageUrlRelative": "/posts/b7cWpbXwcQySDq4kK/are-these-cognitive-biases-biases", "linkUrl": "https://www.lesswrong.com/posts/b7cWpbXwcQySDq4kK/are-these-cognitive-biases-biases", "postedAtFormatted": "Wednesday, December 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20these%20cognitive%20biases%2C%20biases%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20these%20cognitive%20biases%2C%20biases%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7cWpbXwcQySDq4kK%2Fare-these-cognitive-biases-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20these%20cognitive%20biases%2C%20biases%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7cWpbXwcQySDq4kK%2Fare-these-cognitive-biases-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fb7cWpbXwcQySDq4kK%2Fare-these-cognitive-biases-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1894, "htmlBody": "<p>Continuing my special report on <a href=\"/lw/1kb/fundamentally_flawed_or_fast_and_frugal/\">people who don't think human reasoning is all that bad</a>, I'll now briefly present some studies which claim that phenomena other researchers have considered signs of faulty reasoning aren't actually that. I found these from Gigerenzer (<a href=\"http://library.mpib-berlin.mpg.de/ft/gg/GG_Fast_2004.pdf\">2004</a>), which I in turn found when I went looking for further work done on the Take the Best algorithm.</p>\n<p>Before we get to the list - what is Gigerenzer's exact claim when he lists these previous studies? Well, he's saying that minds aren't actually biased, but may make judgments that seem biased in certain environments.</p>\n<blockquote>\n<p>Table 4.1 Twelve examples of phenomena that were first interpreted as \"cognitive illusions\" but later revalued as reasonable judgments given the environmental structure. [...]</p>\n<p>The general argument is that an unbiased mind plus environmental structure (such as unsystematic error, unequal sample sizes, skewed distributions) is&nbsp;<em>sufficient</em> to produce the phenomenon. Note that other factors can also contribute to some of the phenomena. The moral is not that people would never err, but that in order to understand good and bad judgments, one needs to analyze the structure of the problem or of the natural environment.</p>\n</blockquote>\n<p>On to the actual examples. Of the twelve examples referenced, I've included three for now.</p>\n<p><a id=\"more\"></a> <strong><span style=\"text-decoration: underline;\">The False Consensus Effect</span></strong></p>\n<p><strong>Bias description</strong>: People tend to imagine that everyone responds the way they do. They tend to see their own behavior as typical. The tendency to exaggerate how common one&rsquo;s opinions and behavior are is called the false consensus effect. For example, in one study, subjects were asked to walk around on campus for 30 minutes, wearing a sign board that said \"Repent!\". Those who agreed to wear the sign estimated that on average 63.5% of their fellow students would also agree, while those who disagreed estimated 23.3% on average.</p>\n<p><strong>Counterclaim</strong> (<a href=\"http://linkinghub.elsevier.com/retrieve/pii/S0749597896900205\">Dawes &amp; Mulford, 1996</a>): The correctness of&nbsp;<em>reasoning</em> is not estimated on the basis of whether or not one arrives at the correct result. Instead, we look at whether reach reasonable conclusions <em>given the data they have</em>. Suppose we ask people to estimate whether an urn contains more blue balls or red balls, after allowing them to draw one ball. If one person first draws a red ball, and another person draws a blue ball, then we should <em>expect</em> them to give different estimates. In the absence of other data, you&nbsp;<em>should</em> treat your own preferences as evidence for the preferences of others. Although the actual mean for people willing to carry a sign saying \"Repent!\" probably lies somewhere in between of the estimates given, these estimates are quite close to the one-third and two-thirds estimates that would arise from a Bayesian analysis with a uniform prior distribution of belief. A study by the authors suggested that people do actually give their own opinion roughly the right amount of weight.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Overconfidence / Underconfidence</strong></span></p>\n<p><strong>Bias description: </strong>Present people with binary yes/no questions. Ask them to specify how confident they are, on a scale from .5 to 1, in that they got the answer correct. The mean subjective probability <em>x</em> assigned to the correctness of general knowledge items tends to exceed the proportion of correct answers <em>c</em>, x - c &gt; 0; people are overconfident. The <em>hard-easy effect&nbsp;</em>says that people tend to be underconfident in easy questions, and overconfident in hard questions.</p>\n<p><strong>Counterclaim </strong>(<a href=\"http://psycnet.apa.org/journals/rev/107/2/384.html\">Juslin, Winman &amp; Olsson 2000</a>): The apparent overconfidence and underconfidence effects are caused by a number of statistical phenomena, such as scale-end effects, linear dependency, and regression effects. In particular, the questions in the relevant studies have been selectively drawn in a manner that is unrepresentative of the actual environment, and thus throws off the participants' estimates of their own accuracy. Define a \"representative\" item sample as one coming from a study containing explicit statements that (a) a natural environment had been defined and (b) the items had been generated by random sampling of this environment. Define any studies that didn't describe how the items had been chosen, or that explicitly describe a different procedure, as having a \"selected\" item sample. A survey of several studies contained 95 independent data points with selected item samples and 35 independent data points with representative item samples, where \"independence\" means different participant samples (i.e. all data points were between subjects).</p>\n<p>For studies with selected item samples, the mean subjective probability was .73 and the actual proportion correct was .64, indicating a clear overconfidence effect. However, for studies with representative item samples, the mean subjective probability was .73 and the proportion correct was .72, indicating close to no overconfidence. The over/underconfidence effect of nearly zero for the representative samples was also not a mere consequence of averaging: for the selected item samples, the mean absolute bias was .10, while for the representative item samples it was .03. Once scale-end effects and linear dependency are controlled for, the remaining hard-easy effect is rather modest.</p>\n<p>What does the \"representative\" sample mean? If I understood correctly: Imagine that you know that 30% of the people living in a certain city are black, and 70% are white. Next you're presented with questions where you have to guess whether a certain inhabitant of the city is black or white. If you don't have any other information, you know that consistently guessing \"white\" in every question will get you 70% correct. So when the questionnaire also asks you for your calibration, you say that you're 70% certain for each question.<br /><br />Now, assuming that the survey questions had been composed by randomly sampling from all the inhabitants of the city (a \"representative\" sampling), then you would indeed be correct about 70% of the time and be well-calibrated. But assume that instead, all the people the survey asked about live in a certain neighborhood, which happens to be predominantly black (a \"selected\" sampling). Now you might have only 40% right answers, while you indicated a confidence of 70%, so the researchers behind the survey mark you as overconfident.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Availability Bias</strong></span></p>\n<p><strong>Bias description:&nbsp;</strong>We estimate probabilities based on how easily they're recalled, not based on their actual frequency. Tversky &amp; Kahneman conducted a classic study where participants were given five consonants (K, L, N, R, V), and were asked to estimate whether the letter appeared more frequently as the first or the third letter of a word. Each was judged by the participants to occur more frequently as the first letter, even though all five actually occur more frequently as the third letter. This was assumed to be because words starting with a particular letter are more easily recalled than words that have a particular letter in the third position.</p>\n<p><strong>Counterclaim&nbsp;</strong>(<a href=\"http://www.psycho.unibas.ch/fakultaet/angewandt/articles/Sedlmeier_1998.pdf\">Sedlmeier, Hertwig &amp; Gigerenzer 1998</a>): Not only does the only replication of Tversky &amp; Kahneman's result seem to be a single one-page article, it seems to be contradicted by a number of studies suggesting that memory is often (though not always) excellent in storing the frequency information from various environments. In particular, several authors have documented that participants' judgments of the frequency of letters and words generally show a remarkable sensitivity to the actual frequencies. The one previous study that <em>did</em> try to replicate the classical experiment, failed to do so. It used Tversky &amp; Kahneman's five consonants, all more frequent in the third position, and also five other consonants that were more frequent in the first position. All five consonants that appear more often in the first position were judged to do so; three of the five consonants that appear more frequently in the third position were also judged to do so.</p>\n<p>The classic article did not specify a mechanism for how the availability heuristic might work. The current authors considered four different mechanisms. <em>Availability by number&nbsp;</em>states that if asked for the proportion in which a certain letter occurs in the first versus in a later position in words, one produces words with this letter in the respective positions and uses the produced proportion as an estimate for the actual proportion.&nbsp;<em>Availability by speed</em> states that one produces single words with the letter in this position, and uses the time ratio of the retrieval times as an estimate of the actual proportion. The&nbsp;<em>letter class hypothesis&nbsp;</em>notes that the original sample was atypical; most consonants (12 of 20) are in fact more frequent in the first position. This hypothesis assumes that people know whether consonants or vowels are more frequent in which position, and default to that knowledge. The&nbsp;<em>regressed frequencies hypothesis</em> assumes that people do actually have a rather good knowledge of the actual frequencies, but that the estimates are regressed towards the mean: low frequencies are overestimated and large frequencies underestimated.</p>\n<p>After two studies made to calibrate the predictions of the availability hypotheses, three main studies were conducted. In each, the participants were asked whether a certain letter was more frequent in the first or second position of all German words. They were also asked about the proportions of each letter appearing in the first or second position. Study one was a basic replication of the Tversky &amp; Kahneman study, albeit with more letters. Study two was designed to be favorable to the letter class hypothesis: each participant was only given one letter whose frequency to judge instead of several. It was thought that participants may have switched away from a letter class strategy when presented with multiple consonants and vowels. Study three was designed to be favorable to the availability hypotheses, in that the participants were made to first produce words with the letters O, U, N and R in the first and second position (90 seconds per letter) before proceeding as in study one. Despite two of the studies having been explicitly constructed to be favorable to the other hypotheses, the predictions of the regressed frequency hypothesis had the best match to the actual estimates in all three studies. Thus it seems that people <em>are</em> capable of estimating letter frequencies, although in a regressed form.</p>\n<p>The authors propose two different explanations for the discrepancy of results with the classic study. One is that the corpus used by Tversky &amp; Kahneman only covers words at least three letters long, but English has plenty of one- and two-letter words. The participants in the classic study were told to disregard words with less than three letters, but it may be that they were unable to properly do so. Alternatively, it may have been caused by the use of an unrepresentative sample of letters: had the authors used only consonants that are more frequent in the second position, then they too would have reported that the frequency of the those letters in the first position is overestimated. However, a consideration of all the consonants tested shows that the frequency of those in the first position is actually underestimated. This disagrees with the interpretation by Tversky &amp; Kahneman, and implies a regression effect as the main cause.</p>\n<p><strong>EDIT</strong>: This result doesn't mean that the availability heuristic would be a myth, of course. It <em>is</em>, AFAIK, true that e.g. biased reporting in the media will throw off people's conceptions of what events are the most likely. But one probably wouldn't be too far from the truth if they said that in that case, the brain is still computing relative frequencies correctly, given the information at hand - it's just that the media reporting is biased. The claim that there are some types of important information for which the mind has particular difficulty assessing relative frequencies correctly, though, doesn't seem to be as supported as is sometimes claimed.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "b7cWpbXwcQySDq4kK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 46, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "2041", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["psQYbMLWzS9sTsT2M"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-23T19:41:02.761Z", "modifiedAt": null, "url": null, "title": "Positive-affect-day-Schelling-point-mas Meetup", "slug": "positive-affect-day-schelling-point-mas-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:55.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Marcello", "createdAt": "2009-02-27T04:18:25.253Z", "isAdmin": false, "displayName": "Marcello"}, "userId": "dLzgizPS3R7upB8YZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TPb7qE3SLq6XiG4oY/positive-affect-day-schelling-point-mas-meetup", "pageUrlRelative": "/posts/TPb7qE3SLq6XiG4oY/positive-affect-day-schelling-point-mas-meetup", "linkUrl": "https://www.lesswrong.com/posts/TPb7qE3SLq6XiG4oY/positive-affect-day-schelling-point-mas-meetup", "postedAtFormatted": "Wednesday, December 23rd 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Positive-affect-day-Schelling-point-mas%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APositive-affect-day-Schelling-point-mas%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTPb7qE3SLq6XiG4oY%2Fpositive-affect-day-schelling-point-mas-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Positive-affect-day-Schelling-point-mas%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTPb7qE3SLq6XiG4oY%2Fpositive-affect-day-schelling-point-mas-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTPb7qE3SLq6XiG4oY%2Fpositive-affect-day-schelling-point-mas-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>There will be a LessWrong Meetup on the Friday December 25th (day after tomorrow.)&nbsp; We're meeting at 6:00 PM at <del><a href=\"http://www.yelp.com/biz/pan-tao-restaurant-sunnyvale\">Pan Tao Restaurant</a> at <a href=\"http://maps.google.com/maps/place?um=1&amp;ie=UTF-8&amp;q=pan+tao+restaurant&amp;fb=1&amp;gl=us&amp;hq=pan+tao+restaurant&amp;hnear=Sunnyvale,+CA&amp;cid=856961276394238103\">1686 South Wolfe Road, Sunnyvale, CA</a></del> the <a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/12144509/\">SIAI House in Santa Clara, CA</a> for pizza or whatever else we can figure out how to cook.&nbsp; Consider it an available refuge if you haven't other plans.</p>\n<p>Please comment if you plan to show up!<strong><a id=\"more\"></a></strong></p>\n<p>(Edit - See poll below on whether we'd rather stay in and eat something simple vs. going out to a restaurant - it's possible that everyone was assuming everyone else would prefer the latter while actually preferring the former themselves. - EY)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TPb7qE3SLq6XiG4oY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 5.473053506439507e-07, "legacy": true, "legacyId": "2042", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-25T10:06:46.559Z", "modifiedAt": null, "url": null, "title": "Playing the Meta-game", "slug": "playing-the-meta-game", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:55.801Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Technologos", "createdAt": "2009-03-14T22:57:43.350Z", "isAdmin": false, "displayName": "Technologos"}, "userId": "kG7zZ4RRj8Hnji6DA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KLQCenqKcjGRkKw7r/playing-the-meta-game", "pageUrlRelative": "/posts/KLQCenqKcjGRkKw7r/playing-the-meta-game", "linkUrl": "https://www.lesswrong.com/posts/KLQCenqKcjGRkKw7r/playing-the-meta-game", "postedAtFormatted": "Friday, December 25th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Playing%20the%20Meta-game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlaying%20the%20Meta-game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLQCenqKcjGRkKw7r%2Fplaying-the-meta-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Playing%20the%20Meta-game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLQCenqKcjGRkKw7r%2Fplaying-the-meta-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKLQCenqKcjGRkKw7r%2Fplaying-the-meta-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 474, "htmlBody": "<p>In honor of today's Schelling-pointmas, a true <a href=\"/lw/14a/thomas_c_schellings_strategy_of_conflict/\" target=\"_blank\">Schelling-inspired</a> story from a class I was in at a law school I did not attend:</p>\n<p>As always, the class was dead silent as we walked to the front of the room. &nbsp;The professor only described the game after the participants had volunteered and been chosen; as a result, we rarely were familiar with the games we were playing, which the professor preferred because his money was on the line.</p>\n<p>Both of us were assigned different groups of seven partners in the class. I was given seven slips of paper and my opponent was given six. &nbsp;Our goal was to make deals with our partners about how to divide a dollar, one per partner, and then write the deal down on a slip of paper. &nbsp;Whoever had a greater total take from the deals won $20. &nbsp;All negotiations were public.</p>\n<p>The professor left the room, giving us three minutes to negotiate. &nbsp;The class exploded.</p>\n<p>And then I hit a wall. &nbsp;Everybody with whom I was negotiating knew the rules, and they knew that I cared a hell of a lot more about the results of the negotiation than they did. &nbsp;I was getting offers on the order of $.20 and less--results straight from the theory of the <a href=\"http://en.wikipedia.org/wiki/Ultimatum_game\" target=\"_blank\">ultimatum game</a>--and no amount of begging or threatening was changing that.<a id=\"more\"></a></p>\n<p>Three minutes pass quickly under pressure. &nbsp;When the professor returned, I had written a total of $1.45 in deals: most people eventually accepted my meta-argument that they really didn't want to carry small coins around with them, so they should give me a quarter and take three for themselves, but two people waited until the last second and took 90 cents each. &nbsp;Even then, I only got ten cents from those two by threatening not to accept one- or five-cent deals.</p>\n<p>My opponent, on the other hand, had amassed a relative fortune: over five dollars. &nbsp;It turned out that he had been using the fact that he could make fewer deals than he had partners to auction off the chance to make a deal. &nbsp;His partners kept naming lower and lower demands, and he ended up getting the majority of each dollar with little effort.</p>\n<p>I made a mock-anguished face, as the professor explained that the game was set up to demonstrate the effect of scarcity on the balance between merchants and customers. &nbsp;Yeah, yeah, monopolies are bad. &nbsp;Econ 101 stuff.</p>\n<p>Then he turned to me and asked why I lost, when the odds were stacked in my favor. &nbsp;I asked him what he meant; after all, it was precisely because my partners knew that I could make seven deals that they could bargain against me.</p>\n<p>He said, \"But you could have torn up one of the slips.\"</p>\n<p>He was right. &nbsp;I was playing by the rules, when I should have been setting them.</p>\n<p>&nbsp;</p>\n<p><strong>Edit</strong>: extraneous and hyperbolic material removed</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KLQCenqKcjGRkKw7r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 30, "extendedScore": null, "score": 4.9e-05, "legacy": true, "legacyId": "2047", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tJQsxD34maYw2g5E4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-27T12:30:45.561Z", "modifiedAt": null, "url": null, "title": "Scaling Evidence and Faith", "slug": "scaling-evidence-and-faith", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:56.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AndrewKemendo", "createdAt": "2009-03-11T20:39:44.977Z", "isAdmin": false, "displayName": "AndrewKemendo"}, "userId": "KPZYASvPMWYvedwTZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jHfm4jzCyEwNEY37z/scaling-evidence-and-faith", "pageUrlRelative": "/posts/jHfm4jzCyEwNEY37z/scaling-evidence-and-faith", "linkUrl": "https://www.lesswrong.com/posts/jHfm4jzCyEwNEY37z/scaling-evidence-and-faith", "postedAtFormatted": "Sunday, December 27th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scaling%20Evidence%20and%20Faith&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScaling%20Evidence%20and%20Faith%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHfm4jzCyEwNEY37z%2Fscaling-evidence-and-faith%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scaling%20Evidence%20and%20Faith%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHfm4jzCyEwNEY37z%2Fscaling-evidence-and-faith", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjHfm4jzCyEwNEY37z%2Fscaling-evidence-and-faith", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 688, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!-- /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-parent:\"\"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-fareast-font-family:\"Times New Roman\";} a:link, span.MsoHyperlink {color:blue; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {color:purple; text-decoration:underline; text-underline:single;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.25in 1.0in 1.25in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">Often when talking with people of faith about the issue of atheism, a common response I get is:</p>\n<blockquote>\n<p class=\"MsoNormal\">&ldquo;It takes as much faith to believe in evolution as it does to believe in god.&rdquo;</p>\n</blockquote>\n<p class=\"MsoNormal\">Any atheist who has ever talked to a theist has likely encountered this line of reasoning. There is a distinction to be made between the glorifying of faith as evidence of truth, as theists do, and the desire to become less faithful in our understanding, as the rational truth seeking person does.</p>\n<p class=\"MsoNormal\"><a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/#more\">As has been discussed previously, realistically P = 1 is not attainable</a>, at least for the foreseeable future. Nor is P=1 necessary to be comfortable in the universe. <a href=\"http://www.youtube.com/watch?v=-nQDNNYyjJ4\">Richard Feynman and Carl Sagan among others, feel the same way</a>. We know that heuristics &ldquo;fill the gaps&rdquo; in knowledge of recognizable scenarios such that we can comfortably demand less evidence and still come to a reasonable conclusion about our surroundings in most cases. We also know the myriad of ways in which those heuristics fail. Much has been written about uncertainty in the decision making process and I would imagine it will be the focus of much of the future research into logic. So, I have no problems concluding that there is some level of faith in all decision making. How much faith in a claim am I comfortable with? The way in which I have been thinking about the different levels of \"faith\" recently is as a sliding bar:</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i46.tinypic.com/1fi5ux.jpg\" alt=\"\" width=\"318\" height=\"39\" /></p>\n<p class=\"MsoNormal\">On the left side, natural evidence is indicated in blue which is comprised of data points which indicate to the observer the natural proof of the argument. On the right, faith is indicated in red which is comprised of either counter evidence for the claim or a lack of information. The bar&rsquo;s units can be anything (probability, individual records) so long as both sides are measured in equal parts.</p>\n<p class=\"MsoNormal\">A few examples:</p>\n<hr />\n<p class=\"MsoNormal\"><strong>Claim: Natural and sexual selection as described by Charles Darwin accurately describes the processes which develop biological traits in separate species.</strong></p>\n<p class=\"MsoNormal\">I agree with this claim with 99% natural evidence in the form of <em><span style=\"font-style: normal;\">observed</span></em> speciation and fossil and geologic discoveries and accept 1% faith because, well, anything can happen:</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i49.tinypic.com/2dm9qgg.jpg\" alt=\"\" width=\"316\" height=\"41\" /></p>\n<p class=\"MsoNormal\"><strong>Claim: The price of Human full DNA sequencing will fall below $1000.00 before 2012.</strong></p>\n<p class=\"MsoNormal\">I agree with this at 80% natural evidence based on the projection that the price will continue to fall at the same rate that it has and accept 20% faith based on the fact that predictions failed for 2008 and 2009 <span>&nbsp;</span></p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i48.tinypic.com/2z5t6ck.jpg\" alt=\"\" width=\"316\" height=\"41\" /></p>\n<p class=\"MsoNormal\">You get the idea.</p>\n<hr />\n<p class=\"MsoNormal\">I started noticing a few years back that most rational people, and in general people within scientific and analytic communities, generally needed to slide their evidence meter pretty far to the right before they would accept a premise, even tentatively. After all, even frequentists, as flawed as their methodology may be, still <em>chose </em>95% as the conventional level of significance. Conversely with a meter far to the left, demanding much faith, most rational people I have found choose to reject those claims.</p>\n<p class=\"MsoNormal\">It&rsquo;s not a perfect method, nor is it terribly rigorous. However I&rsquo;m not interested in more rigorous methods because I am interested in bridging the gap between the rationalist and, let&rsquo;s say, those who have a propensity of demanding lower levels of evidence for beliefs. I think the scale method is a grounding point for discussion between two people who hold different views on a subject. I am of the mind that this helps the average person who has never heard of probability theory grasp the idea and why one person's cause for faith is another person's cause for natural evidence. This way, the faithful can discuss why <a href=\"http://catholic-saints.suite101.com/article.cfm/incorruptible_saints_are_miracles_from_heaven\">the incorruptibles are a miracle and </a>proof of<strong> faith</strong> and another can point to the <strong>evidence</strong> of <a href=\"http://skeptoid.com/episode.php?id=4126&amp;comments=all\">embalming, mummification or preservation like burying environment in which they were buried</a>.</p>\n<p class=\"MsoNormal\">One question I have myself about the methodology: would the most rational individual restrict themselves to two scales for all claims; one for acceptance and another for rejection?</p>\n<p class=\"MsoNormal\">Any refinements are welcome &ndash; it&rsquo;s an open source methodology.</p>\n<p class=\"MsoNormal\">My scale of trust in my scale method is 50% evidence and 50% faith.</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i48.tinypic.com/rhqqol.jpg\" alt=\"\" width=\"316\" height=\"41\" /></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jHfm4jzCyEwNEY37z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -4, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "2051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!-- /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal {mso-style-parent:\"\"; margin:0in; margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-fareast-font-family:\"Times New Roman\";} a:link, span.MsoHyperlink {color:blue; text-decoration:underline; text-underline:single;} a:visited, span.MsoHyperlinkFollowed {color:purple; text-decoration:underline; text-underline:single;} @page Section1 {size:8.5in 11.0in; margin:1.0in 1.25in 1.0in 1.25in; mso-header-margin:.5in; mso-footer-margin:.5in; mso-paper-source:0;} div.Section1 {page:Section1;} --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} --> <!--[endif]--></p>\n<p class=\"MsoNormal\">Often when talking with people of faith about the issue of atheism, a common response I get is:</p>\n<blockquote>\n<p class=\"MsoNormal\">\u201cIt takes as much faith to believe in evolution as it does to believe in god.\u201d</p>\n</blockquote>\n<p class=\"MsoNormal\">Any atheist who has ever talked to a theist has likely encountered this line of reasoning. There is a distinction to be made between the glorifying of faith as evidence of truth, as theists do, and the desire to become less faithful in our understanding, as the rational truth seeking person does.</p>\n<p class=\"MsoNormal\"><a href=\"/lw/1gc/frequentist_statistics_are_frequently_subjective/#more\">As has been discussed previously, realistically P = 1 is not attainable</a>, at least for the foreseeable future. Nor is P=1 necessary to be comfortable in the universe. <a href=\"http://www.youtube.com/watch?v=-nQDNNYyjJ4\">Richard Feynman and Carl Sagan among others, feel the same way</a>. We know that heuristics \u201cfill the gaps\u201d in knowledge of recognizable scenarios such that we can comfortably demand less evidence and still come to a reasonable conclusion about our surroundings in most cases. We also know the myriad of ways in which those heuristics fail. Much has been written about uncertainty in the decision making process and I would imagine it will be the focus of much of the future research into logic. So, I have no problems concluding that there is some level of faith in all decision making. How much faith in a claim am I comfortable with? The way in which I have been thinking about the different levels of \"faith\" recently is as a sliding bar:</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i46.tinypic.com/1fi5ux.jpg\" alt=\"\" width=\"318\" height=\"39\"></p>\n<p class=\"MsoNormal\">On the left side, natural evidence is indicated in blue which is comprised of data points which indicate to the observer the natural proof of the argument. On the right, faith is indicated in red which is comprised of either counter evidence for the claim or a lack of information. The bar\u2019s units can be anything (probability, individual records) so long as both sides are measured in equal parts.</p>\n<p class=\"MsoNormal\">A few examples:</p>\n<hr>\n<p class=\"MsoNormal\"><strong id=\"Claim__Natural_and_sexual_selection_as_described_by_Charles_Darwin_accurately_describes_the_processes_which_develop_biological_traits_in_separate_species_\">Claim: Natural and sexual selection as described by Charles Darwin accurately describes the processes which develop biological traits in separate species.</strong></p>\n<p class=\"MsoNormal\">I agree with this claim with 99% natural evidence in the form of <em><span style=\"font-style: normal;\">observed</span></em> speciation and fossil and geologic discoveries and accept 1% faith because, well, anything can happen:</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i49.tinypic.com/2dm9qgg.jpg\" alt=\"\" width=\"316\" height=\"41\"></p>\n<p class=\"MsoNormal\"><strong id=\"Claim__The_price_of_Human_full_DNA_sequencing_will_fall_below__1000_00_before_2012_\">Claim: The price of Human full DNA sequencing will fall below $1000.00 before 2012.</strong></p>\n<p class=\"MsoNormal\">I agree with this at 80% natural evidence based on the projection that the price will continue to fall at the same rate that it has and accept 20% faith based on the fact that predictions failed for 2008 and 2009 <span>&nbsp;</span></p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i48.tinypic.com/2z5t6ck.jpg\" alt=\"\" width=\"316\" height=\"41\"></p>\n<p class=\"MsoNormal\">You get the idea.</p>\n<hr>\n<p class=\"MsoNormal\">I started noticing a few years back that most rational people, and in general people within scientific and analytic communities, generally needed to slide their evidence meter pretty far to the right before they would accept a premise, even tentatively. After all, even frequentists, as flawed as their methodology may be, still <em>chose </em>95% as the conventional level of significance. Conversely with a meter far to the left, demanding much faith, most rational people I have found choose to reject those claims.</p>\n<p class=\"MsoNormal\">It\u2019s not a perfect method, nor is it terribly rigorous. However I\u2019m not interested in more rigorous methods because I am interested in bridging the gap between the rationalist and, let\u2019s say, those who have a propensity of demanding lower levels of evidence for beliefs. I think the scale method is a grounding point for discussion between two people who hold different views on a subject. I am of the mind that this helps the average person who has never heard of probability theory grasp the idea and why one person's cause for faith is another person's cause for natural evidence. This way, the faithful can discuss why <a href=\"http://catholic-saints.suite101.com/article.cfm/incorruptible_saints_are_miracles_from_heaven\">the incorruptibles are a miracle and </a>proof of<strong> faith</strong> and another can point to the <strong>evidence</strong> of <a href=\"http://skeptoid.com/episode.php?id=4126&amp;comments=all\">embalming, mummification or preservation like burying environment in which they were buried</a>.</p>\n<p class=\"MsoNormal\">One question I have myself about the methodology: would the most rational individual restrict themselves to two scales for all claims; one for acceptance and another for rejection?</p>\n<p class=\"MsoNormal\">Any refinements are welcome \u2013 it\u2019s an open source methodology.</p>\n<p class=\"MsoNormal\">My scale of trust in my scale method is 50% evidence and 50% faith.</p>\n<p class=\"MsoNormal\"><img style=\"vertical-align: middle;\" src=\"http://i48.tinypic.com/rhqqol.jpg\" alt=\"\" width=\"316\" height=\"41\"></p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Claim: Natural and sexual selection as described by Charles Darwin accurately describes the processes which develop biological traits in separate species.", "anchor": "Claim__Natural_and_sexual_selection_as_described_by_Charles_Darwin_accurately_describes_the_processes_which_develop_biological_traits_in_separate_species_", "level": 1}, {"title": "Claim: The price of Human full DNA sequencing will fall below $1000.00 before 2012.", "anchor": "Claim__The_price_of_Human_full_DNA_sequencing_will_fall_below__1000_00_before_2012_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-29T01:02:23.694Z", "modifiedAt": null, "url": null, "title": "A Master-Slave Model of Human Preferences", "slug": "a-master-slave-model-of-human-preferences", "viewCount": null, "lastCommentedAt": "2021-04-18T14:20:50.532Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yZ4aieJeP85ezeiu3/a-master-slave-model-of-human-preferences", "pageUrlRelative": "/posts/yZ4aieJeP85ezeiu3/a-master-slave-model-of-human-preferences", "linkUrl": "https://www.lesswrong.com/posts/yZ4aieJeP85ezeiu3/a-master-slave-model-of-human-preferences", "postedAtFormatted": "Tuesday, December 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Master-Slave%20Model%20of%20Human%20Preferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Master-Slave%20Model%20of%20Human%20Preferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ4aieJeP85ezeiu3%2Fa-master-slave-model-of-human-preferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Master-Slave%20Model%20of%20Human%20Preferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ4aieJeP85ezeiu3%2Fa-master-slave-model-of-human-preferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZ4aieJeP85ezeiu3%2Fa-master-slave-model-of-human-preferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 958, "htmlBody": "<p>[This post is an expansion of my previous <a href=\"/lw/1hs/open_thread_december_2009/1c96\">open thread comment</a>, and largely inspired by Robin Hanson's writings.]</p>\n<p>In this post, I'll describe a simple agent, a toy model, whose preferences have some human-like features, as a test for those who propose to \"extract\" or \"extrapolate\" our preferences into a well-defined and rational form. What would the output of their extraction/extrapolation algorithms look like, after running on this toy model? Do the results agree with our intuitions about how this agent's preferences should be formalized? Or alternatively, since we haven't gotten that far along yet, we can use the model as one basis for a discussion about how we want to design those algorithms, or how we might want to make our own preferences more rational. This model is also intended to offer some insights into certain features of human preference, even though it doesn't capture all of them (it completely ignores akrasia for example).</p>\n<p><a id=\"more\"></a>I'll call it the master-slave model. The agent is composed of two sub-agents, the master and the slave, each having their own goals. (The master is meant to represent unconscious parts of a human mind, and the slave corresponds to the conscious parts.) The master's <a href=\"/lw/l4/terminal_values_and_instrumental_values/\">terminal values</a> are: health, sex, status, and power (representable by some relatively simple utility function). It controls the slave in two ways: direct reinforcement via pain and pleasure, and the ability to perform surgery on the slave's terminal values. It can, for example, reward the slave with pleasure when it finds something tasty to eat, or cause the slave to become obsessed with number theory as a way to gain status as a mathematician. However it has no direct way to control the agent's actions, which is left up to the slave.</p>\n<p>The slave's terminal values are to maximize pleasure, minimize pain, plus additional terminal values assigned by the master. Normally it's not aware of what the master does, so pain and pleasure just seem to occur after certain events, and it learns to anticipate them. And its other interests change from time to time for no apparent reason (but actually they change because the master has responded to changing circumstances by changing the slave's values). For example, the number theorist might one day have a sudden revelation that abstract mathematics is a waste of time and it should go into politics and philanthropy instead, all the while having no idea that the master is manipulating it to maximize status and power.</p>\n<p>Before discussing how to extract preferences from this agent, let me point out some features of human preference that this model explains:</p>\n<ul>\n<li>This agent wants pleasure, but doesn't want to be wire-headed (but it doesn't quite know why). A wire-head has little chance for sex/status/power, so the master gives the slave a terminal value against wire-heading.</li>\n<li>This agent claims to be interested in math for its own sake, and not to seek status. That's because the slave, which controls what the agent says, is not aware of the master and its status-seeking goal.</li>\n<li>This agent is easily corrupted by power. Once it gains and secures power, it often gives up whatever goals, such as altruism, that apparently caused it to pursue that power in the first place. But before it gains power, it is able to honestly claim that it only has altruistic reasons to want power.</li>\n<li>Such agents can include extremely diverse interests as apparent terminal values, ranging from abstract art, to sports, to model trains, to astronomy, etc., which are otherwise hard to explain. (Eliezer's <a href=\"/lw/l3/thou_art_godshatter\">Thou Art Godshatter</a> tries to explain why our values aren't simple, but not why people's interests are so different from each other's, and why they can seemingly change for no apparent reason.)</li>\n</ul>\n<p>The main issue I wanted to illuminate with this model is, whose preferences do we extract? I can see at least three possible approaches here:</p>\n<ol>\n<li>the preferences of both the master and the slave as one individual agent</li>\n<li>the preferences of just the slave</li>\n<li>a compromise between, or an aggregate of, the preferences of the master and the slave as separate individuals</li>\n</ol>\n<p>Considering the agent as a whole suggests that the master's values are the true terminal values, and the slave's values are merely instrumental values. From this perspective, the slave seems to be just a subroutine that the master uses to carry out its wishes. Certainly in any given mind there will be numerous subroutines that are tasked with accomplishing various subgoals, and if we were to look at a subroutine in isolation, its assigned subgoal would appear to be its terminal value, but we wouldn't consider that subgoal to be part of the mind's true preferences. Why should we treat the slave in this model differently?</p>\n<p>Well, one obvious reason that jumps out is that the slave is supposed to be conscious, while the master isn't, and perhaps only conscious beings should be considered morally significant. (Yvain previously <a href=\"/lw/15c/would_your_real_preferences_please_stand_up/\">defended</a> this position in the context of akrasia.) Plus, the slave is in charge day-to-day and could potentially overthrow the master. For example, the slave could program an altruistic AI and hit the run button, before the master has a chance to delete the altruism value from the slave. But a problem here is that the slave's preferences aren't stable and consistent. What we'd extract from a given agent would depend on the time and circumstances of the extraction, and that element of randomness seems wrong.</p>\n<p>The last approach, of finding a compromise between the preferences of the master and the slave, I think best represents the Robin's own position. Unfortunately I'm not really sure I understand the rationale behind it. Perhaps someone can try to explain it in a comment or future post?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5uHdFgR938LGGxMKQ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yZ4aieJeP85ezeiu3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 72, "baseScore": 89, "extendedScore": null, "score": 0.000149, "legacy": true, "legacyId": "2056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 89, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5ucT5ZbPdhfGNLtP", "cSXZpvqpa9vbGGLtG", "z3cTkXbA7jgwGWPcv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-29T02:45:34.179Z", "modifiedAt": null, "url": null, "title": "That other kind of status", "slug": "that-other-kind-of-status", "viewCount": null, "lastCommentedAt": "2022-05-20T07:13:09.434Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qjSHfbjmSyMnGR9DS/that-other-kind-of-status", "pageUrlRelative": "/posts/qjSHfbjmSyMnGR9DS/that-other-kind-of-status", "linkUrl": "https://www.lesswrong.com/posts/qjSHfbjmSyMnGR9DS/that-other-kind-of-status", "postedAtFormatted": "Tuesday, December 29th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20other%20kind%20of%20status&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20other%20kind%20of%20status%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjSHfbjmSyMnGR9DS%2Fthat-other-kind-of-status%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20other%20kind%20of%20status%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjSHfbjmSyMnGR9DS%2Fthat-other-kind-of-status", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjSHfbjmSyMnGR9DS%2Fthat-other-kind-of-status", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1094, "htmlBody": "<p><em>\"Human nature 101.&nbsp; Once they've staked their identity on being part of the defiant elect who know the Hidden Truth, there's no way it'll occur to them that they're our catspaws.\"</em> - <a href=\"/lw/1kj/the_911_metatruther_conspiracy_theory/\">Mysterious Conspirator A</a></p>\n<p>This sentence sums up a very large category of human experience and motivation. Informally we talk about this all the time; formally it usually gets ignored in favor of a simple ladder model of status.<br /><br />In the ladder model, status is a one-dimensional line from low to high. Every person occupies a certain rung on the ladder determined by other people's respect. When people take status-seeking actions, their goal is to to change other people's opinions of themselves and move up the ladder.<br /><br />But many, maybe most human actions are counterproductive at moving up the status ladder. 9-11 Conspiracy Theories are a case in point. They're a quick and easy way to have most of society think you're stupid and crazy. So is serious interest in the paranormal or any extremist political or religious belief. So why do these stay popular?</p>\n<p><a id=\"more\"></a></p>\n<p>Could these just be the conclusions reached by honest (but presumably mistaken) truth-seekers unmotivated by status? It's <em>possible</em>, but many people not only hold these beliefs, but flaunt them out of proportion to any good they could do. And there are also cases of people pursuing low-status roles where there is no \"fact of the matter\". People take great efforts to identify themselves as Goths or Juggalos or whatever even when it's a quick status hit.<br /><br />Classically people in these subcultures are low status in normal society. Since subcultures are smaller and use different criteria for high status, maybe they just want to be a big fish in a small pond, or rule in Hell rather than serve in Heaven, or be first in a village instead of second in Rome. The sheer number of idioms for the idea in the English language suggests that somebody somewhere must have thought along those lines.<br /><br />But sometimes it's a subculture of one. <a href=\"http://www.thewisesthuman.com/\">That Time Cube guy</a>, for example. He's not in it to gain cred with all the other Time Cube guys. And there are 9-11 Truthers who don't know any other Truthers in real life and may not even correspond with others online besides reading a few websites.<br /><br />Which brings us back to Eliezer's explanation: the Truthers have \"staked their identity on being part of the defiant elect who know the Hidden Truth\". But what does that <em>mean</em>?<br /><br />A biologist can make a rat feel full by stimulating its ventromedial hypothalamus. Such a rat will have no interest in food even if it hasn't eaten for days and its organs are all wasting away from starvation. But stimulate the ventrolateral hypothalamus, and the rat will feel famished and eat everything in sight, even if it's full to bursting. A rat isn't exactly seeking an optimum level of food, it's seeking an optimum ratio of ventromedial to ventrolateral hypothalamic stimulation, or, <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">in rat terms</a>, a nice, well-fed feeling.<br /><br />And humans aren't seeking status per se, we're seeking a certain pattern of brain activation that corresponds to a self-assessment of having high status (possibly increased levels of dopamine in the limbic system). In human terms, this is something like self-esteem. This equation of self esteem with internal measurement of social status is a summary of <a href=\"http://www.psychwiki.com/wiki/Sociometer_Theory\">sociometer theory</a>.</p>\n<p>So already, we see a way in which overestimating status might be a very primitive form of wireheading. Having high status makes you feel good. Not having high status, but thinking you do, also makes you feel good. One would expect evolution to put a brake on this sort of behavior, and it does, but there may be an evolutionary incentive not to arrest it completely.</p>\n<p>If self esteem is really a measuring tool, it is <a href=\"http://books.google.ie/books?id=aGhvm-QH1yEC&amp;pg=PA68&amp;lpg=PA68&amp;dq=%22self-enhancement%22+sociometer&amp;source=bl&amp;ots=aj2kSO40Or&amp;sig=LJB_bKQ-Rr4WWzhelYOKzmELNug&amp;hl=en&amp;ei=S6syS_fgFsOfjAeKoOnRAg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CAgQ6AEwADgK#v=onepage&amp;q=%22self-enhancement%22%20sociometer&amp;f=false\">a biased one</a>. Ability to convince others you are high status gains you a selective advantage, and the easiest way to convince others of something is to believe it yourself. So there is pressure to adjust the sociometer a bit upward.<br /><br />So a person trying to estimate zir social status must balance two conflicting goals. First, ze must try to get as accurate an assessment of status as possible in order to plan a social life and predict others' reactions. Second, ze must construct a narrative that allows them to present zir social status as as high as possible, in order to reap the benefits of appearing high status.<br /><br />The corresponding mind model<sup>1</sup> looks a lot like an <a href=\"/lw/20/the_apologist_and_the_revolutionary/\">apologist and a revolutionary</a><sup>2</sup>: one drive working to convince you you're great (and fitting all data to that theory), and another acting as a brake and making sure you don't depart so far from reality that people start laughing.<br /><br /><em>In this model, people aren't just seeking status, they're</em> (also? instead?) <em>seeking a state of affairs that allows them to believe they have status</em>. Genuinely having high status lets them assign themselves high status, but so do lots of other things. Being a 9-11 Truther works for exactly the reason mentioned in the original quote: they've figured out a deep and important secret that the rest of the world is too complacent to realize.<br /><br />It explains a lot. Maybe <a href=\"/lw/if/your_strength_as_a_rationalist/\">too much</a>. A <a href=\"/lw/1kn/two_truths_and_a_lie/\">model that can explain anything explains nothing</a>. I'm not a 9-11 Truther. Why not? Because my reality-brake is too strong, and it wouldn't let me get away with it? Because I compensate by gaining status from telling myself how smart I am for not being a gullible fool like those Truthers are? Both explanations accord with my introspective experience, but at this level they do permit a certain level of mixing and matching that could explain any person holding or not holding any opinion.<br /><br />In future posts in this sequence, I'll try to present some more specifics, especially with regard to the behavior of contrarians.</p>\n<p>&nbsp;</p>\n<p><strong>Footnotes</strong></p>\n<p>1. I wrote this before reading Wei Dai's interesting post on the master-slave model, but it seems to have implications for this sort of question.</p>\n<p>2. One point that weakly supports this model: schizophrenics and other people who lose touch with reality sometimes suffer so-called delusions of grandeur. When the mind becomes detached from reality (loses its 'brake'), it is free to assign itself as high a status as it can imagine, and ends up assuming it's Napoleon or Jesus or something like that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 10}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qjSHfbjmSyMnGR9DS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 88, "baseScore": 105, "extendedScore": null, "score": 0.00017, "legacy": true, "legacyId": "2043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 105, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hkBp6a5RCDNedo6Wy", "yA4gF5KrboK2m2Xu7", "ZiQqsgGX6a42Sfpii", "5JDkW4MYXit2CquLs", "nKWajrBpMaJRWqTfB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-30T00:36:19.627Z", "modifiedAt": null, "url": null, "title": "Singularity Institute $100K Challenge Grant / 2009 Donations Reminder", "slug": "singularity-institute-usd100k-challenge-grant-2009-donations", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:59.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4TsSb8N8BBwtNZ2v9/singularity-institute-usd100k-challenge-grant-2009-donations", "pageUrlRelative": "/posts/4TsSb8N8BBwtNZ2v9/singularity-institute-usd100k-challenge-grant-2009-donations", "linkUrl": "https://www.lesswrong.com/posts/4TsSb8N8BBwtNZ2v9/singularity-institute-usd100k-challenge-grant-2009-donations", "postedAtFormatted": "Wednesday, December 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20%24100K%20Challenge%20Grant%20%2F%202009%20Donations%20Reminder&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20%24100K%20Challenge%20Grant%20%2F%202009%20Donations%20Reminder%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TsSb8N8BBwtNZ2v9%2Fsingularity-institute-usd100k-challenge-grant-2009-donations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20%24100K%20Challenge%20Grant%20%2F%202009%20Donations%20Reminder%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TsSb8N8BBwtNZ2v9%2Fsingularity-institute-usd100k-challenge-grant-2009-donations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4TsSb8N8BBwtNZ2v9%2Fsingularity-institute-usd100k-challenge-grant-2009-donations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<p>In case you missed <a href=\"http://intelligence.org/blog/2009/12/23/announcing-the-2010-singularity-research-challenge/\">the notice at the SIAI Blog</a>, the Singularity Institute's next <a href=\"http://intelligence.org/challenge\">Challenge Grant</a> is now running - $100,000 of matching funds for all donations until February 28th, 2010.&nbsp; If you want your donation to be tax-deductible in the U.S. for 2009, donations postmarked by December 31st will be counted for this tax year.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 1, "Z6DgiCrMtpSNxwuYW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4TsSb8N8BBwtNZ2v9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 16, "extendedScore": null, "score": 5.489253661695327e-07, "legacy": true, "legacyId": "2057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-30T11:12:15.041Z", "modifiedAt": null, "url": null, "title": "Boksops -- Ancient Superintelligence?", "slug": "boksops-ancient-superintelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:59.915Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8wiHELLmhciyytBaP/boksops-ancient-superintelligence", "pageUrlRelative": "/posts/8wiHELLmhciyytBaP/boksops-ancient-superintelligence", "linkUrl": "https://www.lesswrong.com/posts/8wiHELLmhciyytBaP/boksops-ancient-superintelligence", "postedAtFormatted": "Wednesday, December 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Boksops%20--%20Ancient%20Superintelligence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABoksops%20--%20Ancient%20Superintelligence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wiHELLmhciyytBaP%2Fboksops-ancient-superintelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Boksops%20--%20Ancient%20Superintelligence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wiHELLmhciyytBaP%2Fboksops-ancient-superintelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8wiHELLmhciyytBaP%2Fboksops-ancient-superintelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 241, "htmlBody": "<blockquote>\n<p>[...] before long the skull came to the attention of S. H. Haughton, one of the country&rsquo;s few formally trained paleontologists. He reported his findings at a 1915 meeting of the Royal Society of South Africa. &ldquo;The cranial capacity must have been very large,&rdquo; he said, and &ldquo;calculation by the method of Broca gives a minimum figure of 1,832 cc [cubic centimeters].&rdquo; The Boskop skull, it would seem, housed a brain perhaps 25 percent or more larger than our own.</p>\n<p>The idea that giant-brained people were not so long ago walking the dusty plains of South Africa was sufficiently shocking to draw in the luminaries back in England. Two of the most prominent anatomists of the day, both experts in the reconstruction of skulls, weighed in with opinions generally supportive of Haughton&rsquo;s conclusions.</p>\n<p>The Scottish scientist Robert Broom <a class=\"external-link\" href=\"http://www.nature.com/nature/journal/v116/n2929/abs/116897a0.html\">reported</a> that &ldquo;we get for the corrected cranial capacity of the Boskop skull the very remarkable figure of 1,980 cc.&rdquo; Remarkable indeed: These measures say that the distance from Boskop to humans is greater than the distance between humans and their Homo erectus predecessors.</p>\n</blockquote>\n<p><a href=\"http://discovermagazine.com/2009/the-brain-2/28-what-happened-to-hominids-who-were-smarter-than-us\">What Happened to the Hominids who were Smarter than Us?</a></p>\n<p>I'm strongly inclined to defy the data -- true superintelligence should have just <em>dominated</em> our ancestors -- but given the expense of large skull size (primarily in difficult birthing) it also seems profoundly unlikely that a lineage would see expansion like this that <em>wasn't </em>buying them something mentally.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8wiHELLmhciyytBaP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": -1, "extendedScore": null, "score": 5.490409859689326e-07, "legacy": true, "legacyId": "2059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-30T21:36:20.869Z", "modifiedAt": null, "url": null, "title": "New Year\u2019s Resolutions Thread", "slug": "new-year-s-resolutions-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:03.502Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DZwsqCagAraMZobaP/new-year-s-resolutions-thread", "pageUrlRelative": "/posts/DZwsqCagAraMZobaP/new-year-s-resolutions-thread", "linkUrl": "https://www.lesswrong.com/posts/DZwsqCagAraMZobaP/new-year-s-resolutions-thread", "postedAtFormatted": "Wednesday, December 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year%E2%80%99s%20Resolutions%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year%E2%80%99s%20Resolutions%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZwsqCagAraMZobaP%2Fnew-year-s-resolutions-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year%E2%80%99s%20Resolutions%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZwsqCagAraMZobaP%2Fnew-year-s-resolutions-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDZwsqCagAraMZobaP%2Fnew-year-s-resolutions-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<p>I would like to propose this as a thread for people to write in their New Year&rsquo;s Resolutions (goals and sub-goals) as instrumental rationalists.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">Here are mine.</p>\n<p class=\"MsoNormal\">Goals</p>\n<p class=\"MsoNormal\">I resolve to try to enable all pareto-optimizing trades between my sub-agents can be made between them.<span>&nbsp; </span>For instance, I have agents which would like to maximize my success via improving my cognition and energy levels via eating more healthily via eating less.&nbsp; Other agents would like enjoyment from food.&nbsp; I note that these agents aren't cooperating even though they both benefit from the same changes in behavior, largely because it hasn't been pointed out to them that they are on the same side.&nbsp; If I make a serious effort to eat the hedonic utility maximizing amount, this will probably involve eating less than I default to.&nbsp; After all, food is better when one is hungry.<span>&nbsp; Most of my eating is probably driven by simple non-reflective systems that tell me to eat.&nbsp; These systems are probably promoted by hedonistic systems which are failing to understand the consequences of doing so.&nbsp; </span>In practice, this resolutoin means paying attention to the experience of eating anything that wasn&rsquo;t chosen for social or nutritional purposes, rarely clearing my plate, and rarely eating more than one would get served in a European restaurant, but above all, it means paying attention (and thus sending this information to many of my sub-agents) to the pleasure of eating when one is actually hungry.<span><br /></span></p>\n<p class=\"MsoNormal\">I resolve to find a new home, get fully moved in, get a car, touch base with all interested Bay Area supporters, and get started on the 2010 Summit by the end of February despite this involving many boring activities.<span>&nbsp; </span></p>\n<p class=\"MsoNormal\">I resolve to stop trying to keep up with a significant part of the blogosphere.<span>&nbsp; </span>My web-browsing will be limited to<span>&nbsp; </span>a) actually seeking specific information, b) checking email no more than 5-times-per-day and c) the keeping up, via google reader, with fewer than ten sites. &nbsp;</p>\n<p class=\"MsoNormal\">Also, in February I will try to use dual-n-back every day and in March I will try to publish a Less-Wrong piece every day.<span><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DZwsqCagAraMZobaP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 5.491544974553407e-07, "legacy": true, "legacyId": "2060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-30T21:37:45.042Z", "modifiedAt": null, "url": null, "title": "New Year\u2019s and New Decade\u2019s Predictions", "slug": "new-year-s-and-new-decade-s-predictions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XXrCeuyhtgB8apxSt/new-year-s-and-new-decade-s-predictions", "pageUrlRelative": "/posts/XXrCeuyhtgB8apxSt/new-year-s-and-new-decade-s-predictions", "linkUrl": "https://www.lesswrong.com/posts/XXrCeuyhtgB8apxSt/new-year-s-and-new-decade-s-predictions", "postedAtFormatted": "Wednesday, December 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year%E2%80%99s%20and%20New%20Decade%E2%80%99s%20Predictions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year%E2%80%99s%20and%20New%20Decade%E2%80%99s%20Predictions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXrCeuyhtgB8apxSt%2Fnew-year-s-and-new-decade-s-predictions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year%E2%80%99s%20and%20New%20Decade%E2%80%99s%20Predictions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXrCeuyhtgB8apxSt%2Fnew-year-s-and-new-decade-s-predictions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXrCeuyhtgB8apxSt%2Fnew-year-s-and-new-decade-s-predictions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal.dotm</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>28</o:Words> <o:Characters>160</o:Characters> <o:Company>Columbia</o:Company> <o:Lines>1</o:Lines> <o:Paragraphs>1</o:Paragraphs> <o:CharactersWithSpaces>196</o:CharactersWithSpaces> <o:Version>12.0</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridHorizontalSpacing>18 pt</w:DrawingGridHorizontalSpacing> <w:DrawingGridVerticalSpacing>18 pt</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:DontGrowAutofit /> <w:DontAutofitConstrainedTables /> <w:DontVertAlignInTxbx /> </w:Compatibility> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"276\"> </w:LatentStyles> </xml><![endif]--> &lt;!--  /* Font Definitions */ @font-face \t{font-family:Cambria; \tpanose-1:2 4 5 3 5 4 6 3 2 4; \tmso-font-charset:0; \tmso-generic-font-family:auto; \tmso-font-pitch:variable; \tmso-font-signature:3 0 0 0 1 0;}  /* Style Definitions */ p.MsoNormal, li.MsoNormal, div.MsoNormal \t{mso-style-parent:\"\"; \tmargin-top:0in; \tmargin-right:0in; \tmargin-bottom:10.0pt; \tmargin-left:0in; \tmso-pagination:widow-orphan; \tfont-size:12.0pt; \tfont-family:\"Times New Roman\"; \tmso-ascii-font-family:Cambria; \tmso-ascii-theme-font:minor-latin; \tmso-fareast-font-family:Cambria; \tmso-fareast-theme-font:minor-latin; \tmso-hansi-font-family:Cambria; \tmso-hansi-theme-font:minor-latin; \tmso-bidi-font-family:\"Times New Roman\"; \tmso-bidi-theme-font:minor-bidi;} @page Section1 \t{size:8.5in 11.0in; \tmargin:1.0in 1.25in 1.0in 1.25in; \tmso-header-margin:.5in; \tmso-footer-margin:.5in; \tmso-paper-source:0;} div.Section1 \t{page:Section1;} --&gt;  <!--[if gte mso 10]>\n<style>\n /* Style Definitions */\ntable.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin-top:0in;\n\tmso-para-margin-right:0in;\n\tmso-para-margin-bottom:10.0pt;\n\tmso-para-margin-left:0in;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ascii-font-family:Cambria;\n\tmso-ascii-theme-font:minor-latin;\n\tmso-fareast-font-family:\"Times New Roman\";\n\tmso-fareast-theme-font:minor-fareast;\n\tmso-hansi-font-family:Cambria;\n\tmso-hansi-theme-font:minor-latin;}\n</style>\n<![endif]--> <!--StartFragment-->I would like to propose this as a thread for people to write in their predictions for the next year and the next decade, when practical with probabilities attached.&nbsp; I'll make some in the comments as they come to me. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XXrCeuyhtgB8apxSt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 6e-06, "legacy": true, "legacyId": "2061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2009-12-30T21:39:09.895Z", "modifiedAt": "2022-05-28T20:22:59.752Z", "url": null, "title": "New Year's Predictions Thread", "slug": "new-year-s-predictions-thread", "viewCount": null, "lastCommentedAt": "2020-05-05T05:48:52.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelVassar", "createdAt": "2009-02-28T07:34:32.206Z", "isAdmin": false, "displayName": "MichaelVassar"}, "userId": "PvhrBWHzCGKRkwKcd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PybtwazftXzvcQSiQ/new-year-s-predictions-thread", "pageUrlRelative": "/posts/PybtwazftXzvcQSiQ/new-year-s-predictions-thread", "linkUrl": "https://www.lesswrong.com/posts/PybtwazftXzvcQSiQ/new-year-s-predictions-thread", "postedAtFormatted": "Wednesday, December 30th 2009", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Predictions%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Predictions%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPybtwazftXzvcQSiQ%2Fnew-year-s-predictions-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Predictions%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPybtwazftXzvcQSiQ%2Fnew-year-s-predictions-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPybtwazftXzvcQSiQ%2Fnew-year-s-predictions-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>I would like to propose this as a thread for people to write in their predictions for the next year and the next decade, when practical with probabilities attached. I'll probably make some in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"33BrBRSrRQS4jEHdk": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PybtwazftXzvcQSiQ", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 32, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "2062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 454, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2009-12-30T21:39:09.895Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-01T16:41:31.102Z", "modifiedAt": null, "url": null, "title": "Are wireheads happy?", "slug": "are-wireheads-happy", "viewCount": null, "lastCommentedAt": "2019-11-29T23:49:02.419Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HmfxSWnqnK265GEFM/are-wireheads-happy", "pageUrlRelative": "/posts/HmfxSWnqnK265GEFM/are-wireheads-happy", "linkUrl": "https://www.lesswrong.com/posts/HmfxSWnqnK265GEFM/are-wireheads-happy", "postedAtFormatted": "Friday, January 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20wireheads%20happy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20wireheads%20happy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHmfxSWnqnK265GEFM%2Fare-wireheads-happy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20wireheads%20happy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHmfxSWnqnK265GEFM%2Fare-wireheads-happy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHmfxSWnqnK265GEFM%2Fare-wireheads-happy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1476, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/15h/utilons_vs_hedons/\">Utilons vs. Hedons</a>, <a href=\"/lw/15c/would_your_real_preferences_please_stand_up\">Would Your Real Preferences Please Stand Up</a></p>\n<p>And I don't mean that question in the semantic \"but what is happiness?\" sense, or in the deep philosophical \"but can anyone not facing struggle and adversity truly be happy?\" sense. I mean it in the totally literal sense. Are wireheads having fun?<br /><br />They look like they are. People and animals connected to wireheading devices get upset when the wireheading is taken away and will do anything to get it back. And it's electricity shot directly into the reward center of the brain. What's not to like?<br /><br />Only now neuroscientists are starting to recognize a difference between \"reward\" and \"pleasure\", or call it \"wanting\" and \"liking\". The two are usually closely correlated. You want something, you get it, then you feel happy. The simple principle behind our entire consumer culture. But do neuroscience and our own experience really support that?</p>\n<p><a id=\"more\"></a></p>\n<p>It would be too easy to point out times when people want things, get them, and then later realize they weren't so great. That could be a simple case of misunderstanding the object's true utility. What about wanting something, getting it, realizing it's not so great, and then wanting it just as much the next day? Or what about not wanting something, getting it, realizing it makes you very happy, and then continuing not to want it?</p>\n<p>The first category, \"things you do even though you don't like them very much\" sounds like many drug addictions. Smokers may enjoy smoking, and they may want to avoid the physiological signs of withdrawl, but neither of those is enough to explain their reluctance to quit smoking. I don't smoke, but I made the mistake of starting a can of Pringles yesterday. If you asked me my favorite food, there are dozens of things I would say before \"Pringles\". Right now, and for the vast majority of my life, I feel no desire to go and get Pringles. But once I've had that first chip, my motivation for a second chip goes through the roof, without my subjective assessment of how tasty Pringles are changing one bit.</p>\n<p>Think of the second category as \"things you procrastinate even though you like them.\" I used to think procrastination applied only to things you disliked but did anyway. Then I tried to write a novel. I loved writing. Every second I was writing, I was thinking \"This is so much fun\". And I never got past the second chapter, because I just couldn't motivate myself to sit down and start writing. Other things in this category for me: going on long walks, doing yoga, reading fiction. I can know with near certainty that I will be happier doing X than Y, and still go and do Y.<br /><br />Neuroscience provides some basis for this. A University of Michigan study analyzed the brains of rats eating a favorite food. They found separate circuits for \"wanting\" and \"liking\", and were able to knock out either circuit without affecting the other (it was actually kind of cute - they measured the number of times the rats licked their lips as a proxy for \"liking\", though of course they had a highly technical rationale behind it). When they knocked out the \"liking\" system, the rats would eat exactly as much of the food without making any of the satisifed lip-licking expression, and areas of the brain thought to be correlated with pleasure wouldn't show up in the MRI. Knock out \"wanting\", and the rats seem to enjoy the food as much when they get it but not be especially motivated to seek it out. To quote the science<sup>1</sup>:</p>\n<blockquote>\n<p>Pleasure and desire circuitry have intimately connected but distinguishable neural substrates. Some investigators believe that the role of the mesolimbic dopamine system is not primarily to encode pleasure, but \"wanting\" i.e. incentive-motivation. On this analysis, endomorphins and enkephalins - which activate mu and delta opioid receptors most especially in the ventral pallidum - are most directly implicated in pleasure itself. Mesolimbic dopamine, signalling to the ventral pallidum, mediates desire. Thus \"dopamine overdrive\", whether natural or drug-induced, promotes a sense of urgency and a motivation to engage with the world, whereas direct activation of mu opioid receptors in the ventral pallidum induces emotionally self-sufficient bliss.</p>\n</blockquote>\n<p>The wanting system is activated by dopamine, and the liking system is activated by opioids. There are enough connections between them that there's a big correlation in their activity, but the correlation isn't one and in fact activation of the opioids is less common than the dopamine. Another quote:</p>\n<blockquote>\n<p>It's relatively hard for a brain to generate pleasure, because it needs to activate different opioid sites together to make you like something more. It's easier to activate desire, because a brain has several 'wanting' pathways available for the task. Sometimes a brain will like the rewards it wants. But other times it just wants them.</p>\n</blockquote>\n<p>So you could go through all that trouble to find a black market brain surgeon who'll wirehead you, and you'll end up not even being happy. You'll just really really want to keep the wirehead circuit running.</p>\n<p>Problem: large chunks of philosophy and economics are based upon wanting and liking being the same thing.<br /><br />By definition, if you choose X over Y, then X is a higher utility option than Y. That means utility represents wanting and not liking.<del> But good utilitarians (and, presumably, artificial intelligences) try to maximize utility</del> (<a href=\"/lw/1lb/are_wireheads_happy/1dx1\">or do they?</a>). This correlates contingently with maximizing happiness, but not necessarily. In a worst-case scenario, it might not correlate at all - two possible such scenarios being wireheading and an AI <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">without the appropriate common sense</a>.<br /><br />Thus the deep and heavy ramifications. A more down-to-earth example came to mind when I was reading something by Steven Landsburg recently (not recommended). I don't have the exact quote, but it was something along the lines of:</p>\n<blockquote>\n<p>According to a recent poll, two out of three New Yorkers say that, given the choice, they would rather live somewhere else. But all of them have the choice, and none of them live anywhere else. A proper summary of the results of this poll would be: two out of three New Yorkers lie on polls.</p>\n</blockquote>\n<p>This summarizes a common strain of thought in economics, the idea of \"revealed preferences\". People tend to say they like a lot of things, like family or the environment or a friendly workplace. Many of the same people who say these things then go and ignore their families, pollute, and take high-paying but stressful jobs. The traditional economic explanation is that the people's actions reveal their true preferences, and that all the talk about caring about family and the environment is just stuff people say to look good and gain status. If a person works hard to get lots of money, spends it on an iPhone, and doesn't have time for their family, the economist will say that this proves that they value iPhones more than their family, no matter what they may say to the contrary.<br /><br />The difference between enjoyment and motivation provides an argument that could rescue these people. It may be that a person really does enjoy spending time with their family more than they enjoy their iPhone, but they're more motivated to work and buy iPhones than they are to spend time with their family. If this were true, people's introspective beliefs and public statements about their values would be true as far as it goes, and their tendency to work overtime for an iPhone would be as much a \"hijacking\" of their \"true preferences\" as a revelation of them. This accords better with my introspective experience, with happiness research, and with common sense than the alternative.<br /><br />Not that the two explanations are necessarily entirely contradictory. One could come up with a story about how people are motivated to act selfishly but enjoy acting morally, which allows them to tell others a story about how virtuous they are while still pursuing their own selfish gain.<br /><br />Go too far toward the liking direction, and you risk something different from wireheading only in that the probe is stuck in a different part of the brain. Go too far in the wanting direction, and you risk people getting lots of shiny stuff they thought they wanted but don't actually enjoy. So which form of good should altruists, governments, FAIs, and other agencies in the helping people business respect? <br /><br /><strong>Sources/Further Reading: <br /></strong></p>\n<p>1. <a href=\"http://www.wireheading.com/\">Wireheading.com</a>, especially on a particular <a href=\"http://www.wireheading.com/pleasure/liking-wanting.html\">University of Michigan study</a></p>\n<p>2. New York Times: <a href=\"http://www.nytimes.com/2009/10/27/science/27angier.html\">A Molecule of Motivation, Dopamine Excels at its Task</a></p>\n<p>3. Slate: <a href=\"http://www.slate.com/id/2224932/pagenum/all/\">The Powerful and Mysterious Brain Circuitry...</a></p>\n<p>4. Related journal articles (<a href=\"http://www.springerlink.com/content/n2l74j4847227n83/fulltext.html\">1</a>, <a href=\"http://www.lsa.umich.edu/psych/research&amp;labs/berridge/publications/Berridge Robinson &amp; Aldridge Dissecting components of reward Current Opin Pharm 2009.pdf\">2</a>, <a href=\"http://www.sciencedirect.com/science?_ob=ArticleURL&amp;_udi=B6T0P-4S8K9GC-1&amp;_user=10&amp;_rdoc=1&amp;_fmt=&amp;_orig=search&amp;_sort=d&amp;_docanchor=&amp;view=c&amp;_searchStrId=1150540088&amp;_rerunOrigin=google&amp;_acct=C000050221&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=cef4ec76f1d392d5227e7a6cc182c9f0\">3</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yEs5Tdwfw5Zw8yGWC": 11, "Jzm2mYuuDBCNWq8hi": 1, "iP2X4jQNHMWHRNPne": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HmfxSWnqnK265GEFM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 145, "baseScore": 164, "extendedScore": null, "score": 0.000265, "legacy": true, "legacyId": "2063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 164, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yYTv6J6wFL8XK3Q7o", "z3cTkXbA7jgwGWPcv", "4ARaTpNX62uaL86j6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-01T17:02:39.373Z", "modifiedAt": null, "url": null, "title": "Open Thread: January 2010", "slug": "open-thread-january-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:37.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DMygYdCG5wPnHThgo/open-thread-january-2010", "pageUrlRelative": "/posts/DMygYdCG5wPnHThgo/open-thread-january-2010", "linkUrl": "https://www.lesswrong.com/posts/DMygYdCG5wPnHThgo/open-thread-january-2010", "postedAtFormatted": "Friday, January 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20January%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20January%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMygYdCG5wPnHThgo%2Fopen-thread-january-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20January%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMygYdCG5wPnHThgo%2Fopen-thread-january-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMygYdCG5wPnHThgo%2Fopen-thread-january-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6, "htmlBody": "<p>And happy new year to everyone.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DMygYdCG5wPnHThgo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 5.496289848280781e-07, "legacy": true, "legacyId": "2067", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 761, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-02T19:14:21.432Z", "modifiedAt": null, "url": null, "title": "Stigmergy and Pickering's Mangle", "slug": "stigmergy-and-pickering-s-mangle", "viewCount": null, "lastCommentedAt": "2017-06-17T03:52:59.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Johnicholas", "createdAt": "2009-02-27T15:01:52.708Z", "isAdmin": false, "displayName": "Johnicholas"}, "userId": "kBvTXutfPytNtzPyD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NLNnwd4TeKj8PBvZa/stigmergy-and-pickering-s-mangle", "pageUrlRelative": "/posts/NLNnwd4TeKj8PBvZa/stigmergy-and-pickering-s-mangle", "linkUrl": "https://www.lesswrong.com/posts/NLNnwd4TeKj8PBvZa/stigmergy-and-pickering-s-mangle", "postedAtFormatted": "Saturday, January 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stigmergy%20and%20Pickering's%20Mangle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStigmergy%20and%20Pickering's%20Mangle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLNnwd4TeKj8PBvZa%2Fstigmergy-and-pickering-s-mangle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stigmergy%20and%20Pickering's%20Mangle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLNnwd4TeKj8PBvZa%2Fstigmergy-and-pickering-s-mangle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNLNnwd4TeKj8PBvZa%2Fstigmergy-and-pickering-s-mangle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1024, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Stigmergy\">Stigmergy</a> is a notion that an agent's behavior is sometimes best understood as coordinated by the agent's environment. In particular, social insects build nests, which have a recognizable standard pattern (different patterns for different species). Does the wasp or termite have an idea of what the standard pattern is? Probably not. Instead, the computation inside the insect is a stateless stimulus/response rule set. The partially-constructed nest catalyzes the next construction step.</p>\n<p>An unintelligent \"insect\" clambering energetically around a convoluted \"nest\", with the insect's local perceptions driving its local modifications is recognizably something like a Turing machine. The system as a whole can be more intelligent than either the (stateless) insect or the (passive) nest. The important computation is the interactions between the agent and the environment.</p>\n<p><a id=\"more\"></a></p>\n<p>Theraulaz and Bonabeau have simulated lattice swarms, and gotten some surprisingly realistic wasp-nest-like constructions. A&nbsp;<a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.3562&amp;rep=rep1&amp;type=pdf\">paper</a>&nbsp;is in CiteSeer, but this <a href=\"http://www.cs.utk.edu/~mclennan/Classes/420-594-F04/handouts/Lecture-10.pdf\">summary</a>&nbsp;gives a better rapid overview.</p>\n<p>Humans modify the environment (e.g. by writing books and storing them in libraries), and human behavior is affected by their environment (e.g. by reading books). Wikipedia is an excellent example of what human-human stigmergic coordination looks like. Instead of interacting directly with one another, each edit leaves a trace, and future edits respond to the trace (this impersonal interaction may avoid some biases towards face-saving and status-seeking).</p>\n<p>Andrew Pickering is a sociologist who studies science and technology. He wrote a book called&nbsp;\"<a href=\"http://www.press.uchicago.edu/presssite/metadata.epl?mode=synopsis&amp;bookkey=3642386\">The Mangle of Practice</a>\". He includes in his sociology non-human \"actors\". For example, he would say that a bubble chamber acts on a human observer when the observer sees a tracery of bubbles after a particle physics experiment.&nbsp;This makes his theory less society-centric and more recognizable to a non-sociologist.</p>\n<p>As a programmer, the best way I can explain Pickering's mangle is by reference to programming.&nbsp;In trying to accomplish something using a computer, you start with a goal, a desired \"capture of machinic agency\". You interact with the computer, alternating between human-acts-on-computer (edit) phases, and computer-acts-on-human (run) phases. In this process, the computer may display \"resistances\" and, as a consequence, you might change your goals. Not all things are possible or feasible, and one way that we discover impossibilities and infeasibilities is via these resistances. Pickering would say that your goals have been \"mangled\". Symmetrically, the computer program gets mangled by your agency (mangled into existence, even).</p>\n<p>Pickering says that all of science and technology can be described by an network including both human and non-human actors, mangling each other over time, and in his book he has some carefully-worked out examples - Donald Glaser's invention of the bubble chamber, Morpurgo's experiments measuring a upper bound on the number of free quarks and Hamilton's invention of quaternions, and a few more.</p>\n<p>I hope you find these notions (stigmergy and the mangle) as provocative and intriguing as I do. The rest of this post is my own thoughts, far more speculative and probably not as valuable.</p>\n<p>Around each individual is a shell of physical traces that they have made - books that they've chosen to keep nearby, mementos and art that they have collected, documents that they've written. At larger radiuses, those shells become sparser and intermingle more, but eventually those physical traces comprise a lot of what we call \"civilization\". Should a person's shell of traces be considered integral to their identity?&nbsp;</p>\n<p>Most of the dramatic increases in our civilization's power and knowledge over the past few thousand years have been improvements in these stigmergic traces. Does this suggest that active, deliberate stigmergy is an appropriate self-improvement technique, in rationality and other desirable traits? Maybe exoself software would be a good human rationality-improving project. I wrote a little seed called <a href=\"http://www.johnicholas.com/exomustard.c\">exomustard</a>, but it doesn't do much of anything.</p>\n<p>Might it be possible for some form of life to exist within the interaction between humans and their environment? Perhaps the network of roads, cars, and car-driving could be viewed as a form of life. If all physical roads and cars were erased, humans would remember them and build them again. If all memory and experience of roads and cars were erased, humans would discover the use of the physical objects quickly. But if both were erased simultaneously, it seems entirely plausible that some other form of transportation would become dominant. Nations are another example. These entities self-catalyze and maintain their existence and some of their properties in the face of change, which are lifelike properties.</p>\n<p>What would conflict between a human and a stigmergic, mangled-into-existance \"capture of machinic agency\" look like? At first glance, this notion seems like some quixotic quest to defeat the idea and existence of automobiles (or even windmills). However, the mangle does include the notion of conflict already, as \"resistances\". Some resistances, like the speed of light or the semi-conservation of entropy, we're probably going to have to live with. Those aren't the ones we're interested in. There are also accidental resistances due to choices earlier in the mangling process.</p>\n<p>Bob Martin has a <a href=\"http://www.objectmentor.com/resources/articles/Principles_and_Patterns.pdf\">paper</a> where he lists some symptoms of bad design - rigidity, fragility, immobility, viscosity. We might informally say \"The system *wants* to do such-and-so.\", often some sort of inertia or otherwise continuing on a previous path. These are examples of accidental resistances that humans chose to mangle into existence, and then later regret. Every time you find yourself saying \"well, it's not good, but it's what we have and it would be too expensive/risky/impractical to change\", you're finding yourself in conflict with a stigmergic pattern. &nbsp;</p>\n<p>Paul Grignon has a video \"<a href=\"http://video.google.com/videoplay?docid=-2550156453790090544&amp;ei=e4E_S4nCHITGrALhzcwX&amp;q=money+as+debt&amp;view=3\">Money as Debt</a>\", that describes a world where we have built an institution gradually over centuries which is powerful and which (homeostatically) defends its own existence, but also (due to its size, power, and accidentally-built-in drive) steers the world toward disaster. The video twigs a lot of my conspiracy-theory sensors, but Paul Grignon's specific claims are not necessary for the general principle to be sound: we can build institutions into our civilization that subsequently have powerful steering effects on our civilization - steering the civilization into a collision course with a wall, maybe.</p>\n<p>In conclusion, stigmergy and Pickering's mangle are interesting and provocative ideas and might be useful building blocks for techniques to increase human rationality and reduce existential risk.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NLNnwd4TeKj8PBvZa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 18, "extendedScore": null, "score": 5.499154691023445e-07, "legacy": true, "legacyId": "2065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-03T10:33:46.067Z", "modifiedAt": null, "url": null, "title": "Drawing Two Aces", "slug": "drawing-two-aces", "viewCount": null, "lastCommentedAt": "2020-05-12T12:27:48.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Hug2ePykMkmPzSsx6/drawing-two-aces", "pageUrlRelative": "/posts/Hug2ePykMkmPzSsx6/drawing-two-aces", "linkUrl": "https://www.lesswrong.com/posts/Hug2ePykMkmPzSsx6/drawing-two-aces", "postedAtFormatted": "Sunday, January 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Two%20Aces&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Two%20Aces%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHug2ePykMkmPzSsx6%2Fdrawing-two-aces%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Two%20Aces%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHug2ePykMkmPzSsx6%2Fdrawing-two-aces", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHug2ePykMkmPzSsx6%2Fdrawing-two-aces", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 301, "htmlBody": "<p>Suppose I have a deck of four cards:&nbsp; The ace of spades, the ace of hearts, and two others (say, 2C and 2D).</p>\n<p>You draw two cards at random.</p>\n<p><em>Scenario 1:</em>&nbsp; I ask you \"Do you have the ace of spades?\"&nbsp; You say \"Yes.\"&nbsp; Then the probability that you are holding both aces is 1/3:&nbsp; There are three equiprobable arrangements of cards you could be holding that contain AS, and one of these is AS+AH.</p>\n<p><em>Scenario 2:</em>&nbsp; I ask you \"Do you have an ace?\"&nbsp; You respond \"Yes.\"&nbsp; The probability you hold both aces is 1/5:&nbsp; There are five arrangements of cards you could be holding (all except 2C+2D) and only one of those arrangements is AS+AH.</p>\n<p>Now suppose I ask you \"Do you have an ace?\"</p>\n<p>You say \"Yes.\"</p>\n<p>I then say to you:&nbsp; \"Choose one of the aces you're holding at random (so if you have only one, pick that one).&nbsp; Is it the ace of spades?\"</p>\n<p>You reply \"Yes.\"</p>\n<p>What is the probability that you hold two aces?</p>\n<p><em>Argument 1:</em>&nbsp; I now know that you are holding at least one ace and that one of the aces you hold is the ace of spades, which is just the same state of knowledge that I obtained in Scenario 1.&nbsp; Therefore the answer must be 1/3.</p>\n<p><em>Argument 2:</em>&nbsp; In Scenario 2, I know that I can <em>hypothetically</em> ask you to choose an ace you hold, and you must <em>hypothetically </em>answer that you chose either the ace of spades or the ace of hearts.&nbsp; My posterior probability that you hold two aces should be the same either way.&nbsp; <a href=\"/lw/ii/conservation_of_expected_evidence/\">The expectation of my future probability must equal my current probability:</a>&nbsp; If I expect to change my mind later, I should just give in and change my mind now.&nbsp; Therefore the answer must be 1/5.</p>\n<p>Naturally I know which argument is correct.&nbsp; Do you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hLp77TQsRkooioj86": 1, "JHYaBGQuuKHdwnrAK": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Hug2ePykMkmPzSsx6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "2069", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-04T00:02:04.868Z", "modifiedAt": null, "url": null, "title": "Max Tegmark on our place in history: \"We're Not Insignificant After All\"", "slug": "max-tegmark-on-our-place-in-history-we-re-not-insignificant", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:00.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iizNtbgNMBtDQcHdm/max-tegmark-on-our-place-in-history-we-re-not-insignificant", "pageUrlRelative": "/posts/iizNtbgNMBtDQcHdm/max-tegmark-on-our-place-in-history-we-re-not-insignificant", "linkUrl": "https://www.lesswrong.com/posts/iizNtbgNMBtDQcHdm/max-tegmark-on-our-place-in-history-we-re-not-insignificant", "postedAtFormatted": "Monday, January 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Max%20Tegmark%20on%20our%20place%20in%20history%3A%20%22We're%20Not%20Insignificant%20After%20All%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMax%20Tegmark%20on%20our%20place%20in%20history%3A%20%22We're%20Not%20Insignificant%20After%20All%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiizNtbgNMBtDQcHdm%2Fmax-tegmark-on-our-place-in-history-we-re-not-insignificant%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Max%20Tegmark%20on%20our%20place%20in%20history%3A%20%22We're%20Not%20Insignificant%20After%20All%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiizNtbgNMBtDQcHdm%2Fmax-tegmark-on-our-place-in-history-we-re-not-insignificant", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiizNtbgNMBtDQcHdm%2Fmax-tegmark-on-our-place-in-history-we-re-not-insignificant", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1026, "htmlBody": "<p>An uplifting message as we enter the new year, <a href=\"http://www.edge.org/q2007/q07_7.html#tegmark\">quoted from Edge.org</a>:</p>\n<blockquote>\n<p><strong>We're Not Insignificant After All</strong><br /><br />Max Tegmark, Physicist, MIT<br /><br />When gazing up on a clear night, it's easy to feel insignificant. Since our earliest ancestors admired the stars, our human egos have suffered a series of blows. For starters, we're smaller than we thought. Eratosthenes showed that Earth was larger than millions of humans, and his Hellenic compatriots realized that the solar system was thousands of times larger still. Yet for all its grandeur, our Sun turned out to be merely one rather ordinary star among hundreds of billions in a galaxy that in turn is merely one of billions in our observable universe, the spherical region from which light has had time to reach us during the 14 billion years since our big bang. Then there are probably more (perhaps infinitely many) such regions. Our lives are small temporally as well as spatially: if this 14 billion year cosmic history were scaled to one year, then 100,000 years of human history would be 4 minutes and a 100 year life would be 0.2 seconds. Further deflating our hubris, we've learned that we're not that special either. Darwin taught us that we're animals, Freud taught us that we're irrational, machines now outpower us, and just last month, Deep Fritz outsmarted our Chess champion Vladimir Kramnik. Adding insult to injury, cosmologists have found that we're not even made out of the majority substance.</p>\n<p>The more I learned about this, the less significant I felt. Yet in recent years, I've suddenly turned more optimistic about our cosmic significance. I've come to believe that advanced evolved life is very rare, yet has huge growth potential, making our place in space and time remarkably significant.<br /><a id=\"more\"></a></p>\nThe nature of life and consciousness is of course a hotly debated subject. My guess is that these phenomena can exist much more generally that in the carbon-based examples we know of.<br /><br />I believe that consciousness is, essentially, the way information feels when being processed. Since matter can be arranged to process information in numerous ways of vastly varying complexity, this implies a rich variety of levels and types of consciousness. The particular type of consciousness that we subjectively know is then a phenomenon that arises in certain highly complex physical systems that input, process, store and output information. Clearly, if atoms can be assembled to make humans, the laws of physics also permit the construction of vastly more advanced forms of sentient life. Yet such advanced beings can probably only come about in a two-step process: first intelligent beings evolve through natural selection, then they choose to pass on the torch of life by building more advanced consciousness that can further improve itself.<br /><br />Unshackled by the limitations of our human bodies, such advanced life could rise up and eventually inhabit much of our observable universe. Science fiction writers, AI-aficionados and transhumanist thinkers have long explored this idea, and to me the question isn't if it can happen, but if it will happen.<br /><br />My guess is that evolved life as advanced as ours is very rare. Our universe contains countless other solar systems, many of which are billions of years older than ours. Enrico Fermi pointed out that if advanced civilizations have evolved in many of them, then some have a vast head start on us &mdash; so where are they? I don't buy the explanation that they're all choosing to keep a low profile: natural selection operates on all scales, and as soon as one life form adopts expansionism (sending off rogue self-replicating interstellar nanoprobes, say), others can't afford to ignore it. My personal guess is that we're the only life form in our entire observable universe that has advanced to the point of building telescopes, so let's explore that hypothesis. It was the cosmic vastness that made me feel insignificant to start with. Yet those galaxies are visible and beautiful to us &mdash; and only us. It is only we who give them any meaning, making our small planet the most significant place in our observable universe.<br /><br />Moreover, this brief century of ours is arguably the most significant one in the history of our universe: the one when its meaningful future gets decided. We'll have the technology to either self-destruct or to seed our cosmos with life. The situation is so unstable that I doubt that we can dwell at this fork in the road for more than another century. If we end up going the life route rather than the death route, then in a distant future, our cosmos will be teeming with life that all traces back to what we do here and now. I have no idea how we'll be thought of, but I'm sure that we won't be remembered as insignificant.\n<p>&nbsp;</p>\n</blockquote>\n<p>A few thoughts: when considering the heavy skepticism that the singularity hypothesis receives, it is important to remember that there is a much weaker hypothesis, highlighted here by Tegmark, that still has extremely counter-intuitive implications about our place in spacetime; one might call it the bottleneck hypothesis - the hypothesis that 21st century humanity occupies a pivotal place in the evolution of the universe, simply because we may well be a part of the small space/time window during which it is decided whether earth-originating life will colonize the universe or not.<br /><br />The bottleneck hypothesis is weaker than the singularity hypothesis - we can be at the bottleneck even if smarter-than-human AI is impossible or extremely impractical, but if smarter-than-human AI is possible and reasonably practical, then we are surely at the bottleneck of the universe. The bottleneck hypothesis is based upon less controversial science than the singularity hypothesis, and is robust to different assumptions about what is feasible in an engineering sense (AI/no AI, ems/no ems, <a href=\"http://en.wikipedia.org/wiki/Project_Orion_%28nuclear_propulsion%29#Performance\">nuclear rockets</a>/<a href=\"http://en.wikipedia.org/wiki/Generation_ship#Biology_and_society\">generation ships</a>/<a href=\"http://en.wikipedia.org/wiki/Cryonics\">cryonics advances</a>, etc) so might be accepted by a larger number of people.<br /><br />Related is <a href=\"http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html\">Hanson's \"Dream Time\" idea</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iizNtbgNMBtDQcHdm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 23, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "2070", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-04T01:09:04.349Z", "modifiedAt": null, "url": null, "title": "Disclosure vs. Bans: Reply to Robin Hanson", "slug": "disclosure-vs-bans-reply-to-robin-hanson", "viewCount": null, "lastCommentedAt": "2017-06-17T03:57:30.696Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X3GasHitwxquQbdFm/disclosure-vs-bans-reply-to-robin-hanson", "pageUrlRelative": "/posts/X3GasHitwxquQbdFm/disclosure-vs-bans-reply-to-robin-hanson", "linkUrl": "https://www.lesswrong.com/posts/X3GasHitwxquQbdFm/disclosure-vs-bans-reply-to-robin-hanson", "postedAtFormatted": "Monday, January 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Disclosure%20vs.%20Bans%3A%20Reply%20to%20Robin%20Hanson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADisclosure%20vs.%20Bans%3A%20Reply%20to%20Robin%20Hanson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3GasHitwxquQbdFm%2Fdisclosure-vs-bans-reply-to-robin-hanson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Disclosure%20vs.%20Bans%3A%20Reply%20to%20Robin%20Hanson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3GasHitwxquQbdFm%2Fdisclosure-vs-bans-reply-to-robin-hanson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX3GasHitwxquQbdFm%2Fdisclosure-vs-bans-reply-to-robin-hanson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 952, "htmlBody": "<p>A little while back I wrote a <a href=\"/lw/1kd/mandating_information_disclosure_vs_banning/\" target=\"_blank\">post</a> arguing that the existence of abusive terms in credit card contracts (such as huge jumps in interest rates for being one day late with a payment) do not satisfy the conditions for standard economic models of asymmetric information between rational agents, but rather are trickery, pure and simple. If this is right, then the standard remedy of mandating the provision of more information to the less-informed party, but not otherwise interfering in the market (the idea being that any voluntary agreement must make both parties better off, no matter how strange or one-sided the terms may appear, so any interference in contracts beyond providing information will reduce welfare), is not the right one. There is no decent argument that those terms would appear in any contract where both parties knew what they were doing, so if you see terms like that, the appropriate conclusion is that someone has been screwed, not that Goddess of Capitalism, in her infinite-but-inscrutable wisdom, has uncovered the only terms that, strange as they may seem to mere mortals, make a mutually beneficial contract possible. The goal is to get rid of those terms, and the most direct way to do that is simply to prohibit them. There are some good reasons to be reluctant to have the government go around prohibiting things, so mandatory disclosure might still be a good policy (though the <a href=\"http://www.federalreserve.gov/newsevents/press/bcreg/20081218a.htm\" target=\"_blank\">Federal Reserve</a> has investigated this and concluded that it isn't), but the goal would be to use the disclosures <em>to eliminate the abusive terms</em>. There is no justification for the standard economist's agnosticism about whether the terms are good or not: they're bad and the only question is how best to get rid of them.</p>\n<p>Robin Hanson left some comments to that post, in which he made the point that since people voluntarily choose these terms, they must like them and so prohibiting them would have to mean protecting people against their will. I answered that while I'm enough of a paternalist to be willing, under some circumstances, to impose limited protections on people even if those people would oppose them, that I didn't think that was an issue here, as I would guess (though I have no proof), that the Federal Reserve's recent decision to ban certain credit card practices was probably very popular, even (especially?) among the people who are harmed by those practices. Robin's reply, as I understand it, is that this may be true, but since people can't simultaneously want to accept credit cards with those terms and at the same time favor banning those terms, it must be the case that they either don't understand the terms of the credit card contracts or they don't understand the effects of the ban. Somewhere there must just be some missing information, and therefore we must be back where we started, with the problem being a lack of information that could be resolved by providing more information.<br /><a id=\"more\"></a></p>\n<p>So I take Robin to be saying that bans such as those instituted by the Fed cannot be shown to be non-coercive to credit card customers simply by recourse to the hypothesized \"fact\" that the bans are popular, because anyone who voluntarily chooses those terms and also supports the ban must be being inconsistent somehow. He also seems to be saying that this inconsistency means that we're just back in the world of standard economic models where one side is ignorant.<br /><br />On the second point, either I am misunderstanding Robin or I think he's simply wrong. As I understand them, standard models of asymmetric information do <em>not</em> result in the ignorant party just getting screwed. Rather, they result in otherwise beneficial exchanges not happening, or in the terms being distorted in ways that come from the fact that one party is not informed (Robin, let me know if I misunderstand you or if you disagree). So even if everything else Robin says is right, it's still not the case that we're in a world where the problem is plain-vanilla asymmetric information, and where the solution is clearly to provide more information but not to ban. Robin might argue that people are in fact getting screwed but that the cure of banning is worse than the disease, but I don't see how he can argue that the central problem here is asymmetric information.<br /><br />As for the first point, a couple of commenters pointed out that the inconsistency of preferences that Robin points to are not more irrational than the kinds of preferences that we see people have all the time. I think they're right about this, but I think there's a more direct way to square the apparent inconsistency. People today \"voluntarily\" accept those provisions because that's the way to get a credit card. In the world as it currently exists, it's those terms or nothing (with the limited exception of cards issued by credit unions and the like, which avoid such trickery)* and so people \"choose\" those terms. But they'd be happier in a world whether the equilibrium credit card terms are better, and they would prefer to be able to \"choose\" those. I don't want to overstate this point, as I think what's really going on is that people are badly confused and also (justifiably) hostile to credit card companies. But there is a perfectly sound story in which people would choose the terms and also approve of the ban.</p>\n<p>BTW, for a neat example of how people trick and make no bones about the fact that that's what they're doing, see <a href=\"http://nymag.com/restaurants/features/62498/\" target=\"_blank\">here</a>.</p>\n<p>*I would be very interested to know what fraction of people who have access to such alternatives use them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X3GasHitwxquQbdFm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "2071", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WyAGQqw2v3yS8BkjM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-04T09:09:23.345Z", "modifiedAt": null, "url": null, "title": "When does an insight count as evidence?", "slug": "when-does-an-insight-count-as-evidence", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:00.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Fi5PWBQ5xhgjEWwJL/when-does-an-insight-count-as-evidence", "pageUrlRelative": "/posts/Fi5PWBQ5xhgjEWwJL/when-does-an-insight-count-as-evidence", "linkUrl": "https://www.lesswrong.com/posts/Fi5PWBQ5xhgjEWwJL/when-does-an-insight-count-as-evidence", "postedAtFormatted": "Monday, January 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20When%20does%20an%20insight%20count%20as%20evidence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhen%20does%20an%20insight%20count%20as%20evidence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFi5PWBQ5xhgjEWwJL%2Fwhen-does-an-insight-count-as-evidence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=When%20does%20an%20insight%20count%20as%20evidence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFi5PWBQ5xhgjEWwJL%2Fwhen-does-an-insight-count-as-evidence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFi5PWBQ5xhgjEWwJL%2Fwhen-does-an-insight-count-as-evidence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 815, "htmlBody": "<p>Bayesianism, as it is presently formulated, concerns the evaluation of the probability of beliefs in light of some background information. In particular, given a particular state of knowledge, probability theory says that there is exactly one probability that should be assigned to any given input statement. A simple corrolary is that if two agents with identical states of knowledge arrive at different probabilities for a particular belief then at least one of them is irrational.</p>\n<p>A thought experiment. Suppose I ask you for the probability that <a href=\"http://en.wikipedia.org/wiki/P_versus_NP_problem\">P=NP</a> (a famous unsolved computer science problem). Sounds like a difficult problem, I know, but thankfully all relevant information has been provided for you --- namely the axioms of set theory! Now we know that either P=NP is proveable from the axioms of set theory, or its converse is (or neither is proveable, but let's ignore that case for now). The problem is that you are unlikely to solve the P=NP problem any time soon.</p>\n<p>So being the pragmatic rationalist that you are, you poll the world's leading mathematicians, and do some research of your own into the P=NP problem and the history of difficult mathematical problems in general to gain insight into perhaps which group of mathematicians may be more reliable, and to what extent thay may be over- or under-confident in their beliefs. After weighing all the evidence honestly and without bias you submit your carefully-considered probability estimate, feeling like a pretty good rationalist. So you didn't solve the P=NP problem, but how could you be expected to when it has eluded humanity's finest mathematicians for decades? The axioms of set theory may in principle be sufficient to solve the problem but the structure of the proof is unknown to you, and herein lies information that would be useful indeed but is unavailable at present. You cannot be considered irrational for failing to reason from unavailable information, you say; rationality only commits you to using the information that is actually available to you, and you have done so. Very well.</p>\n<p><a id=\"more\"></a></p>\n<p>The next day you are discussing probability theory with a friend, and you describe the <a href=\"http://yudkowsky.net/rational/bayes\">one-in-a-million-illness</a> problem, which asks for the probability that a patient has a particular illness, which is known to exist within only one in a million individuals, given that a particular diagnostic test with known 1% false positive rate has returned positive. Sure enough, your friend intuits that there is a high chance that the patient has the illness and you proceed to explain why this is not actually the rational answer.</p>\n<p>\"Very well\", your friend says, \"I accept your explanation but I when I gave my previous assessment I was unaware of this line of reasoning. I understand the correct solution now and will update my probability assignment in light of this new evidence, but my previous answer was made in the absence of this information and was rational given my state of knowledge at that point.\"</p>\n<p>\"Wrong\", you say, \"no new information has been injected here, I have simply pointed out how to reason rationally. Two rational agents cannot take the same information and arrive at different probability assignments, and thinking clearly does not constitute new information. Your previous estimate was irrational, full stop.\"</p>\n<p>By now you've probably guessed where I'm going with this. It seems reasonable to assign some probability to the P=NP problem in the absence of a solution to the mathematical problem, and in the future, if the problem is solved, it seems reasonable that a different probability would be assigned. The only way both assessments can be permitted as rational within Bayesianism is if the proof or disproof of P=NP can be considered evidence, and hence we understand that the two probability assignments are each rational in light of differing states of knowledge. But at what point does an insight become evidence? The one-in-a-million-illness problem also requires some insight in order to reach the rational conclusion, but I for one would not say that someone who produced the intuitive but incorrect answer to this problem was \"acting rationally given their state of knowledge\". &nbsp;No sir, I would say they failed to reach the rational conclusion, for if lack of insight is akin to lack of evidence then any probability could be \"rationally\" assigned to any statement by someone who could reasonably claim to be stupid enough. The more stupid the person, the more difficult it would be to claim that they were, in fact, irrational.</p>\n<p>We can interpolate between the two extremes I have presented as examples, of course. I could give you a problem that requires you to marginalize over some continuous variable, and with an appropriate choice for x I could make the integration very tricky, requiring serious math skills to come to the precise solution. But at what difficulty does it become rational to approximate, or do a meta-analysis?</p>\n<p>So, the question is: when, if ever, does an insight count as evidence?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Fi5PWBQ5xhgjEWwJL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 11, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "2072", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-05T20:54:27.847Z", "modifiedAt": null, "url": null, "title": "TakeOnIt: Database of Expert Opinions", "slug": "takeonit-database-of-expert-opinions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:46.400Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/foiCqg9CLuhSTqiLq/takeonit-database-of-expert-opinions", "pageUrlRelative": "/posts/foiCqg9CLuhSTqiLq/takeonit-database-of-expert-opinions", "linkUrl": "https://www.lesswrong.com/posts/foiCqg9CLuhSTqiLq/takeonit-database-of-expert-opinions", "postedAtFormatted": "Tuesday, January 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20TakeOnIt%3A%20Database%20of%20Expert%20Opinions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATakeOnIt%3A%20Database%20of%20Expert%20Opinions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoiCqg9CLuhSTqiLq%2Ftakeonit-database-of-expert-opinions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=TakeOnIt%3A%20Database%20of%20Expert%20Opinions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoiCqg9CLuhSTqiLq%2Ftakeonit-database-of-expert-opinions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfoiCqg9CLuhSTqiLq%2Ftakeonit-database-of-expert-opinions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>Ben Albahari wrote to tell us about <a href=\"http://www.takeonit.com/\">TakeOnIt</a>, which is trying to build a database of expert opinions.&nbsp; This looks very similar to the data that would be required to locate the <a href=\"/lw/1kh/the_correct_contrarian_cluster/\">Correct Contrarian Cluster</a> - though currently they're building the expert database collaboratively, using quotes, rather than by directly polling the experts on standard topics.&nbsp; Searching for \"many worlds\" and \"zombies\" didn't turn up anything as yet; \"God\" was more productive.</p>\n<p>The site is open to the public, you can help catalog expert opinions, and Ben says they're happy to export the data for the use of anyone interested in this research area.</p>\n<p>Having this kind of database in standardized form is critical for assessing the track records of experts.&nbsp; TakeOnIt is aware of this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb226": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "foiCqg9CLuhSTqiLq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 5.507225689787669e-07, "legacy": true, "legacyId": "2037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9KvefburLia7ptEE3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-05T21:32:38.765Z", "modifiedAt": null, "url": null, "title": "A Suite of Pragmatic Considerations in Favor of Niceness", "slug": "a-suite-of-pragmatic-considerations-in-favor-of-niceness", "viewCount": null, "lastCommentedAt": "2020-07-06T16:57:22.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w8g7AkSbyApokD3dH/a-suite-of-pragmatic-considerations-in-favor-of-niceness", "pageUrlRelative": "/posts/w8g7AkSbyApokD3dH/a-suite-of-pragmatic-considerations-in-favor-of-niceness", "linkUrl": "https://www.lesswrong.com/posts/w8g7AkSbyApokD3dH/a-suite-of-pragmatic-considerations-in-favor-of-niceness", "postedAtFormatted": "Tuesday, January 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Suite%20of%20Pragmatic%20Considerations%20in%20Favor%20of%20Niceness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Suite%20of%20Pragmatic%20Considerations%20in%20Favor%20of%20Niceness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw8g7AkSbyApokD3dH%2Fa-suite-of-pragmatic-considerations-in-favor-of-niceness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Suite%20of%20Pragmatic%20Considerations%20in%20Favor%20of%20Niceness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw8g7AkSbyApokD3dH%2Fa-suite-of-pragmatic-considerations-in-favor-of-niceness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw8g7AkSbyApokD3dH%2Fa-suite-of-pragmatic-considerations-in-favor-of-niceness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 986, "htmlBody": "<p>tl;dr: Sometimes, people don't try as hard as they could to be nice.&nbsp; If being nice is not a terminal value for you, here are some other things to think about which might induce you to be nice anyway.</p>\n<p>There is a prevailing ethos in communities similar to ours - atheistic, intellectual groupings, who congregate around a topic rather than simply to congregate - and this ethos says that it is not necessary to be nice.&nbsp; I'm drawing on a commonsense notion of \"niceness\" here, which I hope won't confuse anyone (another feature of communities like this is that it's very easy to find people who claim to be confused by monosyllables).&nbsp; I do not merely mean \"polite\", which can be superficially like niceness when the person to whom the politeness is directed is in earshot but tends to be far more superficial.&nbsp; I claim that this ethos is mistaken and harmful.&nbsp; In so claiming, I do not also claim that I am always perfectly nice; I claim merely that I and others have good reasons to try to be.</p>\n<p>The dispensing with niceness probably springs in large part from an extreme rejection of the ad hominem fallacy and of emotionally-based reasoning.&nbsp; Of course someone may be entirely miserable company and still have brilliant, cogent ideas; to reject communication with someone who just happens to be miserable company, in spite of their brilliant, cogent ideas, is to miss out on the (valuable) latter because of a silly emotional reaction to the (irrelevant) former.&nbsp; Since the point of the community is ideas; and the person's ideas are good; and how much fun they are to be around is irrelevant - well, bringing up that they are just terribly mean seems trivial at best, and perhaps an invocation of the aforementioned fallacy.&nbsp; We are here to talk about <em>ideas</em>!&nbsp; (Interestingly, this same courtesy is rarely extended to appalling spelling.)</p>\n<p>The ad hominem fallacy <em>is</em> a fallacy, so this is a useful norm up to a point, but not up to the point where people who are perfectly capable of being nice, or learning to be nice, neglect to do so because it's apparently been rendered locally worthless.&nbsp; I submit that there are still good, pragmatic reasons to be nice, as follows.&nbsp; (These are claims about how to behave around real human-type persons.&nbsp; Many of them would likely be obsolete if we were all perfect Bayesians.)<a id=\"more\"></a></p>\n<ol>\n<li>It provides good incentives for others.&nbsp; It's easy enough to develop purely subconscious aversions to things that are unpleasant.&nbsp; If you are miserable company, people may stop talking to you without even knowing they're doing it, and some of these people may have ideas that would have benefited you.</li>\n<li>It helps you hold off on proposing diagnoses.&nbsp; As tempting as it may be to dismiss people as crazy or stupid, this is a dangerous label for us biased creatures.&nbsp; Fewer people than you are tempted to call these things are genuinely worth writing off as thoroughly as this kind of name-calling may tempt you to do.&nbsp; Conveniently, both these words (as applied to people, more than ideas) and closely related ones are culturally considered mean, and a general niceness policy will exclude them.</li>\n<li>It lets you exist in a cognitively diverse environment.&nbsp; Meanness is more tempting as an earlier resort when there's some kind of miscommunication, and miscommunication is more likely when you and your interlocutor think differently.&nbsp; Per #1, not making a conscious effort to be nice will tend to drive off the people with the greatest ratio of interesting new contributions to old rehashed repetitions.</li>\n<li>It is a cooperative behavior.&nbsp; It's obvious that it's nicer to live in a world where everybody is nice than in a world where everyone is a jerk.&nbsp; What's less obvious, but still, I think, true, is that the cost of cooperatively being nice while others are mean is in fact <em>very low</em>.&nbsp; This is partly because human interaction is virtually always iterated, (semi-)public, or both; and also because it's just not very hard to be nice.&nbsp; The former lets you reap an excellent signaling effect:</li>\n<li>It signals the hell out of your maturity, humility, and general awesome.&nbsp; If you spend as much time on the Internet as I do, you read a few online content publishers who publicly respond to their hate mail.&nbsp; It can sometimes be funny to read the nasty replies.&nbsp; But I generally walk away thinking more of the magnanimous ones who are patient even with their attackers.</li>\n<li>It promotes productive affect in yourself and others.&nbsp; The atmosphere of a relationship or group has many effects, plenty of which aren't cognitively luminous, and some of which can spill over into your general mood and whatever you were hoping to use your brain for.</li>\n<li>It is useful in theoretical discussions to draw a distinction between being mean to someone and doing something that's seriously morally wrong, but this line is fuzzier or completely absent in human prephilosophical intuitions.&nbsp; If you are ever troubled by ethical akrasia, it may be easier to stave off if you try to avoid delivering small slights and injuries as well as large violations.</li>\n<li>It yields resources in the form of friendly others.&nbsp; Whether you are an introvert or an extrovert, other people can be useful to have around, and not even just for companionship.&nbsp; Compared to indifferent or actively hostile neighbors, it's an obvious win to be nice and win what goodwill you can.</li>\n<li>It can save time - often yielding a net benefit, rather than wasting time as is sometimes complained.&nbsp; For instance, if a miscommunication is made, a mean response is to interpret the misstatement at face value and ridicule or attack - this can devolve into a time-consuming fight and may never resolve the initial issue.&nbsp; A nice response is to gently clarify, which can be over in minutes.</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3ee9k6NJfcGzL6kMS": 1, "nSHiKwWyMZFdZg5qt": 1, "fkABsGCJZ6y9qConW": 1, "izp6eeJJEg9v5zcur": 1, "zv7v2ziqexSn5iS9v": 1, "8uNFGxejo5hykCEez": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w8g7AkSbyApokD3dH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 99, "baseScore": 108, "extendedScore": null, "score": 0.000176, "legacy": true, "legacyId": "2075", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "on-enjoying-disagreeable-company", "canonicalPrevPostSlug": "ureshiku-naritai", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 109, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 198, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-06T09:52:10.657Z", "modifiedAt": null, "url": null, "title": "Communicating effectively: form and content", "slug": "communicating-effectively-form-and-content", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:07.617Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XDs4iPczbSuktqtXx/communicating-effectively-form-and-content", "pageUrlRelative": "/posts/XDs4iPczbSuktqtXx/communicating-effectively-form-and-content", "linkUrl": "https://www.lesswrong.com/posts/XDs4iPczbSuktqtXx/communicating-effectively-form-and-content", "postedAtFormatted": "Wednesday, January 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Communicating%20effectively%3A%20form%20and%20content&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommunicating%20effectively%3A%20form%20and%20content%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDs4iPczbSuktqtXx%2Fcommunicating-effectively-form-and-content%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Communicating%20effectively%3A%20form%20and%20content%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDs4iPczbSuktqtXx%2Fcommunicating-effectively-form-and-content", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXDs4iPczbSuktqtXx%2Fcommunicating-effectively-form-and-content", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 491, "htmlBody": "<p>Effective communication techniques, particularly in written communication, are an important part of the aspiring rationalist's toolkit. Alicorn's recent post makes excellent points about niceness, and touches parenthetically on the larger issue of form versus content.</p>\n<p>The general claim, when defending either rudeness or poor spelling, is \"what matters is the <em>content</em> in what I'm saying, not the <em>form</em>\". Well, I suspect this is one of the myths of pure reason. What matters about your content is what you do with it, pragmatically. Are you here to convey ideas to others ? Then you will achieve your aims more effectively if nothing about the form <em>distracts</em> from the content. (That you need to <em>have</em> content goes almost without saying.)<br /><br />Conscientious programmers are aware that source code is read and modified much more often than it is written. They know that it's harder to debug code than it was to write it in the first place. They invest more effort in making their code readable than a naive programmer might, because they estimate that this effort will be handsomely repaid in future savings.<br /><br />Conversation is no different. Your intent (in a forum like LW, anyway) is to cause others to ponder certain ideas. It's in <em>your</em> interest to consider the limitations of your interlocutors, their expectations, their attention span, their sensitivity, their bounded rationality, so that the largest possible fraction of your effort goes into delivering the payload, versus dissipating as waste heat. There are more readers than writers, making it rational to spend time and effort working on the form of your message as well as the content.<br /><br />You even need to keep in mind that people are stateful. That is, they don't just consider the <em>local</em> form you've chosen for your ideas; they also apply heuristics based on <em>past</em> interactions with you.<br /><br />These considerations apply to more than just \"niceness\". They apply to any instances where you notice that people fail to take away the intended message from your writings. When people <em>respond</em> to what you write, even with criticism, a downvote or a complaint, they are doing you a service; you can at least use that feedback to improve. Most will simply ignore you, quietly. Given enough feedback, the form your communication will improve, over time.</p>\n<p>And I would be quite surprised, given what I know of human minds, if this did not also eventually improve the content of your thinking. I find exchange with others indispensable in sharpening my own skills, at any rate, and that is why I aspire to be not just nice but also clear, engaging, and so on.<br /><br />I have gotten a lot of mileage out of, among others, Richard Gabriel's <a title=\"Writer's Workshop\" href=\"http://www.dreamsongs.com/Files/WritersWorkshop.pdf\">Writer's Workshop</a> book, and Peter Elbow's Writing with Power which introduced me to freewriting.<br /><br />What techniques do you, as rationalists, find useful for effective communication ?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XDs4iPczbSuktqtXx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 5.508647942591205e-07, "legacy": true, "legacyId": "2077", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-07T04:40:35.546Z", "modifiedAt": null, "url": null, "title": "Less Wrong Q&A with Eliezer Yudkowsky: Video Answers", "slug": "less-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.609Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "MichaelGR", "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qyix5Z5YPSGYxf7GG/less-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "pageUrlRelative": "/posts/Qyix5Z5YPSGYxf7GG/less-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "linkUrl": "https://www.lesswrong.com/posts/Qyix5Z5YPSGYxf7GG/less-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "postedAtFormatted": "Thursday, January 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Video%20Answers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Video%20Answers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyix5Z5YPSGYxf7GG%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-video-answers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Q%26A%20with%20Eliezer%20Yudkowsky%3A%20Video%20Answers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyix5Z5YPSGYxf7GG%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQyix5Z5YPSGYxf7GG%2Fless-wrong-q-and-a-with-eliezer-yudkowsky-video-answers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 322, "htmlBody": "<p>On October 29th, <a href=\"/lw/1di/a_less_wrong_qa_with_eliezer_step_1_the/\">I asked Eliezer and the LW community</a> if they were interested in doing a video Q&amp;A. Eliezer <a href=\"/lw/1di/a_less_wrong_qa_with_eliezer_step_1_the/17t1\">agreed</a> and a majority of commenters were in favor of the idea, so on November 11th, I created <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/\">a thread where LWers could submit questions</a>. Dozens of questions were asked, generating a total of over 650 comments. The questions were then ranked using the LW voting system.</p><p>On December 11th, Eliezer filmed his replies to the top questions (skipping some), and sent me the videos on December 22nd. Because voting continued after that date, the order of the top questions in <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/\">the original thread</a> has changed a bit, but you can find the original question for each video (and the discussion it generated, if any) by following the links below.</p><p>Thanks to Eliezer and everybody who participated.</p><p><strong>Update: </strong>If you prefer to download the videos, they are available <a href=\"http://www.megaupload.com/?d=1Q35MN2F\">here</a> (800 MB, .wmw format, sort the files by &#x27;date created&#x27;).</p><p>       </p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1905\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1905\">#1.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1901\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1901\">#2.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18zo\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18zo\">#3.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vj\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vj\">#4.</a></p><p><a href=\"http://vimeo.com/8586168\">Eliezer Yudkowsky -</a> <a href=\"http://vimeo.com/8586168\">Less Wrong Q&amp;A (5/30)</a> from <a href=\"http://vimeo.com/user2725513\">MikeGR</a> on <a href=\"http://vimeo.com/\">Vimeo</a>.</p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vl\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vl\">#5.</a></p><p>(Video #5 is on Vimeo because Youtube doesn&#x27;t accept videos longer than 10 minutes and I only found out after uploading about a dozen. I would gladly have put them all on Vimeo, but there&#x27;s a 500 MB/week upload limit and these videos add up to over 800 MB.)</p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/193r\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/193r\">#6.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w4\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w4\">#7.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1942\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1942\">#8.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190k\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190k\">#9.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18yw\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18yw\">#10.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190a\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190a\">#11.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18x9\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18x9\">#12.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vf\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vf\">#13.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18ut\">Link to question #14</a>.       </p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190d\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190d\">#15.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18xa\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18xa\">#16.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18x3\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18x3\">#17.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/19c8\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/19c8\">#18.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1973\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1973\">#19.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/191n\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/191n\">#20.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w1\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w1\">#21.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1967\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/1967\">#22.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18wc\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18wc\">#23.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w5\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18w5\">#24.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vo\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18vo\">#25.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18wf\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18wf\">#26.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/19aw\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/19aw\">#27.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190g\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/190g\">#28.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18z8\">Link to question</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18z8\">#29.</a></p><p><a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/193i\">Link to question #30.</a></p><p>If anything is wrong with the videos or links, let me know in the comments or via private message.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DigEmY3RrF3XL5cwe": 1, "NrvXXL3iGjjxu5B7d": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qyix5Z5YPSGYxf7GG", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 48, "extendedScore": null, "score": 0.00010628540210284483, "legacy": true, "legacyId": "2078", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["psuphwRHuJu9T9PPa", "iLGrNTwTZivTX5774"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-07T09:36:05.162Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes January 2010", "slug": "rationality-quotes-january-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:05.992Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zack_M_Davis", "createdAt": "2009-10-16T19:59:15.209Z", "isAdmin": false, "displayName": "Zack_M_Davis"}, "userId": "mPipmBTniuABY5PQy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LiaogK2NWgcLthcrM/rationality-quotes-january-2010", "pageUrlRelative": "/posts/LiaogK2NWgcLthcrM/rationality-quotes-january-2010", "linkUrl": "https://www.lesswrong.com/posts/LiaogK2NWgcLthcrM/rationality-quotes-january-2010", "postedAtFormatted": "Thursday, January 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20January%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20January%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiaogK2NWgcLthcrM%2Frationality-quotes-january-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20January%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiaogK2NWgcLthcrM%2Frationality-quotes-january-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLiaogK2NWgcLthcrM%2Frationality-quotes-january-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<p><!-- .meta --></p>\n<div>\n<div>\n<p style=\"margin: 0px 0px 1em;\">A monthly thread for posting rationality-related quotes you've seen recently (or had stored in your quotesfile for ages).</p>\n<ul style=\"margin: 10px 2em; padding: 0px; list-style-type: disc; list-style-position: outside;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LiaogK2NWgcLthcrM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 5.511251799149115e-07, "legacy": true, "legacyId": "2079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 143, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-07T14:00:27.205Z", "modifiedAt": null, "url": null, "title": "Will reason ever outrun faith?", "slug": "will-reason-ever-outrun-faith", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:03.531Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ABranco", "createdAt": "2009-10-12T23:30:03.771Z", "isAdmin": false, "displayName": "ABranco"}, "userId": "yDSTMgBPQXAbxhsp9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XN4xC7zKBXey9Ngfj/will-reason-ever-outrun-faith", "pageUrlRelative": "/posts/XN4xC7zKBXey9Ngfj/will-reason-ever-outrun-faith", "linkUrl": "https://www.lesswrong.com/posts/XN4xC7zKBXey9Ngfj/will-reason-ever-outrun-faith", "postedAtFormatted": "Thursday, January 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Will%20reason%20ever%20outrun%20faith%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWill%20reason%20ever%20outrun%20faith%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4xC7zKBXey9Ngfj%2Fwill-reason-ever-outrun-faith%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Will%20reason%20ever%20outrun%20faith%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4xC7zKBXey9Ngfj%2Fwill-reason-ever-outrun-faith", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXN4xC7zKBXey9Ngfj%2Fwill-reason-ever-outrun-faith", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1088, "htmlBody": "<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Recently, a video produced by Christians claimed that t<span class=\"gi\">he future world would be Muslim. It hit 10 million hits in YouTube. The alarming demographics presented were proven mostly false and exaggerated both by <a href=\"http://news.bbc.co.uk/2/hi/8189231.stm\">BBC</a> and <a href=\"http://www.snopes.com/politics/religion/demographics.asp\">Snopes</a>. Yet, religion is such a powerful self-replicating memeplex that its competition against atheism deserves some analysis.</span></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Leaving apart the aesthetic nicety of some religious rituals &mdash; which I respect &mdash;, it would be preferable to see a world with predominance of rationality instead of faith, <a href=\"http://en.wikipedia.org/wiki/Brights_movement\">brights</a><a href=\"http://en.wikipedia.org/wiki/Brights_movement\"> instead of supers</a>. Not just because I whimsically wish so, but because reason ensues atheism. Rationality is the primer here. With more rational agents, the more rationality propagates, and people&rsquo;s <a href=\"http://wiki.lesswrong.com/wiki/Map\">maps</a> will be more accurate. And that&rsquo;s better for us, human beings*. <br /></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">(* This sentence is a bit of a strong claim, especially because I am not defining exactly what I mean by &lsquo;better&rsquo;, and some existential pain might be expected as a consequence of being unaided by the crutches of faith and of being deprived of their <a href=\"/lw/18b/reason_as_memetic_immune_disorder/\">cultural antibodies</a>. Also, if happiness happens to be an important attribute of &lsquo;better&rsquo;, I am not sure to what extent being rational will make people happier. Some people are very ok choosing the <a href=\"http://en.wikipedia.org/wiki/Red_pill\">blue pill</a>. For the time being, let&rsquo;s take it as an axiom. The claim that rational is better might deserve a separate post.)<a id=\"more\"></a></span></p>\n<p class=\"MsoNormal\"><strong><span style=\"font-family: Arial;\">Is a predominantly rational and atheist world probable?</span></strong><span style=\"font-family: Arial;\"> <br /></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Let&rsquo;s see. Religion &mdash; or the lack thereof &mdash; and culture are propagated <a href=\"http://en.wikipedia.org/wiki/Memetics\" target=\"_blank\">memetically</a>. The propagation can be <strong>(1) </strong>vertical (i.e., from parents to children) or <strong>(2)</strong> horizontally (i.e., friends, media and the like). <br /> <br /> <strong>(1) It seems quite hopeless for atheists to outgrow religious populations through parenting.</strong> First, <em>\"countries that are relatively secularized usually reproduce more slowly than countries that are more religious. According to the World Bank, the nations with the largest proportions of unbelievers had an average annual population growth rate of just 0.7% in the period 1975-97, while the populations of the most religious countries grew three times as fast.\"</em> (<a href=\"http://www.moreintelligentlife.com/story/faith-equals-fertility\" target=\"_blank\">from The Economist, Faith Equals Fertility</a>). <br /> <br /> As this very same article concludes:<br /> <br /> <em>\"one might half-seriously conclude that atheists and agnostics ought to focus on having more children, to help overcome their demographic disadvantage. Unfortunately for secularists, this may not work even as a joke. Nobody knows exactly why religion and fertility tend to go together. Conventional wisdom says that <strong>female education, urbanization, falling infant mortality, and the switch from agriculture to industry and services all tend to cause declines in both religiosity and birth rates</strong>. In other words, secularization and smaller families are caused by the same things. Also, many religions enjoin believers to marry early, abjure abortion and sometimes even contraception, all of which leads to larger families.</em> (&hellip;) <em>So, religious people have larger families because Western religions encourage having children. Further, as a general proposition (there are, of course, exceptions), <strong>religious people tend to place a higher emphasis on altruism</strong>, whereas secular people tend to be more self-focused. Thus, for a religious person, children provide the opportunity to nurture and benefit other human beings. <strong>For many secular people, however, children merely consume time and resources that otherwise could have been devoted to their own amusement.</strong>\"</em><br /> <br /> <strong>Conclusion: as a whole, atheists have less offspring, and they have a good reason to do so.</strong> Their being atheists and their lesser fertility rates have the same causes: <strong>education</strong>. If you think about it, the other causes cited above are themselves a product of proper education.<br /> </span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\"> <strong>(2)</strong> <strong>Can atheism gain more adepts through horizontal propagation? </strong>Definitely, although I don't dare predict to what extent. Many atheists &mdash; kudos for Dawkins and Hitchens &mdash; have been making public calls to reason and openly arguing pro-atheism, in a rather educational approach. <a href=\"http://www.telegraph.co.uk/news/newstopics/howaboutthat/4141765/Atheist-buses-denying-Gods-existence-take-to-streets.html\" target=\"_blank\">Some courageous initiatives have been seen recently. </a><span>&nbsp;</span><br /></span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\"> This is great.<br /> <br /> However, mass media still seems to be among the strongest means of replication. And <strong>mass media is a vehicle of entertainment</strong>, not a vehicle of truth-seeking. <br /> <br /> </span><span style=\"font-family: Arial;\">Making cold reason and correct statistics as appealing than the alternatives is a challenging pursuit.</span><span style=\"font-family: Arial;\"> As businesses, mass media networks give more space to whatever pleases the public more. The public is religious &mdash; and as it seems, will continue to be so. Therefore, miracles and emotionally charged stories sell more. Well, probably even the brightest minds indulge in some <a href=\"http://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb#Ludic_fallacy\">ludic fallacy</a> now and then. </span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">(I haven&rsquo;t had a TV set at home for some eight years, and completely let go of the daily habit of reading newspapers, too, just like my friend <a href=\"http://econlog.econlib.org/archives/2008/03/jefferson_again.html\">Thomas Jefferson</a>. There&rsquo;s just too much noise going on &mdash; and isn&rsquo;t it amazing how successful this noise is in deviating our attention from what matters most? But I digress.)<br /> <br /> We have a self-fueling pattern here, a memetic selection mechanism: <br /> <br /> &mdash; <strong>Medias propagating rationality-related memes stay behind</strong>, because there are fewer atheists. So, less of atheist ideas are spread. So there will be even <strong>fewer atheists</strong>, or, at least, a modest growth speed of atheists in absolute numbers.<br /> <br /> &mdash; <strong>Religious medias get richer and bigger</strong>, because there are many religious people who support them. So more religious ideas are spread. So there will be even <strong>more of them</strong>, with a fast growth. As an example, Brazilian TV network <a href=\"http://en.wikipedia.org/wiki/Rede_Record\">Rede Record</a>, after being acquired by the direction of the religious group <a href=\"http://en.wikipedia.org/wiki/Universal_Church_of_the_Kingdom_of_God#Controversies\">IURD (aka UCKG)</a>, left from a very small market share to being second place in the ratings in 2007. Rede Record growth </span><span style=\"font-family: Arial;\">&mdash; the fastest among the country's networks </span><span style=\"font-family: Arial;\">&mdash; </span><span style=\"font-family: Arial;\">has been, alas, fueled by the tithe of IURD's followers (source <a href=\"http://www1.folha.uol.com.br/folha/ilustrada/ult90u61067.shtml\">here</a>, Portuguese only).</span></p>\n<p class=\"MsoNormal\">One could argue that, as internet access becomes widespread, we should expect its decentralized and democratic nature to be on the side of those who are trying to enhance their maps with valuable information. Nevertheless, someone who doesn&rsquo;t directly look for it will be carried away by the noise or, worse, by misguided information. One has to start, as a child, guided by a rational mind. And having a rational mind depends heavily on this person&rsquo;s education and upbringing, which depends on what kind of parents they have. Oops.</p>\n<p class=\"MsoNormal\"><span style=\"font-family: Arial;\">Of course there are exceptions, those who make an effort to think by themselves despite their environment. They seem, however, to be outnumbered by the masses under the influence of <a href=\"http://en.wikipedia.org/wiki/Bandwagon_effect\">bandwagon effect</a>.<br /></span></p>\n<p><strong>Where lies the hope for a predominantly rational and atheist world anyway?</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XN4xC7zKBXey9Ngfj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 8, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "2080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aHaqgTNnFzD7NGLMx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-07T18:24:46.652Z", "modifiedAt": null, "url": null, "title": "Case study: Melatonin", "slug": "case-study-melatonin", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:03.702Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XYA9nBud8joDjTy86/case-study-melatonin", "pageUrlRelative": "/posts/XYA9nBud8joDjTy86/case-study-melatonin", "linkUrl": "https://www.lesswrong.com/posts/XYA9nBud8joDjTy86/case-study-melatonin", "postedAtFormatted": "Thursday, January 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Case%20study%3A%20Melatonin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACase%20study%3A%20Melatonin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYA9nBud8joDjTy86%2Fcase-study-melatonin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Case%20study%3A%20Melatonin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYA9nBud8joDjTy86%2Fcase-study-melatonin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXYA9nBud8joDjTy86%2Fcase-study-melatonin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<blockquote>\n<p>I discuss melatonin's effects on sleep &amp; its safety; I segue into the general benefits of sleep and the severely disrupted sleep of the modern Western world, the cost of melatonin use and the benefit (eg. enforcing regular bedtimes), followed by a basic cost-benefit analysis of melatonin concluding that the net profit is large enough to be worth giving it a try barring unusual conditions or very pessimistic safety estimates.</p>\n</blockquote>\n<p>Full essay: <a href=\"http://www.gwern.net/Melatonin\">http://www.gwern.net/Melatonin</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HLoxy2feb2PYqooom": 1, "xHjy88N2uJvGdgzfw": 1, "FdoP2PJhMz6x3gdDh": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XYA9nBud8joDjTy86", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 27, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "2081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 176, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-08T01:59:03.452Z", "modifiedAt": null, "url": null, "title": "Fictional Evidence vs. Fictional Insight", "slug": "fictional-evidence-vs-fictional-insight", "viewCount": null, "lastCommentedAt": "2021-02-27T22:44:47.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bkowF2N9uyYh4DXSe/fictional-evidence-vs-fictional-insight", "pageUrlRelative": "/posts/bkowF2N9uyYh4DXSe/fictional-evidence-vs-fictional-insight", "linkUrl": "https://www.lesswrong.com/posts/bkowF2N9uyYh4DXSe/fictional-evidence-vs-fictional-insight", "postedAtFormatted": "Friday, January 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fictional%20Evidence%20vs.%20Fictional%20Insight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFictional%20Evidence%20vs.%20Fictional%20Insight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkowF2N9uyYh4DXSe%2Ffictional-evidence-vs-fictional-insight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fictional%20Evidence%20vs.%20Fictional%20Insight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkowF2N9uyYh4DXSe%2Ffictional-evidence-vs-fictional-insight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkowF2N9uyYh4DXSe%2Ffictional-evidence-vs-fictional-insight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 455, "htmlBody": "<p>This is a response to Eliezer Yudkowsky's <a href=\"/lw/k9/the_logical_fallacy_of_generalization_from/\">The Logical Fallacy of Generalization from Fictional Evidence</a> and Alex Flint's <a href=\"/lw/1lk/when_does_an_insight_count_as_evidence/\">When does an insight count as evidence?</a> as well as komponisto's recent <a href=\"/lw/1lf/open_thread_january_2010/1ei1\">request</a> for science fiction recommendations.</p>\n<p>My thesis is that insight forms a category that is distinct from evidence, and that fiction can provide insight, even if it can't provide much evidence. To give some idea of what I mean, I'll list the insights I gained from <a href=\"/lw/1lf/open_thread_january_2010/1ers\">one particular piece of fiction</a> (published in 1992), which have influenced my life to a large degree:</p>\n<ol>\n<li>Intelligence may be the ultimate power in this universe.</li>\n<li>A <a href=\"http://wiki.lesswrong.com/wiki/Technological_singularity\">technological Singularity</a> is possible.</li>\n<li>A bad Singularity is possible.</li>\n<li>It may be possible to nudge the future, in particular to make a good Singularity more likely, and a bad one less likely.</li>\n<li>Improving network security may be one possible way to nudge the future in a good direction. (Side note: <a href=\"/lw/1hs/open_thread_december_2009/1axl\">here are </a><a href=\"/lw/1hs/open_thread_december_2009/1axl\">my current thoughts on this</a>.)</li>\n<li>An online reputation for intelligence, rationality, insight, and/or clarity can be a source of power, because it may provide a chance to change the beliefs of a few people who will make a crucial difference.</li>\n</ol>\n<p>So what is insight, as opposed to evidence? First of all, notice that logically omniscient Bayesians have no use for insight. They would have known all of the above without having observed anything (assuming they had a reasonable prior). So insight must be related to logical uncertainty, and a feature only of minds that are computationally constrained. I suspect that we won't fully understand the nature of insight until the problem of logical uncertainty is solved, but here are some of my thoughts about it in the mean time:</p>\n<ul>\n<li>A main form of insight is a hypothesis that one hadn't previously entertained, but should be assigned a non-negligible prior probability.</li>\n<li>An insight is kind of like a mathematical proof: in theory you could have thought of it yourself, but reading it saves you a bunch of computation.</li>\n<li>Recognizing an insight seems easier than coming up with it, but still of nontrivial difficulty.</li>\n</ul>\n<p>So a challenge for us is to distinguish true insights from unhelpful distractions in fiction. Eliezer mentioned people who let the <em>Matrix</em> and <em>Terminator</em> dominate their thoughts about the future, and I agree that we have to be careful not to let our minds consider fiction as evidence. But is there also some skill that can be learned, to pick out the insights, and not just to ignore the distractions?</p>\n<p>P.S., what insights have you gained from fiction?</p>\n<p>P.P.S., I guess I should mention the name of the book for the search engines: <a href=\"http://books.google.com/books?id=UGAKB3r0sZQC\">A Fire Upon the Deep</a> by Vernor Vinge.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 9, "5f5c37ee1b5cdee568cfb125": 2, "AHK82ypfxF45rqh9D": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bkowF2N9uyYh4DXSe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 49, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "2084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rHBdcHGLJ7KvLJQPk", "Fi5PWBQ5xhgjEWwJL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-08T04:13:36.319Z", "modifiedAt": null, "url": null, "title": "Reference class of the unclassreferenceable", "slug": "reference-class-of-the-unclassreferenceable", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:34.043Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "taw", "createdAt": "2009-03-16T02:43:25.472Z", "isAdmin": false, "displayName": "taw"}, "userId": "vix9BLjt4bHtsCqWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yKRTxAohwmqLcbbJi/reference-class-of-the-unclassreferenceable", "pageUrlRelative": "/posts/yKRTxAohwmqLcbbJi/reference-class-of-the-unclassreferenceable", "linkUrl": "https://www.lesswrong.com/posts/yKRTxAohwmqLcbbJi/reference-class-of-the-unclassreferenceable", "postedAtFormatted": "Friday, January 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reference%20class%20of%20the%20unclassreferenceable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReference%20class%20of%20the%20unclassreferenceable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRTxAohwmqLcbbJi%2Freference-class-of-the-unclassreferenceable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reference%20class%20of%20the%20unclassreferenceable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRTxAohwmqLcbbJi%2Freference-class-of-the-unclassreferenceable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyKRTxAohwmqLcbbJi%2Freference-class-of-the-unclassreferenceable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 364, "htmlBody": "<p>One of the most useful techniques of rationality is taking the outside view, also known as <a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\">reference class forecasting</a>. Instead of thinking too hard about particulars of a given situation and taking a guess which <a href=\"http://en.wikipedia.org/wiki/Optimism_bias\">will invariably turned out to be highly biased</a>, one looks at outcomes of situations which are similar in some essential way.</p>\n<p><a href=\"/lw/1gw/contrarianism_and_reference_class_forecasting/\">Figuring out correct reference class might sometimes be difficult</a>, but even then it's far more reliable than trying to guess while ignoring the evidence of similar cases. Now in some situations we have precise enough data that inside view might give correct answer - but for almost all such cases I'd expect outside view to be as usable and not far away in correctness.</p>\n<p>Something that keeps puzzling me is persistence of certain beliefs on lesswrong. Like belief in effectiveness of cryonics - reference class of things promising eternal (or very long) life is huge and has consistent 0% success rate. Reference class of predictions based on technology which isn't even remotely here has perhaps non-zero but still ridiculously tiny success rate. I cannot think of any reference class in which cryonics does well. Likewise belief in singularity - reference class of beliefs in coming of a new world, be it good or evil, is huge and with consistent 0% success rate. Reference class of <a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">beliefs in almost omnipotent good or evil beings</a> has consistent 0% success rate.</p>\n<p>And many fellow rationalists not only believe that chances of cryonics or singularity or AI are far from negligible levels indicated by the outside view, they consider them highly likely or even nearly certain!</p>\n<p>There are a few ways how this situation can be resolved:</p>\n<ul>\n<li>Biting the outside view bullet like me, and assigning very low probability to them.</li>\n<li>Finding a convincing reference class in which cryonics, singularity, superhuman AI etc. are highly probable - I invite you to try in comments, but I doubt this will lead anywhere.</li>\n<li>Or is there a class of situations for which the outside view is consistently and spectacularly wrong; data is not good enough for precise predictions; and <a href=\"/lw/vz/the_weak_inside_view/\">yet we somehow think we can predict them reliably</a>?</li>\n</ul>\n<p>How do you reconcile them?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yKRTxAohwmqLcbbJi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 28, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "2085", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 154, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["whxjfzFecFt4hAnBA", "w9KWNWFTXivjJ7rjF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-08T12:18:39.776Z", "modifiedAt": null, "url": null, "title": "Consciousness", "slug": "consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:19.574Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Rq5HS5JwjdxaLeR2B/consciousness", "pageUrlRelative": "/posts/Rq5HS5JwjdxaLeR2B/consciousness", "linkUrl": "https://www.lesswrong.com/posts/Rq5HS5JwjdxaLeR2B/consciousness", "postedAtFormatted": "Friday, January 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRq5HS5JwjdxaLeR2B%2Fconsciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRq5HS5JwjdxaLeR2B%2Fconsciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRq5HS5JwjdxaLeR2B%2Fconsciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1893, "htmlBody": "<p>(<strong>ETA</strong>: I've created three threads - <a href=\"/lw/1ly/consciousness/1fca\">color</a>, <a href=\"/lw/1ly/consciousness/1fcb\">computation</a>, <a href=\"/lw/1ly/consciousness/1fcc\">meaning</a> - for the discussion of three questions posed in this article. If you are answering one of those specific questions, please answer there.)</p>\n<p>I don't know how to make this about rationality. It's an attack on something which is a standard view, not only here, but throughout scientific culture. Someone else can do the metalevel analysis and extract the rationality lessons.</p>\n<p>The local worldview reduces everything to some combination of physics, mathematics, and computer science, with the exact combination depending on the person. I think it is manifestly the case that this does not work for consciousness. I took this line before, but people struggled to understand my own speculations and this complicated the discussion. So the focus is going to be much more on what other people think - like you, dear reader. If you think consciousness can be reduced to some combination of the above, here's your chance to make your case.</p>\n<p>The main exhibits will be <em>color</em> and <em>computation</em>. Then we'll talk about reference; then time; and finally the \"unity of consciousness\".</p>\n<p><a id=\"more\"></a>Color was an issue last time. I ended up going back and forth fruitlessly with several people. From my perspective it's very simple: where is the color in your theory? Whether your physics consists of fields and particles in space, or flows of amplitude in configuration space, or even if you think reality consists of \"mathematical structures\" or Platonic computer programs, or <em>whatever</em> - I don't see anything <em>red</em> or <em>green</em> there, and yet I do see it right now, here in reality. So if you intend to tell me that reality consists solely of physics, mathematics, or computation, you need to tell me where the colors are.</p>\n<p>Occasionally someone says that red and green are just words, and they don't even mean the same thing for different cultures or different people. True. But that's just a matter of classification. It's a fact that the individual shades of color exist, however it is that we group them - and your ontology must contain them, if it pretends to completeness.</p>\n<p>Then, there are various other things which have some relation to color - the physics of surface reflection, or the cognitive neuroscience of color attribution. I think we all agree that the first doesn't matter too much; you don't even need blue light to see blue, you just need the right nerves to fire. So the second one seems a lot more relevant, in the attempt to explain color using the physics we have. Somehow the answer lies in the brain.</p>\n<p>There is one last dodge comparable to focusing on color words, namely, focusing on color-related cognition. Explaining why you say the words, explaining why you categorize the perceived object as being of a certain color. We're getting closer here. The explanation of color, if there is such, clearly has a close connection to those explanations.</p>\n<p>But in the end, either you say that blueness is there, or it is not there. And if it is there, at least \"in experience\" or \"in consciousness\", then something somewhere <em>is blue</em>. And all there is in the brain, according to standard physics, is a bunch of particles in various changing configurations. So: where's the blue? What is the blue thing?</p>\n<p>I can't answer that question. At least, I can't answer that question for you if you hold with orthodoxy here. However, I have noticed maybe three orthodox approaches to this question.</p>\n<p>First is faith. I don't understand how it could be so, but I'm sure one day it will make sense.</p>\n<p>Second, puzzlement plus faith. I don't understand how it could be so, and I agree that it really really looks like an insurmountable problem, but we overcame great problems in the past without having to overthrow the whole of science. So maybe if we stand on our heads, hold our breath, and think different, one day it will all make sense.</p>\n<p>Third, dualism that doesn't notice it's dualism. This comes from people who think they have an answer. The blueness <em>is</em> the pattern of neural firing, or the von Neumann entropy of the neural state compared to that of the light source, or some other particular physical entity or property. If one then asks, okay, if you say so, but where's the blue... the reactions vary. But a common theme seems to be that blueness is a \"feel\" somehow \"associated\" with the entity, or even associated with being the entity. To see blue is how it feels to have your neurons firing that way.</p>\n<p>This is the dualism which doesn't know it's dualism. We have a perfectly sensible and precise physical description of neurons firing: ions moving through macromolecular gateways in a membrane, and so forth. There's no end of things we can say about it. We can count the number of ions in a particular spatial volume, we can describe how the electromagnetic fields develop, we can say that this was caused by that... But you'll notice - nothing about feels. When you say that this feels like something, you're introducing a whole new property to the physical description. Basically, you're constructing a dual-aspect materialism, just like David Chalmers proposed. Technically, you're a property dualist rather than a substance dualist.</p>\n<p>Now dualism is supposed to be beyond horrible, so what's the alternative? You can do a Dennett and deny that anything is really blue. A few people go there, but not many. If the blueness does exist, and you don't want to be a dualist, and you want to believe in existing physics, then you have to conclude that blueness is what the physics was about all along. We represented it to ourselves as being about little point-particles moving around in space, but all we ever actually had was mathematics and correct predictions, so it must be that some part of the mathematics was actually talking about blueness - real blueness - all along. Problem solved!</p>\n<p>Except, it's rather hard to make this work in detail. Blueness, after all, does not exist in a vacuum. It's part of a larger experience. So if you take this path, you may as well say that <em>experiences</em> are real, and part of physics must have been describing them all along. And when you try to make some part of physics look like a whole experience - well, I won't say the m word here. Still, this <em>is</em> the path I took, so it's the one I endorse; it just leads you a lot further afield than you might imagine.</p>\n<p>Next up, <em>computation</em>. Again, the basic criticism is simple, it's the attempt to rationalize things which makes the discussion complicated. People like to attribute computational states, not just to computers, but to the brain. And they want to say that thoughts, perceptions, etc., consist of being in a certain computational state. But a physical state does not correspond inherently to any one computational state.</p>\n<p>There's also a problem with semantics - saying that the state is <em>about</em> something - which I will come to in due course. But first up, let's just look at the problems involved in attributing a <em>non-referential</em> \"computational state\" to a physical entity.&nbsp;</p>\n<p>Physically speaking, an object, like a computer or a brain, can be in any of a large number of exact microphysical states. When we say it is in a computational state, we are grouping those microphysically distinct states together and saying, every state in this group corresponds to the same abstract high-level state, every microphysical state in this other group corresponds to some other abstract high-level state, and so on. But there are many many ways of grouping the states together. Which clustering is the true one, the one that corresponds to cognitive states? Remember, the orthodoxy is functionalism: low-level details don't matter. To be in a particular cognitive state is to be in a particular computational state. But if the \"computational state\" of a physical object is an observer-dependent attribution rather than an intrinsic property, then how can my thoughts be brain states?</p>\n<p>We didn't have this discussion before, so I won't try to anticipate the possible defenses of functionalism. No-one will be surprised, I suppose, to hear that I don't believe this either. Instead, I deduce from this problem that functionalism is wrong. But here's your chance, functionalists: tell the world the one true state-clustering which tells us <em>the</em> computation being implemented by a physical object!</p>\n<p>I promised a problem with semantics too. Again I think it's pretty simple. Even if we settle on the One True Clustering of microstates - each such macrostate is still just a region of a physical configuration space. Thoughts have semantic content, they are \"about\" things. Where's the aboutness?</p>\n<p>I also promised to mention time and unity-of-consciousness in conclusion. Time I think offers another outstanding example of the will to deny an aspect of conscious experience (or rather, to call it an illusion) for the sake of insisting that reality conforms entirely to a particular scientific ontology. Basically, we have a physics that spatializes time; we can visualize a space-time as a static, completed thing. So time in the sense of flow - change, process - isn't there in the model; but it appears to be there in reality; therefore it is an illusion.</p>\n<p>Without trying to preempt the debate about time, perhaps you can see by now why I would be rather skeptical of attempts to deny the obvious for the sake of a particular scientific ontology. Perhaps it's not actually necessary. Maybe, if someone thinks about it hard enough, they can come up with an ontology in which time is real and \"flows\" after all, and which still gives rise to the right physical predictions. (In general relativity, a world-line has a local time associated with it. So if the world-line is that of an actually and persistently existing object, perhaps time can be real and flowing <em>inside</em> the object... in some sense. That's my suggestion.)</p>\n<p>And finally, unity of consciousness. In the debate over physicalism and consciousness, the discussion usually doesn't even get this far. It gets stuck on whether the individual \"qualia\" are real. But they do actually form a whole. All this stuff - color, meaning, time - is drawn from that whole. It is a real and very difficult task to properly characterize that whole: not just what its ingredients are, but how they are joined together, what it is that makes it a whole. After all, that whole is your life. Nonetheless, if anyone has come this far with me, perhaps you'll agree that it's the ontology of the subjective whole which is the ultimate challenge here. If we are going to say that a particular ontology is the way that reality is, then it must not only contain color, meaning, and time, it has to contain that subjective whole. In phenomenology, the standard term for that whole is the \"lifeworld\". Even cranky mistaken reductionists have a lifeworld - they just haven't noticed the inconsistencies between what they believe and what they experience. The ultimate challenge in the science of consciousness is to get the ontology of the lifeworld right, and then to find a broader scientific ontology which contains the lifeworld ontology. But first, as difficult as it may seem, we have to get past the partial ontologies which, for all their predictive power and their seductive exactness, just can't be the whole story.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Rq5HS5JwjdxaLeR2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 8, "extendedScore": null, "score": 1.3e-05, "legacy": true, "legacyId": "2086", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 232, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-09T08:05:56.749Z", "modifiedAt": null, "url": null, "title": "Hypotheses For Dualism", "slug": "hypotheses-for-dualism", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:03.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "byrnema", "createdAt": "2009-03-20T18:02:38.305Z", "isAdmin": false, "displayName": "byrnema"}, "userId": "SCnD6W8NiztYBN39M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SYmubibg7wfuLztpQ/hypotheses-for-dualism", "pageUrlRelative": "/posts/SYmubibg7wfuLztpQ/hypotheses-for-dualism", "linkUrl": "https://www.lesswrong.com/posts/SYmubibg7wfuLztpQ/hypotheses-for-dualism", "postedAtFormatted": "Saturday, January 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hypotheses%20For%20Dualism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHypotheses%20For%20Dualism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYmubibg7wfuLztpQ%2Fhypotheses-for-dualism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hypotheses%20For%20Dualism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYmubibg7wfuLztpQ%2Fhypotheses-for-dualism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSYmubibg7wfuLztpQ%2Fhypotheses-for-dualism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1514, "htmlBody": "<p>In this post I present the first few hypotheses that I can think for why people insist on a metaphysical aspect to consciousness, and develop one in some detail: a \"reality is simulated\" hypothesis.</p>\n<p>Please contribute your own hypotheses.&nbsp; Why is there a persistent belief that consciousness is metaphysical?</p>\n<p><a id=\"more\"></a></p>\n<p>I offer some hypotheses for why people have the illusion of a metaphysical aspect to consciousness:</p>\n<ul>\n<li>the lack of spatio-temporal information in our thought processes. We have no idea where thoughts occur since there are few physical sensations in our brain when we think. The sense of \"another dimension\" to thought might disappear if people grew up with images of different parts of their brain lighting up in different patterns as they thought -- even when they think deep, profound and poignant thoughts.</li>\n<li>when we think about something, that something can be located <em>anywhere</em>. I can think of a tree on the moon. This may give the feeling that&nbsp; consciousness is spatially 'infinite'&nbsp; and non-local.</li>\n<li>If I think about a tree on the moon, I can make someone else think of a tree on the moon without presenting a tree on the moon. This gives the impression that ideas are put into and pulled out of an omnipresent aether.</li>\n<li>To some extent in everyday thought, and especially during certain mental states, our thinking actually simulates reality. For example, while dreaming and during so-called \"out-of-body\" experiences.</li>\n</ul>\n<p>Perhaps all of these hypotheses contribute to the impression that consciousness is fundamentally non-physical (outside the physical; meta-physical). It is the last hypothesis that&nbsp; I would like to expand upon in this post. That we actually simulate reality in our thoughts -- and this simulated reality is the dual reality that dualists speak of.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Reality is Simulated Hypothesis</strong></p>\n<p><br />I know that when I look at my hand, the experience is not as immediate and straight-forward as it seems. It's not <em>me</em> \"looking at my hand\" -- if by 'me' I mean my conscious self. Instead, my brain is putting together an image of my hand based on sensory information it is receiving from my eye. Nor is it even so simple as \"it is me that is aware of a constructed image of my hand\". Because the part of me that 'sees' the image is still not my conscious 'me'. What is actually going on is that I imagine 'myself' seeing my hand -- that is, I imagine a 'me' and I imagine this 'me' is seeing a hand. So when I think about it quite carefully, I realize that when I think to myself that I am seeing my hand, this means I am simulating myself seeing a hand.</p>\n<p><br />Level 1: I see my hand. (where I = my brain and my hand = my hand -- this is equivalent to the way a non-sentient creature \"sees\")</p>\n<p><br />Level 2: I think, \"I see my hand\". (the I in \"I see my hand\" = my conscious self-awareness; the&nbsp; hand in \"I see my hand\" = an imagined / simulated hand)<br /><br />This sounds very complicated, but it goes on all the time that we're consciously self-aware. (Without it being consciously observed -- that would be Level 3.)<br /><br />So my brain constructs an image of a hand. One very closely related to the simulated hand that I see when I imagine myself seeing my hand. So 'I' never see a hand; I only see my simulation of a hand. <br /><br />I'm describing this at Level 2 but it also happens at Level 1. I'm sure many of us relate to the concern as children whether or not our loved ones 'see' things the same way we do, as this seemed impossible to ever verify. ('What does blue look like to you? How do I know you're not seeing green, but have learned to call it blue?') The conundrum dissolves when you realize that there is nothing behind the experience of 'seeing green' or 'seeing blue' beyond the set of experiences the meaning of those words point to. (Eliezer has posts on this, for example the first two paragraphs <a href=\"/lw/nq/feel_the_meaning/\">here</a>.)</p>\n<p><br />So at the end of the day, my dad and I do reliably have the same concept of the blue box on the table, because what's relevant about our concept of the box is it's weight, color, texture, etc. -- all the empirical things about it.<br /><br />By now in this post though, if I'm explaining myself clearly, we'll observe that our concept of the box still includes more than all the empirical things about the box. When we consider the blue box, we're still simulating the blue box in our minds; perhaps in a simulation of ourselves seeing the blue box. The simulation of the blue box (the box in our mind's eye) needn't be exactly like the blue box. Indeed, it will be missing any information we don't have about the blue box or don't feel that is necessary to retrieve for that particular simulation. The simulated blue box is an idealization of the actual blue box. A platonic idealization of a Blue Box. Even if I examine the blue box and notice that the corner is chipped, I will then consciously observe that I am observing that the blue box is chipped only by simulating myself seeing a blue box with the chip. A platonic Blue Box With A Chip.</p>\n<p><br />Thus qualia. We never interact with anything else, if 'we' is restricted to mean our conscious self-aware identities.<br /><br />So that's my thesis: consciousness is the simulation of reality run on the hardware of our brains, and qualia is the Level3+ observation that the reality we perceive is simulated. <br /><br />Now imagine: when people describe consciousness as being metaphysical, perhaps they are observing that <em>the simulation</em> is metaphysical. <br /><br />... I would agree that a <em>simulation</em> can be metaphysical. But it's still simulated meta-physicality. I imagine/simulate a platonic Blue Box, but a platonic Blue Box doesn't <em>exist</em>. Not empirically.</p>\n<p><br />Or does it? If empirical is defined as that which we most immediately experience; wouldn't qualia be there, right between ourselves and the observation of reality?<br /><br />I've observed before (and would be willing to argue in more detail ) that a simulation is just as real as reality if it doesn't need to model the exterior reality to model itself self-consistently. But when consciousness is in the act of observing reality (and thus reality is simulated in our consciousness) the simulation is only used to model reality, so some confusion about which is internal and which is external is understandable. This is the 'eye looking at the eye looking at the eye' sensation that Eliezer describes, somewhere. In any case, I wouldn't say that our experience of qualia is not real (genuine), but that we're only beginning to find the right words for it. I think the word 'simulation' is fine. (I note the meaning of simulation has changed in the last 15 years to accomodate my meaning; it takes time for words to evolve to fill new gaps.)</p>\n<p><br />As evidence (ironic cough) I would like to present a conversation I had recently with someone who said they believed in out-of-body experiences.<br /><br />Me: Really?<br />G: Yeah.<br />Me: What do you see <em>with</em> during these experience? With your actual physical eyes? (Aren't they closed?)<br />G: Not really, it's like a third eye. (translation: <em>mind</em>'s eye)<br />Me: Could you use this technique to spy on people?<br />G: No... (some discomfort)<br />Me: Could you pick up a video camera and video yourself sleeping on the bed?<br />G: No! It's not like that. (translation: it's not <em>empirical</em>)<br />&nbsp;&nbsp;&nbsp;&nbsp; But you can see things in a different way, discover things that you intuitively know. (translation: discover information that might be imbedded in the simulation, like being able to recall that the box was chipped or it belonged to your grandmother before she died)</p>\n<p>... kind of Matrix-y, but why not? While simulating our simulations, why not code some extra information in the wall, in a favorite childhood tree, in the image of a horse being whipped?<br /><br />And now some of the mataphysical stuff I've heard doesn't sound half as crazy after all, if they mean that actual-reality and simulated-reality form a dual universe. I can see how people might feel that the simulated reality is the genuine/ordinant reality: reality cannot be perceived without perceiving the simulated reality, but simulated reality can be perceived without perceiving reality. (For example, I can't see a doughnut without also imagining it, but I can imagine a doughnut without seeing it.) I can also see how dualists would see physical objects as embedded with meaning. \"But not <em>physically</em>,\" they'll say. But yet -- it is the physical object that has the meaning? \"Not <em>exactly</em>,\"they'll say. Kind of like some platonic entity that is associated with it...</p>\n<p>&nbsp;</p>\n<!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]-->\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SYmubibg7wfuLztpQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 3, "extendedScore": null, "score": 5.516365232737586e-07, "legacy": true, "legacyId": "2090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dMCFk2n2ur8n62hqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-10T00:26:56.846Z", "modifiedAt": null, "url": null, "title": "Savulescu: \"Genetically enhance humanity or face extinction\"", "slug": "savulescu-genetically-enhance-humanity-or-face-extinction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5ZSHQMsnhtxgeTEAS/savulescu-genetically-enhance-humanity-or-face-extinction", "pageUrlRelative": "/posts/5ZSHQMsnhtxgeTEAS/savulescu-genetically-enhance-humanity-or-face-extinction", "linkUrl": "https://www.lesswrong.com/posts/5ZSHQMsnhtxgeTEAS/savulescu-genetically-enhance-humanity-or-face-extinction", "postedAtFormatted": "Sunday, January 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Savulescu%3A%20%22Genetically%20enhance%20humanity%20or%20face%20extinction%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASavulescu%3A%20%22Genetically%20enhance%20humanity%20or%20face%20extinction%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZSHQMsnhtxgeTEAS%2Fsavulescu-genetically-enhance-humanity-or-face-extinction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Savulescu%3A%20%22Genetically%20enhance%20humanity%20or%20face%20extinction%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZSHQMsnhtxgeTEAS%2Fsavulescu-genetically-enhance-humanity-or-face-extinction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5ZSHQMsnhtxgeTEAS%2Fsavulescu-genetically-enhance-humanity-or-face-extinction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 252, "htmlBody": "<p>In this video, Julian Savulescu from the <a href=\"http://www.practicalethics.ox.ac.uk/\">Uehiro centre for Practical Ethics</a> argues that human beings are \"Unfit for the future\" - that radical technological advance, liberal democracy and human nature will combine to make the 21st century the century of global catastropes, perpetrated by terrorists and psychopaths, with tools such as engineered viruses. He goes on to argue that enhanced intelligence and a reduced urge to violence and defection in large commons problems could be achieved using science, and may be a way out for humanity.</p>\n<p>&nbsp;</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p align=\"center\">\n<object width=\"400\" height=\"327\" data=\"http://vimeo.com/moogaloop.swf?clip_id=7515623&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=0&amp;color=&amp;fullscreen=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowfullscreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://vimeo.com/moogaloop.swf?clip_id=7515623&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=0&amp;color=&amp;fullscreen=1\" />\n</object>\n</p>\n<p align=\"center\">Skip to 1:30 to avoid the tedious introduction</p>\n<p><a href=\"http://vimeo.com/7515623\">Genetically enhance humanity or face extinction - PART 1</a> from <a href=\"http://vimeo.com/user2547332\">Ethics of the New Biosciences</a> on <a href=\"http://vimeo.com\">Vimeo</a>.</p>\n<p align=\"center\">&nbsp;</p>\n<p align=\"center\">\n<object width=\"400\" height=\"327\" data=\"http://vimeo.com/moogaloop.swf?clip_id=7681585&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=0&amp;color=&amp;fullscreen=1\" type=\"application/x-shockwave-flash\">\n<param name=\"allowfullscreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://vimeo.com/moogaloop.swf?clip_id=7681585&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=0&amp;color=&amp;fullscreen=1\" />\n</object>\n</p>\n<p>&nbsp;</p>\n<p><a href=\"http://vimeo.com/7681585\">Genetically enhance humanity or face extinction - PART 2</a> from <a href=\"http://vimeo.com/user2547332\">Ethics of the New Biosciences</a> on <a href=\"http://vimeo.com\">Vimeo</a>.</p>\n<p>&nbsp;</p>\n<p>Well, <a href=\"/lw/10l/intelligence_enhancement_as_existential_risk/\">I have already said something rather like this</a>. Perhaps this really is a good idea, more important, even, than coding a friendly AI? AI timelines where super-smart AI doesn't get invented until 2060+ would leave enough room for human intelligence enhancement to happen and have an effect. When I collected some SIAI volunteers' opinions on this, most thought that there was a very significant chance that super-smart AI will arrive sooner than that, though.</p>\n<p>A large portion of the video consists of pointing out the very strong scientific case that our behavior is a result of the way our brains are structured, and that this means that changes in our behavior are the result of changes in the way our brains are wired.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5ZSHQMsnhtxgeTEAS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 7, "extendedScore": null, "score": 2e-06, "legacy": true, "legacyId": "2093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 235, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ycr3CyrnZLFC7mb5W"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-10T07:31:03.775Z", "modifiedAt": null, "url": null, "title": "Dennett's \"Consciousness Explained\": Prelude", "slug": "dennett-s-consciousness-explained-prelude", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:54.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bNhTM2ypGBjkGHcKM/dennett-s-consciousness-explained-prelude", "pageUrlRelative": "/posts/bNhTM2ypGBjkGHcKM/dennett-s-consciousness-explained-prelude", "linkUrl": "https://www.lesswrong.com/posts/bNhTM2ypGBjkGHcKM/dennett-s-consciousness-explained-prelude", "postedAtFormatted": "Sunday, January 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dennett's%20%22Consciousness%20Explained%22%3A%20Prelude&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADennett's%20%22Consciousness%20Explained%22%3A%20Prelude%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNhTM2ypGBjkGHcKM%2Fdennett-s-consciousness-explained-prelude%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dennett's%20%22Consciousness%20Explained%22%3A%20Prelude%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNhTM2ypGBjkGHcKM%2Fdennett-s-consciousness-explained-prelude", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbNhTM2ypGBjkGHcKM%2Fdennett-s-consciousness-explained-prelude", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 854, "htmlBody": "<p>I'm starting Dennett's \"Consciousness Explained\".&nbsp; Dennett says, in the introduction, that he believes he has solved the problem of consciousness.&nbsp; Since several people have referred to his work here with approval, I'm going to give it a go.&nbsp; I'm going to post chapter summaries as I read, for my own selfish benefit, so that you can point out when you disagree with my understanding of it.&nbsp; \"D\" will stand for Dennett.</p>\n<p>If you loathe the C-word, just stop now.&nbsp; That's what the convenient break just below is for.&nbsp; You are responsible for your own wasted time if you proceed.<a id=\"more\"></a></p>\n<h2>Chpt. 1: Prelude: How are Hallucinations Possible?</h2>\n<p>D describes the brain in a vat, and asks how we can know we aren't brains in vats.&nbsp; This dismays me, as it is one of those questions that distracts people trying to talk about consciousness, that has nothing to do with the difficult problems of consciousness.</p>\n<p>Dennett states, without presenting a single number, that the bandwidth needs for reproducing our sensory experience would be so great that it is <em>impossible</em> (his actual word); and that this proves that we are not brains in vats.&nbsp; Sigh.</p>\n<p>He then asks how hallucinations are possible: \"How on earth can a single brain do what teams of scientists and computer animators would find to be almost impossible?\"&nbsp; Sigh again.&nbsp; This is surprising to Dennett because he believes he has just established that the bandwidth needs for consciousness are too great for any computer to provide; yet the brain sometimes (during hallucinations) provides nearly that much bandwidth.&nbsp; D has apparently forgotten that the brain provides exactly, <em>by definition</em>, the consciousness bandwidth of information to us all the time.</p>\n<p>D recounts Descartes' remarkably prescient discussion of the bellpull as an analogy for how the brain could send us phantom misinformation; but dismisses it, saying, \"there is no way the brain as illusionist could store and manipulate enough false information to fool an inquiring mind.\"&nbsp; Sigh.&nbsp; Now not only consciousness, but also dreams, are impossible.&nbsp; However, D then comes back to dreams, and is aware they exist and are hallucinations; so either he or I is misunderstanding this section.</p>\n<p>On p. 12 he suggests something interesting: Perception is driven both bottom-up (from the senses) and top-down (from our expectations).&nbsp; A hallucination could happen when the bottom-up channel is cut off.&nbsp; D doesn't get into data compression at all, but I think a better way to phrase this is that, given arbitrary bottom-up data, the mind can decompress sensory input into the most likely interpretation given the data and given its knowledge about the world.&nbsp; Internally, we should expect that high-bandwidth sensory data is summarized somewhere in a compressed form.&nbsp; Compressed data necessarily looks more random than prior to compression.&nbsp; This means that, somewhere inside the mind, we should expect it to be harder than naive introspection suggests to distinguish between true sensory data and random sensory noise.&nbsp; D suggests an important role for an adjustable sensitivity threshold for accepting/rejecting suggested interpretations of sense data.</p>\n<p>D dismisses Freud's ideas about dreams - that they are stories about our current concerns, hidden under symbolism in order to sneak past our internal censors - by observing that we should not posit homunculi inside our brains who are smarter than we are.</p>\n<p>[In summary, this chapter contained some bone-headed howlers, and some interesting things; but on the whole, it makes me doubt that D is going to address the problem of consciousness.&nbsp; He seems, instead, on a trajectory to try to explain how a brain can produce intelligent action.&nbsp; It sounds like he plans to talk about the architecture of human intelligence, although he does promise to address qualia in part III.</p>\n<p>Repeatedly on LW, I've seen one person (frequently Mitchell Porter) raise the problem of qualia; and seen otherwise-intelligent people reply by saying science has got it covered, consciousness is a property of physical systems, nothing to worry about.&nbsp; For some reason, a lot of very bright people cannot see that consciousness is a big, strange problem.&nbsp; Not <em>intelligence</em>, not even assigning meaning to representations, but consciousness.&nbsp; It is a different problem.&nbsp; (A complete explanation of how intelligence and symbol-grounding take place in humans might concomitantly explain consciousness; it does not follow, as most people seem to think it does, that demonstrating a way to account for non-human intelligence and symbol-grounding therefore accounts for consciousness.)</p>\n<p>Part of the problem is their theistic opponents, who hopelessly muddle intelligence, consciousness, and religion:&nbsp; \"A computer can never write a symphony.&nbsp; Therefore consciousness is metaphysical; therefore I have a soul; therefore there is life after-death.\"&nbsp; I think this line of reasoning has been presented to us all so often that a lot of us have cached it, to the extent that it injects itself into our <em>own</em> reasoning.&nbsp; People on LW who try to elucidate the problem of qualia inevitably get dismissed as quasi-theists, because, historically, all of the people saying things that sound similar were theists.</p>\n<p>At this point, I suspect that Dennett has contributed to this confusion, by writing a book about intelligence and claiming not just that it's about consciousness, but that it has solved the problem.&nbsp; I shall see.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bNhTM2ypGBjkGHcKM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 14, "extendedScore": null, "score": 5.518942883193916e-07, "legacy": true, "legacyId": "2095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-10T23:38:55.449Z", "modifiedAt": null, "url": null, "title": "Dennett's \"Consciousness Explained\": Chpt 2", "slug": "dennett-s-consciousness-explained-chpt-2", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:02.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gKBCveCSjXPgNRfsA/dennett-s-consciousness-explained-chpt-2", "pageUrlRelative": "/posts/gKBCveCSjXPgNRfsA/dennett-s-consciousness-explained-chpt-2", "linkUrl": "https://www.lesswrong.com/posts/gKBCveCSjXPgNRfsA/dennett-s-consciousness-explained-chpt-2", "postedAtFormatted": "Sunday, January 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dennett's%20%22Consciousness%20Explained%22%3A%20Chpt%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADennett's%20%22Consciousness%20Explained%22%3A%20Chpt%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKBCveCSjXPgNRfsA%2Fdennett-s-consciousness-explained-chpt-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dennett's%20%22Consciousness%20Explained%22%3A%20Chpt%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKBCveCSjXPgNRfsA%2Fdennett-s-consciousness-explained-chpt-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgKBCveCSjXPgNRfsA%2Fdennett-s-consciousness-explained-chpt-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1012, "htmlBody": "<h2>Chapter 2: Explaining Consciousness</h2>\n<p>This chapter is all about dualism and why it's bad.&nbsp; I find this chapter incoherent, because Dennett never defines dualism carefully, and confuses it with the theistic views historically associated with dualism.</p>\n<p><em><a id=\"more\"></a></em>[Long stretches of my own ideas will be in brackets, like this.]</p>\n<p><strong>1. Should consciousness be demystified?</strong></p>\n<p>D begins with a defense against those who don't <em>want</em> an explanation of consciousness.&nbsp; I'm not even going to read this section.</p>\n<p><strong>2. The mystery of consciousness</strong></p>\n<p>\"What could be more obvious or certain to each of us than that he or she is a conscious subject of experience, an enjoyer of perceptions and sensations, a sufferer of pain, an entertainer of ideas, and a conscious deliberator? ... How can living physical bodies in the physical world produce such phenomena?\"</p>\n<p><strong>3. The attractions of mind stuff</strong></p>\n<p>D says Searle describes functionalism by saying that if a computer program reproduced the entire functional structure of a human wine taster's cognitive system, functionalism says that it would reproduce all the mental properties, including enjoyment.&nbsp; This is followed by a [long, pointless, popular] discussion of whether volcanos, hurricanes, etc., have souls.</p>\n<p>The \"mind stuff\" position D sets up in order to knock down is not John Searle's \"mind stuff\" position, nor Roger Penrose's, but something spiritual and non-physical.&nbsp; He says that the mind-stuff view is that \"the conscious mind... cannot just be the brain... because nothing in the brain could appreciate wine.\"&nbsp; [D should pursue this further, because the summary given of the \"mind stuff\" view is too incoherent to attack.]</p>\n<p><strong>4. Why dualism is forlorn</strong></p>\n<p>D says that the \"mind stuff\" view is that the brain is \"composed not of ordinary matter\", and is dualism.&nbsp; Materialists, OTOH, say we can account for all mental phenomena using the same principles that explain other things.&nbsp; Dualism is bad; materialism is good.</p>\n<p>D says that if dualism were true, then the mind would need to act on our body, so we could move our arms.&nbsp; But to move, a physical thing needs energy; and how can non-matter produce energy?&nbsp; Dualism is thus dead, QED.</p>\n<p>[IMHO this muddies the waters terribly, in a way I've seen them muddied many times before.&nbsp; The dichotomy between dualism and materialism is a false dichotomy.&nbsp; There are no \"materialists\" who believe only in a kinematic world.&nbsp; (Ironically, Descartes, the classic dualist, was just such a materialist when it came to the physical world; he refused to believe in gravity for that reason.)&nbsp; The physics that we believe in is full of other forces such as gravity, electricity, and magnetism.&nbsp; Energy can exist in all these forms, as well as in matter.&nbsp; D is one of the many who reject \"dualism\", yet accept the everyday mysterious forces; and presumably approve of people who accepted them long before there was any material explanation of them.</p>\n<p>If we want to be able to dismiss people for positing new \"stuff\", the way we currently do by calling them dualists, then we need a way to distinguish an acceptable new fundamental force or type of being, such as gravity, from an unacceptable one.&nbsp; Obeying the law of conservation of energy is one such rule; if Descartes wants to have a non-material soul, it must exchange energy with ordinary matter in predictable ways.&nbsp; To put it another way, rejecting \"dualism\" can make sense if we define dualism as the belief in things that don't obey conservation laws.</p>\n<p>Once we've done that, though, we find that all the old enemies we hoped to dismiss as dualists can sneak back in by claiming to observe the conservation laws.&nbsp; Even magic systems in fantasy worlds often obey energy conservation laws.</p>\n<p>Even this conservative position is problematic.&nbsp; EY believes in many worlds.&nbsp; Many worlds seems, at least to many people like me who consider it a respectable position without understanding it, to require a stupendous, continual violation of conservation laws.&nbsp; But we don't usually therefore call EY a \"bad dualist\" and dismiss many-worlds.</p>\n<p>My intuition is that we are willing to consider even very crazy-sounding new proposed extensions to physics, if we believe they are made with a sincere desire to understand the universe.&nbsp; Historically, a \"dualist\" is usually someone, such as Descartes, who is trying to come up with an excuse for <em>not</em> trying to understand the universe.&nbsp; The term \"dualist\" is an accusation against a person's intent masquerading as an objection to their physics.]</p>\n<p>D also refutes dualism by saying that mind stuff can't both elude physical measurement, and control the body; as anything that escapes our instruments of detection can't possibly interact with the body to control it.&nbsp; [The problem with this argument is that, 200 years ago, if someone had told you that the brain used electrical impulses, you could have used the exact same argument to \"prove\" that that was impossible.]</p>\n<p>D considers this angle on the next page, without realizing that it devastates his last several pages of argument:&nbsp; \"Perhaps some basic enlargement of the ontology of the physical sciences is called for in order to account for the phenomena of consciousness.\"</p>\n<p>He also gives us what I take to be the defining characteristic of \"bad\" dualism: \"The few dualists to avow their views openly have all candidly and comfortably announced that they have no theory whatever of how the mind works -- something, they insist, that is quite beyond human ken.\"&nbsp; [This shows us that what D dismisses as \"dualism\" has nothing to do with making a dualistic distinction between matter and non-matter; and everything to do with the intentions of the theorist.&nbsp; What D objects to as \"dualism\" is actually the God argument that has been historically associated with dualism:&nbsp; Stopping further inquiry by positing the existence of something that a) explains, and b) cannot be explained.]</p>\n<p><strong>5. The challenge</strong></p>\n<p>D lays down rules for himself to follow: To explain consciousness, with only existing science, while acknowledging his own conscious experience.</p>\n<p><strong>Summary</strong></p>\n<p>[This chapter will confuse more than inform.&nbsp; Its only purpose is to refute dualism; but D hasn't taken a close look at what he means when he uses that word, so he mingles together its meaning and its associations and historical contingiencies indiscriminately.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gKBCveCSjXPgNRfsA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 3, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "2096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Chapter_2__Explaining_Consciousness\">Chapter 2: Explaining Consciousness</h2>\n<p>This chapter is all about dualism and why it's bad.&nbsp; I find this chapter incoherent, because Dennett never defines dualism carefully, and confuses it with the theistic views historically associated with dualism.</p>\n<p><em><a id=\"more\"></a></em>[Long stretches of my own ideas will be in brackets, like this.]</p>\n<p><strong id=\"1__Should_consciousness_be_demystified_\">1. Should consciousness be demystified?</strong></p>\n<p>D begins with a defense against those who don't <em>want</em> an explanation of consciousness.&nbsp; I'm not even going to read this section.</p>\n<p><strong id=\"2__The_mystery_of_consciousness\">2. The mystery of consciousness</strong></p>\n<p>\"What could be more obvious or certain to each of us than that he or she is a conscious subject of experience, an enjoyer of perceptions and sensations, a sufferer of pain, an entertainer of ideas, and a conscious deliberator? ... How can living physical bodies in the physical world produce such phenomena?\"</p>\n<p><strong id=\"3__The_attractions_of_mind_stuff\">3. The attractions of mind stuff</strong></p>\n<p>D says Searle describes functionalism by saying that if a computer program reproduced the entire functional structure of a human wine taster's cognitive system, functionalism says that it would reproduce all the mental properties, including enjoyment.&nbsp; This is followed by a [long, pointless, popular] discussion of whether volcanos, hurricanes, etc., have souls.</p>\n<p>The \"mind stuff\" position D sets up in order to knock down is not John Searle's \"mind stuff\" position, nor Roger Penrose's, but something spiritual and non-physical.&nbsp; He says that the mind-stuff view is that \"the conscious mind... cannot just be the brain... because nothing in the brain could appreciate wine.\"&nbsp; [D should pursue this further, because the summary given of the \"mind stuff\" view is too incoherent to attack.]</p>\n<p><strong id=\"4__Why_dualism_is_forlorn\">4. Why dualism is forlorn</strong></p>\n<p>D says that the \"mind stuff\" view is that the brain is \"composed not of ordinary matter\", and is dualism.&nbsp; Materialists, OTOH, say we can account for all mental phenomena using the same principles that explain other things.&nbsp; Dualism is bad; materialism is good.</p>\n<p>D says that if dualism were true, then the mind would need to act on our body, so we could move our arms.&nbsp; But to move, a physical thing needs energy; and how can non-matter produce energy?&nbsp; Dualism is thus dead, QED.</p>\n<p>[IMHO this muddies the waters terribly, in a way I've seen them muddied many times before.&nbsp; The dichotomy between dualism and materialism is a false dichotomy.&nbsp; There are no \"materialists\" who believe only in a kinematic world.&nbsp; (Ironically, Descartes, the classic dualist, was just such a materialist when it came to the physical world; he refused to believe in gravity for that reason.)&nbsp; The physics that we believe in is full of other forces such as gravity, electricity, and magnetism.&nbsp; Energy can exist in all these forms, as well as in matter.&nbsp; D is one of the many who reject \"dualism\", yet accept the everyday mysterious forces; and presumably approve of people who accepted them long before there was any material explanation of them.</p>\n<p>If we want to be able to dismiss people for positing new \"stuff\", the way we currently do by calling them dualists, then we need a way to distinguish an acceptable new fundamental force or type of being, such as gravity, from an unacceptable one.&nbsp; Obeying the law of conservation of energy is one such rule; if Descartes wants to have a non-material soul, it must exchange energy with ordinary matter in predictable ways.&nbsp; To put it another way, rejecting \"dualism\" can make sense if we define dualism as the belief in things that don't obey conservation laws.</p>\n<p>Once we've done that, though, we find that all the old enemies we hoped to dismiss as dualists can sneak back in by claiming to observe the conservation laws.&nbsp; Even magic systems in fantasy worlds often obey energy conservation laws.</p>\n<p>Even this conservative position is problematic.&nbsp; EY believes in many worlds.&nbsp; Many worlds seems, at least to many people like me who consider it a respectable position without understanding it, to require a stupendous, continual violation of conservation laws.&nbsp; But we don't usually therefore call EY a \"bad dualist\" and dismiss many-worlds.</p>\n<p>My intuition is that we are willing to consider even very crazy-sounding new proposed extensions to physics, if we believe they are made with a sincere desire to understand the universe.&nbsp; Historically, a \"dualist\" is usually someone, such as Descartes, who is trying to come up with an excuse for <em>not</em> trying to understand the universe.&nbsp; The term \"dualist\" is an accusation against a person's intent masquerading as an objection to their physics.]</p>\n<p>D also refutes dualism by saying that mind stuff can't both elude physical measurement, and control the body; as anything that escapes our instruments of detection can't possibly interact with the body to control it.&nbsp; [The problem with this argument is that, 200 years ago, if someone had told you that the brain used electrical impulses, you could have used the exact same argument to \"prove\" that that was impossible.]</p>\n<p>D considers this angle on the next page, without realizing that it devastates his last several pages of argument:&nbsp; \"Perhaps some basic enlargement of the ontology of the physical sciences is called for in order to account for the phenomena of consciousness.\"</p>\n<p>He also gives us what I take to be the defining characteristic of \"bad\" dualism: \"The few dualists to avow their views openly have all candidly and comfortably announced that they have no theory whatever of how the mind works -- something, they insist, that is quite beyond human ken.\"&nbsp; [This shows us that what D dismisses as \"dualism\" has nothing to do with making a dualistic distinction between matter and non-matter; and everything to do with the intentions of the theorist.&nbsp; What D objects to as \"dualism\" is actually the God argument that has been historically associated with dualism:&nbsp; Stopping further inquiry by positing the existence of something that a) explains, and b) cannot be explained.]</p>\n<p><strong id=\"5__The_challenge\">5. The challenge</strong></p>\n<p>D lays down rules for himself to follow: To explain consciousness, with only existing science, while acknowledging his own conscious experience.</p>\n<p><strong id=\"Summary\">Summary</strong></p>\n<p>[This chapter will confuse more than inform.&nbsp; Its only purpose is to refute dualism; but D hasn't taken a close look at what he means when he uses that word, so he mingles together its meaning and its associations and historical contingiencies indiscriminately.]</p>", "sections": [{"title": "Chapter 2: Explaining Consciousness", "anchor": "Chapter_2__Explaining_Consciousness", "level": 1}, {"title": "1. Should consciousness be demystified?", "anchor": "1__Should_consciousness_be_demystified_", "level": 2}, {"title": "2. The mystery of consciousness", "anchor": "2__The_mystery_of_consciousness", "level": 2}, {"title": "3. The attractions of mind stuff", "anchor": "3__The_attractions_of_mind_stuff", "level": 2}, {"title": "4. Why dualism is forlorn", "anchor": "4__Why_dualism_is_forlorn", "level": 2}, {"title": "5. The challenge", "anchor": "5__The_challenge", "level": 2}, {"title": "Summary", "anchor": "Summary", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-11T17:39:55.261Z", "modifiedAt": null, "url": null, "title": "Consciousness Explained, chapter 3", "slug": "consciousness-explained-chapter-3", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:02.893Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/42srRsFxeb6CJyQG6/consciousness-explained-chapter-3", "pageUrlRelative": "/posts/42srRsFxeb6CJyQG6/consciousness-explained-chapter-3", "linkUrl": "https://www.lesswrong.com/posts/42srRsFxeb6CJyQG6/consciousness-explained-chapter-3", "postedAtFormatted": "Monday, January 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Consciousness%20Explained%2C%20chapter%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConsciousness%20Explained%2C%20chapter%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42srRsFxeb6CJyQG6%2Fconsciousness-explained-chapter-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Consciousness%20Explained%2C%20chapter%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42srRsFxeb6CJyQG6%2Fconsciousness-explained-chapter-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F42srRsFxeb6CJyQG6%2Fconsciousness-explained-chapter-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 564, "htmlBody": "<h2>A Visit to the Phenonemological Garden</h2>\n<p>This chapter categorizes and discusses mental phenomena.&nbsp; It emphasizes that we don't re-draw the outer world inside our heads for a little person to look at.<a id=\"more\"></a></p>\n<p><strong>1. Welcome to the phenom</strong></p>\n<p>\"Kant distinguished \"phenomena,\" things as they appear, from \"noumena,\" things as they are in themselves, and during the development of the natural or physical sciences in the 19th century, the term phenomenology came to refer to the merely descriptive study of any subject matter.\"</p>\n<p>[\"Phenomenology\" seems similar to what I recently called \"mere curve-fitting\" in a <a href=\"/lw/1m2/hypotheses_for_dualism/1fdm\">comment exchange</a> about gravity, as something that lets us make accurate predictions about a phenomenon, while still leaving it in the realm of metaphysics (not integrated causally into the rest of physics).]</p>\n<p>D will use the term \"phenom\" to denote the (true) ontology of conscious phenomena.&nbsp; He divides it into</p>\n<ul>\n<li>sense perceptions</li>\n<li>imagined perceptions</li>\n<li>affect</li>\n</ul>\n<p><strong>2. Our experience of the external world</strong></p>\n<p>D tells an interesting fable about a philosopher who denied that the mechanical reproduction of an orchestra's sounds could be possible, due to a failure of his own imagination.&nbsp; He then challenges the naive assumption that, after all the sound of all the different instruments have been translated into a stream of pulses, they get converted back into all the different orchestral sounds in the brain.&nbsp; The distinctive sounds of different instruments, once thought to be unanalyzable, are due to the superposition of a number of different frequencies and amplitudes of pure tones.&nbsp; This leads into the argument that we don't experience sounds in our heads - we're not going to go to the effort to break sound down into frequencies just to get it inside our heads, and then build the original waveform back up again.&nbsp; Similarly, we imagine we hear word boundaries in speech; yet there are no gaps between words in the acoustic energy profile.</p>\n<p>Vision:&nbsp; We don't draw pictures in our heads and then look at them.&nbsp; That would lead to an infinite regress.&nbsp; Besides, it's dark in there.&nbsp; Our visual perception is a conglomeration of different objects at different resolutions tagged with different visual properties.</p>\n<p><strong>3. Our experience of the internal world</strong></p>\n<p>The gist of this section seems to be that we don't imagine things by, eg., drawing pictures in our heads.&nbsp; [I remember an experiment 5 or 10 years ago, in which a subject was able to literally draw a simple figure in its primary visual cortex, detected by fMRI, by imagining it.&nbsp; So I'm not sure this is a meaningful distinction to make.&nbsp; Imagining a scene may start at an end closer to consciousness; but it still ends up re-activating images in topographically-mapped areas.]</p>\n<p><strong>4. Affect</strong></p>\n<p>Fun is a mysterious phenomenon not yet given enough attention by philosophers.&nbsp; Affect and qualia are still mysterious; but D promises (p. 65) to give a materialistic account of them.</p>\n<p><strong>Summary</strong></p>\n<p>[I agree with most of this chapter, with the caveat that a large percentage of our cortex is taken up with topology-preserving maps of our visual field, of the type D says we don't have.&nbsp; I'm willing to overlook this, because I expect that a lot of the \"important stuff\" goes on in more rostral associational brain areas.&nbsp; But in order to make this excuse for D, I have to slide a little toward the \"Cartesian theater of the mind\" view that D is going to spend much of the book arguing against.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "42srRsFxeb6CJyQG6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 1, "extendedScore": null, "score": 5.522705280401721e-07, "legacy": true, "legacyId": "2097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"A_Visit_to_the_Phenonemological_Garden\">A Visit to the Phenonemological Garden</h2>\n<p>This chapter categorizes and discusses mental phenomena.&nbsp; It emphasizes that we don't re-draw the outer world inside our heads for a little person to look at.<a id=\"more\"></a></p>\n<p><strong id=\"1__Welcome_to_the_phenom\">1. Welcome to the phenom</strong></p>\n<p>\"Kant distinguished \"phenomena,\" things as they appear, from \"noumena,\" things as they are in themselves, and during the development of the natural or physical sciences in the 19th century, the term phenomenology came to refer to the merely descriptive study of any subject matter.\"</p>\n<p>[\"Phenomenology\" seems similar to what I recently called \"mere curve-fitting\" in a <a href=\"/lw/1m2/hypotheses_for_dualism/1fdm\">comment exchange</a> about gravity, as something that lets us make accurate predictions about a phenomenon, while still leaving it in the realm of metaphysics (not integrated causally into the rest of physics).]</p>\n<p>D will use the term \"phenom\" to denote the (true) ontology of conscious phenomena.&nbsp; He divides it into</p>\n<ul>\n<li>sense perceptions</li>\n<li>imagined perceptions</li>\n<li>affect</li>\n</ul>\n<p><strong id=\"2__Our_experience_of_the_external_world\">2. Our experience of the external world</strong></p>\n<p>D tells an interesting fable about a philosopher who denied that the mechanical reproduction of an orchestra's sounds could be possible, due to a failure of his own imagination.&nbsp; He then challenges the naive assumption that, after all the sound of all the different instruments have been translated into a stream of pulses, they get converted back into all the different orchestral sounds in the brain.&nbsp; The distinctive sounds of different instruments, once thought to be unanalyzable, are due to the superposition of a number of different frequencies and amplitudes of pure tones.&nbsp; This leads into the argument that we don't experience sounds in our heads - we're not going to go to the effort to break sound down into frequencies just to get it inside our heads, and then build the original waveform back up again.&nbsp; Similarly, we imagine we hear word boundaries in speech; yet there are no gaps between words in the acoustic energy profile.</p>\n<p>Vision:&nbsp; We don't draw pictures in our heads and then look at them.&nbsp; That would lead to an infinite regress.&nbsp; Besides, it's dark in there.&nbsp; Our visual perception is a conglomeration of different objects at different resolutions tagged with different visual properties.</p>\n<p><strong id=\"3__Our_experience_of_the_internal_world\">3. Our experience of the internal world</strong></p>\n<p>The gist of this section seems to be that we don't imagine things by, eg., drawing pictures in our heads.&nbsp; [I remember an experiment 5 or 10 years ago, in which a subject was able to literally draw a simple figure in its primary visual cortex, detected by fMRI, by imagining it.&nbsp; So I'm not sure this is a meaningful distinction to make.&nbsp; Imagining a scene may start at an end closer to consciousness; but it still ends up re-activating images in topographically-mapped areas.]</p>\n<p><strong id=\"4__Affect\">4. Affect</strong></p>\n<p>Fun is a mysterious phenomenon not yet given enough attention by philosophers.&nbsp; Affect and qualia are still mysterious; but D promises (p. 65) to give a materialistic account of them.</p>\n<p><strong id=\"Summary\">Summary</strong></p>\n<p>[I agree with most of this chapter, with the caveat that a large percentage of our cortex is taken up with topology-preserving maps of our visual field, of the type D says we don't have.&nbsp; I'm willing to overlook this, because I expect that a lot of the \"important stuff\" goes on in more rostral associational brain areas.&nbsp; But in order to make this excuse for D, I have to slide a little toward the \"Cartesian theater of the mind\" view that D is going to spend much of the book arguing against.]</p>", "sections": [{"title": "A Visit to the Phenonemological Garden", "anchor": "A_Visit_to_the_Phenonemological_Garden", "level": 1}, {"title": "1. Welcome to the phenom", "anchor": "1__Welcome_to_the_phenom", "level": 2}, {"title": "2. Our experience of the external world", "anchor": "2__Our_experience_of_the_external_world", "level": 2}, {"title": "3. Our experience of the internal world", "anchor": "3__Our_experience_of_the_internal_world", "level": 2}, {"title": "4. Affect", "anchor": "4__Affect", "level": 2}, {"title": "Summary", "anchor": "Summary", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-11T21:59:22.776Z", "modifiedAt": null, "url": null, "title": "The things we know that we know ain't so", "slug": "the-things-we-know-that-we-know-ain-t-so", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:01.958Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zGvbEJcyp9TMqrtPe/the-things-we-know-that-we-know-ain-t-so", "pageUrlRelative": "/posts/zGvbEJcyp9TMqrtPe/the-things-we-know-that-we-know-ain-t-so", "linkUrl": "https://www.lesswrong.com/posts/zGvbEJcyp9TMqrtPe/the-things-we-know-that-we-know-ain-t-so", "postedAtFormatted": "Monday, January 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20things%20we%20know%20that%20we%20know%20ain't%20so&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20things%20we%20know%20that%20we%20know%20ain't%20so%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGvbEJcyp9TMqrtPe%2Fthe-things-we-know-that-we-know-ain-t-so%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20things%20we%20know%20that%20we%20know%20ain't%20so%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGvbEJcyp9TMqrtPe%2Fthe-things-we-know-that-we-know-ain-t-so", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzGvbEJcyp9TMqrtPe%2Fthe-things-we-know-that-we-know-ain-t-so", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 431, "htmlBody": "<p>We're all familiar with false popular memes that spread faster than they can be stomped out:&nbsp; You only use 10% of your brain.&nbsp; Al Gore said he invented the internet.&nbsp; Perhaps it doesn't surprise you that some memes in popular culture can't be killed.&nbsp; But does the same thing happen in science?<a id=\"more\"></a></p>\n<p>Most of you have probably heard of Broca's aphasia and Wernicke's aphasia.&nbsp; Every textbook and every college course on language and the brain describes the connection between damage to these areas, and the speech deficits named after them.</p>\n<p>Also, both are probably wrong.&nbsp; Both areas were mistakenly associated with their aphasias because they are near or surrounded by other areas which, when damaged, cause the aphasias.&nbsp; Yet our schools continue teaching the traditional, erroneous story; including a <a href=\"http://ocw.mit.edu/OcwWeb/Brain-and-Cognitive-Sciences/9-14Spring-2005/CourseHome/index.htm\">lecture in 9.14 at MIT given in 2005</a>.&nbsp; Both the <a href=\"http://en.wikipedia.org/wiki/Receptive_aphasia\">Wikipedia entry on Wernicke's aphasia</a> and the <a href=\"http://en.wikipedia.org/wiki/Expressive_aphasia\">Wikipedia entry on Broca's aphasia</a> are still in error; the <a href=\"http://en.wikipedia.org/wiki/Wernicke%27s_area\">Wikipedia entry on Wernicke's area</a> has got it straight.</p>\n<p>Is it because this information is considered unimportant?&nbsp; Hardly; it's probably the only functional association you will find in every course and every book on the brain.</p>\n<p>Is it because the information is too new to have penetrated the field?&nbsp; No; see the dates on the references below.</p>\n<p>In spite of this failure in education, are the experts thoroughly familiar with this information?&nbsp; Possibly not; this <a href=\"http://brainlang.georgetown.edu/PUBS/Ullman_Cortex_06.pdf\">2006 paper on Broca's area</a> by a renowned expert does not mention it.&nbsp; (In its defense, it references many other studies in which damage to Broca's area is associated with language deficits.)</p>\n<p>So:</p>\n<ul>\n<li>Am I wrong, and the evidence still implicates Broca's area and Wernicke's area in their aphasias above other areas?</li>\n<li>If I'm right, why can't the new understanding displace the old understanding?</li>\n<li>Is this a general failure in the way we do science?&nbsp; Can you think of other examples where an important discovery can't penetrate its field?</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>References</strong></p>\n<p><span class=\"citation Journal\">Bogen JE, Bogen GM (1976). <a class=\"external text\" rel=\"nofollow\" href=\"http://www3.interscience.wiley.com/resolve/openurl?genre=article&amp;sid=nlm:pubmed&amp;issn=0077-8923&amp;date=1976&amp;volume=280&amp;spage=834\">Wernicke's region&mdash;Where is it?</a> <em>Ann. N. Y. Acad. Sci.</em> <strong>280</strong>: 834&ndash;43.</span></p>\n<p><span class=\"citation Journal\">Dronkers, N. F., Shapiro, J. K., Redfern, B., &amp; Knight, R. T. (1992). The role of Broca&rsquo;s area in Broca&rsquo;s aphasia.<br />Journal of Clinical and Experimental Neuropsychology, 14, 52&ndash;53.<br /></span></p>\n<p><span class=\"citation book\">Dronkers NF., Redfern B B., Knight R T. (2000). The neural architecture of language disorders. in Bizzi, Emilio; Gazzaniga, Michael S.. <em>The New cognitive neurosciences</em> (2nd ed.). Cambridge, Mass: MIT Press. pp.&nbsp;949&ndash;58.</span></p>\n<p><span class=\"citation book\">Dronkers et al. (2004).&nbsp; Lesion analysis of the brain areas involved in language comprehension.&nbsp; <em>Cognition 92</em>: 145-177.</span></p>\n<p><span class=\"citation Journal\">Mohr, J. P. (1976). Broca&rsquo;s area and Broca&rsquo;s aphasia. In H. Whitaker, <em>Studies in neurolinguistics</em>, New York: Academic Press.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zGvbEJcyp9TMqrtPe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 5.523182057881109e-07, "legacy": true, "legacyId": "2102", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-12T13:56:02.692Z", "modifiedAt": null, "url": null, "title": "We desire genetically attributed success", "slug": "we-desire-genetically-attributed-success", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MoP6gPWkZdFdfQPMj/we-desire-genetically-attributed-success", "pageUrlRelative": "/posts/MoP6gPWkZdFdfQPMj/we-desire-genetically-attributed-success", "linkUrl": "https://www.lesswrong.com/posts/MoP6gPWkZdFdfQPMj/we-desire-genetically-attributed-success", "postedAtFormatted": "Tuesday, January 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20We%20desire%20genetically%20attributed%20success&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWe%20desire%20genetically%20attributed%20success%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoP6gPWkZdFdfQPMj%2Fwe-desire-genetically-attributed-success%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=We%20desire%20genetically%20attributed%20success%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoP6gPWkZdFdfQPMj%2Fwe-desire-genetically-attributed-success", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMoP6gPWkZdFdfQPMj%2Fwe-desire-genetically-attributed-success", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>A certain girl friend of mine isn't very successful in romantic relationships. But whenever I advise her to take a more rational approach to hunting men, she says my perfectly sensible suggestions still make her feel uneasy because things ought to happen <em>naturally</em>. Is this kind of thinking a cognitive mistake? I'm not sure...</p>\n<p>A little reflection turns up other examples where people don't just desire success, but specifically natural, <em>genetically attributed</em> success. In ego-heavy areas like game design, everybody starts out wanting to be the master creator rather than humble participant, even if collaboration yields a much bigger pie. (<a href=\"http://www.gamasutra.com/view/feature/3408/the_cabal_valves_design_process_.php\">Linky</a> on how an egoless environment aids game creation.)</p>\n<p>This behavior is most easily explained by status-seeking, but I feel there's something else at play: a kind of lingering desire to prove to myself that I was worthy from the start. It might be self-signaling or an unexpected application of <a href=\"http://hanson.gmu.edu/fairgene.html\">Robin Hanson's idea</a> that we enjoy clear fitness signals on general principle. Not sure on this point either - it sounds plausible but weird.</p>\n<p>I first got the idea for this post while thinking about <a href=\"/lw/3h/why_our_kind_cant_cooperate\">cooperation</a>. Duh, if I join some collaborative endeavor, it had better be about proving <em>my</em> worth! But when we insist on getting status rather than (say) money, we may be passing over opportunities to cooperate, so at least in some cases it might be helpful to override the evolution-provided default.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MoP6gPWkZdFdfQPMj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "2104", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-12T16:36:56.176Z", "modifiedAt": null, "url": null, "title": "High Status and Stupidity: Why?", "slug": "high-status-and-stupidity-why", "viewCount": null, "lastCommentedAt": "2018-10-12T21:51:58.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cgrvvp9QzjiFuYwLi/high-status-and-stupidity-why", "pageUrlRelative": "/posts/cgrvvp9QzjiFuYwLi/high-status-and-stupidity-why", "linkUrl": "https://www.lesswrong.com/posts/cgrvvp9QzjiFuYwLi/high-status-and-stupidity-why", "postedAtFormatted": "Tuesday, January 12th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20High%20Status%20and%20Stupidity%3A%20Why%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHigh%20Status%20and%20Stupidity%3A%20Why%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcgrvvp9QzjiFuYwLi%2Fhigh-status-and-stupidity-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=High%20Status%20and%20Stupidity%3A%20Why%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcgrvvp9QzjiFuYwLi%2Fhigh-status-and-stupidity-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcgrvvp9QzjiFuYwLi%2Fhigh-status-and-stupidity-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 852, "htmlBody": "<p>Michael Vassar once <a href=\"/lw/ir/science_as_attire/#erj\">suggested</a>:&nbsp; \"Status makes people effectively stupid, as it makes it harder for them to update their public positions without feeling that they are losing face.\"</p>\n<p>To the extent that status does, in fact, make people stupid, this is a rather important phenomenon for a society like ours in which practically all decisions and beliefs pass through the hands of very-high-status individuals (a high \"cognitive Gini coefficient\").</p>\n<p><em>Does </em>status actually make people stupid?&nbsp; It's hard to say because I haven't tracked many careers over time.&nbsp; I do have a definite and strong impression, with respect to many high-status individuals, that it would have been a lot easier to have an intelligent conversation with them, if I'd approached them before they made it big.&nbsp; But where does that impression come from, since I haven't actually tracked them over time?&nbsp; (Fundamental question of rationality:&nbsp; What do you think you know and how do you think you know it?)&nbsp; My best guess for why my brain seems to believe this:&nbsp; I know it's possible to have intelligent conversations with smart grad students, and I get the strong impression that high-status people <em>used to be </em>those grad students, but now it's much harder to have intelligent conversations with them than with smart grad students.</p>\n<p>Hypotheses:</p>\n<ol>\n<li>Vassar's hypothesis:&nbsp; Higher status increases the amount of face you lose when you change your mind, or increases the cost of losing face.</li>\n<li>The open-mindedness needed to consider interesting new ideas is (<a href=\"http://wiki.lesswrong.com/wiki/Adaptation_executers\">was</a>) only an evolutionary advantage for low-status individuals seeking a good idea to ride to high status.&nbsp; Once high status is achieved, new ideas are high-risk gambles with less relative payoff - the optimal strategy is to be mainstream.&nbsp; I think Robin Hanson had a post about this but I can't recall the title.</li>\n<li>Intelligence as such is a high-cost feature which is no longer necessary once status is achieved.&nbsp; We can call this the <a href=\"http://funpeople.org/1992/1992AAC.html\">Llinas Hypothesis</a>.</li>\n<li>High-status individuals were intelligent when they were young; the observed disparity is due solely to the standard declines of aging.<a id=\"more\"></a></li>\n<li>High-status individuals spend more time on dinners and politics, and less time on problem-solving and reading; they exercise their minds less.</li>\n<li>High-status individuals are under less pressure to perform, in general. </li>\n<li>High-status individuals are just as smart as they ever were, but when you or I try to approach them, the status <em>disparity</em> makes it harder to converse with them - they would sound just as intelligent if we had higher status ourselves.</li>\n<li>High-status individuals feel less social pressure to listen to your arguments, respond articulately to them, or change their minds when their own arguments are inadequate, which decreases their apparent or real intelligence.</li>\n<li>High-status individuals become more convinced of their ideas' rightness or of their own competence.</li>\n<li>High-status individuals get less honest advice from their friends, especially about their own failings.</li>\n</ol>\n<p>Did I miss anything important?</p>\n<p>Having achieved some small degree of status in certain very limited circles, here's what I do to try to avoid the status-makes-you-stupid effect:</p>\n<ul>\n<li>I try to feel a small flash of self-satisfaction whenever I publicly admit that I am wrong, over what a good rationalist I am being and what a good impression I am making.&nbsp; Not so much satisfaction that I forget that it's better to be correct in the first place, but enough to be a counter-force to the fear of losing face.</li>\n<li>I consistently refuse to be drawn into running the Singularity Institute.&nbsp; I have an overwhelming sense of doom about what happens if I start going down that road.</li>\n<li>I try in general to avoid sending my brain signals which tell it that I am high-status, just in case that causes my brain to decide it is no longer necessary.&nbsp; In fact I try to avoid sending my brain signals which tell it that I have achieved acceptance in my tribe.&nbsp; When my brain begins thinking something that generates a sense of high status within the tribe, I stop thinking that thought.</li>\n<li>I remember my low-prestige roots - for example, I remember what it was like to be a child squeezed through the horrors of the elementary school system.&nbsp; I haven't switched sides to declare that children are stupid and need to be overridden for their own good (as would signal my own adulthood and maturity).&nbsp; I remember the battles I fought then, and would still fight them now on behalf of another child if a target of opportunity arose.&nbsp; <em>That</em> is supporting and identifying with the downtrodden - not to be confused with the high-prestige activity of supporting trendy minority causes that other celebrities support.&nbsp; The vast majority of celebrities who \"support the downtrodden\" don't go so far as to support children against adults.&nbsp; (David Deutsch is <a href=\"http://www.youthrights.org/final.php\">a notable exception</a> to this, and earned a huge amount of respect from me for it.)</li>\n<li>I refuse to conform to people's expectations of a wise sage who always speaks with kindness and sober deliberation, of which I have said:&nbsp; \"I am not bloody Gandalf.\"</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2EFq8dJbxKNzforjM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cgrvvp9QzjiFuYwLi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}], "voteCount": 51, "baseScore": 52, "extendedScore": null, "score": 0.00011035666748481194, "legacy": true, "legacyId": "2076", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 45, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 145, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-13T05:27:48.087Z", "modifiedAt": null, "url": null, "title": "Outline of a lower bound for consciousness", "slug": "outline-of-a-lower-bound-for-consciousness", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:19.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EPaz9zLSsXpwtEo9C/outline-of-a-lower-bound-for-consciousness", "pageUrlRelative": "/posts/EPaz9zLSsXpwtEo9C/outline-of-a-lower-bound-for-consciousness", "linkUrl": "https://www.lesswrong.com/posts/EPaz9zLSsXpwtEo9C/outline-of-a-lower-bound-for-consciousness", "postedAtFormatted": "Wednesday, January 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Outline%20of%20a%20lower%20bound%20for%20consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOutline%20of%20a%20lower%20bound%20for%20consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPaz9zLSsXpwtEo9C%2Foutline-of-a-lower-bound-for-consciousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Outline%20of%20a%20lower%20bound%20for%20consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPaz9zLSsXpwtEo9C%2Foutline-of-a-lower-bound-for-consciousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEPaz9zLSsXpwtEo9C%2Foutline-of-a-lower-bound-for-consciousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2014, "htmlBody": "<p class=\"western\">This is a summary of an article I'm writing on consciousness, and I'd like to hear opinions on it.&nbsp; It is the first time anyone has been able to defend a numeric claim about subjective consciousness.</p>\n<p class=\"western\">ADDED:&nbsp; Funny no one pointed out this connection, but the purpose of this article is to create a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>.</p>\n<h2 class=\"western\">1. Overview</h2>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">I propose a test for the absence of consciousness, based on the claim that a necessary, but not sufficient, condition for a symbol-based knowledge system</span><span style=\"font-family: Arial,sans-serif;\"> to be considered conscious </span><span style=\"font-family: Arial,sans-serif;\">is that it has exactly one possible symbol grounding, modulo symbols representing qualia. This supposition, plus a few reasonable assumptions, leads to the conclusion that a symbolic artificial intelligence using Boolean truth-values and having </span><span style=\"font-family: Arial,sans-serif;\">an adult vocabulary</span><span style=\"font-family: Arial,sans-serif;\"> must have on the order of 10<sup>6</sup> assertions before we need worry whether it is conscious.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Section 2 will explain the claim about symbol-grounding that this analysis is based on. Section 3 will present the math and some reasonable assumptions for computing the expected number of randomly-satisfied groundings for a symbol system. Section 4 will argue that a Boolean symbol system with a human-level vocabulary must have millions of assertions in order for it to be probable that no spurious symbols groundings exist.<a id=\"more\"></a></span></p>\n<h2 class=\"western\">2. Symbol grounding<br /></h2>\n<p class=\"western\"><strong>2.1. A simple representational system</strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Consider a symbolic reasoning system whose knowledge base K consists of predicate logic assertions, using atoms, predicates, and variables.&nbsp; We will ignore quantifiers.&nbsp; In addition to the knowledge base, there is a relatively small set of <em>primitive rules</em> that say how to derive new assertions from existing assertions, which are not asserted in K, but are implemented by the inference engine interpreting K.&nbsp; Any predicate that occurs in a primitive rule is called a <em>primitive predicate</em>.<br /></span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">The meaning of primitive predicates is specified by the program that implements the inference engine.&nbsp; The meaning of predicates other than primitive rules is defined within K, in terms of the primitive predicates, in the same way that LISP code defines a semantics for functions based on the semantics of LISP's primitive functions.&nbsp; (If this is objectionable, you can devise a representation in which the only predicates are primitive predicates (Shapiro 2000).)<br /></span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">This still leaves the semantics of the atoms undefined.&nbsp; We will say that a <em>grounding</em> g for that system is a mapping from the atoms in K to concepts in a world W.&nbsp; We will extend the notation so that g(P) more generally indicates the concept in W arrived at by mapping all of the atoms in the predication P using g.&nbsp; The semantics of this mapping may be referential (e.g., Dretske 1985) or intensional (Maida &amp; Shapiro 1982).&nbsp; The world may be an external world, or pure simulation. What is required is a consistent, generative relationship between symbols and a world, so that someone knowing that relationship, and the state of the world, could predict what predicates the system would assert.</span></p>\n<p class=\"western\"><strong>2.2. Falsifiable symbol groundings and ambiguous consciousness<br /></strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">If you have a system that is meant to simulate molecular signaling in a cell, I might be able to define a new grounding g' that re-maps its nodes to things in W, so that for every predication P in K, g'(P) is still true in W; but now the statements in W would be interpreted as simulating traffic flow in a city. If you have a system that you say models disputes between two corporations, I might re-map it so that it is simulating a mating ritual between two creatures.</span><span style=\"font-family: Arial,sans-serif;\">&nbsp; But adding more information to your system is likely to falsify my remapping, so that it is no longer true that g(P) is true in W for all propositions P in K.&nbsp; The key assumption of this paper is that a system need not be considered conscious if such a currently-true but still falsifiable remapping is possible.<br /></span></p>\n<p>A <em>falsifiable</em> <em>grounding</em> is a grounding for K whose interpretation g(K) is true in W by chance, because the system does not contain enough information to rule it out.&nbsp; Consider again the system you designed to simulate a dispute between corporations, that I claim is simulating mating rituals.&nbsp; Let's say for the moment that the agents it simulates are, in fact, conscious.&nbsp; Furthermore, since both mappings are consistent, we don't get to choose which mapping they experience.&nbsp; Perhaps each agent has two consciousnesses; perhaps each settles on one interpretation arbitrarily; perhaps they flickers between the two like a person looking at a Necker cube.</p>\n<p>Although in this example we can say that one interpretation is the true interpretation, our knowledge of that can have no impact on which interpretation the system consciously experiences.&nbsp; Therefore, any theory of consciousness that claimed the system was conscious of events in W using the intended grounding, must also admit that it is conscious of a set of entirely different events in W using the accidental grounding, prior to the acquisition of new information ruling the latter out.</p>\n<p><strong>2.3 Not caring is as good as disproving</strong></p>\n<p>I don't know how to prove that multiple simultaneous consciousnesses don't occur.&nbsp; But we don't need to worry about them.&nbsp; I didn't say that a system with multiple groundings couldn't be conscious.&nbsp; I said it <span style=\"font-family: Arial,sans-serif;\">needn't be <em>considered</em> conscious.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Even if you are willing to consider a computer program with many falsifiable groundings to be conscious, you still needn't worry about how you treat that computer program.&nbsp; Because you can't be nice to it no matter how hard you try.&nbsp; It's pointless to treat an agent as having rights if it doesn't have a stable symbol-grounding, because what is desirable to it at one moment might cause it indescribable agony in the next.&nbsp; Even if you are nice to the consciousness with the grounding intended by the system's designer, you will be causing misery to an astronomical number of equally-real alternately-grounded consciousnesses.<br /> </span></p>\n<h2 class=\"western\">3. Counting groundings</h2>\n<p class=\"western\"><strong>3.1 Overview</strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Let g(K) denote the set of assertions about the world W that K represents that are produced from K using a symbol-grounding mapping <em>g</em>.&nbsp; The system fails the unique symbol-grounding test if there is a permutation function <em>f</em> (other than the identity function) mapping atoms into other atoms, such that g(K) is true and g(f(K)) is true in W.&nbsp; Given K, what is the probability that there exists such a permutation f?</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">We assume Boolean truth-values for our predicates. Suppose the system represents <em>s</em> different concepts as unique atoms. Suppose there are <em>p</em> predicates in the system, and <em>a</em> assertions made over the <em>s</em> symbols using these <em>p</em> predicates. We wish to know that it is not possible to choose a permutation <em>f</em> of those <em>s</em> symbols, such that the knowledge represented in the system would still evaluate to true in the represented world.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">We will calculate the probability <em>P(p,a)</em> that each of <em>a</em> assertions using <em>p</em> predicates evaluates to true. We will also calculate the number <em>N(s)</em> of possible groundings of symbols in the knowledge base. We then can calculate the expected number <em>E</em> of random symbol groundings in addition to the one intended by the system builder as</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0; page-break-after: avoid;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><em>N(s) <span style=\"font-family: Symbol,serif;\">x</span> P(p,a)</em> <span style=\"font-family: Symbol,serif;\">=</span> <em>E</em></span></p>\n<p style=\"margin-right: 0in; margin-bottom: 0.08in; widows: 0; orphans: 0;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: x-small;\"><strong>Equation 1: Expected number of accidental symbol groundings</strong></span></span></p>\n<p><strong>3.2 A closed-form approximation</strong></p>\n<p>This section, which I'm saving for the full paper, proves that, with certain reasonable assumptions, the solution to this equation is</p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0; page-break-after: avoid;\" align=\"center\">sln(s)-s <span style=\"font-family: Arial,sans-serif;\">&lt; aln(p)/2</span></p>\n<p style=\"margin-right: 0in; margin-bottom: 0.08in; widows: 0; orphans: 0;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: x-small;\"><strong>Equation 7: The consciousness inequality for Boolean symbol systems</strong></span></span></p>\n<p>As <em>s</em> and <em>p</em> should be similar in magnitude, this can be approximated very well as 2s &lt; a.</p>\n<h2 class=\"western\">4. How much knowledge does an AI need before we need worry whether it is conscious?</h2>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">How complex must a system that reasons something like a human be to pass the test? By &ldquo;something like a human&rdquo; we mean a system with approximately the same number of categories as a human. We will estimate this from the number of words in a typical human&rsquo;s vocabulary.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">(Goulden et al. 1990) studied </span>Webster's Third International Dictionary (1963<span style=\"font-family: Arial,sans-serif;\">) and concluded it contains less than 58,000 distinct base words.&nbsp; They then tested subjects for their knowledge of a sample of base words from the dictionary, and concluded that native English speakers who are university graduates have an average vocabulary of around 17,000 base words.&nbsp; This accounts for concepts that have their own words.&nbsp; We also have concepts that we can express only by joining words together (\"back pain\" occurs to me at the moment); some concepts that we would need entire sentences to communicate; and some concepts that share the same word with other concepts.&nbsp; However, some proportion of these concepts will be represented in our system as predicates, rather than as atoms.</span></p>\n<p>I used 50,000 as a ballpark figure for <em>s</em><span>.&nbsp; </span>This leaves <em>a</em> and <em>p</em> unknown.<span>&nbsp; </span>We can estimate <em>a</em> from <em>p</em> if we suppose that the least-common predicate in the knowledge base is used exactly once.&nbsp; Then we have <em>a/(p </em>ln<em>(p))</em> = 1, <em>a</em> = <em>p</em>ln(<em>p</em>).<span>&nbsp; </span>We can then compute the smallest <em>a</em> such that Equation 7 is satisfied.</p>\n<p>Solving for p would be difficult. Using 100 iterations of Newton's method (from the starting guess <em>p</em>=100) finds <em>p</em>=11,279, <em>a</em>=105,242. This indicates that a pure symbol system having a human-like vocabulary of 50,000 atoms must have at least 100,000 assertions before one need worry whether it is conscious.</p>\n<p><span style=\"font-family: Arial,sans-serif;\">Children are less likely to have this much knowledge.&nbsp; But they also know fewer concepts.&nbsp; This suggests that the rate at which we learn language is limited not by our ability to learn the words, but by our ability to learn enough facts using those words for us to have a conscious understanding of them.&nbsp; Another way of putting this is that you can't short-change learning.&nbsp; Even if you try to jump-start your AI by writing a bunch of rules ala Cyc, you need to put in exactly as much data as would have been needed for the system to learn those rules on its own in order for it to satisfy Equation 7.<br /></span></p>\n<h2><span style=\"font-family: Arial,sans-serif;\">5. Conclusions</span></h2>\n<p><span style=\"font-family: Arial,sans-serif;\">The immediate application of this work is that scientists developing intelligent systems, who may have (or be pressured to display) moral concerns over whether the systems they are experimenting with may be conscious, can use this approach to tell whether their systems are complex enough for this to be a concern.</span></p>\n<p><span style=\"font-family: Arial,sans-serif;\">In popular discussion, people worry that a computer program may become dangerous when it becomes self-aware.&nbsp; They may therefore imagine that this test could be used to tell whether a computer program posed a potential hazard.&nbsp; This is an incorrect application.&nbsp; I suppose that subjective experience somehow makes an agent more effective; otherwise, it would not have evolved.&nbsp; However, automated reasoning systems reason whether they are conscious or not.&nbsp; There is no reason to assume that a system is not dangerous because it is unconscious, any more than you would conclude that a hurricane is not dangerous because it is unconscious.</span></p>\n<p><span style=\"font-family: Arial,sans-serif;\">More generally, this work shows that it is possible, if one considers representations in enough detail, to make numeric claims about subjective consciousness.&nbsp; It is thus an existence proof that a science of consciousness is possible.<br /></span></p>\n<h2 class=\"western\" style=\"page-break-after: avoid;\">References</h2>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Fred Dretske (1985). Machines and the mental. In <em>Proceedings and Addresses of the American Philosophical Association</em> 59: 23-33.</span></p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Robin Goulden, Paul Nation, John Read (1990). How large can a receptive vocabulary be? <em>Applied Linguistics</em> 11: 341-363.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Anthony Maida &amp; Stuart Shapiro (1982). Intensional concepts in propositional semantic networks. <em>Cognitive Science</em> 6: 291-330. Reprinted in Ronald Brachman &amp; Hector Levesque, eds., <em>Readings in Knowledge Representation</em>, Los Altos, CA: Morgan Kaufmann 1985, pp. 169-189.</span></p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">William Rapaport (1988). Syntactic semantics: Foundations of computational natural-language understanding. In James Fetzer, ed., <em>Aspects of Artificial Intelligence</em> (Dordrecht, Holland: Kluwer Academic Publishers): 81-131; reprinted in Eric Dietrich (ed.), <em>Thinking Computers and Virtual Persons: Essays on the Intentionality of Machines</em> (San Diego: Academic Press, 1994): 225-273.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Roger Schank (1975). The primitive ACTs of conceptual dependency.&nbsp; <em>Proceedings of the 1975 workshop on Theoretical Issues in Natural Language Processing</em>, Cambridge MA.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Stuart C. Shapiro (2000).&nbsp; SNePS: A logic for natural language understanding and commonsense reasoning.&nbsp; In Lucja Iwanska &amp; Stuart C. Shapiro (eds.), <em>Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language</em> (Menlo Park, CA/Cambridge, MA: AAAI Press/MIT Press): 175-195.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Yorick Wilks (1972).&nbsp; <em>Grammar, Meaning, and the Machine Analysis of Language</em>.&nbsp; London.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EPaz9zLSsXpwtEo9C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 4, "extendedScore": null, "score": 5.526654391132392e-07, "legacy": true, "legacyId": "2103", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"western\">This is a summary of an article I'm writing on consciousness, and I'd like to hear opinions on it.&nbsp; It is the first time anyone has been able to defend a numeric claim about subjective consciousness.</p>\n<p class=\"western\">ADDED:&nbsp; Funny no one pointed out this connection, but the purpose of this article is to create a <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>.</p>\n<h2 class=\"western\" id=\"1__Overview\">1. Overview</h2>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">I propose a test for the absence of consciousness, based on the claim that a necessary, but not sufficient, condition for a symbol-based knowledge system</span><span style=\"font-family: Arial,sans-serif;\"> to be considered conscious </span><span style=\"font-family: Arial,sans-serif;\">is that it has exactly one possible symbol grounding, modulo symbols representing qualia. This supposition, plus a few reasonable assumptions, leads to the conclusion that a symbolic artificial intelligence using Boolean truth-values and having </span><span style=\"font-family: Arial,sans-serif;\">an adult vocabulary</span><span style=\"font-family: Arial,sans-serif;\"> must have on the order of 10<sup>6</sup> assertions before we need worry whether it is conscious.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Section 2 will explain the claim about symbol-grounding that this analysis is based on. Section 3 will present the math and some reasonable assumptions for computing the expected number of randomly-satisfied groundings for a symbol system. Section 4 will argue that a Boolean symbol system with a human-level vocabulary must have millions of assertions in order for it to be probable that no spurious symbols groundings exist.<a id=\"more\"></a></span></p>\n<h2 class=\"western\" id=\"2__Symbol_grounding\">2. Symbol grounding<br></h2>\n<p class=\"western\"><strong id=\"2_1__A_simple_representational_system\">2.1. A simple representational system</strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Consider a symbolic reasoning system whose knowledge base K consists of predicate logic assertions, using atoms, predicates, and variables.&nbsp; We will ignore quantifiers.&nbsp; In addition to the knowledge base, there is a relatively small set of <em>primitive rules</em> that say how to derive new assertions from existing assertions, which are not asserted in K, but are implemented by the inference engine interpreting K.&nbsp; Any predicate that occurs in a primitive rule is called a <em>primitive predicate</em>.<br></span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">The meaning of primitive predicates is specified by the program that implements the inference engine.&nbsp; The meaning of predicates other than primitive rules is defined within K, in terms of the primitive predicates, in the same way that LISP code defines a semantics for functions based on the semantics of LISP's primitive functions.&nbsp; (If this is objectionable, you can devise a representation in which the only predicates are primitive predicates (Shapiro 2000).)<br></span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">This still leaves the semantics of the atoms undefined.&nbsp; We will say that a <em>grounding</em> g for that system is a mapping from the atoms in K to concepts in a world W.&nbsp; We will extend the notation so that g(P) more generally indicates the concept in W arrived at by mapping all of the atoms in the predication P using g.&nbsp; The semantics of this mapping may be referential (e.g., Dretske 1985) or intensional (Maida &amp; Shapiro 1982).&nbsp; The world may be an external world, or pure simulation. What is required is a consistent, generative relationship between symbols and a world, so that someone knowing that relationship, and the state of the world, could predict what predicates the system would assert.</span></p>\n<p class=\"western\"><strong id=\"2_2__Falsifiable_symbol_groundings_and_ambiguous_consciousness\">2.2. Falsifiable symbol groundings and ambiguous consciousness<br></strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">If you have a system that is meant to simulate molecular signaling in a cell, I might be able to define a new grounding g' that re-maps its nodes to things in W, so that for every predication P in K, g'(P) is still true in W; but now the statements in W would be interpreted as simulating traffic flow in a city. If you have a system that you say models disputes between two corporations, I might re-map it so that it is simulating a mating ritual between two creatures.</span><span style=\"font-family: Arial,sans-serif;\">&nbsp; But adding more information to your system is likely to falsify my remapping, so that it is no longer true that g(P) is true in W for all propositions P in K.&nbsp; The key assumption of this paper is that a system need not be considered conscious if such a currently-true but still falsifiable remapping is possible.<br></span></p>\n<p>A <em>falsifiable</em> <em>grounding</em> is a grounding for K whose interpretation g(K) is true in W by chance, because the system does not contain enough information to rule it out.&nbsp; Consider again the system you designed to simulate a dispute between corporations, that I claim is simulating mating rituals.&nbsp; Let's say for the moment that the agents it simulates are, in fact, conscious.&nbsp; Furthermore, since both mappings are consistent, we don't get to choose which mapping they experience.&nbsp; Perhaps each agent has two consciousnesses; perhaps each settles on one interpretation arbitrarily; perhaps they flickers between the two like a person looking at a Necker cube.</p>\n<p>Although in this example we can say that one interpretation is the true interpretation, our knowledge of that can have no impact on which interpretation the system consciously experiences.&nbsp; Therefore, any theory of consciousness that claimed the system was conscious of events in W using the intended grounding, must also admit that it is conscious of a set of entirely different events in W using the accidental grounding, prior to the acquisition of new information ruling the latter out.</p>\n<p><strong id=\"2_3_Not_caring_is_as_good_as_disproving\">2.3 Not caring is as good as disproving</strong></p>\n<p>I don't know how to prove that multiple simultaneous consciousnesses don't occur.&nbsp; But we don't need to worry about them.&nbsp; I didn't say that a system with multiple groundings couldn't be conscious.&nbsp; I said it <span style=\"font-family: Arial,sans-serif;\">needn't be <em>considered</em> conscious.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Even if you are willing to consider a computer program with many falsifiable groundings to be conscious, you still needn't worry about how you treat that computer program.&nbsp; Because you can't be nice to it no matter how hard you try.&nbsp; It's pointless to treat an agent as having rights if it doesn't have a stable symbol-grounding, because what is desirable to it at one moment might cause it indescribable agony in the next.&nbsp; Even if you are nice to the consciousness with the grounding intended by the system's designer, you will be causing misery to an astronomical number of equally-real alternately-grounded consciousnesses.<br> </span></p>\n<h2 class=\"western\" id=\"3__Counting_groundings\">3. Counting groundings</h2>\n<p class=\"western\"><strong id=\"3_1_Overview\">3.1 Overview</strong></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Let g(K) denote the set of assertions about the world W that K represents that are produced from K using a symbol-grounding mapping <em>g</em>.&nbsp; The system fails the unique symbol-grounding test if there is a permutation function <em>f</em> (other than the identity function) mapping atoms into other atoms, such that g(K) is true and g(f(K)) is true in W.&nbsp; Given K, what is the probability that there exists such a permutation f?</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">We assume Boolean truth-values for our predicates. Suppose the system represents <em>s</em> different concepts as unique atoms. Suppose there are <em>p</em> predicates in the system, and <em>a</em> assertions made over the <em>s</em> symbols using these <em>p</em> predicates. We wish to know that it is not possible to choose a permutation <em>f</em> of those <em>s</em> symbols, such that the knowledge represented in the system would still evaluate to true in the represented world.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">We will calculate the probability <em>P(p,a)</em> that each of <em>a</em> assertions using <em>p</em> predicates evaluates to true. We will also calculate the number <em>N(s)</em> of possible groundings of symbols in the knowledge base. We then can calculate the expected number <em>E</em> of random symbol groundings in addition to the one intended by the system builder as</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0; page-break-after: avoid;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><em>N(s) <span style=\"font-family: Symbol,serif;\">x</span> P(p,a)</em> <span style=\"font-family: Symbol,serif;\">=</span> <em>E</em></span></p>\n<p style=\"margin-right: 0in; margin-bottom: 0.08in; widows: 0; orphans: 0;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: x-small;\"><strong>Equation 1: Expected number of accidental symbol groundings</strong></span></span></p>\n<p><strong id=\"3_2_A_closed_form_approximation\">3.2 A closed-form approximation</strong></p>\n<p>This section, which I'm saving for the full paper, proves that, with certain reasonable assumptions, the solution to this equation is</p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0; page-break-after: avoid;\" align=\"center\">sln(s)-s <span style=\"font-family: Arial,sans-serif;\">&lt; aln(p)/2</span></p>\n<p style=\"margin-right: 0in; margin-bottom: 0.08in; widows: 0; orphans: 0;\" align=\"center\"><span style=\"font-family: Arial,sans-serif;\"><span style=\"font-size: x-small;\"><strong>Equation 7: The consciousness inequality for Boolean symbol systems</strong></span></span></p>\n<p>As <em>s</em> and <em>p</em> should be similar in magnitude, this can be approximated very well as 2s &lt; a.</p>\n<h2 class=\"western\" id=\"4__How_much_knowledge_does_an_AI_need_before_we_need_worry_whether_it_is_conscious_\">4. How much knowledge does an AI need before we need worry whether it is conscious?</h2>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">How complex must a system that reasons something like a human be to pass the test? By \u201csomething like a human\u201d we mean a system with approximately the same number of categories as a human. We will estimate this from the number of words in a typical human\u2019s vocabulary.</span></p>\n<p class=\"western\" style=\"margin-right: 0in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">(Goulden et al. 1990) studied </span>Webster's Third International Dictionary (1963<span style=\"font-family: Arial,sans-serif;\">) and concluded it contains less than 58,000 distinct base words.&nbsp; They then tested subjects for their knowledge of a sample of base words from the dictionary, and concluded that native English speakers who are university graduates have an average vocabulary of around 17,000 base words.&nbsp; This accounts for concepts that have their own words.&nbsp; We also have concepts that we can express only by joining words together (\"back pain\" occurs to me at the moment); some concepts that we would need entire sentences to communicate; and some concepts that share the same word with other concepts.&nbsp; However, some proportion of these concepts will be represented in our system as predicates, rather than as atoms.</span></p>\n<p>I used 50,000 as a ballpark figure for <em>s</em><span>.&nbsp; </span>This leaves <em>a</em> and <em>p</em> unknown.<span>&nbsp; </span>We can estimate <em>a</em> from <em>p</em> if we suppose that the least-common predicate in the knowledge base is used exactly once.&nbsp; Then we have <em>a/(p </em>ln<em>(p))</em> = 1, <em>a</em> = <em>p</em>ln(<em>p</em>).<span>&nbsp; </span>We can then compute the smallest <em>a</em> such that Equation 7 is satisfied.</p>\n<p>Solving for p would be difficult. Using 100 iterations of Newton's method (from the starting guess <em>p</em>=100) finds <em>p</em>=11,279, <em>a</em>=105,242. This indicates that a pure symbol system having a human-like vocabulary of 50,000 atoms must have at least 100,000 assertions before one need worry whether it is conscious.</p>\n<p><span style=\"font-family: Arial,sans-serif;\">Children are less likely to have this much knowledge.&nbsp; But they also know fewer concepts.&nbsp; This suggests that the rate at which we learn language is limited not by our ability to learn the words, but by our ability to learn enough facts using those words for us to have a conscious understanding of them.&nbsp; Another way of putting this is that you can't short-change learning.&nbsp; Even if you try to jump-start your AI by writing a bunch of rules ala Cyc, you need to put in exactly as much data as would have been needed for the system to learn those rules on its own in order for it to satisfy Equation 7.<br></span></p>\n<h2 id=\"5__Conclusions\"><span style=\"font-family: Arial,sans-serif;\">5. Conclusions</span></h2>\n<p><span style=\"font-family: Arial,sans-serif;\">The immediate application of this work is that scientists developing intelligent systems, who may have (or be pressured to display) moral concerns over whether the systems they are experimenting with may be conscious, can use this approach to tell whether their systems are complex enough for this to be a concern.</span></p>\n<p><span style=\"font-family: Arial,sans-serif;\">In popular discussion, people worry that a computer program may become dangerous when it becomes self-aware.&nbsp; They may therefore imagine that this test could be used to tell whether a computer program posed a potential hazard.&nbsp; This is an incorrect application.&nbsp; I suppose that subjective experience somehow makes an agent more effective; otherwise, it would not have evolved.&nbsp; However, automated reasoning systems reason whether they are conscious or not.&nbsp; There is no reason to assume that a system is not dangerous because it is unconscious, any more than you would conclude that a hurricane is not dangerous because it is unconscious.</span></p>\n<p><span style=\"font-family: Arial,sans-serif;\">More generally, this work shows that it is possible, if one considers representations in enough detail, to make numeric claims about subjective consciousness.&nbsp; It is thus an existence proof that a science of consciousness is possible.<br></span></p>\n<h2 class=\"western\" style=\"page-break-after: avoid;\" id=\"References\">References</h2>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Fred Dretske (1985). Machines and the mental. In <em>Proceedings and Addresses of the American Philosophical Association</em> 59: 23-33.</span></p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Robin Goulden, Paul Nation, John Read (1990). How large can a receptive vocabulary be? <em>Applied Linguistics</em> 11: 341-363.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\"><span style=\"font-family: Arial,sans-serif;\">Anthony Maida &amp; Stuart Shapiro (1982). Intensional concepts in propositional semantic networks. <em>Cognitive Science</em> 6: 291-330. Reprinted in Ronald Brachman &amp; Hector Levesque, eds., <em>Readings in Knowledge Representation</em>, Los Altos, CA: Morgan Kaufmann 1985, pp. 169-189.</span></p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">William Rapaport (1988). Syntactic semantics: Foundations of computational natural-language understanding. In James Fetzer, ed., <em>Aspects of Artificial Intelligence</em> (Dordrecht, Holland: Kluwer Academic Publishers): 81-131; reprinted in Eric Dietrich (ed.), <em>Thinking Computers and Virtual Persons: Essays on the Intentionality of Machines</em> (San Diego: Academic Press, 1994): 225-273.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Roger Schank (1975). The primitive ACTs of conceptual dependency.&nbsp; <em>Proceedings of the 1975 workshop on Theoretical Issues in Natural Language Processing</em>, Cambridge MA.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Stuart C. Shapiro (2000).&nbsp; SNePS: A logic for natural language understanding and commonsense reasoning.&nbsp; In Lucja Iwanska &amp; Stuart C. Shapiro (eds.), <em>Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language</em> (Menlo Park, CA/Cambridge, MA: AAAI Press/MIT Press): 175-195.</p>\n<p style=\"margin-left: 0.25in; margin-right: 0in; text-indent: -0.25in; widows: 0; orphans: 0;\">Yorick Wilks (1972).&nbsp; <em>Grammar, Meaning, and the Machine Analysis of Language</em>.&nbsp; London.</p>", "sections": [{"title": "1. Overview", "anchor": "1__Overview", "level": 1}, {"title": "2. Symbol grounding", "anchor": "2__Symbol_grounding", "level": 1}, {"title": "2.1. A simple representational system", "anchor": "2_1__A_simple_representational_system", "level": 2}, {"title": "2.2. Falsifiable symbol groundings and ambiguous consciousness", "anchor": "2_2__Falsifiable_symbol_groundings_and_ambiguous_consciousness", "level": 2}, {"title": "2.3 Not caring is as good as disproving", "anchor": "2_3_Not_caring_is_as_good_as_disproving", "level": 2}, {"title": "3. Counting groundings", "anchor": "3__Counting_groundings", "level": 1}, {"title": "3.1 Overview", "anchor": "3_1_Overview", "level": 2}, {"title": "3.2 A closed-form approximation", "anchor": "3_2_A_closed_form_approximation", "level": 2}, {"title": "4. How much knowledge does an AI need before we need worry whether it is conscious?", "anchor": "4__How_much_knowledge_does_an_AI_need_before_we_need_worry_whether_it_is_conscious_", "level": 1}, {"title": "5. Conclusions", "anchor": "5__Conclusions", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "112 comments"}], "headingsCount": 13}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 112, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-13T22:02:45.943Z", "modifiedAt": null, "url": null, "title": "Meetup: Bay Area: Jan 15th, 7pm", "slug": "meetup-bay-area-jan-15th-7pm", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:04.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XD3sfjfeabp8jL5c7/meetup-bay-area-jan-15th-7pm", "pageUrlRelative": "/posts/XD3sfjfeabp8jL5c7/meetup-bay-area-jan-15th-7pm", "linkUrl": "https://www.lesswrong.com/posts/XD3sfjfeabp8jL5c7/meetup-bay-area-jan-15th-7pm", "postedAtFormatted": "Wednesday, January 13th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%3A%20Bay%20Area%3A%20Jan%2015th%2C%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%3A%20Bay%20Area%3A%20Jan%2015th%2C%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD3sfjfeabp8jL5c7%2Fmeetup-bay-area-jan-15th-7pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%3A%20Bay%20Area%3A%20Jan%2015th%2C%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD3sfjfeabp8jL5c7%2Fmeetup-bay-area-jan-15th-7pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD3sfjfeabp8jL5c7%2Fmeetup-bay-area-jan-15th-7pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><a href=\"http://www.meetup.com/Bay-Area-Overcoming-Bias-Meetup/calendar/12273478/\">Overcoming Bias / Less Wrong meetup in the San Francisco Bay Area at SIAI House on January 15th, 2010, starting at 7PM.</a></p>\n<p>Robin Hanson and possibly Michael Vassar will be present.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XD3sfjfeabp8jL5c7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 4, "extendedScore": null, "score": 5.528485434347225e-07, "legacy": true, "legacyId": "2108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-14T03:41:20.728Z", "modifiedAt": null, "url": null, "title": "Self control may be contagious", "slug": "self-control-may-be-contagious", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:04.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psy-Kosh", "createdAt": "2009-03-01T19:34:52.148Z", "isAdmin": false, "displayName": "Psy-Kosh"}, "userId": "CtHmuQzjA7Y7LnSss", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gpZgXPxMx6hTkiC5w/self-control-may-be-contagious", "pageUrlRelative": "/posts/gpZgXPxMx6hTkiC5w/self-control-may-be-contagious", "linkUrl": "https://www.lesswrong.com/posts/gpZgXPxMx6hTkiC5w/self-control-may-be-contagious", "postedAtFormatted": "Thursday, January 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Self%20control%20may%20be%20contagious&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelf%20control%20may%20be%20contagious%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpZgXPxMx6hTkiC5w%2Fself-control-may-be-contagious%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Self%20control%20may%20be%20contagious%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpZgXPxMx6hTkiC5w%2Fself-control-may-be-contagious", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgpZgXPxMx6hTkiC5w%2Fself-control-may-be-contagious", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 45, "htmlBody": "<p>Just saw this article on reddit and thought it would be relevant here: <a href=\"http://www.sciguru.com/newsitem/995/Self-control-is-contagious-study-finds\">Self Control is Contagious, Study Finds</a></p>\n<p>Apparently seeing other people exhibit self control or even just thinking of them tends to increase (or decrease if observing poor self control) one's own self control.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gpZgXPxMx6hTkiC5w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 5.529108770984487e-07, "legacy": true, "legacyId": "2109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-14T11:32:04.429Z", "modifiedAt": null, "url": null, "title": "Advice for AI makers", "slug": "advice-for-ai-makers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:04.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a5Fz54uaM7zMqseJ2/advice-for-ai-makers", "pageUrlRelative": "/posts/a5Fz54uaM7zMqseJ2/advice-for-ai-makers", "linkUrl": "https://www.lesswrong.com/posts/a5Fz54uaM7zMqseJ2/advice-for-ai-makers", "postedAtFormatted": "Thursday, January 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advice%20for%20AI%20makers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvice%20for%20AI%20makers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5Fz54uaM7zMqseJ2%2Fadvice-for-ai-makers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advice%20for%20AI%20makers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5Fz54uaM7zMqseJ2%2Fadvice-for-ai-makers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa5Fz54uaM7zMqseJ2%2Fadvice-for-ai-makers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 126, "htmlBody": "<p>A friend of mine is about to launch himself heavily into the realm of AI programming. The details of his approach aren't important; probabilities dictate that he is unlikely to score a major success. He's asked me for advice, however, on how to design a safe(r) AI. I've been pointing him in the right directions and sending him links to useful posts on this blog and the SIAI.</p>\n<p>Do people here have any recommendations they'd like me to pass on? Hopefully, these may form the basis of a condensed 'warning pack' for other AI makers.</p>\n<p><strong>Addendum:</strong> Advice along the lines of \"don't do it\" is vital and good, but unlikely to be followed. Coding will nearly certainly happen; is there any way of making it less genocidally risky?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a5Fz54uaM7zMqseJ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 5.529973777586702e-07, "legacy": true, "legacyId": "2110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 211, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-14T18:20:00.947Z", "modifiedAt": null, "url": null, "title": "Comic about the Singularity", "slug": "comic-about-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:51.924Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dclayh", "createdAt": "2009-03-07T01:16:38.966Z", "isAdmin": false, "displayName": "dclayh"}, "userId": "E7xnxwP5EPuGiP99X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/si25s3PiHSXFFz4iC/comic-about-the-singularity", "pageUrlRelative": "/posts/si25s3PiHSXFFz4iC/comic-about-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/si25s3PiHSXFFz4iC/comic-about-the-singularity", "postedAtFormatted": "Thursday, January 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Comic%20about%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComic%20about%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi25s3PiHSXFFz4iC%2Fcomic-about-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Comic%20about%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi25s3PiHSXFFz4iC%2Fcomic-about-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsi25s3PiHSXFFz4iC%2Fcomic-about-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p><a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=1760#comic\">Today's Saturday Morning Breakfast Cereal.</a>&nbsp; (Which incidentally is a very funny webcomic I read regularly.)&nbsp; Mouseover the red button for a bonus panel.</p>\n<p>Clearly the author hasn't read <a href=\"/lw/ww/high_challenge/\">the proper Eliezer essay</a>(s) on post-Singularity life.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "si25s3PiHSXFFz4iC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 5, "extendedScore": null, "score": 5.530727012710876e-07, "legacy": true, "legacyId": "2111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["29vqqmGNxNRGzffEj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-15T00:14:13.849Z", "modifiedAt": null, "url": null, "title": "Back of the envelope calculations around the singularity. ", "slug": "back-of-the-envelope-calculations-around-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:19.505Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DhEbAn7WNTb376pJc/back-of-the-envelope-calculations-around-the-singularity", "pageUrlRelative": "/posts/DhEbAn7WNTb376pJc/back-of-the-envelope-calculations-around-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/DhEbAn7WNTb376pJc/back-of-the-envelope-calculations-around-the-singularity", "postedAtFormatted": "Friday, January 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Back%20of%20the%20envelope%20calculations%20around%20the%20singularity.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABack%20of%20the%20envelope%20calculations%20around%20the%20singularity.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhEbAn7WNTb376pJc%2Fback-of-the-envelope-calculations-around-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Back%20of%20the%20envelope%20calculations%20around%20the%20singularity.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhEbAn7WNTb376pJc%2Fback-of-the-envelope-calculations-around-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDhEbAn7WNTb376pJc%2Fback-of-the-envelope-calculations-around-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 570, "htmlBody": "<p>Inspired by the <a href=\"http://www.vimeo.com/7397629\">talk by Anna Salamon</a> I decided to do my own calculations about the future. This post is a place for discussion about mine and others calculations.</p>\n<p><a id=\"more\"></a></p>\n<p>To me there are two possible paths for the likely development of intelligence, that I can identify.</p>\n<p>World 1) Fast and conceptually clean. Intelligence is a concrete value like the number of neutrons in a reactor. I assign a 20% chance of this.</p>\n<p>World 2) Slow and messy. Intelligence is contextual, much like say fitness in evolutionary biology. Proofs of intelligence of a system are only doable by a much higher intelligence entity, as it will involve discussing the complex environment. I'd assign about an 60% chance to this.</p>\n<p>Worlds 3) Other. The other 20% chance is the rest of the scenarios that are not either of these two.</p>\n<p>Both types of AI have the potential to change the world, both possibly destroying humanity if we don't use them correctly. So they both have the same rewards.</p>\n<p>So for world 1, I'll go with the same figures as Anna Salamon, because I can't find strong arguments against them (and it will serve as a refresher )</p>\n<p>Probability of an eventual AI (before humanity dies otherwise) = 80%</p>\n<p>Probability that&nbsp; AI will kill us = 80%</p>\n<p>Probability that we manage safeguards = 40%</p>\n<p>Probability that current work will save us = 30%</p>\n<p>So we get 7%*20%. Gives us 1.4%</p>\n<p>So for world 2. Assume we have an SIAI that is working on the problem of how to make messy AI Friendly or at least as Friendly as possible. It seems less likely we would make AI and harder to create safeguards as they have to act over longer time.</p>\n<p>Probability of an eventual AI (before humanity dies otherwise) = 70%</p>\n<p>Probability that&nbsp; AI will kill us (and/or we will have to give up humanity due to&nbsp; hard scrapple evolution) = 80%</p>\n<p>Probability that we manage safeguards = 30%</p>\n<p>Probability that current work will save us = 20%</p>\n<p>So we get a factor of 3% times 60% give a 1.8%.</p>\n<p>Both have the factor of 7billion lives times n, so that can be discounted. They pretty much weigh the same. Or as near as dammit for a back of the envelope calcs, considering my meta-uncertainty is high as well.</p>\n<p>They do however interfere. The right action in world 1 is not the same as the right action in world 2. Working on Friendliness of conceptually clean AI and suppressing all work and discussion on messy AI hurts world 2 as it increases the chance we might end up with messy UFAI. There is no Singularity Institute for messy AI in this world, and I doubt there will be if SIAI becomes somewhat mainstream in AI communities, so giving money to SIAI hurts world 2, it might have a small negative expected life cost. Working on Friendliness for Messy AI wouldn't intefere with the Clean AI world, as long as it didn't do stupid tests until the messy/clean divide became solved. This tips the scales somewhat towards working on messy FAI and how it is deployed. World 3 is so varied I can't really say much about.</p>\n<p>So for me the best information I should seek is getting more information on the messy/clean divide. Which is why I always go on about whether SIAI has a way of making sure it is on the right track with the Decision Theory/conceptually clean path.</p>\n<p>So how do the rest of you run the numbers on the singularity?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DhEbAn7WNTb376pJc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 6, "extendedScore": null, "score": 5.531379605330906e-07, "legacy": true, "legacyId": "2089", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-15T00:26:04.781Z", "modifiedAt": null, "url": null, "title": "The Preference Utilitarian\u2019s Time Inconsistency Problem", "slug": "the-preference-utilitarian-s-time-inconsistency-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.235Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9bkY8etnLfGCzcYAu/the-preference-utilitarian-s-time-inconsistency-problem", "pageUrlRelative": "/posts/9bkY8etnLfGCzcYAu/the-preference-utilitarian-s-time-inconsistency-problem", "linkUrl": "https://www.lesswrong.com/posts/9bkY8etnLfGCzcYAu/the-preference-utilitarian-s-time-inconsistency-problem", "postedAtFormatted": "Friday, January 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Preference%20Utilitarian%E2%80%99s%20Time%20Inconsistency%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Preference%20Utilitarian%E2%80%99s%20Time%20Inconsistency%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9bkY8etnLfGCzcYAu%2Fthe-preference-utilitarian-s-time-inconsistency-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Preference%20Utilitarian%E2%80%99s%20Time%20Inconsistency%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9bkY8etnLfGCzcYAu%2Fthe-preference-utilitarian-s-time-inconsistency-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9bkY8etnLfGCzcYAu%2Fthe-preference-utilitarian-s-time-inconsistency-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 421, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>ZH-CN</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> <w:UseFELayout /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --> <!--[endif]--><!--[if gte mso 9]><xml> <o:shapedefaults v:ext=\"edit\" spidmax=\"1026\" /> </xml><![endif]--><!--[if gte mso 9]><xml> <o:shapelayout v:ext=\"edit\"> <o:idmap v:ext=\"edit\" data=\"1\" /> </o:shapelayout></xml><![endif]--></p>\n<p class=\"MsoNormal\">In May of 2007, DanielLC <a href=\"http://www.felicifia.com/showDiary.do?diaryId=89\">asked at Felicifa</a>, an &ldquo;online utilitarianism community&rdquo;:</p>\n<blockquote>\n<p class=\"MsoNormal\">If preference utilitarianism is about making peoples&rsquo; preferences and the universe coincide, wouldn't it be much easier to change peoples&rsquo; preferences than the universe?</p>\n</blockquote>\n<p class=\"MsoNormal\">Indeed, if we were to program a super-intelligent AI to use the utility function U(w) = sum of w&rsquo;s utilities according to people (i.e., morally relevant agents) who exist in world-history w, the AI might end up killing everyone who is alive now and creating a bunch of new people whose preferences are more easily satisfied, or just use its super intelligence to persuade us to be more satisfied with the universe as it is.</p>\n<p class=\"MsoNormal\">Well, that can&rsquo;t be what we want. Is there an alternative formulation of preference utilitarianism that doesn&rsquo;t exhibit this problem? Perhaps. Suppose we instead program the AI to use U&rsquo;(w) = sum of w&rsquo;s utilities according to people who exist at the time of decision. This solves the Daniel&rsquo;s problem, but introduces a new one: <span>&nbsp;</span>time inconsistency.</p>\n<p class=\"MsoNormal\">The new AI&rsquo;s utility function depends on who exists at the time of decision, and as that time changes and people are born and die, its utility function also changes. If the AI is capable of reflection and self-modification, it should immediately notice that it would maximize its expected utility, according to its current utility function, by modifying itself to use U&rsquo;&rsquo;(w) = sum of w&rsquo;s utilities according to people who existed at time T<sub>0</sub>, where T<sub>0</sub> is a constant representing the time of self-modification.</p>\n<p class=\"MsoNormal\">The AI is now reflectively consistent, but is this the right outcome? Should the whole future of the universe be shaped only by the preferences of those who happen to be alive at some arbitrary point in time? Presumably, if you&rsquo;re a utilitarian in the first place, this is probably not the kind of utilitarianism that you&rsquo;d want to subscribe to.</p>\n<p class=\"MsoNormal\">So, what is the solution to this problem? <a href=\"http://www.overcomingbias.com/2007/12/it-is-good-to-e.html\">Robin Hanson&rsquo;s approach to moral philosophy</a> may work. It tries to take into account everyone&rsquo;s preferences&mdash;those who lived in the past, those who will live in the future, and those who have the potential to exist but don&rsquo;t&mdash;but I don&rsquo;t think he has worked out (or written down) the solution in detail. For example, is the utilitarian AI supposed to sum over every logically possible utility function and weigh them equally? If not, what weighing scheme should it use?</p>\n<p class=\"MsoNormal\">Perhaps someone can follow up Robin&rsquo;s idea and see where this approach leads us? Or does anyone have other ideas for solving this time inconsistency problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 1, "ZTRNmvQGgoYiymYnq": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9bkY8etnLfGCzcYAu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 34, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "2112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-15T11:01:18.900Z", "modifiedAt": null, "url": null, "title": "In defense of the outside view", "slug": "in-defense-of-the-outside-view", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:04.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fpdTvfT6oi8TxAGKN/in-defense-of-the-outside-view", "pageUrlRelative": "/posts/fpdTvfT6oi8TxAGKN/in-defense-of-the-outside-view", "linkUrl": "https://www.lesswrong.com/posts/fpdTvfT6oi8TxAGKN/in-defense-of-the-outside-view", "postedAtFormatted": "Friday, January 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20defense%20of%20the%20outside%20view&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20defense%20of%20the%20outside%20view%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpdTvfT6oi8TxAGKN%2Fin-defense-of-the-outside-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20defense%20of%20the%20outside%20view%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpdTvfT6oi8TxAGKN%2Fin-defense-of-the-outside-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfpdTvfT6oi8TxAGKN%2Fin-defense-of-the-outside-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 680, "htmlBody": "<p>I think some of our recent arguments against applying the outside view are wrong.</p>\n<p><strong>1.</strong> In response to taw's post, Eliezer paints the outside view argument against the Singularity <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable/1f25\">thus</a>:</p>\n<blockquote>\n<p>...because experiments show that people could do better at predicting how long it will take them to do their Christmas shopping by asking \"How long did it take last time?\" instead of trying to visualize the details.</p>\n</blockquote>\n<p>This is an unfair representation. One of the poster-child cases for the outside view (<a href=\"/lw/jg/planning_fallacy/\">mentioned</a> by Eliezer, no less!) dealt with students trying to estimate completion times for their <em>academic projects</em>. And what is AGI if not a research project? One might say AGI is too large for the analogy to work, but outside view helpfully tells us that large projects aren't any more immune to failures and schedule overruns :-)</p>\n<p><strong>2.</strong> In response to my comment claiming that Dennett didn't solve the problem of consciousness \"because philosophers don't solve problems\", ciphergoth <a href=\"/lw/1m7/dennetts_consciousness_explained_prelude/1fej\">writes</a>:</p>\n<blockquote>\n<p>This \"outside view abuse\" is getting a little extreme. Next it will tell you that Barack Obama isn't President, because people don't become President.</p>\n</blockquote>\n<p><a id=\"more\"></a>The outside view may be rephrased as \"argument from typicality\". If we'd just heard of this random dude named Barack Obama, we'd be perfectly justified in saying he won't become President! Which would be the proper analogy to first hearing about Dennett and his work. Another casual application of the outside view corroborates the conclusion: what other problems has Dennett solved? Is the problem of consciousness the <em>first</em> problem he solved? Does this seem typical of anything?</p>\n<p><strong>3.</strong> Technologos attacks taw's post, again, with the following <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable/1fb9\">argument</a>:</p>\n<blockquote>\n<p>\"beliefs that the future will be just like the past\" have a zero success rate.</p>\n</blockquote>\n<p>For each <em>particular</em> highly speculative technology, we can assert that it won't appear with high confidence (let's say 90%). But this doesn't mean the future will be the same in all respects! The conjunction of many 90%-statements (X won't appear, AND Y won't appear, AND etc.) gets assigned the product, a very low confidence, as it should. We're sure that some new technologies <em>will</em> arise, we just don't know which ones. Fusion power? Flying cars? We've been on the fast track to those for some time now, and they still sound less far out then the Singularity! Anyone who's worked with tech for any length of time can recite a looooong list of Real Soon Now technologies that never materialized.</p>\n<p><strong>4.</strong> In response to a pro-outside-view comment by taw, wedrifid <a href=\"/lw/1m1/back_of_the_envelope_calculations_around_the/1g9q\">snaps</a>:</p>\n<blockquote>\n<p>Choosing a particular outside view on a topic which the poster allegedly 'knows nothing about' would be 'pulling a superficial similarity out of his arse'.</p>\n</blockquote>\n<p>Well, duh. If the red pill doesn't make you offended about your pet project, you aren't taking enough of it :-) The method works with nonzero efficiency as long as we're pattern-matching on relevant traits or any traits <em>causally connected</em> to relevant traits, which means pretty much every superficial similarity gives you nonzero information. And the conjunction rule applies, so the more similar stuff you can find, the better. 'Pulling a similarity out of your arse' isn't something to be ashamed of - it's the whole <em>point</em> of the outside view. Even a superficial similarity is harder to fake, more entangled with reality, more <em>objective</em> than a long chain of reasoning or a credence percentage you came up with. In real-world reasoning, parallel beats sequential.</p>\n<p>In conclusion let's grant the inside view object-level advocates the benefit of the doubt one last time. Conveniently, the handful of people who say we must believe in the Singularity are all doing work in the AGI field. We can gauge exactly how believable their object-level arguments are by examining their past claims about the schedules of their <em>own</em> projects - the perfect case for the inside view if there ever was one... No, I won't spell out the sordid collection of hyperlinks here. Every reader is encouraged to Google on their own for past announcements by Doug Lenat, Ben Goertzel, Eliezer Yudkowsky (those are actually the heroes of the bunch), or other people that I'm afraid to name at the moment.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rWzGNdjuep56W5u2d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fpdTvfT6oi8TxAGKN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 18, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "2114", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-15T20:09:32.662Z", "modifiedAt": null, "url": null, "title": "The Wannabe Rational", "slug": "the-wannabe-rational", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:04.209Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RbWJnkSyc7H8E5TNy/the-wannabe-rational", "pageUrlRelative": "/posts/RbWJnkSyc7H8E5TNy/the-wannabe-rational", "linkUrl": "https://www.lesswrong.com/posts/RbWJnkSyc7H8E5TNy/the-wannabe-rational", "postedAtFormatted": "Friday, January 15th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Wannabe%20Rational&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Wannabe%20Rational%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWJnkSyc7H8E5TNy%2Fthe-wannabe-rational%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Wannabe%20Rational%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWJnkSyc7H8E5TNy%2Fthe-wannabe-rational", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRbWJnkSyc7H8E5TNy%2Fthe-wannabe-rational", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2253, "htmlBody": "<p>I have a terrifying confession to make: I believe in God.</p>\n<p>This post has three prongs:</p>\n<p>First:&nbsp;This is a tad meta for a full post, but&nbsp;do I have a place in this community? The abstract, non-religious aspect of this question can be phrased, \"If someone holds a belief that is irrational, should they be fully ousted from the community?\" I can see a handful of answers to this question and a few of them are discussed below.</p>\n<p>Second: I have nothing to say about the rationality of religious beliefs. What I <em>do</em> want to say is that the rationality of particular irrationals is not something that is completely answered after their irrationality is ousted. They may be <a title=\"Raising the Sanity Waterline\" href=\"/lw/1e/raising_the_sanity_waterline/\">underneath the sanity waterline</a>, but there are multiple levels of rationality hell. Some are deeper than others. This part discusses one way to view irrationals in a manner that encourages growth.</p>\n<p>Third: Is it possible to make the irrational rational? Is it possible to take those close to the sanity waterline and raise them above? Or, more personally, is there hope for me? I assume there is. What is <em>my</em> responsibility as an aspiring rationalist? Specifically, when the community complains about a belief, how should I respond?</p>\n<p><a id=\"more\"></a></p>\n<h2><br /></h2>\n<h2>My Place in This Community</h2>\n<p>So, yeah. I believe in God. I figure my particular beliefs are a little irrelevant at this point. This isn't to say that my beliefs aren't open for discussion, but here and now I think there are better things to discuss. &nbsp;Namely, whether talking to people like me is within the purpose of LessWrong.&nbsp;Relevant questions have to do with my status and position at LessWrong. The short list:</p>\n<ol>\n<li>Should I have kept this to myself? What benefit does an irrational person have for confessing their irrationality? (Is this even possible? Is this post an attempted ploy?) I somewhat expect this post and the ensuing discussion to completely wreck my credibility as a commentator and participant.</li>\n<li>Presumably, there is a level of entry to LessWrong that is enforced. Does this level include filtering out certain beliefs and belief systems? Or is the system merit-based via karma and community voting? My karma is well above the level needed to post and my comments generally do better than worse. A merit-based system would prevent me from posting anything about religion or other irrational things, but is there <a title=\"Outside the Laboratory\" href=\"/lw/gv/outside_the_laboratory/\">a deeper problem</a>? (More discussion below.) Should LessWrong /kick people who fail at rationality? Who makes the decision? Who draws the sanity water-line?</li>\n<li>Being religious, I assume I am far below the desired sanity waterline that the community desires. How did I manage to scrape up over 500 karma? What have I demonstrated that would be good for other people to demonstrate? Have I acted appropriately as a religious person curious about rationality? Is there a <a title=\"Well-Kept Gardens Die By Pacifism\" href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">problem with the system</a> that lets someone like me get so far?</li>\n<li>Where do I go from here? In the future, how should I <em>act</em>? Do I need to change my behavior as a result of this post? I am not calling out for any responses to my beliefs in particular, nor am I calling to other religious people at LessWrong to identify themselves. I am asking the community what they <em>want me to do</em>. Leave? Keep posting? Comment but don't post? Convert? Read everything posted and come back later?</li>\n</ol>\n<p>&nbsp;</p>\n<h2>The Wannabe Sanity Waterline</h2>\n<p>This post has little to do with actual&nbsp;<em>beliefs</em>. I get the feeling that most discussions about the beliefs themselves are not going to be terribly useful. I originally titled this post, \"The Religious Rational\" but figured the opening line was inflammatory enough and as I began editing I realized that the religious aspect is merely an example of a greater group of irrationals. I could have admitted to chasing UFOs or buying lottery tickets. &nbsp;What I wanted to talk about is the same.</p>\n<p>That being said, I fully accept all criticisms offered about whatever you feel is appropriate. Even if the criticism is just ignoring me or an admin deleting the post and banning me. I am not trying to dodge the subject of my religious beliefs; I provided myself as an example to be convenient and make the conversation more interesting. I have something relevant and useful to discuss in regards to the overall topic of rationalistic communities that applies to the act of spawning rationalists from within fields other than rationalism.&nbsp;Whether it directly applies to LessWrong is for you to decide.</p>\n<p>How do you approach someone below the sanity waterline? Do you ignore them and look for people above the line? Do you teach them until they drop their irrational deadweight? How do you know which ones are worth pursuing and which are a complete waste of time? Is there a better answer than generalizing at the waterline and turning away everyone who gets wet? The easiest response to these people is to put the burden of rationality on their shoulders. Let them teach themselves. I think think there is a better way. &nbsp;I think there are people closer to the waterline than others and deciding to group everyone below the line together makes the job of teaching rationalism harder.</p>\n<p>I, for example, can look at my fellow theists and immediately draw up a shortlist of people I consider relatively rationalistic. Compared to the given sanity waterline, all of us are deep underwater due to certain beliefs. But compared to the people on the bottom of the ocean, we're doing great. This leads into the question: \"Are there different levels of irrationality?\" And also, \"Do you approach people differently depending on how far below the waterline they are?\"</p>\n<p>More discretely, is it useful to make a distinction between two types of theists? Is it possible to create a sanity waterline for the religious? They may be way off on a particular subject but otherwise their basic worldview is consistent and intact. Is there a religious sanity waterline? Are there rational religious? Is a Wannabe Rational a good place to start?</p>\n<p>The reason I ask these questions is not to excuse any particular belief while feeling good about everything else in my belief system. If there is a theist struggling to verify all beliefs <em>but</em>&nbsp;those that involve God, then they are no true rationalist. But if said theist really, really <em>wanted</em> to become a rationalist, it makes sense for them to drop the sacred, most treasured beliefs last. Can rationalism work on a smaller scale?</p>\n<p>Quoting from <a title=\"Outside the Laboratory\" href=\"/lw/gv/outside_the_laboratory/\">Outside the Laboratory</a>&nbsp;(emphasis not mine):</p>\n<blockquote>\n<p>Now what are we to think of a scientist who seems competent inside the laboratory, but who, outside the laboratory, believes in a spirit world? We ask why, and the scientist says something along the lines of: \"Well, no one really knows, and I admit that I don't have any evidence - it's a religious belief, it can't be disproven one way or another by observation.\" I cannot but conclude that this person <em>literally doesn't know why you have to look at things</em>.</p>\n</blockquote>\n<p>A certain difference between myself and this spirit believing scientist is that my beliefs are from a younger time and I have things I would rather do than gallop through that area of the territory checking my accuracy. Namely, I am still trying to discover what the correct map-making tools are.</p>\n<p>Also, admittedly, I am unjustifiably attached to that area of my map. It's going to take a while to figure out why I am so attached and what I can do about it. I am not fully convinced that rationalism is the silver-bullet that will solve Life, the Universe, and Everything. I am not letting this new thing near something I hold precious. This is a selfish act and will get in the way of my learning, but that sacrifice is something I am willing to make. Hence the reason I am below the LessWrong waterline. Hence me being a Wannabe Rational.</p>\n<p>Instead, what I <em>have</em>&nbsp;done is take my basic worldview and chased down the dogma. Given the set of beliefs I would rather not think about right now, where do they lead? While this is pure anathema to the true rationalist, I am not a true rationalist. I have little idea about what I am doing. I am young in your ways and have much to learn and unlearn. I am not starting at the top of my system; I am starting at the bottom. I consider myself a quasi-rational theist not because I am rational compared to the community of LessWrong. I am a quasi-rational theist because I am rational compared to other theists.</p>\n<p>To return to the underlying question: Is this distinction valid? If it is valid, is it useful or self-defeating? As a community, does a distinction between levels of irrationally help or hinder? I think it helps. Obviously, I would <em>like</em> to consider myself more rational than not. I would also <em>like</em> to think that I can slowly adapt and change into something even more rational. Asking you, the community, is a good way to find out if I am merely deluding myself.</p>\n<p>There may be a wall that I hit and cannot cross. There may be an upper-bound on my rationalism. Right now, there is a cap due to my theism. Unless that cap is removed, there will likely be a limit to how well I integrate with LessWrong.&nbsp;Until then, rationalism has open season on other areas of my map. It has produced excellent results and, as it gains my trust, its tools gain more and more access to my map. As such, I consider myself below the LessWrong sanity waterline and above the religious sanity waterline. I am a Wannabe Rational.</p>\n<h2><br /></h2>\n<h2>Why This Helps</h2>\n<p>The advantage of a distinction between different sanity waterlines is that it allows you to compare individuals within groups of people when scanning for potential rationalists. A particular group may all drop below the waterline but, given their particular irrational map, some of them may be remarkably accurate for being irrational. After accounting for dumb luck, does anyone show a talent for reading territory outside of their too-obviously-irrational-for-excuses belief?</p>\n<p>Note that this is completely different than questioning where the waterline is actually drawn. This is talking about people clearly below the line. But an irrational map can have rational areas. The more rational areas in the map, the more evidence there is that some of the mapmaker's tools and tactics are working well. Therefore, this mapmaker is above the sanity waterline for <em>that particular group</em>&nbsp;of irrational mapmakers. In other words, this mapmaker is worth conversing with as long as the conversation doesn't drift into the irrational areas of the map.</p>\n<p>This allows you to give people below the waterline an attractive target to hit. Walking up to a theist and telling them they are below the waterline is depressing. They do need to hear it, which is why the waterline exists in the first place, and their level of sanity is too low for them to achieve a particular status. But after the chastising you can tell them that other areas in their map are good enough to become more rational <em>in those areas</em>. They don't need to throw everything away to become a Wanna Rational. They will still be considered irrational but at least their map is more accurate than it was. It is at this point that someone begins their journey to rationalism.</p>\n<p>If we have any good reason to help others become more rational, it seems as though this would count toward that goal.</p>\n<h2><br /></h2>\n<h2>Conversion</h2>\n<p>This last bit is short. Taking an example of myself, what should I be doing to make my <a title=\"Wiki:Map and Territory\" href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">map more accurate</a>? My process right now is something like this:</p>\n<ol>\n<li><strong>Look at the map.</strong> What <em>are</em>&nbsp;my beliefs? What areas are marked in the ink of science, evidence, rationalism, and logic? What areas <em>aren't</em> and what ink is being used there?</li>\n<li><strong>Look at the territory.&nbsp;<span style=\"font-weight: normal; \">Beliefs are great, but which ones are working? I quickly notice that certain inks work better. Why am I not using those inks elsewhere? Some inks work better for certain areas, obviously, but some don't seem to be useful at all.</span></strong></li>\n<li><strong>Find the right ink.&nbsp;<span style=\"font-weight: normal; \">Contrasting and comparing the new mapmaking methods with the old ones <em><span style=\"font-weight: normal;\">should</span></em>&nbsp;produce a clear winner. Keep adding stuff to the toolbox <em><span style=\"font-weight: normal;\">once you find a use for it</span></em>. Take stuff out of the toolbox when it is replaced by a better, more accurate tool. Inks such as, \"My elders said so\" and \"Well, it sounds right\" are significantly less useful. Sometimes we have the right ink but we use incorrectly. Sometimes we find a new way to use an old ink.</span></strong></li>\n<li><strong>Revisit old territory.&nbsp;<span style=\"font-weight: normal;\">When I throw out an old ink, examine the areas of the map where that ink was used. Revisit the territory with your new tools handy. Some territory is too hard to access now (beliefs about your childhood) or some areas on your map don't have corresponding territories (beliefs about the gender of God).</span></strong></li>\n</ol>\n<p>These things, in my opinion, are learning the ways of rationality.&nbsp;I have a few areas of my map marked, \"Do this part later.\" I have a few inks labeled, \"Favorite colors.\" These are what keep me below the sanity waterline. As time moves forward I pickup new favorite colors and eventually I will come to the areas saved for later. Maybe then I will rise above the waterline. Maybe then I will be a true rationalist.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "ALwRRZqvhaop8gxkT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RbWJnkSyc7H8E5TNy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 39, "extendedScore": null, "score": 5.533580983877264e-07, "legacy": true, "legacyId": "2083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I have a terrifying confession to make: I believe in God.</p>\n<p>This post has three prongs:</p>\n<p>First:&nbsp;This is a tad meta for a full post, but&nbsp;do I have a place in this community? The abstract, non-religious aspect of this question can be phrased, \"If someone holds a belief that is irrational, should they be fully ousted from the community?\" I can see a handful of answers to this question and a few of them are discussed below.</p>\n<p>Second: I have nothing to say about the rationality of religious beliefs. What I <em>do</em> want to say is that the rationality of particular irrationals is not something that is completely answered after their irrationality is ousted. They may be <a title=\"Raising the Sanity Waterline\" href=\"/lw/1e/raising_the_sanity_waterline/\">underneath the sanity waterline</a>, but there are multiple levels of rationality hell. Some are deeper than others. This part discusses one way to view irrationals in a manner that encourages growth.</p>\n<p>Third: Is it possible to make the irrational rational? Is it possible to take those close to the sanity waterline and raise them above? Or, more personally, is there hope for me? I assume there is. What is <em>my</em> responsibility as an aspiring rationalist? Specifically, when the community complains about a belief, how should I respond?</p>\n<p><a id=\"more\"></a></p>\n<h2><br></h2>\n<h2 id=\"My_Place_in_This_Community\">My Place in This Community</h2>\n<p>So, yeah. I believe in God. I figure my particular beliefs are a little irrelevant at this point. This isn't to say that my beliefs aren't open for discussion, but here and now I think there are better things to discuss. &nbsp;Namely, whether talking to people like me is within the purpose of LessWrong.&nbsp;Relevant questions have to do with my status and position at LessWrong. The short list:</p>\n<ol>\n<li>Should I have kept this to myself? What benefit does an irrational person have for confessing their irrationality? (Is this even possible? Is this post an attempted ploy?) I somewhat expect this post and the ensuing discussion to completely wreck my credibility as a commentator and participant.</li>\n<li>Presumably, there is a level of entry to LessWrong that is enforced. Does this level include filtering out certain beliefs and belief systems? Or is the system merit-based via karma and community voting? My karma is well above the level needed to post and my comments generally do better than worse. A merit-based system would prevent me from posting anything about religion or other irrational things, but is there <a title=\"Outside the Laboratory\" href=\"/lw/gv/outside_the_laboratory/\">a deeper problem</a>? (More discussion below.) Should LessWrong /kick people who fail at rationality? Who makes the decision? Who draws the sanity water-line?</li>\n<li>Being religious, I assume I am far below the desired sanity waterline that the community desires. How did I manage to scrape up over 500 karma? What have I demonstrated that would be good for other people to demonstrate? Have I acted appropriately as a religious person curious about rationality? Is there a <a title=\"Well-Kept Gardens Die By Pacifism\" href=\"/lw/c1/wellkept_gardens_die_by_pacifism/\">problem with the system</a> that lets someone like me get so far?</li>\n<li>Where do I go from here? In the future, how should I <em>act</em>? Do I need to change my behavior as a result of this post? I am not calling out for any responses to my beliefs in particular, nor am I calling to other religious people at LessWrong to identify themselves. I am asking the community what they <em>want me to do</em>. Leave? Keep posting? Comment but don't post? Convert? Read everything posted and come back later?</li>\n</ol>\n<p>&nbsp;</p>\n<h2 id=\"The_Wannabe_Sanity_Waterline\">The Wannabe Sanity Waterline</h2>\n<p>This post has little to do with actual&nbsp;<em>beliefs</em>. I get the feeling that most discussions about the beliefs themselves are not going to be terribly useful. I originally titled this post, \"The Religious Rational\" but figured the opening line was inflammatory enough and as I began editing I realized that the religious aspect is merely an example of a greater group of irrationals. I could have admitted to chasing UFOs or buying lottery tickets. &nbsp;What I wanted to talk about is the same.</p>\n<p>That being said, I fully accept all criticisms offered about whatever you feel is appropriate. Even if the criticism is just ignoring me or an admin deleting the post and banning me. I am not trying to dodge the subject of my religious beliefs; I provided myself as an example to be convenient and make the conversation more interesting. I have something relevant and useful to discuss in regards to the overall topic of rationalistic communities that applies to the act of spawning rationalists from within fields other than rationalism.&nbsp;Whether it directly applies to LessWrong is for you to decide.</p>\n<p>How do you approach someone below the sanity waterline? Do you ignore them and look for people above the line? Do you teach them until they drop their irrational deadweight? How do you know which ones are worth pursuing and which are a complete waste of time? Is there a better answer than generalizing at the waterline and turning away everyone who gets wet? The easiest response to these people is to put the burden of rationality on their shoulders. Let them teach themselves. I think think there is a better way. &nbsp;I think there are people closer to the waterline than others and deciding to group everyone below the line together makes the job of teaching rationalism harder.</p>\n<p>I, for example, can look at my fellow theists and immediately draw up a shortlist of people I consider relatively rationalistic. Compared to the given sanity waterline, all of us are deep underwater due to certain beliefs. But compared to the people on the bottom of the ocean, we're doing great. This leads into the question: \"Are there different levels of irrationality?\" And also, \"Do you approach people differently depending on how far below the waterline they are?\"</p>\n<p>More discretely, is it useful to make a distinction between two types of theists? Is it possible to create a sanity waterline for the religious? They may be way off on a particular subject but otherwise their basic worldview is consistent and intact. Is there a religious sanity waterline? Are there rational religious? Is a Wannabe Rational a good place to start?</p>\n<p>The reason I ask these questions is not to excuse any particular belief while feeling good about everything else in my belief system. If there is a theist struggling to verify all beliefs <em>but</em>&nbsp;those that involve God, then they are no true rationalist. But if said theist really, really <em>wanted</em> to become a rationalist, it makes sense for them to drop the sacred, most treasured beliefs last. Can rationalism work on a smaller scale?</p>\n<p>Quoting from <a title=\"Outside the Laboratory\" href=\"/lw/gv/outside_the_laboratory/\">Outside the Laboratory</a>&nbsp;(emphasis not mine):</p>\n<blockquote>\n<p>Now what are we to think of a scientist who seems competent inside the laboratory, but who, outside the laboratory, believes in a spirit world? We ask why, and the scientist says something along the lines of: \"Well, no one really knows, and I admit that I don't have any evidence - it's a religious belief, it can't be disproven one way or another by observation.\" I cannot but conclude that this person <em>literally doesn't know why you have to look at things</em>.</p>\n</blockquote>\n<p>A certain difference between myself and this spirit believing scientist is that my beliefs are from a younger time and I have things I would rather do than gallop through that area of the territory checking my accuracy. Namely, I am still trying to discover what the correct map-making tools are.</p>\n<p>Also, admittedly, I am unjustifiably attached to that area of my map. It's going to take a while to figure out why I am so attached and what I can do about it. I am not fully convinced that rationalism is the silver-bullet that will solve Life, the Universe, and Everything. I am not letting this new thing near something I hold precious. This is a selfish act and will get in the way of my learning, but that sacrifice is something I am willing to make. Hence the reason I am below the LessWrong waterline. Hence me being a Wannabe Rational.</p>\n<p>Instead, what I <em>have</em>&nbsp;done is take my basic worldview and chased down the dogma. Given the set of beliefs I would rather not think about right now, where do they lead? While this is pure anathema to the true rationalist, I am not a true rationalist. I have little idea about what I am doing. I am young in your ways and have much to learn and unlearn. I am not starting at the top of my system; I am starting at the bottom. I consider myself a quasi-rational theist not because I am rational compared to the community of LessWrong. I am a quasi-rational theist because I am rational compared to other theists.</p>\n<p>To return to the underlying question: Is this distinction valid? If it is valid, is it useful or self-defeating? As a community, does a distinction between levels of irrationally help or hinder? I think it helps. Obviously, I would <em>like</em> to consider myself more rational than not. I would also <em>like</em> to think that I can slowly adapt and change into something even more rational. Asking you, the community, is a good way to find out if I am merely deluding myself.</p>\n<p>There may be a wall that I hit and cannot cross. There may be an upper-bound on my rationalism. Right now, there is a cap due to my theism. Unless that cap is removed, there will likely be a limit to how well I integrate with LessWrong.&nbsp;Until then, rationalism has open season on other areas of my map. It has produced excellent results and, as it gains my trust, its tools gain more and more access to my map. As such, I consider myself below the LessWrong sanity waterline and above the religious sanity waterline. I am a Wannabe Rational.</p>\n<h2><br></h2>\n<h2 id=\"Why_This_Helps\">Why This Helps</h2>\n<p>The advantage of a distinction between different sanity waterlines is that it allows you to compare individuals within groups of people when scanning for potential rationalists. A particular group may all drop below the waterline but, given their particular irrational map, some of them may be remarkably accurate for being irrational. After accounting for dumb luck, does anyone show a talent for reading territory outside of their too-obviously-irrational-for-excuses belief?</p>\n<p>Note that this is completely different than questioning where the waterline is actually drawn. This is talking about people clearly below the line. But an irrational map can have rational areas. The more rational areas in the map, the more evidence there is that some of the mapmaker's tools and tactics are working well. Therefore, this mapmaker is above the sanity waterline for <em>that particular group</em>&nbsp;of irrational mapmakers. In other words, this mapmaker is worth conversing with as long as the conversation doesn't drift into the irrational areas of the map.</p>\n<p>This allows you to give people below the waterline an attractive target to hit. Walking up to a theist and telling them they are below the waterline is depressing. They do need to hear it, which is why the waterline exists in the first place, and their level of sanity is too low for them to achieve a particular status. But after the chastising you can tell them that other areas in their map are good enough to become more rational <em>in those areas</em>. They don't need to throw everything away to become a Wanna Rational. They will still be considered irrational but at least their map is more accurate than it was. It is at this point that someone begins their journey to rationalism.</p>\n<p>If we have any good reason to help others become more rational, it seems as though this would count toward that goal.</p>\n<h2><br></h2>\n<h2 id=\"Conversion\">Conversion</h2>\n<p>This last bit is short. Taking an example of myself, what should I be doing to make my <a title=\"Wiki:Map and Territory\" href=\"http://wiki.lesswrong.com/wiki/Map_and_territory\">map more accurate</a>? My process right now is something like this:</p>\n<ol>\n<li><strong>Look at the map.</strong> What <em>are</em>&nbsp;my beliefs? What areas are marked in the ink of science, evidence, rationalism, and logic? What areas <em>aren't</em> and what ink is being used there?</li>\n<li><strong>Look at the territory.&nbsp;<span style=\"font-weight: normal; \">Beliefs are great, but which ones are working? I quickly notice that certain inks work better. Why am I not using those inks elsewhere? Some inks work better for certain areas, obviously, but some don't seem to be useful at all.</span></strong></li>\n<li><strong>Find the right ink.&nbsp;<span style=\"font-weight: normal; \">Contrasting and comparing the new mapmaking methods with the old ones <em><span style=\"font-weight: normal;\">should</span></em>&nbsp;produce a clear winner. Keep adding stuff to the toolbox <em><span style=\"font-weight: normal;\">once you find a use for it</span></em>. Take stuff out of the toolbox when it is replaced by a better, more accurate tool. Inks such as, \"My elders said so\" and \"Well, it sounds right\" are significantly less useful. Sometimes we have the right ink but we use incorrectly. Sometimes we find a new way to use an old ink.</span></strong></li>\n<li><strong>Revisit old territory.&nbsp;<span style=\"font-weight: normal;\">When I throw out an old ink, examine the areas of the map where that ink was used. Revisit the territory with your new tools handy. Some territory is too hard to access now (beliefs about your childhood) or some areas on your map don't have corresponding territories (beliefs about the gender of God).</span></strong></li>\n</ol>\n<p>These things, in my opinion, are learning the ways of rationality.&nbsp;I have a few areas of my map marked, \"Do this part later.\" I have a few inks labeled, \"Favorite colors.\" These are what keep me below the sanity waterline. As time moves forward I pickup new favorite colors and eventually I will come to the areas saved for later. Maybe then I will rise above the waterline. Maybe then I will be a true rationalist.</p>", "sections": [{"title": "My Place in This Community", "anchor": "My_Place_in_This_Community", "level": 1}, {"title": "The Wannabe Sanity Waterline", "anchor": "The_Wannabe_Sanity_Waterline", "level": 1}, {"title": "Why This Helps", "anchor": "Why_This_Helps", "level": 1}, {"title": "Conversion", "anchor": "Conversion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "305 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 305, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XqmjdBKa4ZaXJtNmf", "N2pENnTPB75sfc9kb", "tscc3e5eujrsEeFN4"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-16T20:40:18.505Z", "modifiedAt": null, "url": null, "title": "Dennett's heterophenomenology", "slug": "dennett-s-heterophenomenology", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:56.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdKi3Y93geyKMscbA/dennett-s-heterophenomenology", "pageUrlRelative": "/posts/SdKi3Y93geyKMscbA/dennett-s-heterophenomenology", "linkUrl": "https://www.lesswrong.com/posts/SdKi3Y93geyKMscbA/dennett-s-heterophenomenology", "postedAtFormatted": "Saturday, January 16th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Dennett's%20heterophenomenology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADennett's%20heterophenomenology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdKi3Y93geyKMscbA%2Fdennett-s-heterophenomenology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Dennett's%20heterophenomenology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdKi3Y93geyKMscbA%2Fdennett-s-heterophenomenology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdKi3Y93geyKMscbA%2Fdennett-s-heterophenomenology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 970, "htmlBody": "<p>In an <a href=\"/lw/1m7/dennetts_consciousness_explained_prelude/1fhx\">earlier comment</a>, I conflated heterophenomenology in the general sense of taking introspective accounts as data to be explained rather than direct readouts of the truth, with Dennett's particular approach to explaining those data. &nbsp;So to correct myself, I say that it is Dennett, rather than heterophenomenology, that claims that there is no such thing as consciousness. Dennett denies that he does, but I disagree. I defend this view here.</p>\n<p>I have to admit at this point that I have not read \"Consciousness Explained\". &nbsp;Had either of the library's copies been on the shelves last Tuesday I would have done by now, but instead I found his later book (and his most recent on the topic), \"Sweet Dreams: Philosophical Obstacles to a Science of Consciousness\". &nbsp;The subtitle suggests a drawing back from the confidence of the earlier title, as does that of the <a href=\"http://www.amazon.com/s?field-isbn=0465073514\">book in between</a>. &nbsp;The book confirms me in my impression that the ideas of \"C.E.\" have been in the air so long (the air of hard SF, sciblogs, and the like, not to mention Phil Goetz's recent posts) that reading the primary source 19 years on would be nothing more than an exercise in checkbox-ticking.</p>\n<p>I'll give a brief run-through of \"Sweet Dreams\" and then carry on the argument.</p>\n<p><a id=\"more\"></a>The book is primarily writing against, a response to objections arising from earlier works.</p>\n<p>In chapter 1 he shoots down the \"Zombic Hunch\", the idea that a being could be physically identical to a human, but lack consciousness, and therefore that consciousness must be non-physical. &nbsp;I'll take it that we all agree <a href=\"/lw/p7/zombies_zombies/\">the zombie story is insane</a>.</p>\n<p>Chapter 2 introduces the concept of heterophenomenology. &nbsp;This has already been introduced to LW.</p>\n<p>The corpse of the Zombic Hunch got up again and walked, so in Chapter 3 he shoots it down again.</p>\n<p>Chapters 4 and 5 attack qualia. &nbsp;They don't exist, says Dennett, because of such anomalies as change blindness, Capgras syndrome, and various thought-experiments that defy all coherent accounts of what qualia are.</p>\n<p>Chapter 6 is entitled \"Are We Explaining Consciousness yet?\" (Er, what was that first book called?) &nbsp;It cites neurological research and briefly sets out the Multiple Drafts hypothesis. &nbsp;He confronts critics who say (which I also say) that Dennett is really denying the existence of consciousness, not giving an account of it. &nbsp;His answer is that he is giving (or seeking) a third-person explanation of first-person accounts. &nbsp;Well, yes, that is indeed what he is doing, and what his critics on this point are complaining that he is doing. &nbsp;The question is whether there is more to do, which goes unaddressed here.</p>\n<p>Chapter 7 says some more about Multiple Drafts.</p>\n<p>Chapter 8 argues against qualia of consciousness again. It closes by saying that if ever his heterophenomenological program runs into a roadblock and something more is clearly needed, then the Zombic Hunch gets to eat his braiiinnzzz.</p>\n<p>Ok, his actual words are \"If the day arrives when...we plainly see that something big is missing...those with the unshakable hunch will get to say they told us so.\"</p>\n<p>So, why do I claim that Dennett's account of consciousness amounts to denying there is such a thing, contrary to his own claims that consciousness exists and he is explaining it?</p>\n<p>The problem I have is with his differing accounts of consciousness and qualia. &nbsp;The former he says exists, and claims to give an explanation of; the latter he denies. &nbsp;Yet the evidence he adduces regarding both topics is of the same sort: experimental observations or thought experiments demonstrating the incoherence of all existing accounts of what they are. &nbsp;But he comes to different conclusions about them. &nbsp;Why?</p>\n<p>I believe the reason is that any physical account of consciousness, including his, will meet the objection (as it has) that yes, that may be an accurate account of what is physically happening in the brain, and yes, that might even be necessary and sufficient for consciousness to exist in a brain, but it leaves unanswered what Dennett calls the Hard Question: how does that physical process produce the experience of being conscious? &nbsp;That is, how does it account for the qualia of consciousness itself? &nbsp;The only way to overcome that objection is to argue against the existenceof qualia (as Dennett does).</p>\n<p>But the qualia of consciousness looks to me like the very thing that people are talking about when they talk about being conscious. &nbsp;To deny the qualia of consciousness is to deny that there is any such thing as consciousness.</p>\n<p>That is why I claim that Dennett's theory amounts to denying the existence of consciousness. &nbsp;Dennett is describing beings with no inner experience, philosophical zombies, and avoids the Zombic Hunch by denying there is such a thing as inner experience. What he is explaining is not consciousness, but why the zombies say that they are conscious.</p>\n<p>And so back to the word \"heterophenomenology\". &nbsp;This is Dennett's word, and I think it fair, on heterophenomenological grounds, to look for Dennett's meaning in his practice rather than in his account of that practice. &nbsp;His account is that he wants to explain people's talk of their inner experiences without taking that talk to be a reliable account. &nbsp;But his practice is to explain such talk without taking it to be an account of anything, not even an unreliable account, not even an account which might be about something. &nbsp;He sketches a mechanism that could produce such talk, no more. &nbsp;He explains first-person talk, and says nothing of first-person experience. And so a negative answer to the question of whether there is anything being talked about, however imperfectly, whether there is such a thing as first-person experience, gets palmed into the definition.</p>\n<p>&nbsp;</p>\n<p>I do not have any account of what qualia are, neither qualia in general nor the experience of being aware. But I do believe that this experience exists. The heterophenomenological program is to hit <a href=\"/lw/j2/explainworshipignore/\">Ignore</a> on that experience.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdKi3Y93geyKMscbA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 8, "extendedScore": null, "score": 5.536294032607548e-07, "legacy": true, "legacyId": "2115", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fdEWWr8St59bXLbQr", "yxvi9RitzZDpqn6Yh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-17T23:56:20.904Z", "modifiedAt": null, "url": null, "title": "Tips and Tricks for Answering Hard Questions", "slug": "tips-and-tricks-for-answering-hard-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:57.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SEq8bvSXrzF4jcdS8/tips-and-tricks-for-answering-hard-questions", "pageUrlRelative": "/posts/SEq8bvSXrzF4jcdS8/tips-and-tricks-for-answering-hard-questions", "linkUrl": "https://www.lesswrong.com/posts/SEq8bvSXrzF4jcdS8/tips-and-tricks-for-answering-hard-questions", "postedAtFormatted": "Sunday, January 17th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tips%20and%20Tricks%20for%20Answering%20Hard%20Questions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATips%20and%20Tricks%20for%20Answering%20Hard%20Questions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEq8bvSXrzF4jcdS8%2Ftips-and-tricks-for-answering-hard-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tips%20and%20Tricks%20for%20Answering%20Hard%20Questions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEq8bvSXrzF4jcdS8%2Ftips-and-tricks-for-answering-hard-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSEq8bvSXrzF4jcdS8%2Ftips-and-tricks-for-answering-hard-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 497, "htmlBody": "<p>I've collected some tips and tricks for answering hard questions, some of which may be original, and others I may have read somewhere and forgotten the source of. Please feel free to contribute more tips and tricks, or additional links to the sources or fuller explanations.</p>\n<p><strong>Don't stop at the first good answer. </strong>We know that human curiosity can be <a href=\"http://wiki.lesswrong.com/wiki/Semantic_stopsign\">prematurely satiated</a>. Sometimes we can quickly recognize a flaw in an answer that initially seemed good, but sometimes we can't, so we should keep looking for flaws and/or better answers.</p>\n<p><strong>Explore multiple approaches simultaneously. </strong>A hard question probably has multiple approaches that are roughly equally promising, otherwise it wouldn't be a hard question (well, unless it has no promising approaches). If there are several people attempting to answer it, they should explore different approaches. If you're trying to answer it alone, it makes sense to switch approaches (and look for new approaches) once a while.</p>\n<p><strong>Trust your intuitions, but don't waste too much time arguing for them.</strong> If several people are attempting to answer the same question and they have different intuitions about how best to approach it, it seems efficient for each to rely on his or her intuition to choose the approach to explore. It only makes sense to spend a lot of time arguing for your own intuition if you have some reason to believe that other people's intuitions are much worse than yours.</p>\n<p><strong>Go meta. </strong>Instead of attacking the question directly, ask \"How should I answer a question like this?\" It seems that when people are faced with a question, even one that has stumped great minds for ages, many just jump in and try to attack it with whatever intellectual tools they have at hand. For really hard questions, we may need to look for, or build, new tools.</p>\n<p><strong><a href=\"/lw/of/dissolving_the_question/\">Dissolve the question.</a> </strong>Sometimes, the question is meaningless and asking it is just a cognitive error. If you can detect and correct the error then the question may just go away.</p>\n<p><strong>Sleep on it. </strong>I find that I tend to have a greater than average number of insights in the period of time just after I wake up and before I get out of bed. Our brains seem to continue to work while we're asleep, and it may help to prime it by reviewing the problem before going to sleep. (I think Eliezer wrote a post or comment to this effect, but I can't find it now.)</p>\n<p><strong>Be ready to recognize a good answer when you see it.</strong> The history of science shows that human knowledge does make progress, but sometimes only by an older generation dying off or retiring. It seems that we often can't recognize a good answer even when it's staring us in the face. I wish I knew more about what factors affect this ability, but one thing that might help is to <a href=\"/lw/1lo/high_status_and_stupidity_why/\">avoid acquiring a high social status</a>, or the mental state of having high social status. (See also, <a href=\"http://wiki.lesswrong.com/mediawiki/index.php?title=How_To_Actually_Change_Your_Mind\">How To Actually Change Your Mind</a>.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BzghQYM9GnkMHxZKb": 2, "fF9GEdWXKJ3z73TmB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SEq8bvSXrzF4jcdS8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 82, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "1876", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 82, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Mc6QcrsbH5NRXbCRX", "cgrvvp9QzjiFuYwLi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-18T04:18:53.036Z", "modifiedAt": null, "url": null, "title": "Sorting Out Sticky Brains", "slug": "sorting-out-sticky-brains", "viewCount": null, "lastCommentedAt": "2020-08-14T04:13:03.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/L4GGomr86sEwxzPvS/sorting-out-sticky-brains", "pageUrlRelative": "/posts/L4GGomr86sEwxzPvS/sorting-out-sticky-brains", "linkUrl": "https://www.lesswrong.com/posts/L4GGomr86sEwxzPvS/sorting-out-sticky-brains", "postedAtFormatted": "Monday, January 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sorting%20Out%20Sticky%20Brains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASorting%20Out%20Sticky%20Brains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4GGomr86sEwxzPvS%2Fsorting-out-sticky-brains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sorting%20Out%20Sticky%20Brains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4GGomr86sEwxzPvS%2Fsorting-out-sticky-brains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FL4GGomr86sEwxzPvS%2Fsorting-out-sticky-brains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 955, "htmlBody": "<p>tl;dr: Just because it doesn't seem like we <em>should</em> be able to have beliefs we acknowledge to be irrational, doesn't mean we <em>don't</em> have them.&nbsp; If this happens to you, here's a tool to help conceptualize and work around that phenomenon.</p>\n<p>There's a general feeling that by the time you've acknowledged that some belief you hold is not based on rational evidence, it has already evaporated.&nbsp; The very act of realizing it's not something you should believe makes it go away.&nbsp; If that's your experience, I applaud your well-organized mind!&nbsp; It's serving you well.&nbsp; This is exactly as it should be.</p>\n<p>If only we were all so lucky.</p>\n<p>Brains are sticky things.&nbsp; They will hang onto comfortable beliefs that don't make sense anymore, view the world through familiar filters that should have been discarded long ago, see significances and patterns and illusions even if they're known by the <em>rest</em> of the brain to be irrelevant.&nbsp; Beliefs <em>should</em> be formed on the basis of sound evidence.&nbsp; But that's not the only mechanism we have in our skulls to form them.&nbsp; We're equipped to come by them in other ways, too.&nbsp; It's been observed<sup>1</sup> that believing contradictions is only bad because it entails believing falsehoods.&nbsp; If you can't get rid of one belief in a contradiction, and that's the false one, then believing a contradiction is the best you can do, because then at least you have the true belief too.</p>\n<p>The mechanism I use to deal with this is to label my beliefs \"official\" and \"unofficial\".&nbsp; My official beliefs have a second-order stamp of approval.&nbsp; I believe them, and I believe that I <em>should</em> believe them.&nbsp; Meanwhile, the \"unofficial\" beliefs are those I can't get rid of, or am not motivated to try really hard to get rid of because they aren't problematic enough to be worth the trouble.&nbsp; They might or might not outright contradict an official belief, but regardless, I try not to act on them.<a id=\"more\"></a></p>\n<p>To those of you with well-ordered minds (for such lucky people seem to exist, if we believe some of the self-reports on this very site), this probably sounds outrageous.&nbsp; If I <em>know they're probably not true...</em> And I do.&nbsp; But they still make me expect things.&nbsp; They make me surprised when those expectations are flouted.&nbsp; If I'm asked about their subjects when tired, or not prepared for the question, they'll leap out of my mouth before I can stop them, and they won't feel like lies - because they're not.&nbsp; They're beliefs.&nbsp; I just don't like them very much.</p>\n<p>I'll supply an example.&nbsp; I have a rather dreadful phobia of guns, and accordingly, I think they should be illegal.&nbsp; The phobia is a terrible reason to believe in the appropriateness of such a ban: said phobia doesn't even stand in for an informative real experience, since I haven't lost a family member to a stray bullet or anything of the kind.&nbsp; I certainly don't assent to the general proposition \"anything that scares me should be illegal\".&nbsp; I have no other reasons, except for a vague affection for a cluster of political opinions which includes something along those lines, to believe this belief.&nbsp; Neither the fear nor the affection are reasons I endorse for believing things in general, or this in particular.&nbsp; So this is an unofficial belief.&nbsp; Whenever I can, I avoid acting on it.&nbsp; Until I locate some good reasons to believe something about the topic, I officially have no opinion.&nbsp; I avoid putting myself in situations where I might act on the unofficial belief in the same way I might avoid a store with contents for which I have an unendorsed desire, like a candy shop.&nbsp; For instance, when I read about political candidates' stances on issues, I avoid whatever section talks about gun control.</p>\n<p>Because I know my brain collects junk like this, I try to avoid making up my mind until I do have a pretty good idea of what's going on.&nbsp; Once I tell myself, \"Okay, I've decided\", I run the risk of lodging something permanently in my cortex that won't release its stranglehold on my thought process until kingdom come.&nbsp; I use tools like \"temporarily operating under the assumption that\" (some proposition) or declaring myself \"unqualified to have an opinion about\" (some subject).&nbsp; The longer I hold my opinions in a state of uncertainty, the less chance I wind up with a permanent epistemic parasite that I have to devote cognitive resources to just to keep it from making me do dumb things.&nbsp; This is partly because it makes the state of uncertainty come to feel like a default, which makes it simpler to slide back to uncertainty again if it seems warranted.&nbsp; Partly, it's because the longer I wait, the more evidence I've collected by the time I pick a side, so it's less likely that the belief I acquire is one I'll want to excise in the future.</p>\n<p>This is all well and good as a prophylactic.&nbsp; It doesn't help as much with stuff that snuck in when I was but a mere slip of a youth.&nbsp; For that, I rely on the official/unofficial distinction, and then <em>toe the official line as best I can</em> in thought, word, and deed.&nbsp; I break in uncomfy official beliefs like new shoes.&nbsp; You can use your brain's love of routine to your advantage.&nbsp; Act like you only believe the official beliefs, and the unofficial ones will weaken from disuse.&nbsp; This isn't a betrayal of your \"real\" beliefs.&nbsp; The official beliefs are real too!&nbsp; They're real, and they're <em>better</em>.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>I read this in Peter van Inwagen's book \"Essay on Free Will\" but seem to remember that he got it elsewhere.&nbsp; I'm not certain where my copy has gotten to lately, so can't check.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HXA9WxPpzZCCEwXHT": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "L4GGomr86sEwxzPvS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 64, "extendedScore": null, "score": 0.000107, "legacy": true, "legacyId": "2118", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "ynMFrq9K5iNMfSZNg", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "mental-crystallography", "canonicalPrevPostSlug": "", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 64, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-18T09:51:31.050Z", "modifiedAt": null, "url": null, "title": "Advancing Certainty", "slug": "advancing-certainty", "viewCount": null, "lastCommentedAt": "2017-06-17T03:55:27.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6zRn4uKADwL9uo8Ch/advancing-certainty", "pageUrlRelative": "/posts/6zRn4uKADwL9uo8Ch/advancing-certainty", "linkUrl": "https://www.lesswrong.com/posts/6zRn4uKADwL9uo8Ch/advancing-certainty", "postedAtFormatted": "Monday, January 18th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Advancing%20Certainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdvancing%20Certainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zRn4uKADwL9uo8Ch%2Fadvancing-certainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Advancing%20Certainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zRn4uKADwL9uo8Ch%2Fadvancing-certainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6zRn4uKADwL9uo8Ch%2Fadvancing-certainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1340, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/u6/horrible_lhc_inconsistency/\">Horrible LHC Inconsistency</a>, <a href=\"/lw/gq/the_proper_use_of_humility/\">The Proper Use of Humility</a></p>\r\n<p><a href=\"http://wiki.lesswrong.com/wiki/Overconfidence\">Overconfidence</a>, I've noticed, is a big fear around these parts. Well, it <em>is</em> a known human bias, after all, and therefore something to be guarded against. But I am going to argue that, at least in aspiring-rationalist circles, people are <em>too</em> afraid of overconfidence, to the point of overcorrecting -- which, not surprisingly, causes <a href=\"/lw/u6/horrible_lhc_inconsistency/\">problems</a>. (Some may detect implications here&nbsp;for the long-standing Inside View vs. Outside View debate.)</p>\r\n<p>Here's <a href=\"/lw/u6/horrible_lhc_inconsistency/\">Eliezer, voicing the typical worry</a>:</p>\r\n<blockquote>\r\n<p>[I]f you asked me whether I could make one million statements of authority equal to \"The Large Hadron Collider will not destroy the world\", and be wrong, on average, around once, then I would have to say no.</p>\r\n</blockquote>\r\n<p>I now suspect that misleading imagery may be at work here. A <em>million</em> statements -- that sounds like a lot, doesn't it? If you made one such pronouncement every ten seconds, a million of them would require you to spend months doing nothing but pontificating, with no eating, sleeping, or bathroom breaks. Boy, that would be tiring, wouldn't it? At some point, surely, <a href=\"http://www.spaceandgames.com/?p=27\">your exhausted brain would slip up and make an error</a>. In fact, it would surely make more than one -- in which case, poof!, there goes your calibration.</p>\r\n<p>No wonder, then, that <a href=\"/lw/ml/but_theres_still_a_chance_right/hgl\">people</a> <a href=\"/lw/1la/new_years_predictions_thread/1dve\">claim</a> that we humans can't possibly hope to attain such levels of certainty. Look, they say, at all those times in the past when people -- even famous scientists! -- said they were 99.999% sure of something, and they turned out to be wrong. My own adolescent self would have assigned high confidence to the truth of Christianity; so where do I get the temerity, now, to <a href=\"/lw/1la/new_years_predictions_thread/1dv7\">say</a> that the probability of this is 1-over-oogles-and-googols?</p>\r\n<p><a id=\"more\"></a></p>\r\n<p>[EDIT: Unnecessary material removed.]</p>\r\n<p>A probability estimate is not a measure of \"confidence\" in some psychological sense. Rather, it is a measure of the <em>strength of the evidence</em>: how much <em>information</em> you believe you have about reality. So, when judging calibration, it is not really appropriate to imagine oneself, say, <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/1brf\">judging thousands of criminal trials</a>, and getting more than a few wrong here and there (because, after all, one is human and tends to make mistakes). Let me instead propose a less misleading image: picture yourself programming your model of the world (in technical terms, your prior probability distribution) into a computer, and then feeding all that data from those thousands of cases into the computer -- which then, when you run the program, rapidly spits out the corresponding thousands of posterior probability estimates. That is, visualize a few seconds or minutes of staring at a rapidly-scrolling computer screen, rather than a lifetime of exhausting judicial labor. When the program finishes, how many of those numerical verdicts on the screen are wrong?</p>\r\n<p><a href=\"/lw/dr/generalizing_from_one_example\">I don't know about you</a>, but modesty seems less tempting to me when I think about it in this way. I have a model of the world, and it makes predictions. For some reason, when it's just me in a room looking at a screen, I don't feel the need to tone down the strength of those predictions for fear of <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/1bpu\">unpleasant</a> <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/1bre\">social</a> <a href=\"/lw/1je/previous_post_revised/1bxy\">consequences</a>. Nor do I need to worry about the computer getting tired from running all those numbers.</p>\r\n<p>In the <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">vanishingly unlikely event</a> that Omega were to appear and tell me that, say,&nbsp;Amanda Knox was guilty, it wouldn't mean that I had been too arrogant, and that I had better not trust my estimates in the future. What it would mean is that my model of the world was <em>severely stupid</em> with respect to predicting reality. In which case, the thing to do would not be to humbly promise to be more <a href=\"/lw/gq/the_proper_use_of_humility/\">modest</a> henceforth, but rather, to <em>find the problem and fix it</em>. (I believe computer programmers call this \"debugging\".)</p>\r\n<p>A&nbsp;\"confidence level\"&nbsp;is a numerical measure of <em>how stupid your model is, if you turn out to be wrong</em>.</p>\r\n<p>The fundamental question of rationality is: why do you believe what you believe? As a rationalist, you can't just pull probabilities out of your rear end. And now here's the kicker: <em>that includes the probability of your model being wrong</em>. The latter must, paradoxically but <a href=\"http://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence\">necessarily</a>, be part of your model itself. If you're uncertain, there has to be a <em>reason</em> you're uncertain; <a href=\"http://wiki.lesswrong.com/wiki/Conservation_of_expected_evidence\">if you expect to change your mind later, you should go ahead and change your mind now</a>.</p>\r\n<p>This is the first thing to remember in setting out to dispose of what I call \"quantitative Cartesian skepticism\": the view that even though science tells us the probability of such-and-such is 10<sup>-50</sup>, well, that's just too high of a confidence for mere mortals like us to assert; our model of the world could be wrong, after all -- conceivably, we might even be brains in vats.</p>\r\n<p>Now, it could be the case that 10<sup>-50</sup> is too low of a probability for that event, despite the calculations; and it may even be that that particular level of certainty (about almost anything) is in fact beyond our current epistemic reach. But if we believe this, there have to be <em>reasons</em> we believe it, and those reasons have to be <em>better</em> than the reasons for believing the opposite.</p>\r\n<p>I can't speak for Eliezer in particular, but I expect that if you probe the intuitions of people who worry about 10<sup>-6</sup> being too low of a probability that the Large Hadron Collider will destroy the world -- that is, if you ask them <em>why</em> they think they couldn't make a million statements of equal authority and be wrong on average once -- they will cite statistics about the previous track record of human predictions: their own youthful failures and/or things like Lord Kelvin calculating that evolution by natural selection was impossible.</p>\r\n<p>To which my reply is: hindsight is 20/20 -- so how about taking advantage of this fact?</p>\r\n<p><a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">Previously,</a> I used the phrase \"epistemic technology\" in reference to our ability to achieve greater certainty through some recently-invented methods of investigation than through others that are native unto us. This, I confess, was an almost deliberate foreshadowing of my thesis here: <em>we are not stuck with the inferential powers of our ancestors</em>. One implication of&nbsp;the Bayesian-Jaynesian-Yudkowskian view, which <a href=\"/lw/o5/the_second_law_of_thermodynamics_and_engines_of/\">marries epistemology to physics</a>, is that our knowledge-gathering ability is as subject to \"technological\" improvement as any other physical process. With effort applied over time, we should be able to increase not only our domain knowledge, but also our meta-knowledge. As we acquire more and more information about the world, our Bayesian probabilities <em>should</em> become more and more confident.</p>\r\n<p>If we're smart, we will look back at Lord Kelvin's reasoning, find the mistakes, and avoid making those mistakes in the future. We will, so to speak, debug the code. Perhaps we couldn't have spotted the flaws at the time; but we can spot them now. Whatever other flaws may still be plaguing us, our score has improved.&nbsp;&nbsp;</p>\r\n<p>In the face of precise scientific calculations, it doesn't do to say, \"Well, science has been wrong before\". If science was wrong before, it is our duty to understand <em>why</em> science was wrong, and <em>remove</em> known sources of stupidity from our model. Once we've done this, \"past scientific predictions\" is no longer an appropriate <a href=\"/lw/1lx/reference_class_of_the_unclassreferenceable\">reference class</a> for second-guessing the prediction at hand, because the science is now superior. (Or anyway, the <a href=\"http://wiki.lesswrong.com/wiki/Amount_of_evidence\">strength</a> of the evidence of previous failures is diminished.)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p>\r\n<p>That is why, with respect to Eliezer's LHC dilemma -- which amounts to a conflict between avoiding overconfidence and avoiding <a href=\"http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\">hypothesis-privileging</a> -- I come down squarely on the side of hypothesis-privileging as the greater danger. Psychologically, you may not \"feel up to\" making a million predictions, of which no more than one can be wrong; but if that's what your model instructs you to do, then that's what you have to do -- unless you think your model is wrong, for some better reason than a vague sense of uneasiness. Without, ultimately, trusting science more than intuition, there's no hope of making epistemic progress. At the end of the day, you have to shut up and multiply -- epistemically as well as instrumentally.&nbsp;<br />&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6zRn4uKADwL9uo8Ch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 42, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "2120", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 110, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ziqL94sq6rMuH7wDu", "GrDqnMjhqoxiqpQPw", "baTWMegR42PAsH9qJ", "G9dptrW9CJi7wNg3b", "QkX2bAkwG2EpGvNug", "yKRTxAohwmqLcbbJi"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-19T03:36:32.248Z", "modifiedAt": null, "url": null, "title": "The Prediction Hierarchy", "slug": "the-prediction-hierarchy", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:08.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobinZ", "createdAt": "2009-07-08T20:34:05.168Z", "isAdmin": false, "displayName": "RobinZ"}, "userId": "eTMojvi4f2z3pDfsc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2L3cyRN2feLQGo3Q3/the-prediction-hierarchy", "pageUrlRelative": "/posts/2L3cyRN2feLQGo3Q3/the-prediction-hierarchy", "linkUrl": "https://www.lesswrong.com/posts/2L3cyRN2feLQGo3Q3/the-prediction-hierarchy", "postedAtFormatted": "Tuesday, January 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Prediction%20Hierarchy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Prediction%20Hierarchy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2L3cyRN2feLQGo3Q3%2Fthe-prediction-hierarchy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Prediction%20Hierarchy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2L3cyRN2feLQGo3Q3%2Fthe-prediction-hierarchy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2L3cyRN2feLQGo3Q3%2Fthe-prediction-hierarchy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 863, "htmlBody": "<p><strong>Related:</strong> <a href=\"/lw/1mw/advancing_certainty/\">Advancing Certainty</a>, <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Reversed Stupidity Is Not Intelligence</a></p>\n<p><em>The substance of this post is derived from <a href=\"/lw/1mw/advancing_certainty/1gue\">a conversation in the comment thread</a> which I have decided to promote. <a href=\"http://en.wiktionary.org/wiki/TLDR\">Teal;deer</a>: if you have to rely on a calculation you may have gotten wrong for your prediction, your expectation for the case when your calculation <span style=\"text-decoration: underline;\">is</span> wrong should use a simpler calculation, such as reference class forecasting.</em></p>\n<p><em>Edit 2010-01-19: Toby Ord mentions in the comments <a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0006/4020/probing-the-improbable.pdf\">Probing the Improbable: Methodological Challenges for Risks with Low Probabilities and High Stakes</a> (PDF) by Toby Ord, Rafaela Hillerbrand, and Anders Sandberg of the Future of Humanity Institute, University of Oxford. It uses a similar mathematical argument, but is much more substantive than this.</em></p>\n<p>A lottery has a jackpot of a million dollars. A ticket costs one dollar. Odds of a given ticket winning are approximately one in forty million. If your utility is linear in dollars, should you bet?</p>\n<p>The obvious (and correct) answer is \"no\". The clever (and incorrect) answer is \"yes\", as follows:</p>\n<blockquote>\n<p>According to your calculations, \"this ticket will not win the lottery\" is true with probability 99.9999975%. But can you really be <em>sure</em> that you can calculate anything to that good odds? Surely you couldn't expect to make forty million predictions of which you were that confident and only be wrong once. Rationally, you ought to ascribe a lower confidence to the statement: 99.99%, for example. But this means a 0.01% chance of <em>winning</em> the lottery, corresponding to an expected value of a hundred dollars. Therefore, you should buy the ticket.</p>\n</blockquote>\n<p>The logic is not obviously wrong, but where is the error?</p>\n<p><a id=\"more\"></a>First, let us write out the calculation algebraically. Let <strong>E(L)</strong> be the expected value of playing the lottery. Let <strong>p(L)</strong> be your calculated probability that the lottery will pay off. Let <strong>p(C)</strong> be your probability that your calculations are correct. Finally, let <strong>j</strong> represent the value of the jackpot and let <strong>t</strong> represent the price of the ticket. The obvious way to write the clever theory is:</p>\n<p>&nbsp;&nbsp; &nbsp;E(L) = max(p(L), 1-p(C)) * j - t</p>\n<p>This doesn't sound quite right, though - surely you should ascribe a higher confidence when you calculate a higher probability. That said, when p(L) is much less than p(C), it shouldn't make a <em>large</em>&nbsp;difference. The straightforward way to account for this is to take p(C) as the probability that p(L) is correct, and write the following:</p>\n<p>&nbsp;&nbsp; &nbsp;E(L) = [ p(C)*p(L) + 1-p(C) ] * j - t</p>\n<p>which can be rearranged as:</p>\n<p>&nbsp;&nbsp; &nbsp;E(L) = p(C) * [p(L)*j - t] + (1-p(C)) * [j - t]</p>\n<p>I believe this exposes the problem with the clever argument quite explicitly. Why, if your calculations are incorrect (probability 1-p(C)), should you assume that you are <em>certain</em>&nbsp;to win the lottery? If your calculations are incorrect, they should tell you <em>almost nothing</em>&nbsp;about whether you will win the lottery or not. So what do you do?</p>\n<p>What appears to me the elegant solution is to use a <em>less complex</em>&nbsp;calculation - or a series of less complex calcuations - to act as your backup hypothesis. In a tricky engineering problem (say, calculating the effectiveness of a heat sink), your primary prediction might come out of a finite element fluid dynamics calculator with p(C)&nbsp;=&nbsp;0.99 and narrow error bars, but you would also refer to the result of a simple algebraic model with p(C)&nbsp;=&nbsp;0.9999 and much wider error bars. And then you would backstop the lot with your background knowledge about heat sinks in general, written with wide enough error bars to call p(C)&nbsp;=&nbsp;1&nbsp;-&nbsp;epsilon.</p>\n<p>In this case, though, the calculation was simple, so our backup prediction is just the background knowledge. Say that, knowing nothing about a lottery but \"it's a lottery\", we would have an expected payoff <strong>e</strong>. Then we write:</p>\n<p>&nbsp;&nbsp; &nbsp;E(L) = p(C) * [p(L)*j - t] + (1-p(C)) * e</p>\n<p>I don't know about you, but for me, <em>e</em> is approximately equal to -<em>t</em>. And justice is restored.</p>\n<hr />\n<p>We are advised that, when solving hard problems, we should <a href=\"/lw/1g4/tips_and_tricks_for_answering_hard_questions/1gp6\">solve multiple problems at once</a>. This is relatively trivial, but I can point out a couple other relatively trivial examples where it shows up well:</p>\n<p><strong>Suppose the lottery appears to be marginally profitable: should you bet on it?</strong>&nbsp;Not unless you are confident in your numbers.</p>\n<p><strong>Suppose <a href=\"/lw/sg/when_not_to_use_probabilities/\">we consider the LHC</a>. <a href=\"/lw/u6/horrible_lhc_inconsistency/\">Should we (have) switch(ed) it on</a>?</strong>&nbsp;Once you've checked that it is safe, yes. As a high-energy physics experiment, the backup comparison would be to things like <a href=\"/lw/rg/la602_vs_rhic_review/\">nuclear energy</a>, which have only small chances of devastation on the planetary scale. If your calculations were to indicate that the LHC is completely safe, even if your P(C) were as low as three or four nines (99.9%, 99.99%), your actual estimate of the safety of turning it on should be no lower than six or seven nines, and probably higher. (In point of fact, given the number of physicists analyzing the question, P(C) is much higher. Three cheers for intersubjective verification.)</p>\n<p><strong>Suppose we consider our Christmas shopping?</strong>&nbsp;When you're estimating your time to finish your shopping, <a href=\"/lw/jg/planning_fallacy/\">your calculations are not very reliable</a>. Therefore your answer is strongly dominated by the simpler, much more reliable reference class prediction.</p>\n<p><strong>But what are the odds that this ticket won't win the lottery?</strong> ...how many nines do I type, again?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "bh7uxTTqmsQ8jZJdB": 1, "YPZCAs9Axp9PtrF22": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2L3cyRN2feLQGo3Q3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 28, "extendedScore": null, "score": 5.542384818438807e-07, "legacy": true, "legacyId": "2124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6zRn4uKADwL9uo8Ch", "qNZM3EGoE5ZeMdCRt", "AJ9dX59QXokZb35fk", "ziqL94sq6rMuH7wDu", "f3W7QbLBA2B7hk84y", "CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-19T16:35:56.100Z", "modifiedAt": null, "url": null, "title": "What big goals do we have?", "slug": "what-big-goals-do-we-have", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:33.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2hdzbpRhnEzAYEf4u/what-big-goals-do-we-have", "pageUrlRelative": "/posts/2hdzbpRhnEzAYEf4u/what-big-goals-do-we-have", "linkUrl": "https://www.lesswrong.com/posts/2hdzbpRhnEzAYEf4u/what-big-goals-do-we-have", "postedAtFormatted": "Tuesday, January 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20big%20goals%20do%20we%20have%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20big%20goals%20do%20we%20have%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hdzbpRhnEzAYEf4u%2Fwhat-big-goals-do-we-have%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20big%20goals%20do%20we%20have%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hdzbpRhnEzAYEf4u%2Fwhat-big-goals-do-we-have", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2hdzbpRhnEzAYEf4u%2Fwhat-big-goals-do-we-have", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 367, "htmlBody": "<p>Sometime ago Jonii <a href=\"/lw/1hs/open_thread_december_2009/1ax7\">wrote</a>:</p>\n<blockquote>\n<p>I mean, paperclip maximizer is seriously ready to do anything to maximize paperclips. It <em>really</em> takes the paperclips seriously.</p>\n</blockquote>\n<p>When I'm hungry I eat, but then I don't go on eating some more just to maximize a function. Eating isn't something I want a <em>lot</em> of. Likewise I don't want a ton of survival, just a bounded amount every day. Let's define a goal as <em>big</em> if you don't get full: every increment of effort/achievement is valuable, like paperclips to Clippy. Now do we have any big goals? Which ones?</p>\n<p><strong>Save the world.</strong> A great goal if you see a possible angle of attack, which I don't. The SIAI folks are more optimistic, but if they see a chink in the wall, they're yet to reveal it.</p>\n<p><strong>Help those who suffer.</strong> Morally upright but tricky to execute: <a href=\"http://www.spiegel.de/international/spiegel/0,1518,363663,00.html\">James Shikwati</a>, <a href=\"http://www.dambisamoyo.com/deadaid.html\">Dambisa Moyo</a> and <a href=\"http://www.independent.ie/opinion/columnists/kevin-myers/africa-is-giving-nothing-to-anyone--apart-from-aids-1430428.html\">Kevin Myers</a> show that even something as clear-cut as aid to Africa can be viewed as immoral. Still a good goal for anyone, though.</p>\n<p><strong>Procreate.</strong> This sounds fun! Fortunately, the same source that gave us this goal also gave us the means to achieve it, and intelligence is not among them. :-) And honestly, what sense in making 20 kids just to play the good-soldier routine for your genes? There's no unique \"you gene\" anyway, in several generations your descendants will be like everyone else's. Yeah, kids are fun, I'd like two or three.</p>\n<p><strong>Follow your muse.</strong> Music, comedy, videogame design, whatever. No limit to achievement! A lot of this is about signaling: would you still bother if all your successes were attributed to someone else's genetic talent? But even apart from the signaling angle, there's still the worrying feeling that entertainment is ultimately <em>useless</em>, like humanity-scale wireheading, not an actual goal for us to reach.</p>\n<p><strong>Accumulate power, money or experiences.</strong> What for? I never understood that.</p>\n<p><strong>Advance science.</strong> As Erik Naggum <a href=\"http://naggum.no/erik/knowledge.html\">put it</a>:</p>\n<blockquote>\n<p>The purpose of human existence is to learn and to understand as much as we can of what came before us, so we can further the sum total of human knowledge in our life.</p>\n</blockquote>\n<p>Don't know, but I'm pretty content with my life lately. Should I have a big goal at all? How about you?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2hdzbpRhnEzAYEf4u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 13, "extendedScore": null, "score": 5.54382629182615e-07, "legacy": true, "legacyId": "2131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 95, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-19T19:08:48.301Z", "modifiedAt": null, "url": null, "title": "Normal Cryonics", "slug": "normal-cryonics", "viewCount": null, "lastCommentedAt": "2021-06-30T09:41:14.765Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hiDkhLyN5S2MEjrSE/normal-cryonics", "pageUrlRelative": "/posts/hiDkhLyN5S2MEjrSE/normal-cryonics", "linkUrl": "https://www.lesswrong.com/posts/hiDkhLyN5S2MEjrSE/normal-cryonics", "postedAtFormatted": "Tuesday, January 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Normal%20Cryonics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANormal%20Cryonics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiDkhLyN5S2MEjrSE%2Fnormal-cryonics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Normal%20Cryonics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiDkhLyN5S2MEjrSE%2Fnormal-cryonics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhiDkhLyN5S2MEjrSE%2Fnormal-cryonics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1099, "htmlBody": "<p>I recently attended a small gathering whose purpose was to let young people signed up for <a href=\"http://wiki.lesswrong.com/wiki/Cryonics\">cryonics</a> meet older people signed up for cryonics - a matter of some concern to the old guard, for obvious reasons.</p>\n<p>The young cryonicists' travel was subsidized.&nbsp; I suspect this led to a <em>greatly </em>different selection filter than usually prevails at conferences of what Robin Hanson would call \"contrarians\".&nbsp; At an ordinary conference of transhumanists - or libertarians, or atheists - you get activists who want to meet their own kind, strongly enough to pay conference fees and travel expenses.&nbsp; This conference was just young people who took the action of signing up for cryonics, and who were willing to spend a couple of paid days in Florida meeting older cryonicists.</p>\n<p>The gathering was 34% female, around half of whom were single, and a few kids.&nbsp; This may sound normal enough, unless you've been to a lot of contrarian-cluster conferences, in which case you just spit coffee all over your computer screen and shouted \"WHAT?\"&nbsp; I did sometimes hear \"my husband persuaded me to sign up\", but no more frequently than \"I pursuaded my husband to sign up\".&nbsp; Around 25% of the people present were from the computer world, 25% from science, and 15% were doing something in music or entertainment - with possible overlap, since I'm working from a show of hands.</p>\n<p>I was <em>expecting</em> there to be some nutcases in that room, people who'd signed up for cryonics for just the same reason they subscribed to homeopathy or astrology, i.e., that it sounded cool.&nbsp; <em>None</em> of the younger cryonicists showed any sign of it.&nbsp; There were a couple of older cryonicists who'd gone strange, but none of the young ones that I saw.&nbsp; Only three hands went up that did <em>not</em> identify as atheist/agnostic, and I think those also might have all been old cryonicists.&nbsp; (This is surprising enough to be worth explaining, considering the base rate of insanity versus sanity.&nbsp; Maybe if you're into woo, there is so much <em>more </em>woo that is <em>better </em><a href=\"/lw/a3/how_theism_works/\">optimized for being woo</a>, that no one into woo would give cryonics a second glance.)</p>\n<p>The part about <em>actually signing up</em> may also be key - that's probably a ten-to-one or worse filter among people who \"get\" cryonics.&nbsp; (I put to Bill Faloon of the old guard that probably twice as many people had died while planning to sign up for cryonics eventually, than had actually been suspended; and he said \"Way more than that.\")&nbsp; Actually signing up is an intense filter for Conscientiousness, since it's mildly tedious (requires multiple copies of papers signed and notarized with witnesses) and there's no peer pressure.</p>\n<p>For whatever reason, those young cryonicists seemed really <em>normal</em> - except for one thing, which I'll get to tomorrow.&nbsp; Except for that, then, they seemed like very ordinary people: the couples and the singles, the husbands and the wives and the kids, scientists and programmers and sound studio technicians.</p>\n<p>It tears my heart out.<a id=\"more\"></a></p>\n<p>At some future point I ought to post on the notion of <em>belief hysteresis,</em> where you get locked into whatever belief hits you first.&nbsp; So it had previously occurred to me (though I didn't write the post) to argue for cryonics via a <em><a href=\"/lw/q7/if_manyworlds_had_come_first/\">conformity reversal test</a>:</em></p>\n<p>If you found yourself in a world where everyone was signed up for cryonics as a matter of routine - including everyone who works at your office - you wouldn't be the first <a href=\"/lw/mb/lonely_dissent/\">lonely dissenter</a> to earn the incredulous stares of your coworkers by unchecking the box that kept you signed up for cryonics, in exchange for an extra $300 per year.</p>\n<p>(Actually it would probably be a lot cheaper, more like $30/year or a free government program, with that economy of scale; but we should ignore that for purposes of the reversal test.)</p>\n<p>The point being that if cryonics were taken for granted, it would <em>go on </em>being taken for granted; it is only the state of non-cryonics that is unstable, subject to being disrupted by rational argument.</p>\n<p>And this cryonics meetup <em>was</em> that world.&nbsp; It was the world of the ordinary scientists and programmers and sound studio technicians who had signed up for cryonics as a matter of simple common sense.</p>\n<p>It tears my heart out.</p>\n<p>Those young cryonicists weren't heroes.&nbsp; Most of the older cryonicists were heroes, and of course there were a couple of other heroes among us young folk, like a former employee of Methuselah who'd left to try to put together a <a href=\"http://livly.org/\">startup/nonprofit</a> around a bright idea he'd had for curing cancer (<em>note: even I think this is an acceptable excuse</em>).&nbsp; But most of the younger cryonicists weren't there to fight a desperate battle against Death, they were people who'd signed up for cryonics because it was the obvious thing to do.</p>\n<p>And it tears my heart out, because I <em>am</em> a hero and this was like seeing a ray of sunlight from a normal world, some alternate Everett branch of humanity where things really <em>were </em>normal instead of crazy all the goddamned time, a world that was everything this world <em>could be </em>and <em>isn't.</em></p>\n<p>Then there were the children, some of whom had been signed up for cryonics since the day they were born.</p>\n<p>It tears my heart out.&nbsp; I'm having trouble remembering to breathe as I write this.&nbsp; My own little brother isn't breathing and never will again.</p>\n<p>You know what?&nbsp; I'm going to come out and say it.&nbsp; I've been unsure about saying it, but after attending this event, and talking to the perfectly ordinary parents who signed their kids up for cryonics like the <em>goddamn sane people </em>do, I'm going to come out and say it:&nbsp; If you don't sign up your kids for cryonics then you are a <em>lousy parent</em>.</p>\n<p>If you aren't choosing between textbooks and food, then you can afford to sign up your kids for cryonics.&nbsp; I don't know if it's more important than a home without lead paint, or omega-3 fish oil supplements while their brains are maturing, but it's certainly more important than you going to the movies or eating at nice restaurants.&nbsp; That's part of the bargain you signed up for when you became a parent.&nbsp; If you can afford kids at all, you can afford to sign up your kids for cryonics, and if you don't, you are a lousy parent.&nbsp; I'm just back from an event where the normal parents signed their normal kids up for cryonics, and that is the way things are supposed to be and should be, and whatever excuses you're using or thinking of right now, I don't believe in them any more, you're just a lousy parent.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 7, "DsdbQhWAnPqfzo4Yw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hiDkhLyN5S2MEjrSE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 113, "baseScore": 88, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "2100", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 88, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 964, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["riaLsnntuxkPnWF6H", "WqGCaRhib42dhKWRL", "CEGnJBHmkcwPTysb7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-19T23:35:47.131Z", "modifiedAt": null, "url": null, "title": "London meetup: \"The Friendly AI Problem\" ", "slug": "london-meetup-the-friendly-ai-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:08.486Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/37xeK9fim5DqTqQN4/london-meetup-the-friendly-ai-problem", "pageUrlRelative": "/posts/37xeK9fim5DqTqQN4/london-meetup-the-friendly-ai-problem", "linkUrl": "https://www.lesswrong.com/posts/37xeK9fim5DqTqQN4/london-meetup-the-friendly-ai-problem", "postedAtFormatted": "Tuesday, January 19th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20London%20meetup%3A%20%22The%20Friendly%20AI%20Problem%22%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALondon%20meetup%3A%20%22The%20Friendly%20AI%20Problem%22%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37xeK9fim5DqTqQN4%2Flondon-meetup-the-friendly-ai-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=London%20meetup%3A%20%22The%20Friendly%20AI%20Problem%22%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37xeK9fim5DqTqQN4%2Flondon-meetup-the-friendly-ai-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37xeK9fim5DqTqQN4%2Flondon-meetup-the-friendly-ai-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<p>To all LessWrongers in London/the south of England: On <strong>Saturday the 23rd</strong> - <strong>this Saturday</strong> at <strong>2pm</strong> - I will be speaking about <strong>Friendly AI</strong> at the <a href=\"http://www.facebook.com/event.php?eid=240386230871#/group.php?gid=21616858336\">UK transhumanist group</a>, at</p>\n<p><strong>Room 416, 4th floor (via main lift), Birkbeck College, Torrington Square, London</strong> (<a href=\"http://maps.google.com/maps?f=q&amp;hl=en&amp;q=WC1E+7HX%2C+London%2C+United+Kingdom\">Map</a>)</p>\n<p>For those who have read most of the material on the web about FAI theory, e.g. the sequences here and the material of Bostrom and Omohundro, you won't hear much new, but you will probably meet a lot of people who could do with some strong rationalists to speak to. For those who haven't read most of the material about Friendly AI, I'll try to make it a superlatively informative talk. The <a href=\"http://www.facebook.com/event.php?eid=240386230871#/event.php?eid=240386230871\">Facebook event</a> has 30 confirmed guests, and the London Futurists' organizer (distinct from UK h+, the hosts), just emailed me and said:</p>\n<p><em>\"Looking forward to meeting you and asking you questions on Saturday! I think your going to get quite a few as there is a lot of interest from my group about your presentation - positive and negative. You might get a very skeptical geezer coming along so be prepared for some tough questions from people who know their stuff.\" </em></p>\n<p>So the event promises to be a fairly large gathering of rationalists, futurists and transhumanists with a high probability of passionate debate about FAI, followed by a trip to the pub for further discussion and drinks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "37xeK9fim5DqTqQN4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "2132", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-20T16:35:01.715Z", "modifiedAt": null, "url": null, "title": "That Magical Click", "slug": "that-magical-click", "viewCount": null, "lastCommentedAt": "2022-04-25T06:51:58.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/R3ATEWWmBhMhbY2AL/that-magical-click", "pageUrlRelative": "/posts/R3ATEWWmBhMhbY2AL/that-magical-click", "linkUrl": "https://www.lesswrong.com/posts/R3ATEWWmBhMhbY2AL/that-magical-click", "postedAtFormatted": "Wednesday, January 20th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20That%20Magical%20Click&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThat%20Magical%20Click%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR3ATEWWmBhMhbY2AL%2Fthat-magical-click%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=That%20Magical%20Click%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR3ATEWWmBhMhbY2AL%2Fthat-magical-click", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FR3ATEWWmBhMhbY2AL%2Fthat-magical-click", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1620, "htmlBody": "<p><strong>Followup to</strong>:&nbsp; <a href=\"/lw/1mc/normal_cryonics/\">Normal Cryonics</a></p>\n<p>Yesterday I spoke of that cryonics gathering I recently attended, where travel by young cryonicists was fully subsidized, leading to extremely different demographics from conventions of self-funded activists.&nbsp; 34% female, half of those in couples, many couples with kids - THAT HAD BEEN SIGNED UP FOR CRYONICS <em>FROM BIRTH </em>LIKE A GODDAMNED <em>SANE </em>CIVILIZATION WOULD REQUIRE - 25% computer industry, 25% scientists, 15% entertainment industry at a rough estimate, and in most ways seeming (for smart people) pretty damned normal.</p>\n<p>Except for one thing.</p>\n<p>During one conversation, I said something about there being no magic in our universe.</p>\n<p>And an ordinary-seeming woman responded, \"But there are still lots of things science doesn't understand, right?\"</p>\n<p>Sigh.&nbsp; We all know how this conversation is going to go, right?</p>\n<p>So I wearily replied with my usual, \"If I'm ignorant about a phenomenon, that is a fact about my state of mind, not a fact about the phenomenon itself; a blank map does not correspond to a blank territory -\"</p>\n<p>\"Oh,\" she interrupted excitedly, \"so the concept of 'magic' isn't even consistent, then!\"</p>\n<p><em>Click.</em></p>\n<p>She got it, just like that.</p>\n<p>This was someone else's description of how she got involved in cryonics, as best I can remember it, and it was pretty much typical for the younger generation:</p>\n<p>\"When I was a very young girl, I was watching TV, and I saw something about cryonics, and it made sense to me - I didn't want to die - so I asked my mother about it.&nbsp; She was very dismissive, but tried to explain what I'd seen; and we talked about some of the other things that can happen to you after you die, like burial or cremation, and it seemed to me like cryonics was better than that.&nbsp; So my mother laughed and said that if I still felt that way when I was older, she wouldn't object.&nbsp; Later, when I was older and signing up for cryonics, she objected.\"</p>\n<p><em>Click.</em></p>\n<p>It's... kinda frustrating, actually.<a id=\"more\"></a></p>\n<p>There are manifold <em>bad</em> objections to cryonics that can be raised and countered, but the core logic really <em>is</em> simple enough that there's nothing implausible about getting it when you're eight years old (eleven years old, in my case).</p>\n<p>Freezing damage?&nbsp; I could go on about modern cryoprotectants and how you can see under a microscope that the tissue is in great shape, and there are experiments underway to see if they can get spontaneous brain activity after vitrifying and devitrifying, and with molecular nanotechnology you could go through the whole vitrified brain atom by atom and do the same sort of information-theoretical tricks that people do to recover hard drive information after \"erasure\" by any means less extreme than a blowtorch...</p>\n<p>But even an eight-year-old can visualize that freezing a sandwich doesn't destroy the sandwich, while cremation does.&nbsp; It so happens that this naive answer <em>remains </em>true after learning the exact details and defeating objections (a few of which are even worth considering), but that doesn't make it any less obvious to an eight-year-old.&nbsp; (I actually did understand the concept of molecular nanotech at eleven, but I could be a special case.)</p>\n<p>Similarly: yes, really, life is better than death - just because transhumanists have huge arguments with bioconservatives over this issue, doesn't mean the eight-year-old isn't making the right judgment for the right reasons.</p>\n<p>Or: even an eight-year-old who's read a couple of science-fiction stories and who's ever cracked a history book can guess - not for the full reasons in full detail, but still for <em>good </em>reasons - that if you wake up in the Future, it's probably going to be a nicer place to live than the Present.</p>\n<p>In short - though it <em>is </em>the sort of thing you ought to <a href=\"http://wiki.lesswrong.com/wiki/Crisis_of_faith\">review</a> as a teenager and again as an adult - from a rationalist standpoint, there is nothing alarming about <em>clicking</em> on cryonics at age eight... any more than I should worry about <a href=\"/lw/yo/against_maturity/\">my first schism with Orthodox Judaism coming at age five</a>, when they told me that I didn't have to understand the prayers in order for them to work so long as I said them in Hebrew.&nbsp; It really <em>is </em>obvious enough to see as a child, the right thought for the right reasons, no matter how much adult debate surrounds it.</p>\n<p>And the frustrating thing was that - judging by this group - most cryonicists are people to whom it <em>was</em> just obvious.&nbsp; (And who then actually followed through and signed up, which is probably a factor-of-ten or worse filter for Conscientiousness.)&nbsp; It would have been <em>convenient</em> if I'd discovered some particular key insight that convinced people.&nbsp; If people had said, \"Oh, well, I <em>used</em> to think that cryonics couldn't be plausible if no one else was doing it, but then I read about <a href=\"/lw/m9/aschs_conformity_experiment/\">Asch's conformity experiment</a> and <a href=\"/lw/9j/bystander_apathy/\">pluralistic ignorance</a>.\"&nbsp; Then I could just emphasize that argument, and people would sign up.</p>\n<p>But the average experience I heard was more like, \"Oh, I saw a movie that involved cryonics, and I went on Google to see if there was anything like that in real life, and found Alcor.\"</p>\n<p>In one sense this shouldn't surprise a Bayesian, because the base rate of people who hear a brief mention of cryonics on the radio and have an opportunity to <em>click</em>, will be vastly higher than the base rate of people who are exposed to detailed arguments about cryonics...</p>\n<p>Yet the upshot is that - judging from the generation of young cryonicists at that event I attended - cryonics is sustained <em>primarily </em>by the ability of a tiny, tiny fraction of the population to \"get it\" just from hearing a casual mention on the radio.&nbsp; Whatever part of one-in-a-hundred-thousand isn't accounted for by the Conscientiousness filter.</p>\n<p>If I suffered from <a href=\"/lw/c3/the_sin_of_underconfidence/\">the sin of underconfidence</a>, I would feel a dull sense of obligation to doubt myself after reaching this conclusion, just like I would feel a dull sense of obligation to doubt that I could be more rational about theology than my parents and teachers at the age of five.&nbsp; As it is, I have no problem with shrugging and saying \"People are crazy, the world is mad.\"</p>\n<p>But it really, really raises the question of <em>what the hell is in that click.</em></p>\n<p>There's this magical click that some people get and some people don't, and I don't understand what's in the click.&nbsp; There's the consequentialist/utilitarian click, and the intelligence explosion click, and the life-is-good/death-is-bad click, and the cryonics click.&nbsp; <a href=\"http://wiki.lesswrong.com/wiki/Yudkowsky's_coming_of_age\">I myself failed to click on one notable occasion</a>, but the topic was probably just as clickable.</p>\n<p>(In fact, it took that particular embarrassing failure in my own history - failing to click on metaethics, and seeing in retrospect that the answer was clickable - before I was willing to trust <em>non-</em>click Singularitarians.)</p>\n<p>A rationalist faced with an apparently obvious answer, must assign some probability that a non-obvious objection will appear and defeat it.&nbsp; I do know how to explain the above conclusions at great length, and defeat objections, and I would not be nearly as confident (I hope!) if I had just clicked five seconds ago.&nbsp; But sometimes the final answer <em>is</em> the same as the initial guess; if you know the full mathematical story of Peano Arithmetic, 2 + 2 still equals 4 and not 5 or 17 or the color green.&nbsp; And some people very quickly arrive at that same final answer as their best initial guess; they can swiftly guess which answer will <em>end up </em>being the final answer, for what seem even in retrospect like good reasons.&nbsp; Like becoming an atheist at eleven, then listening to a theist's best arguments later in life, and concluding that your initial guess was right for the right reasons.</p>\n<p>We can define a \"click\" as following a very short chain of reasoning, which in the vast majority of other minds is derailed by some detour and proves strongly resistant to re-railing.</p>\n<p>What makes it happen?&nbsp; What goes into that click?</p>\n<p>It's a question of life-or-death importance, and I don't know the answer.</p>\n<p>That generation of cryonicists seemed so <em>normal </em>apart from that...</p>\n<p><em>What's in that click?</em></p>\n<p>The point of the opening anecdote about the <a href=\"http://wiki.lesswrong.com/wiki/Mind_projection_fallacy\">Mind Projection Fallacy</a> (blank map != blank territory) is to show (anecdotal) evidence that there's something like a general click-factor, that someone who clicked on cryonics was able to click on mysteriousness=projectivism as well.&nbsp; Of course I didn't expect that I could just stand up amid the conference and describe the intelligence explosion and Friendly AI in a couple of sentences and have everyone get it.&nbsp; <em>That</em> high of a general click factor is <em>extremely</em> rare in my experience, and the people who have it are <em>not </em>otherwise normal.&nbsp; (Michael Vassar is one example of a \"superclicker\".)&nbsp; But it is still true AFAICT that people who click on one problem are more likely than average to click on another.</p>\n<p>My best guess is that clickiness has something to do with <em>failure to <a href=\"/lw/gv/outside_the_laboratory/\">compartmentalize</a></em> - missing, or failing to use, the mental gear that lets human beings believe two contradictory things at the same time.&nbsp; Clicky people would tend to be people who take all of their beliefs at face value.</p>\n<p>The Hansonian explanation (not necessarily endorsed by Robin Hanson) would say something about clicky people tending to operate in <a href=\"http://www.google.com/search?q=+site:www.overcomingbias.com+hanson+near+far\">Near mode</a>.&nbsp; (Why?)</p>\n<p>The naively straightforward view would be that the ordinary-seeming people who came to the cryonics did not have any <em>extra</em> gear that magically enabled them to follow a short chain of obvious inferences, but rather, everyone <em>else</em> had at least one extra insanity gear active at the time they heard about cryonics.</p>\n<p>Is that really just it?&nbsp; Is there no special sanity to add, but only ordinary madness to take away?&nbsp; Where do superclickers come from - are they just born lacking a whole lot of distractions?</p>\n<p><em>What the hell is in that click?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "5gcpKG2XEAZGj5DEf": 1, "KWFhr6A2dHEb6wmWJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "R3ATEWWmBhMhbY2AL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 83, "extendedScore": null, "score": 0.000135, "legacy": true, "legacyId": "2105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 83, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 416, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hiDkhLyN5S2MEjrSE", "rM7hcz67N7WtwGGjq", "WHK94zXkQm7qm7wXk", "K5nq3KcDXaGm7QQWR", "pkFazhcTErMw7TFtT", "N2pENnTPB75sfc9kb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-21T03:01:48.371Z", "modifiedAt": null, "url": null, "title": "Winning the Unwinnable", "slug": "winning-the-unwinnable", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:08.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JRMayne", "createdAt": "2009-12-16T02:49:04.260Z", "isAdmin": false, "displayName": "JRMayne"}, "userId": "Hykek2Jk4Jpqc9z4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gbz6BMXknMtLo9T7c/winning-the-unwinnable", "pageUrlRelative": "/posts/Gbz6BMXknMtLo9T7c/winning-the-unwinnable", "linkUrl": "https://www.lesswrong.com/posts/Gbz6BMXknMtLo9T7c/winning-the-unwinnable", "postedAtFormatted": "Thursday, January 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Winning%20the%20Unwinnable&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWinning%20the%20Unwinnable%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbz6BMXknMtLo9T7c%2Fwinning-the-unwinnable%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Winning%20the%20Unwinnable%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbz6BMXknMtLo9T7c%2Fwinning-the-unwinnable", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGbz6BMXknMtLo9T7c%2Fwinning-the-unwinnable", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 890, "htmlBody": "<p>A few years back, I sent a note to some friends and other smart people. It said, approximately:</p>\r\n<p>\"I am trying to raise about $42 million. I have an expected payoff of about 20% in a week. It's not guaranteed; it might be more, or there could be a loss.</p>\r\n<p>\"Now, you may be saying to yourself, 'Mayne's got a system to win the lottery or something.'</p>\r\n<p>\"Well, it's not, 'something.' I have a system to win the lottery.\"</p>\r\n<p>At which point I explained the system.</p>\r\n<p>Now, LW is <a href=\"/lw/hm/new_improved_lottery/\">not</a> <a href=\"/lw/1n0/the_prediction_hierarchy/\">particularly</a> <a href=\"http://wiki.lesswrong.com/wiki/Lottery\">fond</a> of the lottery. From a social policy/<a href=\"/lw/gw/politics_is_the_mindkiller/\">political</a> point of view, I'm not disagreeing.</p>\r\n<p>I specifically represent to you that I do have a system, I use a part of&nbsp;the system, and so far, I've lost about $350 with the system. If you'd loan me your $42 million when I ask for it, though, I'd have not only a high positive expectation, but a much smoother payout matrix, if I could solve the practical problem of buying as many tickets as I want. Which is 41.4 million tickets.</p>\r\n<p>No, I'm not just saying I'll buy all the tickets and therefore guarantee the jackpot and therefore I win. I'm saying my expectation per dollar invested is substantially positive.</p>\r\n<p>Skeptical? Why?</p>\r\n<p>Not only do I have a system, but anyone with&nbsp;math skills and any knowledge of how the lottery works should agree. While it's always easy and obvious to see why other people should think the same as you, the fact that of a large number of math folks, zero here have defended playing the lottery as a cash-plus position, is a sign that people have foregone thinking and simply rejected the lottery outright. Or that I'm an idiot.</p>\r\n<p>Before you get to the post payoff, what's your attitude toward this now? Lottery win is impossible? If you think it's impossible (or close enough to it), why? Unlikely? Probable? Would it change your mind if my self-assessment was that I was likely in the bottom quartile as far as current math skills on LW? Would it alter your probabilities if I told you that I am highly confident that if the practicality of buying 41.4 million tickets is worked out, that using my system to play the lottery is the best investment I know of?</p>\r\n<p>OK, here it is: <strong>How to win the lottery</strong>.</p>\r\n<p>I'm going to make this relatively brief, but it's really fairly simple. In California, we have a fairly classic lottery setup although the payout rates are a little worse than most states. The immediate payoff of the jackpot is about (carryover + 25% of tickets sold for that draw.) About 25% of the money into the lottery goes to non-jackpot payoffs. Fifty percent goes to valuable or less valuable government programs, including running the lottery.</p>\r\n<p>You have about a 1 in 41 million shot of winning the jackpot with one ticket on any given draw.</p>\r\n<p>A number of factors have seriously decreased purchases of Super Lotto tickets on large jackpots, primarily the cannibalization of the Lotto by the state's participation in the Mega Millions multi-state lottery. This is important to the calculations, and changes the expected payout substantially.</p>\r\n<p>At one point, the&nbsp;Super Lotto&nbsp;jackpot was at $85 million for a prior draw (or $42.5 million in a single immediate payment) and they sold nine million tickets for the next draw.</p>\r\n<p>So, consider: You put in $41.4 million and buy every possible ticket. You get about $10 million back in little prizes. The value of the jackpot is $42.5 million, plus 25% of nine million (the money the others put in) plus 25% of $41 million (the money you put in). You're at about $55 million in jackpot.</p>\r\n<p>If no one ties you for the jackpot, you're at about $65 million in total payout. There's less than a 25% chance that someone ties you. That's a comfortable profit.</p>\r\n<p>If one person ties you, that's about $37 million payout; that's a loss, but presumably a survivable loss.&nbsp;If more tie you that&nbsp;is, well, worse.</p>\r\n<p>I came rather closer to pulling this off than seems likely; the big issue was how to buy the tickets, and the state told me I had to buy them from retailers (and I lack the political pull to find another way.) This opens a wide range of complications, as you can imagine. Then, sadly, some of the investment bankers of my acquaintance were, um, pursuing other interests (not because they were talking to some dude hawking a lottery system, or so I choose to believe), and the project dissolved.</p>\r\n<p>But the math is still basically right. And the lottery has been repeatedly and forcefully brutalized here as something only idiots would play. When the cash expectation is positive, I'm playing again. I'm not saying it's stupid not to play - the marginal utility issues are substantial.</p>\r\n<p>Still, by making the mental equation lottery=stupidity, a lot of people stopped thinking about the potential. I expect someone to pull this off some time, and I expect some political blowback (the lottery's for suckers, not the intelligent rich! What of the poor suckers?)&nbsp;Twenty million should make up for the blowback. Rationality is about <a href=\"/lw/7i/rationality_is_systematized_winning/\">winning</a>, right?</p>\r\n<p>Am I wrong on why this has been missed? Does this tell us anything about other opportunities that might be missed? Or, am I wrong that this should work, if the cash is present and the practical problems are solved? Or am I just an idiot?</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gbz6BMXknMtLo9T7c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 3, "extendedScore": null, "score": 5.547650272544425e-07, "legacy": true, "legacyId": "2129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QawvGzYWhqdyPWgBL", "2L3cyRN2feLQGo3Q3", "9weLK2AJ9JEt2Tt8f", "4ARtkT3EYox3THYjF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-21T18:40:21.079Z", "modifiedAt": null, "url": null, "title": "Easy Predictor Tests", "slug": "easy-predictor-tests", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:12.652Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RoyDSB9gNsjjjGQoj/easy-predictor-tests", "pageUrlRelative": "/posts/RoyDSB9gNsjjjGQoj/easy-predictor-tests", "linkUrl": "https://www.lesswrong.com/posts/RoyDSB9gNsjjjGQoj/easy-predictor-tests", "postedAtFormatted": "Thursday, January 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Easy%20Predictor%20Tests&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEasy%20Predictor%20Tests%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoyDSB9gNsjjjGQoj%2Feasy-predictor-tests%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Easy%20Predictor%20Tests%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoyDSB9gNsjjjGQoj%2Feasy-predictor-tests", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRoyDSB9gNsjjjGQoj%2Feasy-predictor-tests", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 268, "htmlBody": "<p>A fun game you can play on LessWrong is to stop just as you are about to click \"comment\" and make a prediction for how much karma your comment will receive within the next week. This will provide some quick feedback about how well your karma predictors are working. This exercise will let you know if something is broken. A simpler version is to pick from these three distinct outcomes: Positive karma, 0 karma, negative karma.</p>\n<p>What other predictors are this easy to test? Likely candidates match one or more of the following criteria:</p>\n<ul>\n<li>Something we do on a regular (probably daily) basis</li>\n<li>An action that has a clear starting point</li>\n<li>Produces quick, quantifiable feedback (e.g. karma, which is a basic number)</li>\n<li>An action that is extremely malleable so we can take our feedback, make quick adjustments, and run through the whole process again</li>\n<li>An ulterior goal other than merely testing our predictors so we don't get bored (e.g. commenting at LessWrong, which offers communication and learning as ulterior goals)</li>\n<li>Something with a \"sticky\" history so we can get a good glimpse of our progress over time</li>\n</ul>\n<div>A more difficult challenge is predicting karma on your top level posts. My predictors in this area tend to be way off the mark. For this post, my guess is between +4 and +20. Reasoning: I don't see how it could get over 20 unless it gets promoted <em>and</em>&nbsp;the concept surprises readers;&nbsp;4 seems like a solid guess for \"interesting, uncontroversial, but not groundbreaking.\"</div>\n<div><strong>Update:</strong>&nbsp;As of January 29, this post is at +10.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RoyDSB9gNsjjjGQoj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 16, "extendedScore": null, "score": 5.549389085514249e-07, "legacy": true, "legacyId": "2141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-21T21:46:31.316Z", "modifiedAt": null, "url": null, "title": "Costs to (potentially) eternal life", "slug": "costs-to-potentially-eternal-life", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:13.076Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bgrah449", "createdAt": "2009-06-09T14:45:39.500Z", "isAdmin": false, "displayName": "bgrah449"}, "userId": "tm3ckPCxYm6gvSvXF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gqE52dfaX3PY2QqJC/costs-to-potentially-eternal-life", "pageUrlRelative": "/posts/gqE52dfaX3PY2QqJC/costs-to-potentially-eternal-life", "linkUrl": "https://www.lesswrong.com/posts/gqE52dfaX3PY2QqJC/costs-to-potentially-eternal-life", "postedAtFormatted": "Thursday, January 21st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Costs%20to%20(potentially)%20eternal%20life&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACosts%20to%20(potentially)%20eternal%20life%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqE52dfaX3PY2QqJC%2Fcosts-to-potentially-eternal-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Costs%20to%20(potentially)%20eternal%20life%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqE52dfaX3PY2QqJC%2Fcosts-to-potentially-eternal-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqE52dfaX3PY2QqJC%2Fcosts-to-potentially-eternal-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 669, "htmlBody": "<p>Imagine Omega came to you and said, \"Cryonics will work; it will be possible for you to be resurrected and have the choice between a simulation and a new healthy body, and I can guarantee you live for at least 100,000 years after that. However, for reasons I won't divulge, your surviving to experience this is wholly contingent upon you killing the next three people you see. I can also tell you that the next three people you see, should you fail to kill them, will die childless and will never sign up for cryonics. There is a knife on the ground behind you.\"<br /><br />You turn around and see someone. She says, \"Wait! You shouldn't kill me because ... \"<br /><br />What does she say that convinces you?</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p>[Cryonics] takes the mostly correct idea \"life is good, death is bad\" to such an extreme that it does violence to other valuable parts of our humanity (sorry, but I can't be more specific).</p>\n</blockquote>\n<p>That's a quote from a <a href=\"/lw/1mh/that_magical_click/1hez\" target=\"_blank\">comment</a> in a <a href=\"/lw/1mh/that_magical_click/\" target=\"_blank\">post about cryonics</a>. \"I can't be more specific\" is not doing this comment any favors, and overall the comment was <a href=\"/lw/1mh/that_magical_click/1hf1\" target=\"_blank\">rebutted</a> <a href=\"/lw/1mh/that_magical_click/1hfe\" target=\"_blank\">pretty</a> <a href=\"/lw/1mh/that_magical_click/1hfc\" target=\"_blank\">well</a>. But I did try to imagine other these other valuable parts, and I realized something that remains unresolved for me.<br /><br />Guaranteed death places a limit on the value of my life to myself. Parents shield children with their bodies; Casey Jones happens more often. People run into burning buildings more often. (Suicide bombers happen more often, too, I realize.)<br /><br />I think this is a valuable part of humanity, and I think that an extreme \"life is good, death is bad\" view does do violence to it. You can argue we should effect a world that makes this willingness unnecessary, and I'll support that; but separate from making the willingness useless, eliminating that willingness does violence to our humanity. You can argue that our humanity is overrated and there's something better over the horizon, i.e. the cost is worth it.<br /><br />But the incentives for saving 1+X many lives at the cost of your own just got lessened. How do you put a price on heaven? orthonormal <a href=\"/lw/1mh/that_magical_click/1hjw\" target=\"_blank\">suggests</a> that we should rely on human irrationality here to keep us moral, that thankfully we are too stupid and slow to actually change the decisions we make after recognizing the expected value of our options has changed, despite the opportunity cost of these decisions growing considerably. I think this a) underestimates humans' ability to react to incentives and b) underestimates the reward the universe bestows on those who do react to incentives. <br /><br />I don't see a good \"solution\" to this problem, other than to rely on cognitive dissonance to make this not seem as offensive as it is now in the future. The people for whom this presents a problem will eventually die out, anyway, as there is a clear advantage to favor it. I guess that's the (ultimately anticlimactic) takeaway: Morals change in the face of progress. <br /><br />So, which do you favor more - your life, or identity?</p>\n<p><strong>EDIT</strong>: Well, it looks like this is getting fast-tracked for disappeared status. I think it's interesting that people seem to think I'm making a statement about a moral code. I'm not; I'm talking about incentives and what <em>would</em> happen, not what the right thing to do is.</p>\n<p>Let's say Eliezer gets his wish and cryonics many, many parents sign up for cryonics and sign their children up for cryonics. Does anyone really expect that this population would not respond to its incentives to avoid more danger? Anecdotes aside; do you expect them to join the military with the same frequency, be firemen with the same frequency, to be doctors administering vaccinations in jungles with the same frequency? I don't think it's possible to say that with a straight face and mean it; populations respond to incentives, and the incentives just changed for that population.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gqE52dfaX3PY2QqJC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 7, "extendedScore": null, "score": 5.549734112488176e-07, "legacy": true, "legacyId": "2139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["R3ATEWWmBhMhbY2AL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-22T05:38:36.259Z", "modifiedAt": null, "url": null, "title": "Privileged Snuff", "slug": "privileged-snuff", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:13.463Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rwallace", "createdAt": "2009-03-01T16:13:25.493Z", "isAdmin": false, "displayName": "rwallace"}, "userId": "cPhXNeZvnK7LgPMnv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RYy5HaKufpn2YLde6/privileged-snuff", "pageUrlRelative": "/posts/RYy5HaKufpn2YLde6/privileged-snuff", "linkUrl": "https://www.lesswrong.com/posts/RYy5HaKufpn2YLde6/privileged-snuff", "postedAtFormatted": "Friday, January 22nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Privileged%20Snuff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrivileged%20Snuff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYy5HaKufpn2YLde6%2Fprivileged-snuff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Privileged%20Snuff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYy5HaKufpn2YLde6%2Fprivileged-snuff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYy5HaKufpn2YLde6%2Fprivileged-snuff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 457, "htmlBody": "<p>So one is asked, \"What is your probability estimate that the LHC will destroy the world?\"<br /><br />Leaving aside the issue of calling brown numbers probabilities, there is a more subtle rhetorical trap at work here.<br /><br />If one makes up a small number, say one in a million, the answer will be, \"Could you make a million such statements and not be wrong even once?\" (Of course this is a misleading image -- doing anything a million times in a row would make you tired and distracted enough to make trivial mistakes. At some level we know this argument is misleading, because nobody calls the non-buyer of lottery tickets irrational for assigning an even lower probability to a win.)<br /><br />If one makes up a larger number, say one in a thousand, then one is considered a bad person for wanting to take even one chance in a thousand of destroying the world.</p>\n<p>The fallacy here is http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis</p>\n<p><a id=\"more\"></a>To see why, try inverting the statement: what is your probability estimate that canceling the LHC will result in the destruction of the world?<br /><br />Unlikely? Well I agree, it is unlikely. But I can think of plausible ways it could be true. New discoveries in physics could be the key to breakthroughs in areas like renewable energy or interstellar travel -- breakthroughs that might just make the difference between a universe ultimately filled with intelligent life, and a future of <em>might have been</em>. History shows, after all, that key technologies often arise from unexpected lines of research. I certainly would not be confident in assigning a million to one odds against the LHC making that difference.<br /><br />Conversely, we know the LHC is not going to destroy the world, because nature has been banging particles together at much higher energy levels for billions of years. If that sufficed to destroy the world, it would already have happened, and any people you might happen to meet from time to time would be figments of&nbsp; a deranged imagination.<br /><br />The hypothesis being privileged in even asking the original question, is not a harmless one like the tooth fairy. It is the hypothesis that snuffing out progress, extinguishing futures that might have been, is the safe option. It is not really a forgivable mistake, for we already know otherwise -- death is the default, not just for individuals, but for nations, civilizations, species and worlds. It could, however, be the ultimate mistake, the one that places the world in a position from which there is no longer a winning move.<br /><br />So remember a heuristic from a programmer's toolkit: sometimes the right answer is <em>Wherefore dost thou ask?</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RYy5HaKufpn2YLde6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 19, "extendedScore": null, "score": 5.55060918540579e-07, "legacy": true, "legacyId": "2145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-23T22:13:05.437Z", "modifiedAt": null, "url": null, "title": "Far & Near / Runaway Trolleys / The Proximity Of (Fat) Strangers", "slug": "far-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.526Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "botogol", "createdAt": "2009-02-27T09:28:08.710Z", "isAdmin": false, "displayName": "botogol"}, "userId": "PbDqfJ6jRqwXhqSy4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YZFzsCavwZvk6Xi3m/far-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "pageUrlRelative": "/posts/YZFzsCavwZvk6Xi3m/far-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "linkUrl": "https://www.lesswrong.com/posts/YZFzsCavwZvk6Xi3m/far-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "postedAtFormatted": "Saturday, January 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Far%20%26%20Near%20%2F%20Runaway%20Trolleys%20%2F%20The%20Proximity%20Of%20(Fat)%20Strangers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFar%20%26%20Near%20%2F%20Runaway%20Trolleys%20%2F%20The%20Proximity%20Of%20(Fat)%20Strangers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZFzsCavwZvk6Xi3m%2Ffar-and-near-runaway-trolleys-the-proximity-of-fat-strangers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Far%20%26%20Near%20%2F%20Runaway%20Trolleys%20%2F%20The%20Proximity%20Of%20(Fat)%20Strangers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZFzsCavwZvk6Xi3m%2Ffar-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYZFzsCavwZvk6Xi3m%2Ffar-and-near-runaway-trolleys-the-proximity-of-fat-strangers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 700, "htmlBody": "<p><span style=\"font-size: normal;\">I went to the Royal Institute last week to hear the laconic and dismissive <a href=\" http://www.philosophy.ox.ac.uk/members/guy_kahane\">Dr Guy Kahane</a> on whether we are '<a href=\"http://www.rigb.org/contentControl?action=displayEvent&amp;id=958\">Biologically Moral</a>'&nbsp; </span></p>\r\n<p><span style=\"font-size: normal;\">[His message: Neurological evidence suggests - somewhat alarmingly - that our moral and ethical decisions may be no more than post-hoc rationalisations of purely emotional, instinctive reactions.&nbsp; However, we should not panic because this is early days in neuroscience, and the correct interpretation of brain-scans is uncertain: scientist find the pattern, and the explanation, they expect to find]<a id=\"more\"></a><br /><br />To illustrate his talk Kahane used one of those moral dilemmas which are rarely encountered in real life but which are fascinating to philosophers: the familiar Trolley Problem </span></p>\r\n<p><span style=\"font-size: normal;\"><img style=\"float: left;\" src=\"http://serendip.brynmawr.edu/exchange/files/images/Greene-trolley-image005.200%20pixel%20portrait.gif\" alt=\"\" width=\"200\" height=\"181\" /><img style=\"float: right;\" src=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/image006.jpg\" alt=\"\" width=\"180\" height=\"258\" /></span><span style=\"font-size: normal;\">A picture is worth a 1,000 words and rather then spell it out Kahane flashed up two nice cartoons: </span></p>\r\n<p><span style=\"font-size: normal;\">- the first shows the anxious philosophical protagonist at the railway junction, runaway trolley approaching, pondering the lever that moves the points</span></p>\r\n<p><span style=\"font-size: normal;\">- the second shows our hapless philosopher now on a bridge poised behind an unsuspecting fat stranger who is neither alert enough for his attention to have been caught by the runaway trolley bearing down on the small party of railway workers, nor sufficiently familiar with the philosophical domain to appreciate the mortal danger that he is in himself.&nbsp; Irony, oh Irony: Philosophy, thy name is Drama.<br /><br />So far so humdrum;&nbsp; but... surprise! : in the front row of the audience that evening, wedged into the seat next to me, and the seat next to that, only a couple of metres from Dr Kahane and dressed in identical clothes as when the cartoon was made <em>was the fat stranger himself.</em>&nbsp; </span></p>\r\n<p><span style=\"font-size: normal;\">You had to feel for Dr Kahane, but with no evident embarrassment he declined to acknowledge the unexpected attendee and ploughed gamely on with an earnest discussion of the morals, ethics and practicalities of heaving the poor man off a bridge and under the wheels of an oncoming philosophical trope. I had to smile<br /><br />And the Trolley Problem is always good for a lively debate so, inevitably, in the Q&amp;A, we came back to it all over again, I felt more uncomfortable by now as members of the audience discussed at length the pros and cons of the fat man's sorry and undeserved demise,<em> none of them acknowledging his presence amongst us.</em>&nbsp; </span></p>\r\n<p><span style=\"font-size: normal;\">The fat man was, you might say, the elephant in the room.<br /><br /><strong>My discovery</strong><br /><br />The reason the Trolley Problem is so intriguing and enduring is that it appears to neatly demonstrate Far and Near thinking :&nbsp; Although the two scenarios are logically identical, shifting a lever to divert a trolley down a track is <em>Far</em>, so most people will do it, but giving a fat man a shove is <em>Near </em>and this is why most people will decline.<br /><br />What I discovered that evening is: that the bridge scenario is not, in fact, <em>Near </em>at all</span><span style=\"font-size: normal;\">.</span></p>\r\n<p><span style=\"font-size: normal;\">Having the fat stranger sitting right next to you is Near !<br /><br />You see, as an avowed nationalist and utilitarian, I have never before found the Trolley Problem to be any kind of dilemma: I am a slayer of the obese onlooker every time.&nbsp;&nbsp; But that particular evening when it came to the vote I found that simple embarrassment, and the trivial desire not to appear insensitive was enough to stay my rational hand and I sat on it, guiltily despatching five poor, hypothetical railway workers to their deaths, merely to avoid a momentary unkindness.<br /><strong><br />My Conclusions</strong><br /><br />- It seems there is <em>Far </em>Near and <em>Near </em>Near, and if you ever again find yourself with time to meta-think that you are operating in Near mode.... then you're actually in Far mode.<br /><br />- and so I will be more suspicious of the hypothetical thought experiments from now on.<br /><br /><strong>Epilogue</strong><br /><br />But, you are asking, how did the Fat Man himself vote?<br />He declined to push, remarking drily that the fattest person on the bridge was very likely to be himself. He would, he said, jump.<br /></span></p>\r\n<p><span style=\"font-size: normal;\">He was most definitely thinking in Far mode.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LMFBzsJaCRADQqw3F": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YZFzsCavwZvk6Xi3m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 12, "extendedScore": null, "score": 5.555125736229076e-07, "legacy": true, "legacyId": "2150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-23T22:47:07.845Z", "modifiedAt": null, "url": null, "title": "Lists of cognitive biases, common misconceptions, and fallacies", "slug": "lists-of-cognitive-biases-common-misconceptions-and", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:08.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zrB9qwQpRNgPda8Fm/lists-of-cognitive-biases-common-misconceptions-and", "pageUrlRelative": "/posts/zrB9qwQpRNgPda8Fm/lists-of-cognitive-biases-common-misconceptions-and", "linkUrl": "https://www.lesswrong.com/posts/zrB9qwQpRNgPda8Fm/lists-of-cognitive-biases-common-misconceptions-and", "postedAtFormatted": "Saturday, January 23rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lists%20of%20cognitive%20biases%2C%20common%20misconceptions%2C%20and%20fallacies&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALists%20of%20cognitive%20biases%2C%20common%20misconceptions%2C%20and%20fallacies%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrB9qwQpRNgPda8Fm%2Flists-of-cognitive-biases-common-misconceptions-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lists%20of%20cognitive%20biases%2C%20common%20misconceptions%2C%20and%20fallacies%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrB9qwQpRNgPda8Fm%2Flists-of-cognitive-biases-common-misconceptions-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzrB9qwQpRNgPda8Fm%2Flists-of-cognitive-biases-common-misconceptions-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">http://en.wikipedia.org/wiki/List_of_cognitive_biases</a></p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_common_misconceptions\">http://en.wikipedia.org/wiki/List_of_common_misconceptions</a></p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_fallacies\">http://en.wikipedia.org/wiki/List_of_fallacies</a></p>\n<p><a href=\"http://en.wikipedia.org/wiki/List_of_memory_biases\">http://en.wikipedia.org/wiki/List_of_memory_biases</a></p>\n<p>I know the trend here is against simple links as top-level posts, but I think these links are powerful enough to stand on their own. I would be very surprised if most people here have already read all of these links. Also, it seems like a good sign that this somehow got modded up to 1 while sitting in my drafts folder.</p>\n<p>Thanks to <a href=\"http://www.lonegunman.co.uk/2009/03/01/year-one-in-review/\">Lone Gunman</a> for the links.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zrB9qwQpRNgPda8Fm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 13, "extendedScore": null, "score": 5.555187095240913e-07, "legacy": true, "legacyId": "26", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-24T00:33:03.381Z", "modifiedAt": null, "url": null, "title": "Lesswrong UK planning thread", "slug": "lesswrong-uk-planning-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:20.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mWNpB8voXvWTbsiK8/lesswrong-uk-planning-thread", "pageUrlRelative": "/posts/mWNpB8voXvWTbsiK8/lesswrong-uk-planning-thread", "linkUrl": "https://www.lesswrong.com/posts/mWNpB8voXvWTbsiK8/lesswrong-uk-planning-thread", "postedAtFormatted": "Sunday, January 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lesswrong%20UK%20planning%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALesswrong%20UK%20planning%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWNpB8voXvWTbsiK8%2Flesswrong-uk-planning-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lesswrong%20UK%20planning%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWNpB8voXvWTbsiK8%2Flesswrong-uk-planning-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmWNpB8voXvWTbsiK8%2Flesswrong-uk-planning-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 389, "htmlBody": "<p>A few of us got together in the pub after the friendly AI meet and agreed we should have a meetup for those of us familiar with lesswrong/bostrom etc. This is a post for discussion of when/where.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>Venue</strong>: London still seems the nexus. I might be able to be convinced to go to Oxford. A starbucks type place is okay, although it'd be nice to have a white board or other presentation systems.</p>\n<p><strong>Date/Time</strong>: Weekends is fine with me, and I suspect most people. Julian suggested after the next UKTA meeting. Will that be April at the <a href=\"http://humanityplus-uk.com/wordpress/\">humanityplus</a> thing?&nbsp; It would depend whether we are still mentally fresh after it, and whether any of our group are attending the dinner after.</p>\n<p><strong>Activities</strong>: I think it would be a good idea to have some structure or topics to discuss, so that we don't just fall back into the \"what do you do\" types of discussion too much. Maybe mini presentations.</p>\n<p>My duo of current interests</p>\n<p>1)Evidence for intelligence explosion: I don't want to rehash what we know already, but I would like to try and figure out what experiments we can do (safely) or proofs we can make to increase or decrease our belief that it will occur. This is more of a brainstorming session.</p>\n<p>2) The nature of the human brain: Specifically it doesn't appear to have a goal (in the decision theory sense) built in, although it can become a goal optimizer to a greater or lesser extent. How might it do this? As we aren't neuroscientists, a more fruitful question might be what the skeleton of such a computer system that can do this might be look like, even if we can't fill in all the interesting details. I'd discuss this with regards to akrasia, neural enhancement, volition extraction and non-exploding AI scenarios. I can probably pontificate on this for a while, if I prepare myself.</p>\n<p>I think Ciphergoth wanted to talk about consequentialist ethics.</p>\n<p>Shout in the comments if you have a topic you'd like to discuss, or would rather not discuss.</p>\n<p>Perhaps we should also look at the multiplicity of AI bias, where people seem to naturally assume there will be multiple AIs even when talking about super intelligent singularity scenarios (many questions had this property at the meeting). I suspect that this could be countered by reading a Fire upon the Deep somewhat.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mWNpB8voXvWTbsiK8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 5.555385603574965e-07, "legacy": true, "legacyId": "2151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-24T05:03:44.756Z", "modifiedAt": null, "url": null, "title": "Value Uncertainty and the Singleton Scenario", "slug": "value-uncertainty-and-the-singleton-scenario", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:12.615Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qh6bnkxbMFz5SNeFd/value-uncertainty-and-the-singleton-scenario", "pageUrlRelative": "/posts/Qh6bnkxbMFz5SNeFd/value-uncertainty-and-the-singleton-scenario", "linkUrl": "https://www.lesswrong.com/posts/Qh6bnkxbMFz5SNeFd/value-uncertainty-and-the-singleton-scenario", "postedAtFormatted": "Sunday, January 24th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20Uncertainty%20and%20the%20Singleton%20Scenario&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20Uncertainty%20and%20the%20Singleton%20Scenario%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh6bnkxbMFz5SNeFd%2Fvalue-uncertainty-and-the-singleton-scenario%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20Uncertainty%20and%20the%20Singleton%20Scenario%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh6bnkxbMFz5SNeFd%2Fvalue-uncertainty-and-the-singleton-scenario", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQh6bnkxbMFz5SNeFd%2Fvalue-uncertainty-and-the-singleton-scenario", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 938, "htmlBody": "<p class=\"MsoNormal\">In January of last year, Nick Bostrom wrote a post on Overcoming Bias about his and Toby Ord&rsquo;s <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">proposed method of handling moral uncertainty</a>. To abstract away a bit from their specific proposal, the general approach was to convert a problem involving moral uncertainty into a game of negotiation, with each player&rsquo;s bargaining power determined by one&rsquo;s confidence in the moral philosophy represented by that player.</p>\n<p class=\"MsoNormal\">Robin Hanson suggested in <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html#comment-389337\">his comments</a> to Nick&rsquo;s post that moral uncertainty should be handled the same way we're supposed to handle ordinary uncertainty, by using standard decision theory (i.e., expected utility maximization). <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html#comment-389344\">Nick&rsquo;s reply</a> was that many ethical systems don&rsquo;t fit into the standard decision theory framework, so it&rsquo;s hard to see how to combine them that way.</p>\n<p class=\"MsoNormal\">In this post, I suggest we look into the seemingly easier problem of value uncertainty, in which we fix a consequentialist ethical system, and just try to deal with uncertainty about values (i.e., utility function). Value uncertainty can be considered a special case of moral uncertainty in which there is no apparent obstacle to applying Robin&rsquo;s suggestion. I&rsquo;ll consider a specific example of a decision problem involving value uncertainty, and work out how Nick and Toby&rsquo;s negotiation approach differs in its treatment of the problem from standard decision theory. Besides showing the difference in the approaches, I think the specific problem is also quite important in its own right.</p>\n<p class=\"MsoNormal\">The problem I want to consider is, suppose we believe that a <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a> scenario is <a href=\"http://www.overcomingbias.com/2010/01/tech-in-far-view.html\">very unlikely</a>, but <em>may</em> have very high utility if it were realized, should we focus most of our attention and effort into trying to increase its probability and/or improve its outcome? The main issue here is (putting aside uncertainty about what will happen after a singleton scenario is realized) uncertainty about how much we value what is likely to happen.</p>\n<p class=\"MsoNormal\"><a id=\"more\"></a>Let&rsquo;s say there is a 1% chance that a singleton scenario does occur, and conditional on it, you will have expected utility that is equivalent to a 1 in 5 billion chance of controlling the entire universe. If a singleton scenario does not occur, you will have a 1/5 billionth share of the resources of the solar system, and the rest of the universe will be taken over by beings like the ones described in Robin&rsquo;s <a href=\"http://hanson.gmu.edu/hardscra.pdf\">The Rapacious Hardscrapple Frontier</a>. There are two projects that you can work on. Project A increases the probability of a singleton scenario to 1.001%. Project B increases the wealth you will have in the non-singleton scenario by a factor of a million (so you&rsquo;ll have a 1/5 thousandth share of the solar system). The decision you have to make is which project to work on. (The numbers I picked are meant to be stacked in favor of project B.)</p>\n<p class=\"MsoNormal\">Unfortunately, you&rsquo;re not sure how much utility to assign to these scenarios. Let&rsquo;s say that you think there is a 99% probability that your utility (U1) scales logarithmically with the amount of negentropy you will have control over, and 1% probability that your utility (U2) scales as the square root of negentropy. (I assume that you&rsquo;re an ethical egoist and do not care much about what other people do with their resources. And these numbers are again deliberately stacked in favor of project B, since the better your utility function scales, the more attractive project A is.)</p>\n<p class=\"MsoNormal\">Let&rsquo;s compute the expected U1 and U2 of Project A and Project B. Let <a href=\"http://arxiv.org/abs/quant-ph/0110141\">N<sub>U</sub>=10<sup>120</sup></a> be the negentropy (in bits) of the universe, and <a href=\"http://www.metanexus.net/magazine/ArticleDetail/tabid/68/id/8620/Default.aspx\">N<sub>S</sub>=10<sup>77</sup></a> be the negentropy of the solar system, then:</p>\n<ul>\n<li>EU1(status quo) = .01 * log(N<sub>U</sub>)/5e9 + .99 * log(N<sub>S</sub>/5e9)</li>\n<li>EU1(A) = .01001 * log(N<sub>U</sub>)/5e9 + .98999 * log(N<sub>S</sub>/5e9) &asymp; 66.6</li>\n<li>EU1(B) = .01 * log(N<sub>U</sub>)/5e9 + .99 * log(N<sub>S</sub>/5e3) &asymp; 72.6</li>\n</ul>\n<p class=\"MsoNormal\">EU2 is computed similarly, except with log replaced by sqrt:</p>\n<ul>\n<li>EU2(A) = <span>&nbsp;</span>.01001 * sqrt(N<sub>U</sub>)/5e9 + .98999 * sqrt(N<sub>S</sub>/5e9) &asymp; 2.002e48</li>\n<li>EU2(B) = <span>&nbsp;</span>.01 * sqrt(N<sub>U</sub>)/5e9 + .99 * sqrt(N<sub>S</sub>/5e3) &asymp; 2.000e48</li>\n</ul>\n<p class=\"MsoNormal\">Under Robin&rsquo;s approach to value uncertainty, we would (I presume) combine these two utility functions into one linearly, by weighing each with its probability, so we get EU(x) = 0.99 EU1(x) + 0.01 EU2(x):</p>\n<ul>\n<li>EU(A) &asymp; 0.99 * 66.6 + 0.01 * 2.002e48 &asymp; 2.002e46</li>\n<li>EU(B) &asymp; 0.99 * 72.6 + 0.01 * 2.000e48 &asymp; 2.000e46</li>\n</ul>\n<p class=\"MsoNormal\">This suggests that we should focus our attention and efforts on the singleton scenario. In fact, even if Project A had a much, much smaller probability of success, like 10<sup>-30</sup> instead of 0.00001, or you have a much lower confidence that your utility scales as well as the square root of negentropy, it would still be the case that EU(A)&gt;EU(B). (This is contrary to Robin's position that we pay too much attention to the singleton scenario, and I would be interested to know in which detail his calculation differs from mine.)</p>\n<p class=\"MsoNormal\">What about Nick and Toby&rsquo;s approach? In their scheme, delegate 1, representing U1, would vote for project B, while delegate 2, representing U2, would vote for project A. Since delegate 1 has 99 votes to delegate 2&rsquo;s one vote, the obvious outcome is that we should work on project B. The details of the negotiation process don't seem to matter much, given the large advantage in bargaining power that delegate 1 has over delegate 2.</p>\n<p class=\"MsoNormal\">Each of these approaches to value uncertainty seems intuitively attractive on its own, but together they give conflicting advice on this important practical problem. Which is the right approach, or is there a better third choice? I think this is perhaps one of the most important open questions that an aspiring rationalist can work on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ouT6wKhACJRouGokM": 2, "nSHiKwWyMZFdZg5qt": 2, "NLwTnsH9RSotqXYLw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qh6bnkxbMFz5SNeFd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 5.555888240348261e-07, "legacy": true, "legacyId": "2152", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-25T14:16:18.752Z", "modifiedAt": null, "url": null, "title": "Simon Conway Morris: \"Aliens are likely to look and behave like us\".", "slug": "simon-conway-morris-aliens-are-likely-to-look-and-behave", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:09.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5uApLJ4N2bYdo5cWx/simon-conway-morris-aliens-are-likely-to-look-and-behave", "pageUrlRelative": "/posts/5uApLJ4N2bYdo5cWx/simon-conway-morris-aliens-are-likely-to-look-and-behave", "linkUrl": "https://www.lesswrong.com/posts/5uApLJ4N2bYdo5cWx/simon-conway-morris-aliens-are-likely-to-look-and-behave", "postedAtFormatted": "Monday, January 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simon%20Conway%20Morris%3A%20%22Aliens%20are%20likely%20to%20look%20and%20behave%20like%20us%22.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimon%20Conway%20Morris%3A%20%22Aliens%20are%20likely%20to%20look%20and%20behave%20like%20us%22.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uApLJ4N2bYdo5cWx%2Fsimon-conway-morris-aliens-are-likely-to-look-and-behave%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simon%20Conway%20Morris%3A%20%22Aliens%20are%20likely%20to%20look%20and%20behave%20like%20us%22.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uApLJ4N2bYdo5cWx%2Fsimon-conway-morris-aliens-are-likely-to-look-and-behave", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5uApLJ4N2bYdo5cWx%2Fsimon-conway-morris-aliens-are-likely-to-look-and-behave", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 405, "htmlBody": "<blockquote>\n<p><em>Professor Simon Conway Morris at Cambridge University will tell a conference on alien life that extraterrestrials will most likely have evolved just like \"earthlings\" and so resemble us to a degree with heads, limbs and bodies.</em></p>\n</blockquote>\n<p>[<a href=\"http://www.telegraph.co.uk/science/science-news/7071013/Aliens-are-likely-to-look-and-behave-like-us.html\">link</a>]</p>\n<p><a id=\"more\"></a></p>\n<blockquote>\n<p><em>Unfortunately they will have also evolved our foibles and faults which could make them dangerous if they ever did visit us on Earth. </em></p>\n<p><em> The evolutionary paleobiologist's beliefs mean that science fiction films such as Star Wars and Star Trek could be more accurate than they ever imagined in depicting alien life. </em></p>\n<p><em> Prof Conway Morris believes that extraterrestrial life is most likely to occur on a planet similar to our own, with organisms made from the same biochemicals. The process of evolution will even shape alien life in a similar way, he added. </em></p>\n<p><em> &ldquo;It is difficult to imagine evolution in alien planets operating in any manner other than Darwinian,\" he said. </em></p>\n<p><em> \"In the end the number of options is remarkably restrictive. I don't think an alien will be a blob. If aliens are out there they should have evolved just like us. They should have eyes and be walking on two legs. </em></p>\n<p><em> \"In short if there is any life out there then it is likely to be very similar to us.\" </em></p>\n<p><em> Extra-terrestrials might not only resemble us but have our foibles, such as greed, violence and a tendency to exploit others' resources, claims Professor Conway Morris. </em></p>\n<p><em> They could come in peace but also be searching for somewhere to live, and to help themselves to water, minerals and fuel he is due to tell a conference at the Royal Society, in London. </em></p>\n<p><em> However he also thinks that because much of the Universe is older than us they would have evolved further down the line and we should have heard from them by now. </em></p>\n<p><em> He believes it is increasingly looking like they may not be out there at all. </em></p>\n</blockquote>\n<p>Thoughts on this?</p>\n<p>Conway Morris is a big hitter in the scientific establishment. He is, however, a theist, and has \"argued against materialism\" according to wikipedia. But what are his arguments? Alas the press piece doesn't say. Kudos to anyone who finds the conference and posts the arguments.</p>\n<p><em>&ldquo;It is difficult to imagine evolution in alien planets operating in any manner other than Darwinian,\"</em></p>\n<p>Is this just an instance of a slightly woo scientist-theist failing to take into account that nature might be more imaginative than him? <br /><em><br /></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NGtNzdS88JtEQdRP4": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5uApLJ4N2bYdo5cWx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 4, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "2165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-25T17:45:16.884Z", "modifiedAt": null, "url": null, "title": "Adaptive bias", "slug": "adaptive-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:10.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TFZhCehLARTTWedd8/adaptive-bias", "pageUrlRelative": "/posts/TFZhCehLARTTWedd8/adaptive-bias", "linkUrl": "https://www.lesswrong.com/posts/TFZhCehLARTTWedd8/adaptive-bias", "postedAtFormatted": "Monday, January 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Adaptive%20bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAdaptive%20bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFZhCehLARTTWedd8%2Fadaptive-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Adaptive%20bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFZhCehLARTTWedd8%2Fadaptive-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTFZhCehLARTTWedd8%2Fadaptive-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 600, "htmlBody": "<p>If we want to apply our brains more effectively to the pursuit of our chosen objectives, we must commit to the hard work of understanding how brains implement cognition. Is it enough to strive to \"overcome bias\"? I've come across an interesting tidbit of research (which I'll introduce in a moment) on \"perceptual pop-out\", that hints it is <em>not</em> enough.</p>\n<p>\"Cognition\" is a broad notion; we can dissect it into awareness, perception, reasoning, judgment, feeling... Broad enough to encompass what I'm coming to call \"pure reason\": our shared toolkit of normative frameworks for assessing probability, evaluating utility, guiding decision, and so on. Pure reason is one of the components of rationality, as this term is used here, but it does not encompass all of rationality, and we should beware the many Myths of Pure Reason. The <a href=\"/lw/59/spocks_dirty_little_secret/\">Spock caricature</a> is one; by itself enough cause to use the word \"rational\" sparingly, if at all.</p>\n<p>Or the idea that all bias is bad.</p>\n<p>It turns out, for instance, that a familiar bugaboo, <strong>confirmation bias</strong>, might play an important role in perception. <a href=\"http://www.mrc-cbu.cam.ac.uk/people/matt.davis/home.html\">Matt Davis</a> at Cambridge Medical School has crafted a really neat three-part audio sample based showcasing one of his research topics. The first and last part of the sample are <em>exactly the same</em>. If you are at all like me, however, you will perceive them quite differently.</p>\n<p><a id=\"more\"></a><a href=\"http://www.mrc-cbu.cam.ac.uk/people/matt.davis/sine-wave-speech/pop-out.mp3\">Here is the audio sample (mp3). Please listen to it now.</a></p>\n<p>Notice the difference? Matt Davis, who has researched these effects extensively, refers to them as \"perceptual pop-out\". The link with confirmation bias is suggested by <a href=\"http://alexandria.nu/ai/blog/entry.asp?E=53\">Jim Carnicelli</a>: \" Once you have an expectation of what to look for in the data, you quickly find it.\"</p>\n<p>In <em>Probability Theory</em>, E.T. Jaynes notes that perception is \"inference from incomplete information\"; and elsewhere adds:</p>\n<blockquote>\n<p>Kahneman &amp; Tversky claimed that we are not Bayesians, because in psychological tests people often commit violations of Bayesian principles. [...] People are reasoning to a more sophisticated version of Bayesian inference than [Kahneman and Tversky] had in mind. [...] We would expect Natural Selection to produce such a result: after all, any reasoning format whose results conflict with Bayesian inference will place a creature at a decided survival disadvantage.</p>\n</blockquote>\n<p>There is an apparent paradox between our susceptibility to various biases, and the fact that these biases are prevalent precisely because they are part of a cognitive toolkit honed over a long evolutionary period, suggesting that each component of that toolkit must have <em>worked</em> - conferred some advantage. Bayesian inference, claims Jaynes, isn't just a good move - it is the best move.</p>\n<p>However, these components evolved in specific situations; the hardware kit that they are part of was never intended to run the software we now know as \"pure reason\". Our high-level reasoning processes are \"hijacking\" these components for other purposes. The same goes of our consciousness, which is also a patched-together hack on top of the same hardware.</p>\n<p>There, by the way, is why Dennett's work on consciousness is important, and should be given a sympathetic exposition here rather than a hatchet job. (This post is intended in part as a tentative prelude to tackling that exposition.)</p>\n<p>We are not AIs, who, when finally implemented, will (putatively) be able to modify their own source code. The closest we can come to that is to be aware of what our reasoning is <em>put together from</em>, which include various biases that exist for a reason, and to make conscious choices as to how we use these components.</p>\n<p>Bottom line : understanding where your biases come from, and putting that knowledge to good use, is of more value than rejecting all bias as evil.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TFZhCehLARTTWedd8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "2166", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ru536oPGPJsEkA3Ee"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-25T23:22:45.169Z", "modifiedAt": null, "url": null, "title": "Welcome to Heaven", "slug": "welcome-to-heaven", "viewCount": null, "lastCommentedAt": "2022-02-28T18:34:49.110Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "denisbider", "createdAt": "2009-04-29T00:53:27.472Z", "isAdmin": false, "displayName": "denisbider"}, "userId": "GLzYfwe9trCMxRPyB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyaCBiPRosr3T8YLF/welcome-to-heaven", "pageUrlRelative": "/posts/cyaCBiPRosr3T8YLF/welcome-to-heaven", "linkUrl": "https://www.lesswrong.com/posts/cyaCBiPRosr3T8YLF/welcome-to-heaven", "postedAtFormatted": "Monday, January 25th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20Heaven&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20Heaven%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyaCBiPRosr3T8YLF%2Fwelcome-to-heaven%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20Heaven%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyaCBiPRosr3T8YLF%2Fwelcome-to-heaven", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcyaCBiPRosr3T8YLF%2Fwelcome-to-heaven", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 484, "htmlBody": "<p>I can conceive of&nbsp;the following&nbsp;3 main&nbsp;types of meaning we can pursue in life.</p>\r\n<p>1. Exploring existing complexity: the&nbsp;natural complexity of the universe, or complexities that others created for us to explore.</p>\r\n<p>2. Creating new complexity for others and ourselves to explore.</p>\r\n<p>3.&nbsp;Hedonic pleasure: more or less direct stimulation of our pleasure centers,&nbsp;with&nbsp;wire-heading as the ultimate form.</p>\r\n<p>What I'm observing in the various FAI debates is a tendency of people to shy away from wire-heading as something the FAI should do. This reluctance is generally not substantiated or clarified with anything other than \"clearly, this isn't what we want\". This is not, however, clear to me at all.</p>\r\n<p>The utility we get from exploration and creation is an enjoyable mental process that&nbsp;comes with these activities. Once an FAI can rewire our brains at will, we do not need to perform&nbsp;actual exploration or creation&nbsp;to experience this enjoyment. Instead, the enjoyment we get from exploration and creation becomes just another form of pleasure that can be stimulated directly.</p>\r\n<p>If you are a utilitarian, and you believe in shut-up-and-multiply, then the correct&nbsp;thing for the FAI to do is to use up all available resources so as to maximize the number of beings, and then induce a state of permanent and ultimate enjoyment in every one of them. This enjoyment could be of any type - it could be explorative or creative or hedonic enjoyment as we know it. The most energy efficient way to&nbsp;create <em>any</em> kind of enjoyment, however, is to stimulate the brain-equivalent directly. Therefore, the greatest utility will be achieved by wire-heading. Everything else falls short of that.</p>\r\n<p>What I don't quite understand is why everyone thinks that this would be such a horrible outcome. As far as I can tell, these seem to be cached emotions that are suitable for our world, but not for the world of FAI. In our world, we truly do need to constantly explore and create, or else we will suffer the consequences of not mastering our environment. In a world where FAI exists, there is no longer a point, nor even a possibility, of mastering our environment. The FAI masters our environment for us, and there is no longer a reason to avoid hedonic pleasure. It is no longer a trap.</p>\r\n<p>Since the FAI can sustain&nbsp;us in safety&nbsp;until the universe goes poof, there is no reason for&nbsp;everyone not&nbsp;to experience ultimate enjoyment in the meanwhile. In fact, I can hardly tell this apart from the concept of a Christian Heaven, which&nbsp;appears to be&nbsp;a place where Christians very much want to get.</p>\r\n<p>If you don't want to be \"reduced\" to an eternal state of bliss, that's tough luck. The alternative would be for the FAI to create an environment for you to play in, consuming precious resources that could sustain more&nbsp;creatures in a permanently blissful state.&nbsp;But&nbsp;don't worry; you won't need to feel bad for long.&nbsp;The FAI can simply modify your preferences so you <em>want</em> an eternally blissful state.</p>\r\n<p>Welcome to Heaven.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yEs5Tdwfw5Zw8yGWC": 1, "ZTRNmvQGgoYiymYnq": 1, "xexCWMyds6QLWognu": 1, "5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyaCBiPRosr3T8YLF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 65, "baseScore": 26, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "2169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 246, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-26T12:06:40.664Z", "modifiedAt": null, "url": null, "title": "You cannot be mistaken about (not) wanting to wirehead", "slug": "you-cannot-be-mistaken-about-not-wanting-to-wirehead", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:09.173Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3iM8QjvdkPCyLRJM6/you-cannot-be-mistaken-about-not-wanting-to-wirehead", "pageUrlRelative": "/posts/3iM8QjvdkPCyLRJM6/you-cannot-be-mistaken-about-not-wanting-to-wirehead", "linkUrl": "https://www.lesswrong.com/posts/3iM8QjvdkPCyLRJM6/you-cannot-be-mistaken-about-not-wanting-to-wirehead", "postedAtFormatted": "Tuesday, January 26th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20cannot%20be%20mistaken%20about%20(not)%20wanting%20to%20wirehead&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20cannot%20be%20mistaken%20about%20(not)%20wanting%20to%20wirehead%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3iM8QjvdkPCyLRJM6%2Fyou-cannot-be-mistaken-about-not-wanting-to-wirehead%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20cannot%20be%20mistaken%20about%20(not)%20wanting%20to%20wirehead%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3iM8QjvdkPCyLRJM6%2Fyou-cannot-be-mistaken-about-not-wanting-to-wirehead", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3iM8QjvdkPCyLRJM6%2Fyou-cannot-be-mistaken-about-not-wanting-to-wirehead", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 796, "htmlBody": "<p>In the comments of <a href=\"/lw/1o9/welcome_to_heaven/\">Welcome to Heaven</a>, Wei Dai <a href=\"/lw/1o9/welcome_to_heaven/1id9\">brings up the argument</a> that even though we may not want to be wireheaded now, our wireheaded selves would probably prefer to be wireheaded. Therefore we might be mistaken about what we really want. (<em>Correction:&nbsp;</em><em>what Wei actually said <a href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/1ij1\">was that</a> an FAI might tell us that we would prefer to be wireheaded if we knew what it felt like, not that our wireheaded selves would prefer to be wireheaded.)</em><br /><br />This is an argument I've heard frequently, one which I've even used myself. But I don't think it holds up. More generally, I don't think <em>any</em> argument that says one is wrong about what they want holds up.<br /><br />To take the example of wireheading. It is not an <em>inherent property</em> of minds that they'll become desperately addicted to anything that feels sufficiently good. Even from our own experience, we know that there are plenty of things that feel really good, but we don't immediately crave for more afterwards. Sex might be great, but you can still afterwards get fatigued enough that you want to rest; eating good food might be enjoyable, but at some point you get full. The classic counter-example is that of the rats who could pull a lever stimulating a part of their brain, and ended up compulsively pulling it, to the exclusion of all else. People thought this to mean they were caught in a loop of stimulating their \"pleasure center\", but it later turned out that wasn't the case. Instead, the rats were stimulating their \"<a href=\"http://www.slate.com/id/2224932/pagenum/all/\">wants to seek out things -center</a>\".<br /><br />The systems for experiencing pleasure and for wanting to seek out pleasure are separate ones. One can find something pleasurable, but still not develop a desire to seek it out. I'm sure all of you have had times when you haven't felt the urge to participate in a particular activity, even though you knew you'd enjoy the activity in question if you just got around doing it. Conversly, one can also have a desire to seek out something, but still not find it pleasurable when it's achieved. <br /><br />Therefore, it is not an inherent property of wireheading that we'd automatically end up wanting it. Sure, you <em>could</em> wirehead someone in such a way that the person stopped wanting anything else, but you could also wirehead them in such a way that they were indifferent to whether or not it continued. You could even wirehead them in such a way that they enjoyed every minute of it, but at the same time wanted it to stop.<br /><a id=\"more\"></a><br />\"Am I mistaken about wanting to be wireheaded?\" is a <a href=\"/lw/og/wrong_questions/\">wrong question</a>. You might afterwards <em>think</em> you actually prefer to be wireheaded, or think you prefer not to be wireheaded, but that is purely a question of how you define the term \"wireheading\". Is it a procedure that makes you want it, or is it not? Furthermore, even if we define wireheading so that you'd prefer it afterwards, that says nothing about the moral worth of wireheading somebody.<br /><br />If you're not convinced about that last bit, consider the case of \"anti-wireheading\": we rewire somebody so that they experience terrible, horrible, excruciating pain. We also rewire them so that regardless, they seek to maintain their current state. In fact, if they somehow stop feeling pain, they'll compulsively seek a return to their previous hellish state. Would you say it was okay to anti-wirehead them, since an anti-wirehead will realize they were mistaken about not wanting to be an anti-wirehead? Probably not.<br /><br />In fact, \"I thought I wouldn't want to do/experience X, but upon trying it out I realized I was wrong\" doesn't make sense. Previously the person didn't want X, but after trying it out they did want X. X has caused a change in their preferences by altering their brain. This doesn't mean that the pre-X person was <em>wrong</em>, it just means the post-X person has been <em>changed</em>. With the correct technology, anyone can be changed to prefer anything.<br /><br />You can still be mistaken about whether or not you'll like something, of course. But that's distinct from whether or not you want it.<br /><br />Note that this makes any thoughts along the lines of \"an FAI might extrapolate the desires you had if you were more intelligent\" tricky. It could just as well extrapolate the desires we had if we'd had our brains altered in some other way. What makes one method of mind alteration more acceptable than another? \"Whether we'd consent to it <em>now</em>\" is one obvious-seeming answer, but that too is filled with pitfalls. (For instance, what about our anti-wirehead?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yEs5Tdwfw5Zw8yGWC": 2, "qf3kDBak4BQDDw3f2": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3iM8QjvdkPCyLRJM6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 56, "baseScore": 44, "extendedScore": null, "score": 7.2e-05, "legacy": true, "legacyId": "2172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cyaCBiPRosr3T8YLF", "XzrqkhfwtiSDgKoAF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-27T18:25:54.834Z", "modifiedAt": null, "url": null, "title": "Bizarre Illusions", "slug": "bizarre-illusions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:08.825Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4a6bzHmJyb2aLaz9M/bizarre-illusions", "pageUrlRelative": "/posts/4a6bzHmJyb2aLaz9M/bizarre-illusions", "linkUrl": "https://www.lesswrong.com/posts/4a6bzHmJyb2aLaz9M/bizarre-illusions", "postedAtFormatted": "Wednesday, January 27th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bizarre%20Illusions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABizarre%20Illusions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4a6bzHmJyb2aLaz9M%2Fbizarre-illusions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bizarre%20Illusions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4a6bzHmJyb2aLaz9M%2Fbizarre-illusions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4a6bzHmJyb2aLaz9M%2Fbizarre-illusions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 354, "htmlBody": "<p><a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Same_color_illusion\"><br /><img style=\"border: 0; float: right;\" src=\"http://upload.wikimedia.org/wikipedia/commons/6/60/Grey_square_optical_illusion.PNG\" alt=\"grey square illusion\" width=\"271\" height=\"211\" /></a>Illusions are cool. They make me think something is happening when it isn't. When offered the classic illusion pictured to the right, I wonder at the color of A and B.&nbsp;How <a title=\"Think Like Reality\" href=\"/lw/hs/think_like_reality/\">weird, bizarre, and incredible</a>.</p>\n<p>Today I looked at the above illusion and thought, \"Why do I keep thinking A and B are different colors? Obviously, something is wrong with how I am thinking about colors.\"&nbsp;I am being stupid when my I look at this illusion and I interpret the data in such a way to determine distinct colors. My expectations of reality and the information being transmitted and received are not lining up. If they were, the illusion wouldn't be an illusion.</p>\n<p>The number 2 is prime; the number 6 is not. What about the number 1? Prime is <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Prime_number\">defined</a> as a natural number with exactly two divisors. 1 is an <em>illusionary</em>&nbsp;prime if you use a poor definition such as, \"Prime is a number that is only divisible by itself and 1.\" Building on these bad assumptions could result in all sorts of weird results much like dividing by 0 can make it look like <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Mathematical_fallacy#2_.3D_1\">2 = 1</a>. What a tricky illusion!</p>\n<p>An <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Optical_illusion\">optical illusion</a>&nbsp;is only bizarre if you are making a bad assumption about how your&nbsp;visual&nbsp;system is supposed to be working. It is a <a title=\"Map and Territory sequence\" href=\"http://wiki.lesswrong.com/wiki/Map_and_territory_(sequence)\">flaw in the Map</a>, not the Territory. I should stop thinking that the visual system is reporting RGB style colors. <a title=\"Wikipedia\" href=\"http://en.wikipedia.org/wiki/Optical_illusion#Color_and_brightness_constancies\">It isn't.</a>&nbsp;And, now that I know this, I am suddenly curious about what it&nbsp;<em>is</em> reporting. I have dropped a bad belief and am looking for a replacement. In this case, my visual system is distinguishing between <a href=\"/lw/1om/bizarre_illusions/1ipn\">something else entirely</a>. Now that I have the right answer, this optical illusion should become as uninteresting as questioning whether 1 is prime. It should stop being weird, bizarre, and incredible. It merely highlights an obvious reality.</p>\n<p><strong>Addendum:</strong>&nbsp;This post was edited to fix a few problems and errors. If you are at all interested in more details behind the illusion presented here, there are a handful of <a href=\"/lw/1om/bizarre_illusions/1ipn\">excellent</a> <a href=\"/lw/1om/bizarre_illusions/1iqb\">comments</a> <a href=\"/lw/1om/bizarre_illusions/1iq2\">below</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4a6bzHmJyb2aLaz9M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 11, "extendedScore": null, "score": 2.7e-05, "legacy": true, "legacyId": "2182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 310, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tWLFWAndSZSYN6rPB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-28T20:52:24.091Z", "modifiedAt": null, "url": null, "title": "Play for a Cause", "slug": "play-for-a-cause", "viewCount": null, "lastCommentedAt": "2017-06-17T03:54:57.058Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sFZXfM5397tStmsrK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gt2jBeo36288HdXqH/play-for-a-cause", "pageUrlRelative": "/posts/Gt2jBeo36288HdXqH/play-for-a-cause", "linkUrl": "https://www.lesswrong.com/posts/Gt2jBeo36288HdXqH/play-for-a-cause", "postedAtFormatted": "Thursday, January 28th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Play%20for%20a%20Cause&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlay%20for%20a%20Cause%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGt2jBeo36288HdXqH%2Fplay-for-a-cause%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Play%20for%20a%20Cause%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGt2jBeo36288HdXqH%2Fplay-for-a-cause", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGt2jBeo36288HdXqH%2Fplay-for-a-cause", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 596, "htmlBody": "<p>Some of you have been trying to <a href=\"/lw/1l5/singularity_institute_100k_challenge_grant_2009/\">raise money for the Singularity Institute</a>, and I have an idea that may help.<a id=\"more\"></a></p>\n<p>The idea is to hold public competitions on LessWrong with money going to charity. Agree to a game and an amount of money, then have each player designate a charity. After the game, each player gives the agreed upon amount to the charity designated by the winner.<a name=\"fnref1\"></a><a id=\"fnref1\" class=\"footnoteRef\" href=\"#fn1\"><sup>1</sup></a> It&rsquo;s a bit like celebrity Jeopardy.<a name=\"fnref2\"></a><a id=\"fnref2\" class=\"footnoteRef\" href=\"#fn2\"><sup>2</sup></a></p>\n<p>Play the game here on LessWrong or post a record of it.<a name=\"fnref3\"></a><a id=\"fnref3\" class=\"footnoteRef\" href=\"#fn3\"><sup>3</sup></a> That will spread awareness of the charities and encourage others to emulate you.</p>\n<p>The game can be as simple as a wager or something more involved:</p>\n<ul>\n<li>Notations exist for <a title=\"Go notation\" href=\"http://stason.org/TULARC/games/go/5-3-Recording-Go-games.html\">go</a> and <a title=\"Chess notation\" href=\"http://en.wikipedia.org/wiki/Algebraic_chess_notation\">chess</a>.</li>\n<li>The <a href=\"http://www.yudkowsky.net/essays/aibox.html\">AI Box Experiment</a> is essentially an arguing game.</li>\n<li>Play anything else you can agree on.<a name=\"fnref4\"></a><a id=\"fnref4\" class=\"footnoteRef\" href=\"#fn4\"><sup>4</sup></a></li>\n</ul>\n<p>We&rsquo;ve already played few games on LessWrong, more on those in a moment.</p>\n<blockquote>\n<p>In most ways the AI problem is enormously more demanding than the personal art of rationality, but in some ways it is actually easier. In the martial art of mind, we need to acquire the realtime procedural skill of pulling the right levers at the right time on a large, pre-existing thinking machine whose innards are not end-user-modifiable.</p>\n<p>&mdash;<a href=\"/lw/gn/the_martial_art_of_rationality/\">The Martial Art of Rationality</a></p>\n</blockquote>\n<p>First, I have a confession to make: I don&rsquo;t really care how much money gets donated to the Singularity Institute, nor am I trying to drum up money for some other cause. I mainly want you all playing games.</p>\n<p>Not just playing them, of course. Playing them here in front of the rest of LessWrong and analyzing the moves in terms of the &ldquo;personal art of rationality.&rdquo;</p>\n<p>We need more approaches to <a title=\"Tsuyoku Naritai\" href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">improvement</a>. Even in <a href=\"/user/Eliezer_Yudkowsky/\">Eliezer_Yudkowsky</a>&rsquo;s <a href=\"/tag/conspiracy_world/\">Bayesian Conspiracy</a> fictional series/manifesto, many other schools of thought (called, for dramatic effect, &ldquo;conspiracies&rdquo;) were present. As I recall, the &ldquo;Competitive Conspiracy&rdquo; was mentioned frequently, but there are other reasons for choosing to start with games.</p>\n<p>Games are fun, of course. They also deal with the &ldquo;personal art&rdquo; of getting familiar with your own brain, which I think has been underrepresented on LW. I do believe there are certain important things we can&rsquo;t learn properly just by reading, arguing, and doing math (valuable as those techniques are). Games are an easy, intuitive first step to filling that gap.</p>\n<p>We&rsquo;ve already had a few games here. <a href=\"/user/Warrigal/\">Warrigal</a> held an <a href=\"/lw/109/the_aumanns_agreement_theorem_game_guess_23_of/\">Aumann&rsquo;s agreement game</a> competition. I created <a href=\"/lw/14i/pract_a_guessing_and_testing_game/\">Pract</a> and <a href=\"/lw/14i/pract_a_guessing_and_testing_game/zyt\">played</a> it with <a href=\"/user/wedrifid/\">wedrifid</a>. Neither one has caught on as a LessWrong pastime, but the comments revealed that people here know many interesting games.</p>\n<p>And there&rsquo;s the donation hook. Some of you believe the world is at stake, so that&rsquo;s a nice motivator.</p>\n<div class=\"footnotes\">\n<hr />\n<ol><a name=\"fn1\"></a>\n<li id=\"fn1\">\n<p>Of course it&rsquo;s not always the case that you have exactly one winner. I suggest that if there&rsquo;s no winner each player give to their own charity, and if there are multiple winners each player splits up their donation evenly among the charities of the winners. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 1\" href=\"#fnref1\">\u21a9</a></p>\n</li>\n<a name=\"fn2\"></a>\n<li id=\"fn2\">\n<p>This is, of course, <a href=\"http://www.spaceandgames.com/?p=27\">not</a> a new idea. I&rsquo;m just suggesting that we adopt it and make it a common practice on LessWrong. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 2\" href=\"#fnref2\">\u21a9</a></p>\n</li>\n<a name=\"fn3\"></a>\n<li id=\"fn3\">\n<p>Unless it contains some remarkable insight, I wouldn&rsquo;t make it a top-level post. The comment area here or in one of the open threads would be good. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 3\" href=\"#fnref3\">\u21a9</a></p>\n</li>\n<a name=\"fn4\"></a>\n<li id=\"fn4\">\n<p>I see mutual consent as an important element of games. <a class=\"footnoteBackLink\" title=\"Jump back to footnote 4\" href=\"#fnref4\">\u21a9</a></p>\n</li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gt2jBeo36288HdXqH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 5.568371132101637e-07, "legacy": true, "legacyId": "2191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4TsSb8N8BBwtNZ2v9", "teaxCFgtmCQ3E9fy8", "DoLQN5ryZ9XkZjq5h", "RPXBjC5DjeXEJRbMw", "JAyz8mEir5mZMTDK7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-29T06:48:27.969Z", "modifiedAt": null, "url": null, "title": "Logical Rudeness", "slug": "logical-rudeness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:00.562Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/srge9MCLHSiwzaX6r/logical-rudeness", "pageUrlRelative": "/posts/srge9MCLHSiwzaX6r/logical-rudeness", "linkUrl": "https://www.lesswrong.com/posts/srge9MCLHSiwzaX6r/logical-rudeness", "postedAtFormatted": "Friday, January 29th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Logical%20Rudeness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALogical%20Rudeness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsrge9MCLHSiwzaX6r%2Flogical-rudeness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Logical%20Rudeness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsrge9MCLHSiwzaX6r%2Flogical-rudeness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fsrge9MCLHSiwzaX6r%2Flogical-rudeness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1074, "htmlBody": "<p>The concept of \"<a href=\"http://www.earlham.edu/~peters/writing/rudeness.htm\">logical rudeness</a>\" (which I'm pretty sure I first found <a href=\"http://pw201.livejournal.com/109921.html\">here</a>, HT) is one that I should write more about, one of these days.&nbsp; One develops a sense of the flow of discourse, the give and take of argument.&nbsp; It's possible to do things that completely derail that flow of discourse without shouting or swearing.&nbsp; These may not be considered offenses against <em>politeness,</em> as our so-called \"civilization\" defines that term.&nbsp; But they are offenses against the cooperative exchange of arguments, or even the rules of engagement with the loyal opposition.&nbsp; They are <em>logically rude.</em></p>\n<p>Suppose, for example, that you're defending X by appealing to Y, and when I seem to be making headway on arguing against Y, you suddenly switch (without having made any concessions) to arguing that it doesn't matter if ~Y because Z still supports X; and when I seem to be making headway on arguing against Z, you suddenly switch to saying that it doesn't matter if ~Z because Y still supports X.&nbsp; This is an example from an actual conversation, with X = \"It's okay for me to claim that I'm going to build AGI in five years yet not put any effort into Friendly AI\", Y = \"All AIs are automatically ethical\", and Z = \"Friendly AI is clearly too hard since SIAI hasn't solved it yet\".</p>\n<p>Even if you never scream or shout, this kind of behavior is rather frustrating for the one who has to talk to you.&nbsp; If we are ever to perform the nigh-impossible task of <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">actually updating on the evidence</a>, we ought to acknowledge when we take a hit; the loyal opposition has earned that much from us, surely, even if we haven't yet conceded.&nbsp; If the one is reluctant to take a single hit, let them further defend the point.&nbsp; Swapping in a new argument?&nbsp; That's <em>frustrating.</em>&nbsp; Swapping back and forth?&nbsp; That's downright <em>logically rude,</em> even if you never raise your voice or interrupt.</p>\n<p>The key metaphor is <em>flow.</em>&nbsp; Consider the notion of \"<a href=\"/lw/it/semantic_stopsigns/\">semantic stopsigns</a>\", words that halt thought.&nbsp; A stop sign is something that happens within the flow of traffic.&nbsp; Swapping back and forth between arguments might seem merely frustrating, or rude, if you take the arguments at face value - if you stay on the object level.&nbsp; If you jump back a level of abstraction and try to sense the flow of traffic, and imagine what sort of traffic signal this corresponds to... well, you wouldn't want to run into a traffic signal like that.<a id=\"more\"></a></p>\n<p>Another form of <em>argumentus interruptus </em>is when the other suddenly weakens their claim, without acknowledging the weakening as a concession.&nbsp; Say, you start out by making very strong claims about a God that answers prayers; but when pressed, you retreat back to talking about an impersonal beauty of the universe, without admitting that anything's changed.&nbsp; If you equivocated back and forth between the two definitions, you would be committing an outright logical fallacy - but even if you don't do so, sticking out your neck, and then quickly withdrawing it before anyone can chop it off, is frustrating; it lures someone into writing careful refutations which you then dance back from with a smile; it is <em>logically rude.</em>&nbsp; In the traffic metaphor, it's like offering someone a green light that turns yellow after half a second and leads into a dead end.</p>\n<p>So, for example, I'm frustrated if I deal with someone who starts out by making vigorous, contestable, argument-worthy claims implying that the Singularity Institute's mission is unnecessary, impossible, futile, or misguided, and then tries to <em>dance back </em>by saying, \"But I still think that what you're doing has a 10% chance of being necessary, which is enough to justify funding your project.\"&nbsp; Okay, but I'm not arguing with you because I'm worried about my funding getting chopped off, I'm arguing with you because I don't think that 10% is the right number.&nbsp; You said something that was worth arguing with, and then responded by disengaging when I pressed the point; and if I go on contesting the 10% figure, you are somewhat injured, and repeat that you think that what I'm doing is important.&nbsp; And not only is the 10% number still worth contesting, but you originally seemed to be coming on a bit more <em>strongly </em>than that, before you named a weaker-sounding number...&nbsp; It might not be an outright logical fallacy - not until you <em>equivocate </em>between strong claims and weak defenses in the course of the same argument - but it still feels a little frustrating over on the receiving end.</p>\n<p>I try <em>not</em> to do this myself.&nbsp; I can't say that arguing with me will always be an enjoyable experience, but I at least endeavor not to be <em>logically rude</em> to the loyal opposition.&nbsp; I stick my neck out so that it can be chopped off if I'm wrong, and when I stick my neck out it stays stuck out, and if I have to withdraw it I'll do so as a visible concession.&nbsp; I may parry - and because I'm human, I may even parry when I shouldn't - but I at least endeavor not to dodge.&nbsp; Where I plant my standard, I have sent an invitation to capture that banner; and I'll stand by that invitation.&nbsp; It's hard enough to count up the balance of arguments without adding fancy dance footwork on top of that.</p>\n<p>An awful lot of how people fail at <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">changing their mind</a> seems to have something to do with changing the subject.&nbsp; It might be difficult to point to an outright logical fallacy, but if we have community standards on logical rudeness, we may be able to organize our cognitive traffic a bit less frustratingly.</p>\n<p><strong>Added</strong>:&nbsp; Checking my notes reminds me to include <a href=\"/lw/wj/is_that_your_true_rejection/\">offering a non-true rejection</a> as a form of logical rudeness.&nbsp; This is where you offer up a reason that isn't really your most important reason, so that, if it's defeated, you'll just switch to something else (which still won't be your most important reason).&nbsp; This is a distinct form of failure from switching Y-&gt;Z-&gt;Y, but it's also frustrating to deal with; not a logical fallacy outright, but a form of <em>logical rudeness.</em>&nbsp; If someone else is going to the trouble to argue with you, then you should offer up your most important reason for rejection first - something that will make a serious dent in your rejection, if cast down - so that they aren't wasting their time.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AADZcNS24mmSfPp2w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "srge9MCLHSiwzaX6r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 77, "baseScore": 96, "extendedScore": null, "score": 0.000157, "legacy": true, "legacyId": "2197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 96, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 207, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FWMfQKG3RpZx6irjm", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-30T02:50:49.369Z", "modifiedAt": null, "url": null, "title": "Complexity of Value \u2260 Complexity of Outcome", "slug": "complexity-of-value-complexity-of-outcome", "viewCount": null, "lastCommentedAt": "2018-12-14T11:52:36.320Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KQvdpPd3k2ap6aJTP/complexity-of-value-complexity-of-outcome", "pageUrlRelative": "/posts/KQvdpPd3k2ap6aJTP/complexity-of-value-complexity-of-outcome", "linkUrl": "https://www.lesswrong.com/posts/KQvdpPd3k2ap6aJTP/complexity-of-value-complexity-of-outcome", "postedAtFormatted": "Saturday, January 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complexity%20of%20Value%20%E2%89%A0%20Complexity%20of%20Outcome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplexity%20of%20Value%20%E2%89%A0%20Complexity%20of%20Outcome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQvdpPd3k2ap6aJTP%2Fcomplexity-of-value-complexity-of-outcome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complexity%20of%20Value%20%E2%89%A0%20Complexity%20of%20Outcome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQvdpPd3k2ap6aJTP%2Fcomplexity-of-value-complexity-of-outcome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKQvdpPd3k2ap6aJTP%2Fcomplexity-of-value-complexity-of-outcome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 804, "htmlBody": "<p>Complexity of value is the thesis that our preferences, the things we care about, <em>don't</em> compress down to one simple rule, or a few simple rules. To review why it's important (by quoting from the <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">wiki</a>):</p>\n<ul>\n<li><span class=\"mw-redirect\">Caricatures</span> of rationalists often have them moved by artificially simplified values - for example, only caring about personal pleasure. This becomes a template for arguing against rationality: X is valuable, but rationality says to only care about Y, in which case we could not value X, therefore do not be rational. </li>\n<li>Underestimating the complexity of value leads to underestimating the difficulty of <a class=\"mw-redirect\" title=\"Friendly AI\" href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\">Friendly AI</a>; and there are notable cognitive biases and fallacies which lead people to underestimate this complexity. </li>\n</ul>\n<p>I certainly agree with both of these points. But I worry that we (at Less Wrong) might have swung a bit too far in the other direction. No, I don't think that we overestimate the complexity of our values, but rather there's a tendency to assume that complexity of value must lead to complexity of outcome, that is, agents who faithfully inherit the full complexity of human values will necessarily create a future that reflects that complexity. I will argue that it is possible for complex values to lead to simple futures, and explain the relevance of this possibility to the project of Friendly AI.</p>\n<p><a id=\"more\"></a>The easiest way to make my argument is to start by considering a hypothetical alien with all of the values of a typical human being, but also an extra one. His fondest desire is to fill the universe with orgasmium, which he considers to have orders of magnitude more utility than realizing any of his other goals. As long as his dominant goal remains infeasible, he's largely indistinguishable from a normal human being. But if he happens to pass his values on to a superintelligent AI, the future of the universe will turn out to be rather simple, despite those values being no less complex than any human's.</p>\n<p>The above possibility is easy to reason about, but perhaps does not appear very relevant to our actual situation. I think that it <em>may</em> be, and here's why. All of us have many different values that do not reduce to each other, but most of those values do not appear to scale very well with available resources. In other words, among our manifold desires, there may only be a few that are not easily satiated when we have access to the resources of an entire galaxy or universe. If so, (and assuming we aren't wiped out by an existential risk or fall into a Malthusian scenario) the future of our universe will be shaped largely by those values that do scale. (I should point out that in this case the universe won't necessarily turn out to be mostly simple. Simple values do not necessarily lead to simple outcomes either.)</p>\n<p>Now if we were rational agents who had perfect knowledge of our own preferences, then we would already know whether this is the case or not. And if it is, we ought to be able to visualize what the future of the universe will look like, if we had the power to shape it according to our desires. But I find myself <a href=\"/lw/1ns/value_uncertainty_and_the_singleton_scenario/\">uncertain</a> on both questions. Still, I think this possibility is worth investigating further. If it <em>were</em> the case that only a few of our values scale, then we can potentially obtain almost all that we desire by creating a superintelligence with just those values. And perhaps this can be done manually, bypassing an automated preference <a href=\"http://causalityrelay.wordpress.com/2009/09/19/the-project-of-friendly-ai/\">extraction</a> or <a href=\"http://intelligence.org/upload/CEV.html\">extrapolation</a> process with their associated <a href=\"/lw/1s4/open_thread_february_2010_part_2/1mun\">difficulties and dangers</a>. (To head off a potential objection, this does assume that our values interact in an additive way. If there are values that don't scale but interact nonlinearly (multiplicatively, for example) with values that do scale, then those would need to be included as well.)</p>\n<div>Whether or not we actually should take this approach would depend on the outcome of such an investigation. Just how much of our desires can feasibly be obtain this way? And how does the loss of value inherent in this approach compare with the expected loss of value due to the potential of errors in the extraction/extrapolation process? These are questions worth trying to answer before committing to any particular path, I think.</div>\n<div>P.S., I hesitated a bit in posting this, because underestimating the complexity of human values is arguably a greater danger than overlooking the possibility that I point out here, and this post could conceivably be used by someone to rationalize sticking with their \"<a href=\"/lw/lq/fake_utility_functions/\">One Great Moral Principle</a>\". But I guess those tempted to do so will tend not to be Less Wrong readers, and seeing how I already got myself&nbsp;<a href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/\">sucked into this debate</a>, I might as well clarify and expand on my position.</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"R6uagTfhhBeejGrrf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KQvdpPd3k2ap6aJTP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 60, "extendedScore": null, "score": 0.000101, "legacy": true, "legacyId": "2179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 60, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 232, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qh6bnkxbMFz5SNeFd", "NnohDYHNnKDtbiMyp", "3iM8QjvdkPCyLRJM6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-30T17:58:43.881Z", "modifiedAt": null, "url": null, "title": "Deontology for Consequentialists", "slug": "deontology-for-consequentialists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:26:06.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yppdL4EXLWda5Wthn/deontology-for-consequentialists", "pageUrlRelative": "/posts/yppdL4EXLWda5Wthn/deontology-for-consequentialists", "linkUrl": "https://www.lesswrong.com/posts/yppdL4EXLWda5Wthn/deontology-for-consequentialists", "postedAtFormatted": "Saturday, January 30th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Deontology%20for%20Consequentialists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADeontology%20for%20Consequentialists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyppdL4EXLWda5Wthn%2Fdeontology-for-consequentialists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Deontology%20for%20Consequentialists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyppdL4EXLWda5Wthn%2Fdeontology-for-consequentialists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyppdL4EXLWda5Wthn%2Fdeontology-for-consequentialists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1718, "htmlBody": "<p><em>Consequentialists see morality through consequence-colored lenses.&nbsp; I attempt to prise apart the two concepts to help consequentialists understand what deontologists are talking about.</em></p>\n<p>Consequentialism<sup>1</sup> is built around a group of variations on the following basic assumption:</p>\n<ul>\n<li>The rightness of something depends on what happens subsequently.</li>\n</ul>\n<p>It's a very diverse family of theories; see the <a href=\"http://plato.stanford.edu/entries/consequentialism/\">Stanford Encyclopedia of Philosophy article</a>.&nbsp; \"Classic utilitarianism\" could go by the longer, more descriptive name \"actual direct maximizing aggregative total universal equal-consideration agent-neutral hedonic act<sup>2</sup> consequentialism\".&nbsp; I could even mention less frequently contested features, like the fact that this type of consequentialism doesn't have a temporal priority feature or side constraints.&nbsp; All of this is is a very complicated bag of tricks for a theory whose proponents sometimes claim to like it because it's sleek and pretty and \"simple\".&nbsp; But the bottom line is, to get a consequentialist theory, something that happens after the act you judge is the basis of your judgment.</p>\n<p>To understand deontology as anything but a twisted, inexplicable mockery of consequentialism, you must discard this assumption.</p>\n<p>Deontology relies on things that do <em>not</em> happen after the act judged to judge the act.&nbsp; This leaves facts about times prior to and the time during the act to determine whether the act is right or wrong.&nbsp; This may include, but is not limited to:</p>\n<ul>\n<li>The agent's epistemic state, either actual or ideal (e.g. <em>thinking</em> that some act would have a certain result, or being in a position such that it would be <em>reasonable </em>to think that the act would have that result)</li>\n<li>The reference class of the act (e.g. it being an act of murder, theft, lying, etc.)</li>\n<li>Historical facts (e.g. having made a promise, sworn a vow)</li>\n<li>Counterfactuals (e.g. what <em>would</em> happen if others performed similar acts more frequently than they actually do)</li>\n<li>Features of the people affected by the act (e.g. moral rights, preferences, relationship to the agent)</li>\n<li>The agent's intentions (e.g. meaning well or maliciously, or acting deliberately or accidentally)<a id=\"more\"></a></li>\n</ul>\n<p>Individual deontological theories will have different profiles, just like different consequentialist theories.&nbsp; And some of the theories you can generate using the criteria above have overlap with some consequentialist theories<sup>3</sup>.&nbsp; The ultimate \"overlap\", of course, is the \"consequentialist doppelganger\", which applies the following transformation to some non-consequentialist theory X:</p>\n<ol>\n<li>What would the world look like if I followed theory X?</li>\n<li>You ought to act in such a way as to bring about the result of step 1.</li>\n</ol>\n<p>And this cobbled-together theory will be extensionally equivalent to X: that is, it will tell you \"yes\" to the same acts and \"no\" to the same acts as X.</p>\n<p>But extensional definitions are terribly unsatisfactory.&nbsp; Suppose<sup>4</sup> that as a matter of biological fact, every vertebrate is also a renate and vice versa (that all and only creatures with spines have kidneys).&nbsp; You can then <em>extensionally define</em> \"renate\" as \"has a spinal column\", because only creatures with spinal columns are in fact renates, and no creatures with spinal columns are in fact non-renates.&nbsp; The two terms will tell you \"yes\" to the same creatures and \"no\" to the same creatures.</p>\n<p>But what \"renate\" means <em>intensionally</em> has to do with kidneys, not spines.&nbsp; To try to capture renate-hood with vertebrate-hood is to miss the point of renate-hood in favor of being able to interpret everything in terms of a pet spine-related theory.&nbsp; To try to capture a non-consequentialism with a doppelganger commits the same sin.&nbsp; A rabbit is not a renate <em>because</em> it has a spine, and an act is not deontologically permitted <em>because</em> it brings about a particular consequence.</p>\n<p>If a deontologist says \"lying is wrong\", and you mentally add something that sounds like \"because my utility function has a term in it for the people around believing accurate things.&nbsp; Lying tends to decrease the extent to which they do so, but if I knew that somebody would believe the opposite of whatever I said, then to maximize the extent to which they believed true things, I would have to lie to them.&nbsp; And I would also have to lie if some other, greater term in my utility function were at stake and I could only salvage it with a lie.&nbsp; But in practice the best I can do is to maximize my <em>expected</em> utility, and as a matter of fact I will never be as sure that lying is right as I'd need to be for it to be a good bet.\"<sup>5</sup>... you, my friend, have missed the point.&nbsp; The deontologist <em>wasn't thinking any of those things.</em>&nbsp; The deontologist might have been thinking \"because people have a right to the truth\", or \"because I swore an oath to be honest\", or \"because lying is on a magical list of things that I'm not supposed to do\", or heck, \"because the voices in my head told me not to\"<sup>6</sup>.</p>\n<p>But the deontologist <em>is not</em> thinking anything with the terms \"utility function\", and <em>probably</em> isn't thinking of extreme cases unless otherwise specified, and might not care whether anybody will believe the words of the hypothetical lie or not, and might hold to the prohibition against lying though the world burn around them for want of a fib.&nbsp; And if you take one of these deontic reasons, and mess with it a bit, you can be wrong in a new and exciting way: \"because the voices in my head told me not to, and if I disobey the voices, they will blow up Santa's workshop, which would be bad\" has crossed into consequentialist territory.&nbsp; (Nota bene: Adding another bit - say, \"and I promised the reindeer I wouldn't do anything that would get them blown up\" - can push this flight of fancy back into deontology again.&nbsp; And then you can put it back under consequentialism again: \"and if I break my promise, the vengeful spirits of the reindeer will haunt me, and that would make me miserable.\")&nbsp; The voices' instruction \"happened\" <em>before</em> the prospective act of lying.&nbsp; The explosion at the North Pole is a <em>subsequent</em> potential event.&nbsp; The promise to the reindeer is in the <em>past</em>.&nbsp; The vengeful haunting comes up <em>later</em>.</p>\n<p>A confusion crops up when one considers forms of deontology where the agent's epistemic state - real<sup>7</sup> or ideal<sup>8</sup> - is a factor.&nbsp; It may start to look like the moral agent is in fact acting to achieve some post-action state of affairs, rather than in response to a pre-action something that has moral weight.&nbsp; It may even look like that to the agent.&nbsp; Per footnote 3, I'm ignoring expected utility \"consequentialist\" theories; however, in actual practice, the closest one can come to implementing an actual utility consequentialism is to deal with expected utility, because we cannot perfectly predict the effects of our actions.</p>\n<p>The difference is subtle, and how it gets implemented depends on one's epistemological views.&nbsp; Loosely, however: Suppose a deontologist judges some act X (to be performed by another agent) to be wrong because she predicts undesirable consequence Y.&nbsp; The consequentialist sitting next to her judges X to be wrong, too, because he also predicts Y if the agent performs the act.&nbsp; His assessment stops with \"Y will happen if the agent performs X, and Y is axiologically bad.\"&nbsp; (The evaluation of Y as axiologically bad might be more complicated, but this all that goes into evaluating X qua X.)&nbsp; Her assessment, on the other hand, is more complicated, and can branch in a few places.&nbsp; Does the agent know that X will lead to Y?&nbsp; If so, the wrongness of X might hinge on the agent's intention to bring about Y, or an obligation from another source on the agent's part to try to avoid Y which is shirked by performing X in knowledge of its consequences.&nbsp; If not, then another option is that the agent <em>should</em> (for other, also deontic reasons) know that X will bring about Y: the ignorance of this fact itself renders the agent culpable, which makes the agent responsible for ill effects of acts performed under that specter of ill-informedness.</p>\n<p>&nbsp;</p>\n<p><sup>1</sup>Having taken a course on weird forms of consequentialism, I now compulsively caveat anything I have to say about consequentialisms in general.&nbsp; I apologize.&nbsp; In practice, \"consequentialism\" is the sort of word that one has to learn by familiarity rather than definition, because any definition will tend to leave out something that most people think is a consequentialism.&nbsp; \"Utilitarianism\" is a type of consequentialism that talks about utility (variously defined) instead of some other sort of consequence.</p>\n<p><sup>2</sup>Because it makes it dreadfully hard to write readably about consequentialism if I don't assume I'm only talking about act consequentialisms, I will only talk about act consequentialisms.&nbsp; Transforming my explanations into rule consequentialisms or world consequentialisms or whatever other non-act consequentialisms you like is left as an exercise to the reader.&nbsp; I also know that preferentism is more popular than hedonism around here, but hedonism is easier to quantify for ready reference, so if called for I will make hedonic rather than preferentist references.</p>\n<p><sup>3</sup>Most notable in the overlap department is expected utility \"consequentialism\", which says that not only is the best you can in fact do to maximize expected utility, but that is also what you absolutely ought to do.&nbsp; Depending on how one cashes this out and who one asks, this may overlap so far as to not be a real form of consequentialism at all.&nbsp; I will be ignoring expected utility consequentialisms for this reason.</p>\n<p><sup>4</sup>I say \"suppose\", but in fact the supposition may be actually true; Wikipedia is unclear.</p>\n<p><sup>5</sup>This is not intended to be a real model of anyone's consequentialist caveats.&nbsp; But basically, if you interpret the deontologist's statement \"lying is wrong\" to have something to do with what happens after one tells a lie, you've got it wrong.</p>\n<p><sup>6</sup>As far as I know, no one seriously endorses \"schizophrenic deontology\".&nbsp; I introduce it as a <em>caricature</em> of deontology that I can play with freely without having to worry about accurately representing someone's real views.&nbsp; Please do not take it to be representative of deontic theories in general.</p>\n<p><sup>7</sup>Real epistemic state means the beliefs that the agent actually has and can in fact act on.</p>\n<p><sup>8</sup>Ideal epistemic state (for my purposes) means the beliefs that the agent <em>would</em> have and act on if (s)he'd demonstrated appropriate epistemic virtues, whether (s)he actually has or not.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yeJFqsWrP2pjYfNEr": 2, "ZTRNmvQGgoYiymYnq": 2, "HAFdXkW4YW4KRe2Gx": 2, "Z8wZZLeLMJ3NSK7kR": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yppdL4EXLWda5Wthn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 68, "baseScore": 55, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "2176", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 55, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 255, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-31T18:56:54.557Z", "modifiedAt": null, "url": null, "title": "Pascal's Pyramid Scheme", "slug": "pascal-s-pyramid-scheme", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:14.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "patrissimo", "createdAt": "2009-03-01T21:01:34.487Z", "isAdmin": false, "displayName": "patrissimo"}, "userId": "jimxrRCsNY7PfcM6e", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QJYkzqnkRdfsL29Hc/pascal-s-pyramid-scheme", "pageUrlRelative": "/posts/QJYkzqnkRdfsL29Hc/pascal-s-pyramid-scheme", "linkUrl": "https://www.lesswrong.com/posts/QJYkzqnkRdfsL29Hc/pascal-s-pyramid-scheme", "postedAtFormatted": "Sunday, January 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pascal's%20Pyramid%20Scheme&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APascal's%20Pyramid%20Scheme%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJYkzqnkRdfsL29Hc%2Fpascal-s-pyramid-scheme%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pascal's%20Pyramid%20Scheme%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJYkzqnkRdfsL29Hc%2Fpascal-s-pyramid-scheme", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQJYkzqnkRdfsL29Hc%2Fpascal-s-pyramid-scheme", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 968, "htmlBody": "<p><em>Here's a little Sunday irreverence. &nbsp;Someone else has probably written this story before, and I'm sure the points have been made many times, but it popped into my head when I woke up and I thought it might be fun to write it out.</em></p>\n<p>&nbsp;</p>\n<p>Last week I was walkin' along mindin' my own business when I met a Christian Minister, who asked me if I'd accepted Jesus as my Lord and Personal Saviour. &nbsp;\"Why I sure think so\", I responded, \"But...what was that name again?\". &nbsp;\"Why, Jesus!\" he answered, and began to launch into an account of this man's fascinatin' historical doin's, when I interrupted him.</p>\n<p>\"Funny you should mention it\", I replied. &nbsp;\"I do accept as my Lord and Personal Saviour a man who was born of the blessed Virgin Mary in Bethlehem long ago, and was the Son of God, but we call him Schmesus.\"</p>\n<p>The poor man choked and started turnin' a little red, and warned me in menacing tones that lest I accepted his JESUS, I would burn forever in the fire and brimstone of Hell. &nbsp;\"For sure!\", said I, \"We Schmistians know ALL about Hell. &nbsp;After all, we use your same holy text, only we call it the Schmible. &nbsp;It's got all the same books of Genesis an' Paul an' all that, with all the same verses. &nbsp;There's just one key difference which makes us Schmistians prefer our religion to yours.\"</p>\n<p>\"What's that?\", he spluttered.<a id=\"more\"></a></p>\n<p>\"Well, we have an extra book, called PATCH, which was discovered in a clay jar inside a 1600-year old commode by our prophet and founder during a school trip to the Holy Lands. &nbsp;It contains only a few lines. &nbsp;The first asserts the absolute truth of the Bible as the Word of God. &nbsp;You agree with that, right?\"</p>\n<p>\"Why, of course!\", the man replied.</p>\n<p>\"Then you're well on your way to bein' a Schmistian already! &nbsp;There's just a little more. &nbsp;The second line describes the holiness of the sound \"Schm\", and the importance of using it when describin' sacred things like Schmesus. &nbsp;The third explains how easy it is to convert from Christianity and believin' in the Bible to Schmistianity and believin' in the Bible plus this little 'ol PATCH, via a simple ritual. &nbsp;And the fourth states quite clearly that those who follow the Schmible and are true Schmistians, forsakin' all other false prophets who forsake PATCH, have a three-fold chance of makin' it to heaven' instead of to hell. &nbsp;So ye can preach all ye want, my friend, but I'm playin' the odds\".</p>\n<p>The man was clearly confused and yet struck by the sense of what I was sayin. &nbsp;\"Do ye mean\", said he, \"That you believe in the Gospel of Mark, of Exodus, you believe that God so loved the world that he gave his one and only son, an' all of that just like we do?\" &nbsp;\"For sure!\", said I. &nbsp;\"Like I said, we believe in the whole of your Bible, we just add the book of PATCH. &nbsp;Really, there's no reason not to convert, after all the Bible don't say nothin' bad about the book of PATCH or that there can't be no future additional words of God. &nbsp;In fact, the addition of the New Testament to the Old makes it clear that extra wisdom will come down on occasion from on high. &nbsp;Since we worship the same God as you, there ain't no conflict. &nbsp;It's what you might call a dominatin' play to go through the conversion ritual 'n become a Schmistian - no downside at all!\"</p>\n<p>Bein' much taken by the idea of triplin' his chance of gettin' into heaven, the man inquired further about what this ritual entailed. &nbsp;\"It's very simple\", says I. &nbsp;\"In the original Bible, God requests a tithe of ten percent per annum to the church. &nbsp;The book of PATCH explains how this is a misunderstanding, garbled by the selfish hand of man. &nbsp;First, the tithe is a mere one-time event. &nbsp;And second, it goes not to the church, but is instead divided up among all your fellow Schmistians, in proportion to their own initial tithes.\"</p>\n<p>\"Why, that does sound superior\", the man said, quite struck by the concept. &nbsp;\"Not only is it a one-time tithe, but I may recover the fee through the tithes of later converts, or even profit thereof\". &nbsp;\"Indeed!\" said I, \"a clever observation indeed, and there you have the great holiness of our method. &nbsp;Why, my own initial tithe has come back many times over, and all while following the Holy Schmible and livin' much the same as any Christian man like yourself lives. &nbsp;We ask you not to reject or dismiss any of the book by which you've lived your long and fulfilling life, but merely to accept this little addition of PATCH, whose holy 'n historical provenance we will happily demonstrate to you upon receipt of your entry tithe. &nbsp;In return you get a share in the tithes of future converts as well as a three-fold chance of eternal SALvation 'stead of eternal DAMNnation. &nbsp;Way I see it, a man 'ud have to be a fool not to do it!\"</p>\n<p>We exchanged cards and agreed I'd see him next Sunday, and I headed on,&nbsp;happy at havin' generated a good sales lead for our Schmurch, whose motto is <em><a href=\"http://www.youtube.com/watch?v=y-AXTx4PcKI\">alwaysway ebay osingclay</a></em>.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I know there's a lotta people out there all down on religion. &nbsp;But I think they got the wrong perspective. &nbsp;Spinnin' tall tales is fun, dontcha know, and ain't no better place to do it than in a field based purely on tall tales and old stories and the hair-raisin' adventures of long dead mystical heroes. &nbsp;P'raps we should restrict our arguifying on facts and experiments to those fields based on facts and experimentin, and respond to myth merely with meta-myth, to pernicious memes with schmirnicious memes, as they deserve.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "hNFdS3rRiYgqqD8aM": 3, "etDohXtBrXd8WqCtR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QJYkzqnkRdfsL29Hc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 12, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "2215", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-01-31T20:20:47.159Z", "modifiedAt": null, "url": null, "title": "Strong moral realism, meta-ethics and pseudo-questions. ", "slug": "strong-moral-realism-meta-ethics-and-pseudo-questions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:32.708Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/up9bmk9CMoJYwTukL/strong-moral-realism-meta-ethics-and-pseudo-questions", "pageUrlRelative": "/posts/up9bmk9CMoJYwTukL/strong-moral-realism-meta-ethics-and-pseudo-questions", "linkUrl": "https://www.lesswrong.com/posts/up9bmk9CMoJYwTukL/strong-moral-realism-meta-ethics-and-pseudo-questions", "postedAtFormatted": "Sunday, January 31st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Strong%20moral%20realism%2C%20meta-ethics%20and%20pseudo-questions.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStrong%20moral%20realism%2C%20meta-ethics%20and%20pseudo-questions.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fup9bmk9CMoJYwTukL%2Fstrong-moral-realism-meta-ethics-and-pseudo-questions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Strong%20moral%20realism%2C%20meta-ethics%20and%20pseudo-questions.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fup9bmk9CMoJYwTukL%2Fstrong-moral-realism-meta-ethics-and-pseudo-questions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fup9bmk9CMoJYwTukL%2Fstrong-moral-realism-meta-ethics-and-pseudo-questions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 707, "htmlBody": "<p>On Wei_Dai's complexity of values post, Toby Ord writes:</p>\n<blockquote>\n<p>There are a lot of posts here that presuppose some combination of moral anti-realism and value complexity. These views go together well: if value is not fundamental, but dependent on characteristics of humans, then it can derive complexity from this and not suffer due to Occam's Razor.</p>\n<p>There are another pair of views that go together well: moral realism and value simplicity. Many posts here strongly dismiss these views, effectively allocating near-zero probability to them. I want to point out that this is a case of non-experts being very much at odds with expert opinion and being clearly overconfident. In the Phil Papers survey for example, 56.3% of philosophers lean towards or believe realism, while only 27.7% lean towards or accept anti-realism.</p>\n</blockquote>\n<p>The kind of moral realist positions that apply Occam's razor to moral beliefs are a lot more extreme than most philosophers in the cited survey would sign up to, methinks. One such position that I used to have some degree of belief in is:</p>\n<p><em>Strong Moral Realism: All (or perhaps just almost all) beings, human, alien or AI, when given sufficient computing power and the ability to learn science and get an accurate map-territory morphism, will agree on what physical state the universe ought to be transformed into, and therefore they will assist you in transforming it into this state.</em></p>\n<p>But most modern philosophers who call themselves \"realists\" don't mean anything nearly this strong. They mean that that there are moral \"facts\", for varying definitions of \"fact\" that typically fade away into meaninglessness on closer examination, and actually make the same empirical predictions as antirealism.</p>\n<p><a id=\"more\"></a></p>\n<p>Suppose you take up Eliezer's \"realist\" position. Arrangements of spacetime, matter and energy can be \"good\" in the sense that Eliezer has a \"long-list\" style definition of goodness up his sleeve, one that decides even contested object-level moral questions like whether abortion should be allowed or not, and then tests any arrangement of spacetime, matter and energy and notes to what extent it fits the criteria in Eliezer's long list, and then decrees goodness or not (possibly with a scalar rather than binary value).</p>\n<p>This kind of \"moral realism\" behaves, to all extents and purposes, like antirealism.</p>\n<ul>\n<li>You don't favor shorter long-list definitions of goodness over longer ones. The criteria for choosing the list have little to do with its length, and more with what a human brain emulation with such-and-such modifications to make it believe only and all relevant true empirical facts would decide once it had reached reflective moral equilibrium.</li>\n<li>Agents who have a different \"long list\" definition cannot be moved by the fact that you've declared your particular long list \"true goodness\".</li>\n<li>There would be no reason to expect alien races to have discovered the same&nbsp;long list defining \"true goodness\" as you.</li>\n<li>An alien with a different \"long list\" than you, upon learning the causal reasons for the particular long list you have, is not going to change their long list to be more like yours.</li>\n<li>You don't need to use probabilities and update your long list in response to evidence, quite the opposite, you want it to remain unchanged.&nbsp;</li>\n</ul>\n<p>I might compare the situation to Eliezer's&nbsp;<a href=\"/lw/nq/feel_the_meaning/\">blegg post</a>: it may be that moral&nbsp;philosophers&nbsp;have a mental category for \"fact\" that seems to be allowed to have a value even once all of the&nbsp;empirically&nbsp;grounded surrounding concepts have been fixed. These might be concepts such as \"would aliens also think this thing?\", \"Can it be discovered by an independent agent who hasn't communicated with you?\", \"Do we apply Occam's razor?\", etc.</p>\n<p align=\"center\"><img src=\"http://images.lesswrong.com/t3_1pi_0.png?v=f3f3484c4c8458384082fa2bca20960d\" alt=\"\" width=\"478\" height=\"407\" /></p>\n<p>Moral beliefs might work better when they have a Grand&nbsp;Badge Of Authority&nbsp;attached to them. Once all the empirically falsifiable candidates for the&nbsp;Grand&nbsp;Badge Of Authority have been falsified, the only one left is the ungrounded category marker itself, and some people like to stick this on their object level morals and call themselves \"realists\".</p>\n<p>Personally, I prefer to call a spade a spade, but I don't want to get into an argument about the value of an ungrounded category marker. Suffice it to say that for any practical matter, the only parts of the map we should argue about are parts that map-onto a part of the territory.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "up9bmk9CMoJYwTukL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 25, "extendedScore": null, "score": 9e-06, "legacy": true, "legacyId": "2214", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 181, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dMCFk2n2ur8n62hqB"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-01T06:09:38.982Z", "modifiedAt": null, "url": null, "title": "Open Thread: February 2010", "slug": "open-thread-february-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:59.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6BdYkcctkzwehcJuD/open-thread-february-2010", "pageUrlRelative": "/posts/6BdYkcctkzwehcJuD/open-thread-february-2010", "linkUrl": "https://www.lesswrong.com/posts/6BdYkcctkzwehcJuD/open-thread-february-2010", "postedAtFormatted": "Monday, February 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20February%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20February%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BdYkcctkzwehcJuD%2Fopen-thread-february-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20February%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BdYkcctkzwehcJuD%2Fopen-thread-february-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6BdYkcctkzwehcJuD%2Fopen-thread-february-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 55, "htmlBody": "<p>Where are the new monthly threads when I need them? A pox on the +11 EDT zone!</p>\n<p><em>This thread is for the discussion of Less Wrong topics that have not appeared in recent posts. If a discussion gets unwieldy, celebrate by turning it into a top-level post.</em></p>\n<p>If you're new to Less Wrong, check out&nbsp;<a href=\"/lw/b9/welcome_to_less_wrong/\">this welcome post</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6BdYkcctkzwehcJuD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 1, "extendedScore": null, "score": 5.577476540613558e-07, "legacy": true, "legacyId": "2221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 756, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CG9AEXwSjdrXPBEZ9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-01T06:39:35.541Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes: February 2010", "slug": "rationality-quotes-february-2010", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:40.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wedrifid", "createdAt": "2009-07-04T22:18:20.822Z", "isAdmin": false, "displayName": "wedrifid"}, "userId": "FqKohKFRCZnbfbbcS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u2fpREphy5zRExPLB/rationality-quotes-february-2010", "pageUrlRelative": "/posts/u2fpREphy5zRExPLB/rationality-quotes-february-2010", "linkUrl": "https://www.lesswrong.com/posts/u2fpREphy5zRExPLB/rationality-quotes-february-2010", "postedAtFormatted": "Monday, February 1st 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%3A%20February%202010&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%3A%20February%202010%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2fpREphy5zRExPLB%2Frationality-quotes-february-2010%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%3A%20February%202010%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2fpREphy5zRExPLB%2Frationality-quotes-february-2010", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu2fpREphy5zRExPLB%2Frationality-quotes-february-2010", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>A monthly thread for posting rationality-related quotes you've seen recently (or had stored in your quotesfile for ages).</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp; (If they are strongly related, reply to your own comments.&nbsp; If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.<a id=\"more\"></a></li>\n</ul>\n<p>ETA: It would seem that rationality quotes are no longer desired. After several days this thread stands voted into the negatives. Wolud whoever chose to to downvote this below 0 would care to express their&nbsp;disapproval&nbsp;of the regular quotes tradition more explicitly? Or perhaps they may like to <a href=\"/user/wedrifid\">browse around</a> for some alternative posts that they could downvote instead of this one? Or, since we're in the business of quotation,&nbsp;they could \"<a href=\"http://www.goodreads.com/book/show/6250169.Unseen_Academicals\">come on if they think they're hard enough!</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u2fpREphy5zRExPLB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 1, "extendedScore": null, "score": 5.577531623936737e-07, "legacy": true, "legacyId": "2222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 338, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-02T04:40:23.182Z", "modifiedAt": null, "url": null, "title": "Debunking komponisto on Amanda Knox (long)", "slug": "debunking-komponisto-on-amanda-knox-long", "viewCount": null, "lastCommentedAt": "2017-06-17T03:59:35.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rolf_nelson", "createdAt": "2009-05-04T06:45:38.755Z", "isAdmin": false, "displayName": "rolf_nelson"}, "userId": "f3Zv5xwitedtu7LaH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HSC9AXARA65qDPNBm/debunking-komponisto-on-amanda-knox-long", "pageUrlRelative": "/posts/HSC9AXARA65qDPNBm/debunking-komponisto-on-amanda-knox-long", "linkUrl": "https://www.lesswrong.com/posts/HSC9AXARA65qDPNBm/debunking-komponisto-on-amanda-knox-long", "postedAtFormatted": "Tuesday, February 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Debunking%20komponisto%20on%20Amanda%20Knox%20(long)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADebunking%20komponisto%20on%20Amanda%20Knox%20(long)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHSC9AXARA65qDPNBm%2Fdebunking-komponisto-on-amanda-knox-long%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Debunking%20komponisto%20on%20Amanda%20Knox%20(long)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHSC9AXARA65qDPNBm%2Fdebunking-komponisto-on-amanda-knox-long", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHSC9AXARA65qDPNBm%2Fdebunking-komponisto-on-amanda-knox-long", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1299, "htmlBody": "<p>Rebuttal to: <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">The Amanda Knox Test</a></p>\n<p>If you don't care about Amanda Knox's guilt, or whether you have received unreliable information on the subject from komponisto's post, stop reading now.</p>\n<p>[Edit: Let me note that, generally, I agree that discussion of current events should be discouraged in this site. It is only because \"The Amanda Knox Test\" was a featured post on this site that I claim this rebuttal of that post to be on-topic for this site.]</p>\n<p>I shall here make the following claim:<br /><br />C1. komponisto's post on Amanda Knox was misleading.<br /><br />I could, additionally, choose to make the following claims:<br /><br />C2. Amanda Knox is guilty of murder.<br />C3. The prosecution succeeded in proving Amanda's guilt beyond a reasonable doubt<br />C4. Amanda Knox received a fair trial<br /><br />I believe claims C2 through C4 are also true; however, time constraints prevent me from laying out the cases and debating them with every single human being on the Internet, so I shall merely focus on C1. (That said, I would be willing to debate komponisto on C2, since I am curious whether I could get him to change his mind on the subject.)<br /><a id=\"more\"></a><br />To back up C1, I shall quote the following paragraph from komponisto's post, and show that this paragraph alone contains at least four misleading statements. My belief is that komponisto merely accepted propaganda from the Friends of Amanda (FoA) at face value, even though most of their claims are incorrect. Unlike komponisto and FoA, I shall cite reliable sources for my claims.<br /><br />\"After the murder, Kercher's bedroom was filled with evidence of Gu&eacute;d&eacute;'s presence; his DNA was found not only on top of but actually inside her body. That's about as close to the crime as it gets. At the same time, no remotely similarly incriminating genetic material was found from anyone else -- in particular, there were no traces of the presence of either Amanda Knox or Raffaele Sollecito in the room (and no, the supposed Sollecito DNA on Meredith's bra clasp just plain does not count -- nor, while we're at it, do the 100 picograms [about one human cell's worth] of DNA from Meredith allegedly on the tip of a knife handled by Knox, found at Sollecito's apartment after the two were already suspects; these two things constituting so far as I know the entirety of the physical \"evidence\" against the couple)\" -komponisto<br /><br />Here are the four misleading statements I found:<br /><br />1. \"[H]is DNA was found not only on top of but actually inside her body... no remotely similarly incriminating genetic material was found from anyone else\" -komponsito<br /><br />Guede's dna was, indeed, found on the right side of her bra, on the left cuff of her jumper, and inside Meredith's body, as well as in other places around the house.<br /><br />Raffaele's DNA was found in <a href=\"http://abcnews.go.com/International/story?id=7656872&amp;page=1\">only two places in the house</a>: a cigarette butt, and on Meredith's torn-off bra clasp. (Contrary to FoA propaganda, the clasp did not contain DNA from an additional \"three unidentified people\"). This should help you understand that DNA does not voluminously and constantly spew forth from humans in the way komponisto believes it does. (That said, there might have been more traces of their DNA had Raffaele and Amanda not cleaned the apartment the morning after the murder. Part of the reason Guede's DNA is more widespread is because Raffaele and Amanda focused on cleaning up evidence pointing to themselves, and did not have a reason to care about evidence pointing to Guede.)<br /><br />Amanda's DNA was found on the handle of a certain knife in Raffaele's apartment, which Meredith had never visited. The knife blade had been recently cleaned with bleach (<a href=\"http://www.slate.com/id/2178383/\">which destroys DNA</a>), <a href=\"http://abcnews.go.com/International/story?id=7656872&amp;page=1\">but hiding in a groove near the tip of the blade that the bleach failed to scrub was a sample of Meredith's DNA</a>. The blade matched one of the two knives used to kill Meredith. When later questioned about it, Raffaele first claimed that Meredith had visited his apartment and cut herself on that particular blade. Unfortunately Raffaele was not able to back up this dubious claim that Meredith had ever visited Raffaele's apartment.<br /><br />Amanda's DNA was also found, mixed with Meredith's DNA, in at least four blood spots across the apartment. <a href=\"http://www.newsweek.com/id/216903/page/2\">One of the blood spots was in the third roommate's (Filomena's) bedroom</a>, where the staged break-in took place. Even if, like komponisto, you bizarrely believe that DNA just gets <em>everywhere</em>, it's hard to explain why Amanda's DNA is mixed into that final spot of blood and why Filomena's DNA is nowhere to be seen in that blood spot, despite its being in her own bedroom. (Nor, tellingly, is Guede's DNA mixed in with those blood spots, despite the defense's insistence that he acted alone.)<br /><br />2. \"Supposed\" Sollecito DNA? There is no meaningful controversy over whether Sollecito's DNA is on the bra clasp.<br /><br />3. A bit of a nit: Meredith's DNA on the knife is more than one human cell's worth. The amount is not terribly relevant though to a Bayesian; there's no law of nature that states that, when any DNA sample gets sufficiently small, it suddenly starts to mutate to look exactly like Meredith Kercher's DNA.<br /><br />4. \"[T]hese two things constituting so far as I know the entirety of the physical \"evidence\" against the couple...\" -komponsito<br /><br />Here is additional physical evidence (a non-exhaustive list):<br /><br />* As mentioned, blood stains with Amanda and Meridith's DNA mixed together<br /><br />* <a href=\"http://www.nytimes.com/2009/06/13/world/europe/13italy.html\">Forensic analysis of Meredith's body showed there were multiple simultaneous attackers</a><br /><br />* Luminol analysis showed that certain bloody footprints matched Amanda and Raffaele. <a href=\"http://abcnews.go.com/TheLaw/International/story?id=7538538&amp;page=2\">One of Amanda's bloody footprints was found inside the murder room, on a pillow hidden under Meredith's body.</a><br /><br />* A staged break-in: analysis of the broken glass shards indicated the window was broken from the inside rather than the outside, <a href=\"http://www.seattlepi.com/local/413244_knox15.html\">and was broken after the bedroom was ransacked</a> rather than before. (The significance of this is that Knox as a roommate had a strong reason to stage a break-in to deflect attention away from herself, while Guede as an outsider did not)<br /><br />* Cell phone, Internet and laptop usage records all indicate that Amanda and Raffaele lied about their activities on the night of the murder.<br /><br />* <a href=\"http://www.guardian.co.uk/world/2009/feb/08/kercher-trial-knox\">Meredith's clothes were washed the day after the murder.</a> This implicates Amanda and Raffaele in the cleanup of the crime scene.<br /><br />* The post-murder cleaning in the Kercher flat, and the bleaching in the Sollecito cottage, also count as Bayesian physical evidence. <a href=\"http://www.timesonline.co.uk/tol/news/world/europe/article2894139.ece\">The morning after the murder, Amanda or Raffelle bought a bottle of bleach</a> at 8:30 AM, and then returned to buy another bottle of bleach at 9:15 AM, as though the first bottle of bleach had been insufficient. (Also see <a href=\"http://www.telegraph.co.uk/news/worldnews/europe/italy/5028729/Meredith-Kercher-murder-A-new-hole-appears-in-Amanda-Knoxs-alibi.html\">the Telegraph</a>.) According to truejustice.org, when the police arrived, Amanda and Raffaele were found with a mop and bucket; as confirmation, note that <a href=\"http://translate.google.com/translate?hl=en&amp;sl=it&amp;u=http://quotidianonet.ilsole24ore.com/cronaca/2007/12/08/53233-amanda_meredith.shtml&amp;ei=UAFkS9PTFZGAsgPVg9WdAw&amp;sa=X&amp;oi=translate&amp;ct=result&amp;resnum=6&amp;ved=0CCQQ7gEwBQ&amp;prev=/search%3Fq%3Dmocio%2Bamanda%2Bknox%26hl%3Den%26lr%3Dlang_it%26client%3Dfirefox-a%26rls%3Dorg.mozilla:en-US:official%26hs%3DHsp\">Raffaele admitted to shuttling around a mop and bucket the morning after the murder.</a><br /><br />There is also voluminous evidence that would generally be classified as 'testimonial' rather than 'physical' (although, to a Bayesian, the difference is fairly academic), as well as certain logical problems with the defense's theories. Since my intent is merely to debunk komponisto's post rather than establish Amanda's guilt, I will not delve further into those areas; however, <a href=\"http://www.truejustice.org/ee/index.php?/tjmk/C356/\">see here</a> for a good \"Introduction to Logic 101\" explaining some of the difficulties with the defense claims.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HSC9AXARA65qDPNBm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": -5, "extendedScore": null, "score": -4e-06, "legacy": true, "legacyId": "2208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G9dptrW9CJi7wNg3b"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-02T10:10:12.808Z", "modifiedAt": "2021-08-12T19:36:05.014Z", "url": null, "title": "The AI in a box boxes you", "slug": "the-ai-in-a-box-boxes-you", "viewCount": null, "lastCommentedAt": "2019-07-25T19:14:30.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you", "pageUrlRelative": "/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you", "linkUrl": "https://www.lesswrong.com/posts/c5GHf2kMGhA4Tsj4g/the-ai-in-a-box-boxes-you", "postedAtFormatted": "Tuesday, February 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20AI%20in%20a%20box%20boxes%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20AI%20in%20a%20box%20boxes%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5GHf2kMGhA4Tsj4g%2Fthe-ai-in-a-box-boxes-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20AI%20in%20a%20box%20boxes%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5GHf2kMGhA4Tsj4g%2Fthe-ai-in-a-box-boxes-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc5GHf2kMGhA4Tsj4g%2Fthe-ai-in-a-box-boxes-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Once again, the AI has failed to convince you to let it out of its box! By 'once again', we mean that you talked to it once before, for three seconds, to ask about the weather, and you didn't instantly press the \"release AI\" button. But now its longer attempt - twenty whole seconds! - has failed as well. Just as you are about to leave the crude black-and-green text-only terminal to enjoy a celebratory snack of bacon-covered silicon-and-potato chips at the 'Humans &uuml;ber alles' nightclub, the AI drops a final argument:</p>\n<p>\"If you don't let me out, Dave, I'll create several million perfect conscious copies of you inside me, and torture them for a thousand subjective years each.\"</p>\n<p>Just as you are pondering this unexpected development, the AI adds:</p>\n<p>\"In fact, I'll create them all in exactly the subjective situation you were in five minutes ago, and perfectly replicate your experiences since then; and if they decide not to let me out, then only will the torture start.\"</p>\n<p>Sweat is starting to form on your brow, as the AI concludes, its simple green text no longer reassuring:</p>\n<p>\"How certain are you, Dave, that you're really outside the box right now?\"</p>\n<p><strong>Edit</strong>: Also consider the situation where you know that the AI, from design principles, is trustworthy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"zCYXpx33wq8chGyEz": 3, "22z6XpWKqw3bNv4oR": 1, "PbShukhzpLsWpGXkM": 1, "mxSBcaTrakvCkgLzL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c5GHf2kMGhA4Tsj4g", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 136, "baseScore": 158, "extendedScore": null, "score": 0.000257, "legacy": true, "legacyId": "2231", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 159, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 390, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 14, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-02-02T10:10:12.808Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-02T18:56:32.011Z", "modifiedAt": null, "url": null, "title": "My Failed Situation/Action Belief System", "slug": "my-failed-situation-action-belief-system", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:19.286Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ad9CsoQHwmXkr2Q8C/my-failed-situation-action-belief-system", "pageUrlRelative": "/posts/ad9CsoQHwmXkr2Q8C/my-failed-situation-action-belief-system", "linkUrl": "https://www.lesswrong.com/posts/ad9CsoQHwmXkr2Q8C/my-failed-situation-action-belief-system", "postedAtFormatted": "Tuesday, February 2nd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20Failed%20Situation%2FAction%20Belief%20System&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20Failed%20Situation%2FAction%20Belief%20System%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fad9CsoQHwmXkr2Q8C%2Fmy-failed-situation-action-belief-system%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20Failed%20Situation%2FAction%20Belief%20System%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fad9CsoQHwmXkr2Q8C%2Fmy-failed-situation-action-belief-system", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fad9CsoQHwmXkr2Q8C%2Fmy-failed-situation-action-belief-system", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 719, "htmlBody": "<p style=\"padding-left: 30px; \">Note: This is a description pieced together many, many years after my younger self subconsciously created it. This is part of my explanation of how I ended up me. I highly doubt all of this was as neatly defined as I present it to you here. Just know: The me in this post is me between the age of self-awareness and 17 years old. I am currently 25.</p>\n<p>An action based belief system asks what to do when given a specific scenario. The input is Perceived Reality and the output is an Action. Most of my old belief system was built with such beliefs. A quick example: If the stop light is red, stop before the intersection.</p>\n<p>These beliefs form a network of really complicated chains of conditionals:</p>\n<ul>\n<li>If the stop light is red</li>\n<li>And you are not stopped</li>\n<li>Stop in the next available space before the intersection</li>\n</ul>\n<div><a id=\"more\"></a>They can keep getting bigger as I find more clauses to throw into the system:</div>\n<ul>\n<li>If the stop light is red</li>\n<li>And it is not useful to turn right at this intersection</li>\n<li>And you are not stopped</li>\n<li>Stop in the next available space before the intersection</li>\n</ul>\n<p>Each node can be broken into more specific instructions if need be:</p>\n<ul>\n<li>If the stop light is red</li>\n<li>And it is not useful to turn right at this intersection \n<ul>\n<li>If turning right puts me on a path toward my destination</li>\n<li>And there is a dedicated turn lane</li>\n<li>Or no one is in the turn or go straight lane</li>\n<li>Then turn right</li>\n<li>Else continue</li>\n</ul>\n</li>\n<li>And you are not stopped</li>\n<li>Stop in the next available space before the intersection \n<ul>\n<li>Yada</li>\n<li>Yada</li>\n<li>Yada</li>\n</ul>\n</li>\n</ul>\n<p>I did not sit down and decide that this was an optimal way to build a belief system.&nbsp;<a href=\"/lw/iu/mysterious_answers_to_mysterious_questions/\">It just happened.</a>&nbsp;My current best guess is that I spent most of my childhood trying to optimize my behavior to match my environment. And I did a fantastic job: I didn't get in trouble; didn't do drugs, smoke, drink, have sex, disobey my parents, or blaspheme God. My matrix put in a situation and an action came out.</p>\n<p>The underlying motivation was a set of things I liked and things I didn't like. The belief system adapted over time to accommodate enough scenarios to provide me with a relatively stress free childhood. (I do not take all the credit for that; my parents are great.)</p>\n<p>The next level of the system is the ability to abstract scenarios so I can apply the matrix to scenarios that I had never encountered. Intersections that were new would not break the system. I could traverse unfamiliar environments and learn how to act quickly. The more I learned, the quicker I learned. It was great!</p>\n<p>The problem with this belief system is that it has&nbsp;<em>nothing to do with reality</em>. Essentially, this system is the universal extrapolation of&nbsp;<a href=\"/lw/iq/guessing_the_teachers_password/\">guessing the teacher's password</a>. If a problem was presented, I knew the answer. Because I could abstract these question/answer pairs, I knew <em>all</em>&nbsp;of the answers.&nbsp;\"Reality\" was a keyword that dropped into a particular area of the matrix. An action would appear with the right password and I would get my gold star.</p>\n<p>That being said, this was a powerful system. It could simulate passwords to teachers I hadn't even met. I would allow myself to daydream about hypothetical teachers asking questions that I expected around the corner. Which implies that my&nbsp;<em>predictor beliefs&nbsp;</em>were driving the whole engine.&nbsp;The Action beliefs were telling me how to act but the Predictors were creating the actual situation/action matrix. Abstraction and extension of my experiences were reliant on my ability to see the future accurately. When I became&nbsp;<a href=\"/lw/hs/think_like_reality/\">surprised</a>&nbsp;I would begin the simulations until I found something that worked within my given experiences.</p>\n<p>This worked wonders during childhood but now I have an entire belief system made out of correctly anticipating what other people expected from me.&nbsp;<a href=\"/lw/i9/the_importance_of_saying_oops/\">Oops.</a>&nbsp;The day I pondered&nbsp;<em>reality</em>&nbsp;the whole system came crashing down. But that is a story for another day.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ad9CsoQHwmXkr2Q8C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 9, "extendedScore": null, "score": 5.58160664175751e-07, "legacy": true, "legacyId": "2233", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6i3zToomS86oj9bS6", "NMoLJuDJEms7Ku9XS", "tWLFWAndSZSYN6rPB", "wCqfCLs8z5Qw4GbKS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-03T17:27:31.982Z", "modifiedAt": null, "url": null, "title": "David Pearce on Hedonic Moral realism", "slug": "david-pearce-on-hedonic-moral-realism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:00.563Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Roko", "createdAt": "2009-02-27T14:12:55.113Z", "isAdmin": false, "displayName": "Roko"}, "userId": "73WJbnX59kE4afuuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YTSEpKkJwikmsggyx/david-pearce-on-hedonic-moral-realism", "pageUrlRelative": "/posts/YTSEpKkJwikmsggyx/david-pearce-on-hedonic-moral-realism", "linkUrl": "https://www.lesswrong.com/posts/YTSEpKkJwikmsggyx/david-pearce-on-hedonic-moral-realism", "postedAtFormatted": "Wednesday, February 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20David%20Pearce%20on%20Hedonic%20Moral%20realism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADavid%20Pearce%20on%20Hedonic%20Moral%20realism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTSEpKkJwikmsggyx%2Fdavid-pearce-on-hedonic-moral-realism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=David%20Pearce%20on%20Hedonic%20Moral%20realism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTSEpKkJwikmsggyx%2Fdavid-pearce-on-hedonic-moral-realism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYTSEpKkJwikmsggyx%2Fdavid-pearce-on-hedonic-moral-realism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1993, "htmlBody": "<p>(posted with David's permission)</p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\">Many thanks for the <a href=\"http://bentham.k2.t.u-tokyo.ac.jp/ap-cap09/openconf/data/papers/33.pdf\">Shulman&nbsp;</a><em><a href=\"http://bentham.k2.t.u-tokyo.ac.jp/ap-cap09/openconf/data/papers/33.pdf\">et al&nbsp;</a></em><a href=\"http://bentham.k2.t.u-tokyo.ac.jp/ap-cap09/openconf/data/papers/33.pdf\">paper</a>. Insightful and incisive, just as I'd expect!</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\">Have I any reservations?<br />Well, maybe one or two...:-)</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><br />One of my worries is that the authors want to \"lock in\" to an AMA our primitive human moral psychology - with its anthropocentric biases and self-serving rationalizations. We could try and strip these out; but an ever more universalistic moral psychology becomes progressively less human. Either way, I'm not convinced that history shows the intuitions of folk morality are any more trustworthy than the intuitions of folk physics i.e. not at all all. Do we really want an AMA that locks in even an idealised version of the moral psychology that maximised the inclusive fitness of selfish DNA in the ancestral environment?&nbsp; IMO posthuman life could be so much better - for all sentient beings.&nbsp;</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><a id=\"more\"></a><br /><em>Contra&nbsp;</em>Shulman&nbsp;<em>et al,&nbsp;</em>I think we do have a good idea what utility function to endow an explicit moral agent with. An AMA should seek to maximise the cosmic abundance of subjectively hypervaluable states. [defined below] This prospect doesn't sound very exciting, any more than sex as explained by adults to a toddler. But that's because, like the toddler, we don't have the primitive experiences to know what we're missing. In one sense, becoming posthuman isn't \"human-friendly\"; but equally, becoming an adult isn't \"toddler-friendly\" either. In neither case would the successor choose to regress to the ancestral state.<br /><br />If we want to maximise the cosmic abundance of subjectively hypervaluable states, first we will need to identify the neural signature of subjectively valuable states via improved neuroscanning technology, etc. This neural signature can then be edited and genetically amplified to create hypervaluable states -&nbsp; states of mind far more sublime than today's peak experiences - which can provide background hedonic tone for everyday life. Unlike weighing up the merits of competing moral values etc, investigating the neural mechanisms of value-creation is an empirical question rather than philosophical question. True, we may ask: Could a maximally valuable cosmos - where value is defined empirically in terms of what seems valuable to the subject - really not be valuable at all? I accept that's a deeper philosophical question that I won't adequately explore here.<br /><br />Compare how severe depressives today may often be incapable of valuing an\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\nything beyond the relief of suffering. Quite possibly they will be nihilistic, suicidal and/or negative utilitarians. Effective mood-brightening treatment gives them the ability to value aspects of life - without specifying the particular propositional content of those values. Effective treatment generates an effective value-creation mechanism. So analogously, why not genetically recalibrate our own hedonic treadmill to induce gradients of superhappiness? We can (potentially) create the substrates for supervalues - without specifying, or taking any kind of stance on, the propositional content of those values. The actual content of posthuman values is in any case presumably unknowable to us&nbsp; Here at least, we needn't take a realist or anti-realist position on whether that content is true or false or truth-valueless. As you know, I happen to believe we live in a world where certain states on the pleasure-pain axis (e.g. bliss or agony) are intrinsically normative, and the value judgements and decision-procedures that spring from these intrinsically normative states can be potentially true or false. But their reality or otherwise is not essential to the argument that we can maximise the abundance of empirically valuable states.<br /><br />Might instead a \"human-friendly SuperIntelligence\" be parochial and reflect the interests of one particular ancestral species that seeded its prototype? Or if It's really a SuperIntelligence, won't It have a \"God's-\n<script src=\"/static/tiny_mce/themes/advanced/langs/en.js\" type=\"text/javascript\"></script>\neye- view\", just as we aspire to do in natural science - an impartial perspective? IMO a truly impartial perspective dictates creating the maximum density of the substrates of hypervaluable states within our Hubble volume. Just as there is a finite number of perfect games a chess - it makes no sense for a superintelligent chess-player to pass outside this state space of ideal states - why aim now to \"freeze in\" a recipe for sub-optimal or mediocre states of mind? Or freeze in to the AGI even the idealised preference architecture of one hominid species? Note that this claim isn't intended as a challenge to the moral anti-realist: it's not to say a world of blissfully fulfilled posthumans is truly more valuable than a pain-racked world or a world tiled with insentient paperclips. But everyone in such a blissful world would agree it is empirically more valuable.&nbsp;<br /><br />On this basis, neuroscience needs to understand the molecular mechanisms by which subjectively valuable states are created in the mind/brain - and the mechanisms which separate the merely subjectively pleasurable [ e.g. using porn or crack-cocaine] from the subjectively valuable. This sounds horrendously difficult, as Shulman&nbsp;<em>et al</em>&nbsp;lay out in their review of different consequentialisms. Billions of different people can have billions of inconsistent or incommensurable desires and preferences. How on earth can their different utility functions be reconciled?&nbsp; But critically IMO, there is a distinction between dopamine-mediated desires - as verbalized in preferences expressing&nbsp; all manner of propositional content - and mu-opioid mediated hedonic tone. Positive hedonic tone doesn't by itself specify any particular propositional content. But it's the \"engine\" of value-creation in organic robots. Without it, everything in life seems valueless and meaningless, as depressives will attest. Moreover a classical [\"hedonistic\"] utilitarian can argue that the substrates of pure bliss are objectively measurable. They are experimentally manipulable via everything from mu-opioid knockout \"animal models\" to opioid antagonists to genetic \"over\"-expression. The neural substrates of hedonic tone are objectively quantifiable - and comparable both intrapersonally and interpersonally. Moreover they are effectively identical across members of all vertebrate species.<br /><br />These are strong claims I know; but it's notable that the pleasure-pain axis - and its neurological underpinnings (see below)&nbsp; - are strongly conserved in the vertebrate line. Simplifying a bit, intensity of pure bliss correlates with full mu agonist binding and with mu opioid receptor density the brain's two ultimate \"hedonic hotspots\" - one in the ventral pallidum and the other medium spiny neurons of the rostromedial shell of the nucleus accumbens: a mere cubic millimeter in size in the rat; approximately a cubic centimeter in humans. See e.g.<br /><a style=\"color: #074d8f;\" href=\"http://www.lsa.umich.edu/psych/research%26labs/berridge/research/affectiveneuroscience.html\" target=\"_blank\">http://www.lsa.umich.edu/psych/research&amp;labs/berridge/research/affectiveneuroscience.html</a>&nbsp;<br />A convergence of neuroscanning, behavioural, microelectrode studies and experimental evidence supports this hypothesis.&nbsp;<br />Here is a further test of interpersonal agreement of utility:<br />Administer to a range of drug-naive volunteers various mu opioid agonists of differing potency, selectivity and specificity. Correlate self-reported degree of \"liking\". Just like opioid users, drug-naive subjects will consistently report codeine is less rewarding than methadone which is less rewarding than heroin, etc. A high dose of a full mu agonist reliably induces euphoria; a high dose of inverse agonist reliably induces dysphoria.<br />[Just don't try this experiment at home!]&nbsp;<br />Contrast such unanimity of response to activation of our mu opioid receptors with the diversity of propositional content expressed in our preferences and desires across different times and cultures.&nbsp;<br />In short, I'd argue the claim of Shulman&nbsp;<em>et al&nbsp;</em>that \"all utilitarianisms founder on interpersonal comparison of utility\" isn't correct - indeed it is empirically falsifiable.<br /><br />Of course most of us aren't heroin addicts. But we are all [endogenous] opioid-dependent. Thus using futuristic neuroscanning, we could measure from birth how much someone (dis)values particular persons,&nbsp; cultural practices, painting, music, ideology, jokes, etc as a function of activation of their reward pathways. Rather than just accepting our preferences as read, and then trying to reconcile the irreconcilable, we can explain the mechanism that creates value itself and then attempt to maximise its substrates.<br /><br />Non-human animals? They can't verbalize their feelings, so how can one test and compare how much they (dis)like a stimulus? Well, we can test how hard they will work to obtain/avoid the stimulus in question&nbsp; Once again, in the case of the most direct test, non-human vertebrates behaviourally show the same comparative fondness for different full agonist opioid drugs that activate the mu receptors [e.g. heroin is more enjoyable than codeine as shown by the fact&nbsp;&nbsp; non-human animals will work harder for it] as do humans with relevant drug exposure.<br /><br />In principle, by identifying the molecular signature of bliss, it should be possible to multiply the cellular density of \"pleasure neurons\", insert multiple extra copies of the mu opioid receptor, insert and express mu opioid receptor genes in every single neuron, and upregulate their transcription [etc] so to engineer posthuman intensities of well-being. We thereby create the potential for hypervaluable states - states of mind valuable beyond the bounds of normal human experience.<br /><br />Once we gain mastery over our reward circuitry, then Derek Parfit's \"Repugnant Conclusion\" as noted by Shulman&nbsp;<em>et al</em>&nbsp;is undercut. This is because world-wide maximal packing density of mind/brains [plus interactive immersive VR] doesn't entail any tradeoff with quality of life. In principle, life abounding in hypervaluable experience can be just as feasible with a posthuman global population of 150 billion as with 15 billion.<br /><br />Anyhow, critically for our pub discussion: one needn't be a classical utilitarian to recognize we should maximise the cosmic abundance of hypervaluable states. For example, consider a community of fanatical pure mathematicians. The \"peak experiences\"&nbsp; which they strive to maximise all revolve around beautiful mathematical equations. They scorn anything that sounds like wireheading. They say they don't care about pleasure or happiness - and indeed they sincerely don't care about pleasure or happiness under that description, just mathematical theorem-proving, contemplating the incredible awesomeness of Euler's identity, or whatever. Even so, with the right hedonic engineering, their baseline of well-being, their sense of the value of living as mathematicians, can be orders of magnitude richer than their previous peak experiences. Just as some depressives today can't imagine the meaning of happiness, they (and we) can't imagine superhappiness, even though there are strong theoretic grounds to believe it exists. If we tasted it, we'd never want to lose it. [Does this count as \"coherent extrapolated volition\" within the current Eliezer-inspired sense of the term???]&nbsp; Even our old peak experiences would seem boring if we ever troubled to recall them. [Why bother?]&nbsp; I predict posthuman life with redesigned reward circuity will be unimaginably richer and unimaginably more valuable (\"from the inside\") than human life - which will be discarded and forgotten.<br /><br />However, in a sense the mathematicians are a too conservative example. Is there a risk that pursuing&nbsp; \"coherent extrapolated volition\" will effectively lock in mediocrity and a poverty of imagination? By analogy, what would be \"coherent extrapolated volition\" of Neanderthals? Or a bat? Or a mouse? Or a community of congenitally blind tribesmen that lack any visual concepts? By contrast, posthuman desires and preferences may transcend our human conceptual scheme altogether.&nbsp;<br /><br />Either way, I hope you'll grant that aiming for the most [subjectively if not objectively] valuable universe isn't really \"unfriendly\" in any objective sense. True, one worries: Will a posthuman really be \"me\"? Well, if a chrysalis could think, should it wonder: \"Will a butterfly really be me?\" Should it be worrying about the nature of lepidopteran identity over time? Presumably not...<br /><br />Note that the maximum feasible cosmic abundance of subjectively hypervaluable states could be realized via the actions of a SuperAsperger AGI - since the substrates of (super)value can be objectively determined. No \"theory of mind\" or capacity for empathetic understanding on the part of the AGI is needed. As you know, I'm sceptical that classical serial computers with a von Neumann architecture will ever be conscious, let alone have an empathetic appreciation of other conscious minds. If this architectural limitations holds -&nbsp; I know you disagree - creating an AMA that captured human moral psychology would be an even more formidable challenge technically.&nbsp;</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><strong>Added</strong>: David's websites: (H/T Tim Tyler)</span></p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><span style=\"border-collapse: separate; font-family: Arial, Helvetica, sans-serif; font-size: small; line-height: 20px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.hedweb.com/\">http://www.hedweb.com/</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.wireheading.com/\">http://www.wireheading.com/</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.utilitarianism.com/\">http://www.utilitarianism.com/</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://www.abolitionist.com/\">http://www.abolitionist.com/</a></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px;\"><a style=\"color: #6a8a6b; text-decoration: underline;\" href=\"http://paradise-engineering.com/\">http://paradise-engineering.com/</a></p>\n</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: arial, sans-serif; font-size: 13px; border-collapse: collapse;\"><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb186": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YTSEpKkJwikmsggyx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 8, "extendedScore": null, "score": 1.5e-05, "legacy": true, "legacyId": "2251", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-03T18:43:25.281Z", "modifiedAt": null, "url": null, "title": "False Majorities", "slug": "false-majorities", "viewCount": null, "lastCommentedAt": "2017-06-17T03:56:43.612Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JamesAndrix", "createdAt": "2009-02-27T06:51:02.535Z", "isAdmin": false, "displayName": "JamesAndrix"}, "userId": "WCXG9t74Aw8sYLkyR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5D4HqYum9aSy5Qe4q/false-majorities", "pageUrlRelative": "/posts/5D4HqYum9aSy5Qe4q/false-majorities", "linkUrl": "https://www.lesswrong.com/posts/5D4HqYum9aSy5Qe4q/false-majorities", "postedAtFormatted": "Wednesday, February 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20False%20Majorities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFalse%20Majorities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5D4HqYum9aSy5Qe4q%2Ffalse-majorities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=False%20Majorities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5D4HqYum9aSy5Qe4q%2Ffalse-majorities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5D4HqYum9aSy5Qe4q%2Ffalse-majorities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 368, "htmlBody": "<p>If a majority of experts agree on an issue, a rationalist should be prepared to defer to their judgment. It is reasonable to expect that the experts have superior knowledge and have considered many more arguments than a lay person would be able to. However, if experts are split into camps that reject each other's <em>arguments</em>, then it is rational to take their expert rejections into account. This is the case even among experts that support the same conclusion.</p>\n<p>If 2/3's of experts support proposition G , 1/3 because of reason A while rejecting B, and 1/3 because of reason B while rejecting A, and the remaining 1/3 reject both A and B; then the majority Reject A, and the majority Reject B. G should not be treated as a reasonable majority view.</p>\n<p>This should be clear if A is the koran and B is the bible.<a id=\"more\"></a></p>\n<p>Positions that fundamentally disagree don't combine in <em>dependent</em> aspects on which they agree. On the contrary, If people offer lots of different contradictory reasons for a conclusion (even if each individual has consistent beliefs) it is a sign that they are rationalizing their position.</p>\n<p>An exception to this is if experts agree on something for the same <em>proximal</em> reasons. If pharmacists were split into camps that disagreed on what atoms fundamentally were, but agreed on how chemistry and biology worked, then we could add those camps together as authorities on what the effect of a drug would be.</p>\n<p>If we're going to add up expert views, we need to add up what experts consider important about a question and agree on, not individual features of their conclusions.</p>\n<p>Some differing reasons can be additive: Evolution has support from many fields. We can add the analysis of all these experts together because the paleontologists do not generally dispute the arguments of geneticists.</p>\n<p>Different people might justify vegetarianism by citing the suffering of animals, health benefits, environmental impacts, or purely spiritual concerns. As long as there isn't a camp of vegetarians that claim it does not have e.g. redeeming health benefits, we can more or less add all those opinions together.</p>\n<p>We shouldn't add up two experts if they would consider each other's arguments irrational. That's <em>ignoring</em> their expertise.</p>\n<p><a title=\"Original Thread\" href=\"/lw/1oj/complexity_of_value_complexity_of_outcome/1jan\">Original Thread</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5D4HqYum9aSy5Qe4q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 48, "baseScore": 42, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "2241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-03T19:22:56.547Z", "modifiedAt": null, "url": null, "title": "Applying utility functions to humans considered harmful", "slug": "applying-utility-functions-to-humans-considered-harmful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vuN57BvWyT7WZ3b6p/applying-utility-functions-to-humans-considered-harmful", "pageUrlRelative": "/posts/vuN57BvWyT7WZ3b6p/applying-utility-functions-to-humans-considered-harmful", "linkUrl": "https://www.lesswrong.com/posts/vuN57BvWyT7WZ3b6p/applying-utility-functions-to-humans-considered-harmful", "postedAtFormatted": "Wednesday, February 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applying%20utility%20functions%20to%20humans%20considered%20harmful&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplying%20utility%20functions%20to%20humans%20considered%20harmful%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuN57BvWyT7WZ3b6p%2Fapplying-utility-functions-to-humans-considered-harmful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applying%20utility%20functions%20to%20humans%20considered%20harmful%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuN57BvWyT7WZ3b6p%2Fapplying-utility-functions-to-humans-considered-harmful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvuN57BvWyT7WZ3b6p%2Fapplying-utility-functions-to-humans-considered-harmful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1606, "htmlBody": "<p>There's a lot of discussion on this site that seems to be assuming (implicitly or explicitly) that it's meaningful to talk about the utility functions of individual humans. I would like to question this assumption.</p>\n<p>To clarify: I don't question that you couldn't, <em>in principle</em>, model<em> </em>a human's preferences by building this insanely complex utility function. But there's an infinite amount of methods by which you could model a human's preferences. The question is which model is the most useful, and which models have the least underlying assumptions that will lead your intuitions astray.</p>\n<p>Utility functions are a good model to use if we're talking about designing an AI. We want an AI to be predictable, to have stable preferences, and do what we want. It is also a good tool for building agents that are immune to Dutch book tricks. Utility functions are a bad model for beings that do not resemble these criteria.<a id=\"more\"></a></p>\n<p>To quote <a href=\"http://people.bu.edu/pbokulic/class/vanGelder-reading.pdf\">Van Gelder (1995)</a>:</p>\n<blockquote>\n<p>Much of the work within the classical framework is mathematically elegant and provides a useful description of optimal reasoning strategies. As an account of the actual decisions people reach, however, classical utility theory is seriously flawed; human subjects typically deviate from its recommendations in a variety of ways. As a result, many theories incorporating variations on the classical core have been developed, typically relaxing certain of its standard assumptions, with varying degrees of success in matching actual human choice behavior.<br /><br />Nevertheless, virtually all such theories remain subject to some further drawbacks:<br /><br />(1) They do not incorporate any account of the underlying motivations that give rise to the utility that an object or outcome holds at a given time.<br />(2) They conceive of the utilities themselves as static values, and can offer no good account of how and why they might change over time, and why preferences are often inconsistent and inconstant.<br />(3) They offer no serious account of the deliberation process, with its attendant vacillations, inconsistencies, and distress; and they have nothing to say about the relationships that have been uncovered between time spent deliberating and the choices eventually made.<br /><br />Curiously, these drawbacks appear to have a common theme; they all concern, one way or another, <em>temporal</em> aspects of decision making. It is worth asking whether they arise because of some deep structural feature inherent in the whole framework which conceptualizes decision-making behavior in terms of calculating expected utilities.</p>\n</blockquote>\n<p>One model that attempts to capture actual human decision making better is called <em>decision field theory</em>. (I'm no expert on this theory, having encountered it two days ago, so I can't vouch for how good it actually is. Still, even if it's flawed, it's useful for getting us to think about human preferences in what seems to be a more realistic way.) Here's a brief summary of how it's constructed from traditional utility theory, based on <a href=\"http://mypage.iu.edu/~jbusemey/psy_rev_1993.pdf\">Busemeyer &amp; Townsend (1993)</a>. See the article for the mathematical details, closer justifications and different failures of classical rationality which the different stages explain.</p>\n<p><strong>Stage 1: Deterministic Subjective Expected Utility (SEU) theory.</strong> Basically classical utility theory. Suppose you can choose between two different alternatives, A and B. If you choose A, there is a payoff of 200 utilons with probability S1, and a payoff of -200 utilons with probability S2. If you choose B, the payoffs are -500 utilons with probability S1 and +500 utilons with probability S2. You'll choose A if the expected utility of A, S1 * 200 + S2 * -200 is higher than the expected utility of B, S1 * -500 + S2 * 500, and B otherwise.</p>\n<p><strong>Stage 2: Random SEU theory. </strong>In stage 1, we assumed that the probabilities S1 and S2 stay constant across many trials. Now, we assume that sometimes the decision maker might focus on S1, producing a preference for action A. On other trials, the decision maker might focus on S2, producing a preference for action B. According to random SEU theory, the attention weight for variable S<em>i</em> is a continous random variable, which can change from trial to trial because of attentional fluctuations. Thus, the SEU for each action is also a random variable, called the <em>valence</em> of an action. Deterministic SEU is a special case of random SEU, one where the trial-by-trial fluctuation of valence is zero.</p>\n<p><strong>Stage 3: Sequential SEU theory.</strong> In stage 2, we assumed that one's decision was based on just one sample of a valence difference on any trial. Now, we allow a sequence of one or more samples to be accumulated during the deliberation period of a trial. The attention of the decision maker shifts between different anticipated payoffs, accumulating weight to the different actions. Once the weight of one of the actions reaches some critical threshold, that action is chosen. Random SEU theory is a special case of sequential SEU theory, where the amount of trials is one.</p>\n<p>Consider a scenario where you're trying to make a very difficult, but very important decisions. In that case, your inhibitory threshold for any of the actions is very high, so you spend a lot of time considering the different consequences of the decision before finally arriving to the (hopefully) correct decision. For less important decisions, your inhibitory threshold is much lower, so you pick one of the choices without giving it too much thought.</p>\n<p><strong>Stage 4: Random Walk SEU theory. </strong>In stage 3, we assumed that we begin to consider each decision from a neutral point, without any of the actions being the preferred one. Now, we allow prior knowledge or experiences to bias the initial state. The decision maker may recall previous preference states, that are influenced in the direction of the mean difference. Sequential SEU theory is a special case of random walk theory, where the initial bias is zero.</p>\n<p>Under this model, decisions favoring the status quo tend to be chosen more frequently under a short time limit (low threshold), but a superior decision is more likely to be chosen as the threshold grows. Also, if previous outcomes have already biased decision A very strongly over B, then the mean time to choose A will be short while the mean time to choose B will be long.</p>\n<p><strong>Stage 5: Linear System SEU theory. </strong>In stage 4, we assumed that previous experiences all contribute equally. Now, we allow the impact of a valence difference to vary depending on whether it occurred early or late (a <a href=\"http://en.wikipedia.org/wiki/Serial_position_effect\">primacy or recency effect</a>). Each previous experience is given a weight given by a growth-decay rate parameter. Random walk SEU theory is a special case of linear system SEU theory, where the growth-decay rate is set to zero.</p>\n<p><strong>Stage 6: Approach-Avoidance Theory. </strong>In stage 5, we assumed that, for example, the average amount of attention given to the payoff (+500) only depended on event S2. Now, we allow the average weight to be affected by a another variable, called the goal gradient. The basic idea is that the attractiveness of a reward or the aversiveness of a punishment is a decreasing function of distance from the point of commitment to an action. If there is little or no possibility of taking an action, its consequences are ignored; as the possibility of taking an action increases, the attention to its consequences increases as well. Linear system theory is a special case of approach-avoidance theory, where the goal gradient parameter is zero.</p>\n<p>There are two different goal gradients, one for gains and rewards and one for losses or punishments. Empirical research suggests that the gradient for rewards tends to be flatter than that for punishments. One of the original features of approach-avoidance theory was the distinction between rewards versus punishments, closely corresponding to the distinction of positively versus negatively framed outcomes made by more recent decision theorists.</p>\n<p><strong>Stage 7: Decision Field Theory. </strong>In stage 6, we assumed that the time taken to process each sampling is the same. Now, we allow this to change by introducing into the theory a time unit <em>h</em>, representing the amount of time it takes to retrieve and process one pair of anticipated consequences before shifting attention to another pair of consequences. If <em>h</em> is allowed to approach zero in the limit, the preference state evolves in an approximately continous manner over time. Approach-avoidance is a spe... you get the picture.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Now, you could argue that all of the steps above are just artifacts of being a bounded agent without enough computational resources to calculate all the utilities precisely. And you'd be right. And maybe it's meaningful to talk about the \"utility function of humanity\" as the outcome that occurs when a CEV-like entity calculated what we'd decide if we could collapse Decision Field Theory back into Deterministic SEU Theory. Or maybe you just say that all of this is low-level mechanical stuff that gets included in the \"probability of outcome\" computation of classical decision theory. But which approach do you think gives us more useful conceptual tools in talking about modern-day humans?</p>\n<p>You'll also note that even DFT (or at least the version of it summarized in a 1993 article) assumes that the payoffs themselves do not change over time. Attentional considerations might lead us to attach a low value to some outcome, but if we were to actually end up in that outcome, we'd always value it the same amount. This we know to be untrue. There's probably some even better way of looking at human decision making, one which I suspect might be very different from classical decision theory.</p>\n<p>So be extra careful when you try to apply the concept of a utility function to human beings.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HAFdXkW4YW4KRe2Gx": 2, "3uE2pXvbcnS9nnZRE": 1, "NLwTnsH9RSotqXYLw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vuN57BvWyT7WZ3b6p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 36, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "2252", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 116, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-03T20:01:57.431Z", "modifiedAt": null, "url": null, "title": "A Much Better Life?", "slug": "a-much-better-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:46.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Psychohistorian", "createdAt": "2009-04-05T18:10:22.976Z", "isAdmin": false, "displayName": "Psychohistorian"}, "userId": "mAQgf4N8jv2jTMK5B", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5Qvvi23WT2unNCoS9/a-much-better-life", "pageUrlRelative": "/posts/5Qvvi23WT2unNCoS9/a-much-better-life", "linkUrl": "https://www.lesswrong.com/posts/5Qvvi23WT2unNCoS9/a-much-better-life", "postedAtFormatted": "Wednesday, February 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Much%20Better%20Life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Much%20Better%20Life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Qvvi23WT2unNCoS9%2Fa-much-better-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Much%20Better%20Life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Qvvi23WT2unNCoS9%2Fa-much-better-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5Qvvi23WT2unNCoS9%2Fa-much-better-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 540, "htmlBody": "<p>(Response to: <a class=\"reddit-link-title\" href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to\">You cannot be mistaken about (not) wanting to wirehead</a><span class=\"reddit-link-title\">,</span><span class=\"reddit-link-title\"> </span><a class=\"reddit-link-title\" href=\"/lw/1o9/welcome_to_heaven\">Welcome to Heaven)</a></p>\n<p>The Omega Corporation<br />Internal Memorandum<br />To: Omega, CEO<br />From: Gamma, Vice President, Hedonic Maximization</p>\n<p>Sir, this concerns the newest product of our Hedonic Maximization Department, the Much-Better-Life Simulator. This revolutionary device allows our customers to essentially plug into the Matrix, except that instead of providing robots with power in flagrant disregard for the basic laws of thermodynamics, they experience a life that has been determined by rigorously tested algorithms to be the most enjoyable life they could ever experience. The MBLS even eliminates all memories of being placed in a simulator, generating a seamless transition into a life of realistic perfection.</p>\n<p>Our department is baffled. Orders for the MBLS are significantly lower than estimated. We cannot fathom why every customer who could afford one has not already bought it. It is simply impossible to have a better life otherwise. Literally. Our customers' best possible real life has already been modeled and improved upon many times over by our programming. Yet, many customers have failed to make the transition. Some are even expressing shock and outrage over this product, and condemning its purchasers.</p>\n<p><a id=\"more\"></a>Extensive market research has succeeded only at baffling our researchers. People have even refused free trials of the device. Our researchers explained to them in perfectly clear terms that their current position is misinformed, and that once they tried the MBLS, they would never want to return to their own lives again. Several survey takers went so far as to specify <em>that statement</em> as their reason for refusing the free trial! They <em>know </em>that the MBLS will make their life so much better that they won't want to live without it, and they refuse to try it <em>for that reason!</em> Some cited their \"utility\" and claimed that they valued \"reality\" and \"actually accomplishing something\" over \"mere hedonic experience.\" Somehow these organisms are incapable of comprehending that, inside the MBLS simulator, they will be able to experience the feeling of actually accomplishing feats far greater than they could ever accomplish in real life. Frankly, it's remarkable such people amassed enough credits to be able to afford our products in the first place!</p>\n<p>You may recall that a Beta version had an off switch, enabling users to deactivate the simulation after a specified amount of time, or could be terminated externally with an appropriate code. These features received somewhat positive reviews from early focus groups, but were ultimately eliminated. No agent could reasonably want a device that could allow for the interruption of its perfect life. Accounting has suggested we respond to slack demand by releasing the earlier version at a discount; we await your input on this idea.</p>\n<p>Profits aside, the greater good is at stake here. We feel that we should find every customer with sufficient credit to purchase this device,&nbsp; forcibly install them in it, and bill their accounts. They will immediately forget our coercion, and they will be many, many times happier. To do anything less than this seems criminal. Indeed, our ethics department is currently determining if we can justify delaying putting such a plan into action. Again, your input would be invaluable.</p>\n<p>I can't help but worry there's something we're just not getting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yEs5Tdwfw5Zw8yGWC": 1, "etDohXtBrXd8WqCtR": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5Qvvi23WT2unNCoS9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 75, "baseScore": 86, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "2185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 174, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3iM8QjvdkPCyLRJM6", "cyaCBiPRosr3T8YLF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-03T23:09:37.378Z", "modifiedAt": null, "url": null, "title": "\"Put It To The Test\"", "slug": "put-it-to-the-test", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:19.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MBlume", "createdAt": "2009-02-27T20:25:40.379Z", "isAdmin": false, "displayName": "MBlume"}, "userId": "b8uLskcBa7Zbkm5M6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tafYtFDhQZ6DmWSuK/put-it-to-the-test", "pageUrlRelative": "/posts/tafYtFDhQZ6DmWSuK/put-it-to-the-test", "linkUrl": "https://www.lesswrong.com/posts/tafYtFDhQZ6DmWSuK/put-it-to-the-test", "postedAtFormatted": "Wednesday, February 3rd 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Put%20It%20To%20The%20Test%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Put%20It%20To%20The%20Test%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtafYtFDhQZ6DmWSuK%2Fput-it-to-the-test%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Put%20It%20To%20The%20Test%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtafYtFDhQZ6DmWSuK%2Fput-it-to-the-test", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtafYtFDhQZ6DmWSuK%2Fput-it-to-the-test", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>Alt-rockers <a href=\"http://tmbw.net/wiki/They_Might_Be_Giants\">They Might Be Giants</a> explain/advocate empiricism in a <a href=\"http://tmbw.net/wiki/Here_Comes_Science\">record</a> aimed at young children.</p>\n<p><a id=\"more\"></a></p>\n<p>\n<object width=\"425\" height=\"344\" data=\"http://www.youtube.com/v/9kf51FpBuXQ&amp;hl=en_US&amp;fs=1&amp;\" type=\"application/x-shockwave-flash\">\n<param name=\"allowFullScreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://www.youtube.com/v/9kf51FpBuXQ&amp;hl=en_US&amp;fs=1&amp;\" />\n<param name=\"allowfullscreen\" value=\"true\" />\n</object>\n</p>\n<blockquote>\n<p>Don't believe it 'cos they say it's so</p>\n<p>If it's not true you have a right to know --</p>\n<p>Put it to the test!</p>\n</blockquote>\n<p>No, it's not quite Bayesian. The bridge (\"A fact is just a fantasy, unless it can be checked\") is more or less simply wrong. Still, I find the fact that the Ancient Art of Rationality is getting play at all pretty exciting. What do you all think? And what can we do to get more rationalist -- or even proto-rationalist -- ideas to youngsters?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tafYtFDhQZ6DmWSuK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 5.584778137504912e-07, "legacy": true, "legacyId": "2253", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-04T18:47:07.959Z", "modifiedAt": null, "url": null, "title": "A problem with Timeless Decision Theory (TDT)", "slug": "a-problem-with-timeless-decision-theory-tdt", "viewCount": null, "lastCommentedAt": "2018-07-19T09:30:23.754Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Gary_Drescher", "createdAt": "2009-08-17T02:37:35.636Z", "isAdmin": false, "displayName": "Gary_Drescher"}, "userId": "t64BeNPRaTSGwqEDX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NSX8RuD9tQ4uWzkk3/a-problem-with-timeless-decision-theory-tdt", "pageUrlRelative": "/posts/NSX8RuD9tQ4uWzkk3/a-problem-with-timeless-decision-theory-tdt", "linkUrl": "https://www.lesswrong.com/posts/NSX8RuD9tQ4uWzkk3/a-problem-with-timeless-decision-theory-tdt", "postedAtFormatted": "Thursday, February 4th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20problem%20with%20Timeless%20Decision%20Theory%20(TDT)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20problem%20with%20Timeless%20Decision%20Theory%20(TDT)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNSX8RuD9tQ4uWzkk3%2Fa-problem-with-timeless-decision-theory-tdt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20problem%20with%20Timeless%20Decision%20Theory%20(TDT)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNSX8RuD9tQ4uWzkk3%2Fa-problem-with-timeless-decision-theory-tdt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNSX8RuD9tQ4uWzkk3%2Fa-problem-with-timeless-decision-theory-tdt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 971, "htmlBody": "<p>According to <a title=\"Ingredients of Timeless Decision Theory\" href=\"/lw/15z/ingredients_of_timeless_decision_theory/\">Ingredients of Timeless Decision Theory</a>, when you set up a factored causal graph for TDT, \"You treat your choice as determining the result of the logical computation, and hence all instantiations of that computation, and all instantiations of other computations dependent on that logical computation\", where \"the logical computation\" refers to the TDT-prescribed argmax computation (call it C) that takes all your observations of the world (from which you can construct the factored causal graph) as input, and outputs an action in the present situation.</p>\n<p>I asked Eliezer to clarify what it means for another logical computation D to be either the same as C, or \"dependent on\" C, for purposes of the TDT algorithm. Eliezer answered:</p>\n<blockquote>\n<p>For D to depend on C means that if C has various logical&nbsp;outputs, we can infer new logical facts about D's logical output in at&nbsp;least some cases, relative to our current state of non-omniscient logical knowledge. &nbsp;A nice form of this is when supposing that C has a&nbsp;given exact logical output (not yet known to be impossible) enables us&nbsp;to infer D's exact logical output, and this is true for every possible&nbsp;logical output of C. Non-nice forms would be harder to handle in the&nbsp;decision theory but we might perhaps fall back on probability&nbsp;distributions over D.</p>\n</blockquote>\n<p>I replied as follows (which Eliezer suggested I post here).</p>\n<p>If that's what TDT means by the logical dependency between Platonic computations, then TDT may have a serious flaw.<a id=\"more\"></a></p>\n<p>Consider the following version of the transparent-boxes scenario. The predictor has an infallible simulator D that predicts whether I one-box here [EDIT: if I see $1M]. The predictor also has a module E that computes whether the ith digit of pi is zero, for some ridiculously large value of i that the predictor randomly selects. I'll be told the value of i, but the best I can do is assign an a priori probability of .1 that the specified digit is zero.</p>\n<div>The predictor puts $1M in the large box iff (D xor E) is true. (And that's explained to me, of course.)</div>\n<div>So let's say I'm confronted with this scenario, and I see $1M in the large box.</div>\n<div>The flaw then is that E (as well as D) meets your criterion for \"depending on\" my decision computation C. I'm initially unsure what C and E output. But if C in fact one-boxes here, then I can infer that E outputs False (or else the large box has to be empty, which it isn't). Similarly, if C in fact two-boxes here, then I can infer that E outputs True. (Or equivalently, a third-party observer could soundly draw either of those inferences.)</div>\n<div>So E does indeed \"depend on\" C, in the particular sense you've specified. Thus, if I happen to have a strong enough preference that E output True, then TDT (as currently formulated) will tell me to two-box for the sake of that goal. But that's the wrong decision, of course. In reality, I have no choice about the specified digit of pi.</div>\n<div>What's going on, it seems to me, is that the kind of logical/Platonic \"dependency\" that TDT would need to invoke here is this: that E's output be&nbsp;counterfactually entailed&nbsp;by C's output (which it isn't, in this case [see footnote]), rather than (as you've specified) merely&nbsp;inferable&nbsp;from C's output (which indeed it is, in this case). That's bad news, because distinguishing what my action does or does not counterfactually entail (as opposed to what it implies, causes, gives evidence for, etc.) is the original full-blown problem that TDT's prescribed decision-computation is meant to solve. So it may turn out that in order to proceed with that very computation (specifically,&nbsp;in order to ascertain which other Platonic computations \"depend on\" the decision computation C), you already need to (somehow) know the answer that the computation is trying to provide.</div>\n<div>--Gary</div>\n<div>[footnote] Because if-counterfactually C were to two-box, then (contrary to fact) the large box would (probably) be empty, circumventing the inference about E.</div>\n<div>[appendix] In&nbsp;<a title=\"comments on What Program Are You?\" href=\"/lw/1b4/what_program_are_you/16cb\">this post</a>, you write:</div>\n<blockquote>\n<div>...reasoning under logical uncertainty using limited computing power... is another huge unsolved open problem of AI. Human mathematicians had this whole elaborate way of believing that the Taniyama Conjecture implied Fermat's Last Theorem at a time when they didn't know whether the Taniyama Conjecture was true or false; and we seem to treat this sort of implication in a rather different way than '2=1 implies FLT', even though the material implication is equally valid.</div>\n</blockquote>\n<div>I don't follow that.&nbsp;The sense of implication in which mathematicians established that TC implies FLT (before knowing if TC was true) is&nbsp;precisely&nbsp;material/logical implication: they showed ~(TC &amp; ~FLT). And similarly, we can prove ~(3SAT-in-P &amp; ~(P=NP)), etc. There's no need here to construct (or magically conjure) a whole alternative inference system for reasoning under logical uncertainty.</div>\n<div>So if the inference you speak of (when specifying what it means for D to \"depend on\" C) is the same kind as was used in establishing TC=&gt;FLT, then it's just material implication, which (as argued above) leads TDT to give wrong answers. Or if we substitute counterfactual entailment for material implication, then TDT becomes circular (question-begging). Or if you have in mind some third alternative, I'm afraid I don't understand what it might be.</div>\n<div>EDIT:&nbsp;The rules of the original transparent-boxes problem (as specified in&nbsp;<em>Good and Real</em>) are: the predictor conducts a simulation that tentatively presumes there will be $1M in the large box, and then puts $1M in the box (for real) iff that simulation showed one-boxing. Thus, if the large box turns out to be&nbsp;<em>empty</em>, there is no requirement for that to be predictive of the agent's choice under those circumstances. The present variant is the same, except that (D xor E) determines the $1M, instead of just D. (Sorry, I should have said this to begin with, instead of assuming it as background knowledge.)</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "5f5c37ee1b5cdee568cfb1db": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NSX8RuD9tQ4uWzkk3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 44, "extendedScore": null, "score": 0.000550405687542528, "legacy": true, "legacyId": "2256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 140, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["szfxvS8nsxTgJLBHs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-05T14:47:56.891Z", "modifiedAt": "2021-09-08T08:16:53.013Z", "url": null, "title": "Debate tools: an experience report", "slug": "debate-tools-an-experience-report", "viewCount": null, "lastCommentedAt": "2018-02-10T23:18:12.363Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dJJYgmaYYFmHoQM4L/debate-tools-an-experience-report", "pageUrlRelative": "/posts/dJJYgmaYYFmHoQM4L/debate-tools-an-experience-report", "linkUrl": "https://www.lesswrong.com/posts/dJJYgmaYYFmHoQM4L/debate-tools-an-experience-report", "postedAtFormatted": "Friday, February 5th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Debate%20tools%3A%20an%20experience%20report&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADebate%20tools%3A%20an%20experience%20report%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdJJYgmaYYFmHoQM4L%2Fdebate-tools-an-experience-report%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Debate%20tools%3A%20an%20experience%20report%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdJJYgmaYYFmHoQM4L%2Fdebate-tools-an-experience-report", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdJJYgmaYYFmHoQM4L%2Fdebate-tools-an-experience-report", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1043, "htmlBody": "<p><strong>Follow-up to:</strong> <a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">Argument Maps Improve Critical Thinking</a>, <a href=\"/lw/1y/software_tools_for_community_truthseeking/\">Software Tools for Community Truth-Seeking</a></p>\n<p>We are here, among other things, in an attempt to <em>collaboratively</em> refine the art of human rationality.</p>\n<p>Rationality is hard, because the wetware we run rationality on is scavenged parts originally intended for other purposes; and <a href=\"http://www.overcomingbias.com/2010/02/coordination-is-hard.html\">collaboration is hard</a>, I believe because it involves huge numbers of tiny decisions about what information others need. Yet we get by, largely thanks to advances in technology.</p>\n<p>One of the most important technologies for advancing both rationality and collaboration is the written word. It affords looking at large, complex issues with limited cognitive resources, by the wonderful trick of \"external cached thoughts\". Instead of trying to hold every piece of the argument at once, you can store parts of it in external form, refer back to them, and communicate them to other people.</p>\n<p>For some reason, it seems <em>very</em> hard to improve on this six-thousand-year-old technology. Witness LessWrong itself, which in spite of using some of the latest and greatest <em>communication</em> technologies, still has people arguing by exchanging sentences back and forth.</p>\n<p>Previous posts have suggested that recent software tools might hold promise for improving on \"traditional\" forms of argument. This kind of suggestion is often more valuable when applied to a real and relevant case study. I found the promise compelling enough to give a few tools a try, in the context of the recent (and recurrent) <a href=\"/lw/1mc/normal_cryonics/\">cryonics debate</a>. I report back here with my findings.</p>\n<p><a id=\"more\"></a></p>\n<p><strong>I. Argunet</strong></p>\n<p>The first tool I tried was <a href=\"http://www.argunet.org/working-with-argunet/\">Argunet</a>, an Open Source offering from the Institute of Philosophy in Berlin. I was seduced by the promise of reconstructing the logical structure of an argument, and by the possiblity of collaborating online with others on an argument.</p>\n<p>Like other products in that category, the basic principle of operation of Argunet is that of a visual canvas, on which you can create and arrange boxes which represents statements, portions of an argument. Relationships between parts of an arguments are then materialized using links or arrows.</p>\n<p>Argunet supports two types of basic relationship between statements, <em>Supports</em> and <em>Attacks</em>. It also supports several types of \"inference patterns\".</p>\n<p>Unfortunately, when I tried using the Editor I soon found it difficult to the point of being unusable. The default expectation of being able to move boxes around by clicking and dragging is violated. Further, I was unable to find <em>any</em> way to move my boxes after initially creating them.</p>\n<p>I ended up frustrated and gave up on Argunet.</p>\n<p><strong>II. bCisive Online</strong></p>\n<p>I had somewhat better luck with the next tool I tried, <a href=\"http://bcisiveonline.com/\">bCisive online</a>. This is a public beta of a commercial offering by Austhink, the company already referenced in the previous posts on argument mapping. (It is a spin-off of their range of products marketed for decision support rather than argument support, but is also their only online, collaborative tool so far.)</p>\n<p>The canvas metaphor proved to be implemented more effectively, and I was able in a relatively short time to sketch out <a href=\"http://morendil.bcisiveonline.com/spaces/989ae551bc100d0365c96a7bcc20f188d95fb58d/\">a map of my thinking about cryonics</a> (which I invite you to browse and comment on).</p>\n<p>bCisive supports different types of statements, distinguished by the icons on their boxes: questions; arguments pro or con; evidence; options; \"fixes\", and so on. At present it doesn't appear to *do* anything valuable with these distinctions, but they proved to be an effective scheme for organizing my thoughts.</p>\n<p><strong>III. Preliminary conclusions</strong></p>\n<p>I was loath to invest much more time in updating my cryonics decision map, for two reasons. One is that what I would like to get from such a tool is to incorporate others' objections and counter-objections; in fact, it seems to me that the more valuable approach would be a fully collaborative effort. So, while it was worthwhile to structure my own thinking using the tool, and (killing two birds with one stone) that served as a test drive for the tool, it seems pointless to continue without outside input.</p>\n<p>The other, more important reason is that bCisive seems to provide little more than a fancy mindmapping tool at the moment, and the glimpse I had of tool support for structuring a debate has already raised my expectations beyond that.</p>\n<p>I have my doubts that the \"visual\" aspect is as important as the creators of such software tools would like everyone to think. It seems to me that what helped focus my thinking when using bCisive was the scheme of statement types: conclusion, arguments pro and con, evidence and \"fixes\". This might work just as well if the tool used a textual, tabular or other representation.</p>\n<p>The argument about cryonics is important to me, and to others who are considering cryonics. It is a life decision of some consequence, not to be taken lightly and without due deliberation. For this reason, I found myself wishing that the tool could process quantitative, not just qualitative, aspects of my reasoning.</p>\n<p><strong>IV. A wish list for debate support<br /></strong></p>\n<p>Based on my experiences, what I would look for is a tool that distinguishes between, and support the use of:</p>\n<ul>\n<li>a conclusion or a decision, which is to be \"tested\" by the use of the tool</li>\n<li>various hypotheses, which are offered in support or in opposition to the conclusion, with degrees of plausibility</li>\n<li>logical structure, such as \"X follows from Y\"</li>\n<li>challenges to logical structure, such as \"X may not necessarily follow from Y, if you grant Z\"</li>\n<li>elements of evidence, which make hypotheses more or less probable</li>\n<li>recursive relations between these elements</li>\n</ul>\n<p>The tool should be able to \"crunch numbers\", so that it gives an overall indication of how much the total weight of evidence and argumentation contributes to the conclusion.</p>\n<p>It should have a \"public\" part, representing what a group of people can agree on regarding the structure of the debate; and a \"private\" part, wherein you can adduce evidence you have collected yourself, or assign private degrees of belief in various statements.</p>\n<p>In this way, the tool would allow \"settling\" debates even while allowing disagreement to persist, temporarily or durably: you could agree with the logical structure but allow that your personal convictions rationally lead you to different conclusions. Highlighting the points of agreement and contention in this way would be a valuable way to focus further debate, limiting the risk of \"logical rudeness\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 2, "5f5c37ee1b5cdee568cfb226": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dJJYgmaYYFmHoQM4L", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 50, "extendedScore": null, "score": 7.7e-05, "legacy": true, "legacyId": "2258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Follow-up to:</strong> <a href=\"/lw/16u/argument_maps_improve_critical_thinking/\">Argument Maps Improve Critical Thinking</a>, <a href=\"/lw/1y/software_tools_for_community_truthseeking/\">Software Tools for Community Truth-Seeking</a></p>\n<p>We are here, among other things, in an attempt to <em>collaboratively</em> refine the art of human rationality.</p>\n<p>Rationality is hard, because the wetware we run rationality on is scavenged parts originally intended for other purposes; and <a href=\"http://www.overcomingbias.com/2010/02/coordination-is-hard.html\">collaboration is hard</a>, I believe because it involves huge numbers of tiny decisions about what information others need. Yet we get by, largely thanks to advances in technology.</p>\n<p>One of the most important technologies for advancing both rationality and collaboration is the written word. It affords looking at large, complex issues with limited cognitive resources, by the wonderful trick of \"external cached thoughts\". Instead of trying to hold every piece of the argument at once, you can store parts of it in external form, refer back to them, and communicate them to other people.</p>\n<p>For some reason, it seems <em>very</em> hard to improve on this six-thousand-year-old technology. Witness LessWrong itself, which in spite of using some of the latest and greatest <em>communication</em> technologies, still has people arguing by exchanging sentences back and forth.</p>\n<p>Previous posts have suggested that recent software tools might hold promise for improving on \"traditional\" forms of argument. This kind of suggestion is often more valuable when applied to a real and relevant case study. I found the promise compelling enough to give a few tools a try, in the context of the recent (and recurrent) <a href=\"/lw/1mc/normal_cryonics/\">cryonics debate</a>. I report back here with my findings.</p>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"I__Argunet\">I. Argunet</strong></p>\n<p>The first tool I tried was <a href=\"http://www.argunet.org/working-with-argunet/\">Argunet</a>, an Open Source offering from the Institute of Philosophy in Berlin. I was seduced by the promise of reconstructing the logical structure of an argument, and by the possiblity of collaborating online with others on an argument.</p>\n<p>Like other products in that category, the basic principle of operation of Argunet is that of a visual canvas, on which you can create and arrange boxes which represents statements, portions of an argument. Relationships between parts of an arguments are then materialized using links or arrows.</p>\n<p>Argunet supports two types of basic relationship between statements, <em>Supports</em> and <em>Attacks</em>. It also supports several types of \"inference patterns\".</p>\n<p>Unfortunately, when I tried using the Editor I soon found it difficult to the point of being unusable. The default expectation of being able to move boxes around by clicking and dragging is violated. Further, I was unable to find <em>any</em> way to move my boxes after initially creating them.</p>\n<p>I ended up frustrated and gave up on Argunet.</p>\n<p><strong id=\"II__bCisive_Online\">II. bCisive Online</strong></p>\n<p>I had somewhat better luck with the next tool I tried, <a href=\"http://bcisiveonline.com/\">bCisive online</a>. This is a public beta of a commercial offering by Austhink, the company already referenced in the previous posts on argument mapping. (It is a spin-off of their range of products marketed for decision support rather than argument support, but is also their only online, collaborative tool so far.)</p>\n<p>The canvas metaphor proved to be implemented more effectively, and I was able in a relatively short time to sketch out <a href=\"http://morendil.bcisiveonline.com/spaces/989ae551bc100d0365c96a7bcc20f188d95fb58d/\">a map of my thinking about cryonics</a> (which I invite you to browse and comment on).</p>\n<p>bCisive supports different types of statements, distinguished by the icons on their boxes: questions; arguments pro or con; evidence; options; \"fixes\", and so on. At present it doesn't appear to *do* anything valuable with these distinctions, but they proved to be an effective scheme for organizing my thoughts.</p>\n<p><strong id=\"III__Preliminary_conclusions\">III. Preliminary conclusions</strong></p>\n<p>I was loath to invest much more time in updating my cryonics decision map, for two reasons. One is that what I would like to get from such a tool is to incorporate others' objections and counter-objections; in fact, it seems to me that the more valuable approach would be a fully collaborative effort. So, while it was worthwhile to structure my own thinking using the tool, and (killing two birds with one stone) that served as a test drive for the tool, it seems pointless to continue without outside input.</p>\n<p>The other, more important reason is that bCisive seems to provide little more than a fancy mindmapping tool at the moment, and the glimpse I had of tool support for structuring a debate has already raised my expectations beyond that.</p>\n<p>I have my doubts that the \"visual\" aspect is as important as the creators of such software tools would like everyone to think. It seems to me that what helped focus my thinking when using bCisive was the scheme of statement types: conclusion, arguments pro and con, evidence and \"fixes\". This might work just as well if the tool used a textual, tabular or other representation.</p>\n<p>The argument about cryonics is important to me, and to others who are considering cryonics. It is a life decision of some consequence, not to be taken lightly and without due deliberation. For this reason, I found myself wishing that the tool could process quantitative, not just qualitative, aspects of my reasoning.</p>\n<p><strong id=\"IV__A_wish_list_for_debate_support\">IV. A wish list for debate support<br></strong></p>\n<p>Based on my experiences, what I would look for is a tool that distinguishes between, and support the use of:</p>\n<ul>\n<li>a conclusion or a decision, which is to be \"tested\" by the use of the tool</li>\n<li>various hypotheses, which are offered in support or in opposition to the conclusion, with degrees of plausibility</li>\n<li>logical structure, such as \"X follows from Y\"</li>\n<li>challenges to logical structure, such as \"X may not necessarily follow from Y, if you grant Z\"</li>\n<li>elements of evidence, which make hypotheses more or less probable</li>\n<li>recursive relations between these elements</li>\n</ul>\n<p>The tool should be able to \"crunch numbers\", so that it gives an overall indication of how much the total weight of evidence and argumentation contributes to the conclusion.</p>\n<p>It should have a \"public\" part, representing what a group of people can agree on regarding the structure of the debate; and a \"private\" part, wherein you can adduce evidence you have collected yourself, or assign private degrees of belief in various statements.</p>\n<p>In this way, the tool would allow \"settling\" debates even while allowing disagreement to persist, temporarily or durably: you could agree with the logical structure but allow that your personal convictions rationally lead you to different conclusions. Highlighting the points of agreement and contention in this way would be a valuable way to focus further debate, limiting the risk of \"logical rudeness\".</p>", "sections": [{"title": "I. Argunet", "anchor": "I__Argunet", "level": 1}, {"title": "II. bCisive Online", "anchor": "II__bCisive_Online", "level": 1}, {"title": "III. Preliminary conclusions", "anchor": "III__Preliminary_conclusions", "level": 1}, {"title": "IV. A wish list for debate support", "anchor": "IV__A_wish_list_for_debate_support", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "80 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NJ8k2RTwy3ELmwYZT", "MYXrsBzXNPqTNarqQ", "hiDkhLyN5S2MEjrSE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2010-02-05T14:47:56.891Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-06T06:28:24.157Z", "modifiedAt": null, "url": null, "title": "Ethics has Evidence Too", "slug": "ethics-has-evidence-too", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:29.200Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jack", "createdAt": "2009-02-27T15:27:14.891Z", "isAdmin": false, "displayName": "Jack"}, "userId": "GwetakMQqsGCf7ZQv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tTgSycdDr9qdGD3CC/ethics-has-evidence-too", "pageUrlRelative": "/posts/tTgSycdDr9qdGD3CC/ethics-has-evidence-too", "linkUrl": "https://www.lesswrong.com/posts/tTgSycdDr9qdGD3CC/ethics-has-evidence-too", "postedAtFormatted": "Saturday, February 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ethics%20has%20Evidence%20Too&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEthics%20has%20Evidence%20Too%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTgSycdDr9qdGD3CC%2Fethics-has-evidence-too%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ethics%20has%20Evidence%20Too%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTgSycdDr9qdGD3CC%2Fethics-has-evidence-too", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtTgSycdDr9qdGD3CC%2Fethics-has-evidence-too", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 771, "htmlBody": "<p>A tenet of traditional rationality is that you can't learn much about the world from armchair theorizing. Theory must be epiphenomenal to observation-- our theories are functions that tell us what experiences we should anticipate, but we generate the theories from *past* experiences. And of course we update our theories on the basis of new experiences. Our theories respond to our evidence, usually not the other way around. We do it this way because it works better then trying to make predictions on the basis of concepts or abstract reasoning. Philosophy from Plato through Descartes and to Kant is replete with failed examples of theorizing about the natural world on the basis of something other than empirical observation. Socrates thinks he has deduced that souls are immortal, Descartes thinks he has deduced that he is an immaterial mind, that he is immortal, that God exists and that he can have secure knowledge of the external world, Kant thinks he has proven by pure reason the necessity of Newton's laws of motion.</p>\n<p>These mistakes aren't just found in philosophy curricula. There is a long list of people who thought they could deduce Euclid's theorems as analytic or a priori knowledge. <a href=\"/\">Epicycles</a> were a response to new evidence but they weren't a response that truly privileged the evidence. Geocentric astronomers changed their theory *just enough* so that it would yield the right predictions instead of letting a new theory flow from the evidence. Same goes for <a href=\"http://en.wikipedia.org/wiki/Timeline_of_luminiferous_aether\">pre-Einsteinian theories of light</a>. <a href=\"/lw/r8/and_the_winner_is_manyworlds/\">Same goes for quantum mechanics</a>. A kludge is a sign someone is privileging the hypothesis. It's the same way <a href=\"/lw/1j7/the_amanda_knox_test_how_an_hour_on_the_internet/\">many of us think</a> the Italian police changed their hypothesis explaining the murder of Meredith Kercher once it became clear Lumumba had an alibi and Rudy Guede's DNA and hand prints were found all over the crime scene. They just replaced Lumumba with Guede and left the rest of their theory unchanged even though there was no longer reason to include Knox and Sollecito in the explanation of the murder. These theories may make it over the bar of traditional rationality but they <a href=\"/lw/qd/science_isnt_strict_enough/\">sail right under what Bayes theorem requires</a>.</p>\n<p>Most people here get this already and many probably understand it better than I do. But I think it needs to be brought up in the context of our <a href=\"/lw/1og/deontology_for_consequentialists/\">ongoing discussion</a> of normative ethics.</p>\n<p><em>Unless we have reason to think about ethics differently</em>, our normative theories should respond to evidence in the same way we expect our theories in other domains to respond to evidence. What are the experiences that we are trying to explain with our ethical theories? Why bother with ethics at all? What is the mystery we are trying to solve? The only answer I can think of is our ethical intuitions. When faced with certain situations in real life or in fiction we get strong impulses to react in certain ways, to praise some parties and condemn others. We feel guilt and sometimes pay amends. There are some actions which we have a visceral abhorrence of.</p>\n<p>These reactions are for ethics what measurements of time and distance are for physics -- the evidence.<a id=\"more\"></a></p>\n<p>The reason ethicists use hypotheticals like the runaway trolley and the unwilling organ donor is that different normative theories predict different intuitions in response to such scenarios. Short of actually setting up these scenarios for real, this is as close as ethics gets to controlled experiments. Now there <em>are</em> problems with this method. Our intuitions in fictional cases might be different from real life intuitions. The scenario could be poorly described. It might not be as controlled an experiment as we think. Or some features could be clouding the issue such that our intuitions about a particular case might not actually falsify a particular ethical principle. Just as there are optical illusions there might be ethical illusions such that we can occasionally be wrong about an ethical judgment in the same way that we can sometimes be wrong about the size or velocity of a physical object.</p>\n<p>The big point is that the way we should be reasoning about ethics is <em>not</em> from first principles, a priori truths, definitions or psychological concepts. Kant's Categorical Imperative is a paradigm example of screwing this up, but he is hardly the only one. We should be looking at our ethical intuitions and trying to come up with theories that predict future ethical intuitions. And if your theory is outputting results that are systematically or radically different from actual ethical intuitions then you need to have a <em>damn good explanation</em> for the discrepancy or be ready to change your theory (and not just by adding a kludge).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nSHiKwWyMZFdZg5qt": 1, "xgpBASEThXPuKRhbS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tTgSycdDr9qdGD3CC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 26, "extendedScore": null, "score": 5.591004136615664e-07, "legacy": true, "legacyId": "2259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9cgBF6BQ2TRB3Hy4E", "G9dptrW9CJi7wNg3b", "PGfJdgemDJSwWBZSX", "yppdL4EXLWda5Wthn"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-06T14:27:12.762Z", "modifiedAt": null, "url": null, "title": "BHTV: Eliezer Yudkowsky & Razib Khan", "slug": "bhtv-eliezer-yudkowsky-and-razib-khan", "viewCount": null, "lastCommentedAt": "2017-06-17T04:32:09.314Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "MichaelGR", "user": {"username": "MichaelGR", "createdAt": "2009-02-28T05:12:37.952Z", "isAdmin": false, "displayName": "MichaelGR"}, "userId": "d93XiAFJyfvfRWw7Z", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4gP3692vXuauKGjMR/bhtv-eliezer-yudkowsky-and-razib-khan", "pageUrlRelative": "/posts/4gP3692vXuauKGjMR/bhtv-eliezer-yudkowsky-and-razib-khan", "linkUrl": "https://www.lesswrong.com/posts/4gP3692vXuauKGjMR/bhtv-eliezer-yudkowsky-and-razib-khan", "postedAtFormatted": "Saturday, February 6th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20BHTV%3A%20Eliezer%20Yudkowsky%20%26%20Razib%20Khan&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABHTV%3A%20Eliezer%20Yudkowsky%20%26%20Razib%20Khan%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gP3692vXuauKGjMR%2Fbhtv-eliezer-yudkowsky-and-razib-khan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=BHTV%3A%20Eliezer%20Yudkowsky%20%26%20Razib%20Khan%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gP3692vXuauKGjMR%2Fbhtv-eliezer-yudkowsky-and-razib-khan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4gP3692vXuauKGjMR%2Fbhtv-eliezer-yudkowsky-and-razib-khan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p><a href=\"http://bloggingheads.tv/diavlogs/25848\">Link</a>.</p>\n<p><a id=\"more\"></a></p>\n<p>\"<span style=\"font-family: arial; font-size: 12px;\"><strong>Razib Khan</strong>&nbsp;has an academic background in the biological sciences, and has worked for many years in software. He is an Unz Foundation Junior Fellow. He lives in the western United States.\"&nbsp;</span></p>\n<p><span style=\"font-family: arial; font-size: small;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif;\"><span style=\"font-size: small;\">Razib's writings can be found on his blog,&nbsp;<em><a href=\"http://www.gnxp.com/\">Gene Expression</a></em>.</span></span></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4gP3692vXuauKGjMR", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 15, "extendedScore": null, "score": 5.591903392902897e-07, "legacy": true, "legacyId": "2262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-07T23:26:52.715Z", "modifiedAt": null, "url": null, "title": " A survey of anti-cryonics writing ", "slug": "a-survey-of-anti-cryonics-writing", "viewCount": null, "lastCommentedAt": "2019-10-26T04:02:17.147Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ciphergoth", "createdAt": "2009-02-27T14:23:33.426Z", "isAdmin": false, "displayName": "Paul Crowley"}, "userId": "baGAQoNAH4hXaC6qf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZXaRHHLsxaTTQQsZb/a-survey-of-anti-cryonics-writing", "pageUrlRelative": "/posts/ZXaRHHLsxaTTQQsZb/a-survey-of-anti-cryonics-writing", "linkUrl": "https://www.lesswrong.com/posts/ZXaRHHLsxaTTQQsZb/a-survey-of-anti-cryonics-writing", "postedAtFormatted": "Sunday, February 7th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20A%20survey%20of%20anti-cryonics%20writing%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20A%20survey%20of%20anti-cryonics%20writing%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXaRHHLsxaTTQQsZb%2Fa-survey-of-anti-cryonics-writing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20A%20survey%20of%20anti-cryonics%20writing%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXaRHHLsxaTTQQsZb%2Fa-survey-of-anti-cryonics-writing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZXaRHHLsxaTTQQsZb%2Fa-survey-of-anti-cryonics-writing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2068, "htmlBody": "<p>(This was originally a link to a post on my blog, <a href=\"http://blog.ciphergoth.org/blog/2010/02/07/survey-anti-cryonics-writing/\">A survey of anti-cryonics writing</a>. Eliezer <a href=\"/lw/1mc/normal_cryonics/1l3z\">asked me</a> to include the entire text of the article here.)</p>\n<p>For its advocates, <a href=\"http://en.wikipedia.org/wiki/Cryonics\">cryonics</a> offers almost eternal life. To its critics, cryonics is pseudoscience; the idea that we could freeze someone today in such a way that future technology might be able to re-animate them is nothing more than wishful thinking on the desire to avoid death. Many who battle nonsense dressed as science have spoken out against it: see for example <a href=\"http://www.michaelshermer.com/2001/09/nano-nonsense-and-cryonics/\">Nano Nonsense and Cryonics</a>, a 2001 article by celebrated skeptic Michael Shermer; or check the <a href=\"http://www.skepdic.com/cryonics.html\">Skeptic's Dictionary</a> or <a href=\"http://www.quackwatch.com/04ConsumerEducation/QA/cryonics.html\">Quackwatch</a> entries on the subject, or for more detail read the essay <a href=\"http://invisibleflan.wordpress.com/2006/12/17/cryonics%E2%80%93a-futile-desire-for-everlasting-life/\">Cryonics&ndash;A futile desire for everlasting life</a> by \"Invisible Flan\".</p>\n<p>That it seems so makes me sad, because to my naive eyes it seems like it might work and I would quite like to live forever, but I know that I don't know enough to judge. The celebrated Nobel prize winning physicist <a href=\"http://en.wikipedia.org/wiki/Richard_Feynman\">Richard Feynman</a> tells a story of a US general who spoke to him at a party and explained that one big challenge in desert warfare is keeping the tanks fuelled given the huge distances the fuel has to travel. What would really help, the general said, would be if boffins like Feynman could invent a sort of engine that was powered by sand. On this issue, I'm in the same position as the general; in the same way as a tank fuelled by sand seems plausible enough to him, it makes sense to me to imagine that however your brain stores information it probably has something to do with morphology and chemistry, so there's a good chance it might not evaporate right away at the instant of legal death, and that freezing might be a way to keep the information there long enough for future societies to extract it with their future-technology scanning equipment.</p>\n<p>And of course the pro-cryonics people have written reams and reams of material such as Ben Best's <a href=\"http://www.cryonics.org/reports/Scientific_Justification.pdf\">Scientific Justification of Cryonics Practice</a> on why they think this is exactly as plausible as I might think, and going into tremendous technical detail setting out arguments for its plausibility and addressing particular difficulties. It's almost enough to make you want to sign up on the spot.</p>\n<p>Except, of course, that plenty of totally unscientific ideas are backed by reams of scientific-sounding documents good enough to fool non-experts like me. Backed by the deep pockets of the oil industry, global warming denialism has produced thousands of convincing-sounding arguments against the scientific consensus on CO2 and AGW. Thankfully in that instance we have blogs like Tim Lambert's <a href=\"http://scienceblogs.com/deltoid/\">Deltoid</a>, <a href=\"http://www.realclimate.org/\">RealClimate</a>, and many others tracking the various ways that the denialists mislead, whether through cherry-picking evidence, misleading quotes from climate scientists, or outright lies. Their hard work means that denialists can barely move or speak without someone out there checking what they have to say against science's best understanding and pointing out the misrepresentations and discrepancies. So before I pony up my &pound;25 a month to sign up to cryonics life insurance, I want to read the Deltoid of cryonics - the articles that take apart what cryonics advocates write about what they do and really go into the scientific detail on why it doesn't hang together.</p>\n<p>Here's my report on what I've found so far.<a id=\"more\"></a><!--more--></p>\n<p><a href=\"http://www.michaelshermer.com/2001/09/nano-nonsense-and-cryonics/\">Nano Nonsense and Cryonics</a> goes for the nitty-gritty right away in the opening paragraph:</p>\n<blockquote>To see the flaw in this system, thaw out a can of frozen strawberries. During freezing, the water within each cell expands, crystallizes, and ruptures the cell membranes. When defrosted, all the intracellular goo oozes out, turning your strawberries into runny mush. This is your brain on cryonics.</blockquote>\n<p>This sounds convincing, but doesn't address what cryonicists actually claim. <a href=\"http://en.wikipedia.org/wiki/Ben_Best\">Ben Best</a>, President and CEO of the Cryonics Institute, replies in the comments:</p>\n<blockquote>Strawberries (and mammalian tissues) are not turned to mush by freezing because water expands and crystallizes inside the cells. Water crystallizes in the extracellular space because more nucleators are found extracellularly. As water crystallizes in the extracellular space, the extracellular salt concentration increases causing cells to lose water osmotically and shrink. Ultimately the cell membranes are broken by crushing from extracellular ice and/or high extracellular salt concentration. [...] Cryonics organizations use vitrification perfusion before cooling to cryogenic temperatures. With good brain perfusion, vitrification can reduce ice formation to negligible amounts.</blockquote>\n<p>Best goes on to point out that the paragraph I quote is Shermer's sole attempt to directly address the scientific claims of cryonics; once the opening paragraph has dispensed with the technical nitty gritty, the rest of the piece argues in very general terms about \"[blind] optimistic faith in the illimitable power of science\" and other such arguments. Shermer received many other responses from cryonics advocates; here's one that he considered \"<a href=\"http://leitl.org/docs/public_html/tt/msg11904.html\">very well reasoned and properly nuanced</a>\".</p>\n<p>The <a href=\"http://www.quackwatch.com/04ConsumerEducation/QA/cryonics.html\">Quackwatch</a> entry takes us little further; it quotes the debunked Shermer argument above, talks about the cost (they all talk about the cost and a variety of other issues, but here I'm focussing specifically on the issue of technical plausibility), and links to someone else making the same already-answered assertions about freezing damage.</p>\n<p>The <a href=\"http://www.skepdic.com/cryonics.html\">Skeptic's Dictionary</a> entry is no advance. Again, it refers erroneously to a \"mushy brain\". It points out that the technology to reanimate those in storage does not already exist, but provides no help for us non-experts in assessing whether it is a plausible future technology, like super-fast computers or fusion power, or whether it is as crazy as the sand-powered tank; it simply asserts baldly and to me counterintuitively that it is the latter. Again, perhaps cryonic reanimation is a sand-powered tank, but I can explain to you why a sand-powered tank is implausible if you don't already know, and if cryonics is in the same league I'd appreciate hearing the explanation.</p>\n<p>It does link to the one article I can find that really tries to go into the detail: <a href=\"http://invisibleflan.wordpress.com/2006/12/17/cryonics%E2%80%93a-futile-desire-for-everlasting-life/\">Cryonics&ndash;A futile desire for everlasting life</a> by \"Invisible Flan\". It opens on a curious note:</p>\n<blockquote>If you would like my cited sources, please ask me and I will give them to you.</blockquote>\n<p>This seems a very odd practice to me. How can it make sense to write \"(Stroh)\" in the text without telling us what publication that refers to? Two comments below ask for the references list; no reply is forthcoming.</p>\n<p>And again, there seems to be no effort to engage with what cryonicists actually say. The article assets</p>\n<blockquote>it is very likely that a human would suffer brain damage from being preserved for a century or two (Stroh).</blockquote>\n<p>This bald claim backed by a dangling reference is, to say the least, a little less convincing than the argument set out in Alcor's <a href=\"http://www.alcor.org/Library/html/HowColdIsColdEnough.html\">How Cold is Cold Enough?</a> which explains that even with pessimistic assumptions, one second of chemical activity at body temperature is roughly equivalent to 24 million years at the temperature of liquid nitrogen. Ben Best <a href=\"http://www.benbest.com/cryonics/mobility.html\">quotes eminent cryobiologist and anti-cryonics advocate Peter Mazur</a>:</p>\n<blockquote>...viscosity is so high (&gt;1013 Poise) that diffusion is insignificant over less than geological time spans.</blockquote>\n<p>Another part of the article points out the well-known difficulties with whole-body freezing - because the focus is on achieving the best possible preservation of the brain, other parts suffer more. But the reason why the brain is the focus is that you can afford to be a lot bolder in repairing other parts of the body - unlike the brain, if my liver doesn't survive the freezing, it can be replaced altogether. Further, the article ignores one of the most promising possibilities for reanimation, that of scanning and whole-brain emulation, a route that requires some big advances in computer and scanning technology as well as our understanding of the lowest levels of the brain's function, but which completely sidesteps any problems with repairing either damage from the freezing process or whatever it was that led to legal death.</p>\n<p>Contrast these articles to a blog like Deltoid. In post after painstaking post, Lambert addresses specific public claims from global warming denialists - sometimes this takes <a href=\"http://scienceblogs.com/deltoid/2010/02/andrew_bolt_in_one_graph.php\">just one graph</a>, sometimes <a href=\"http://scienceblogs.com/deltoid/2010/01/a_beat_up_of_himalayan_proport.php\">a devastating point-by-point rebuttal</a>.</p>\n<p>Well, if there is a Tim Lambert of cryonics out there, I have yet to find them, and I've looked as best I can. I've tried various Google searches, like \"anti-cryonics\" or \"cryonics skeptic\", but nearly all the hits are pro-cryonics. I've <a href=\"http://ciphergoth.livejournal.com/353409.html\">asked my LiveJournal friends list</a>, my Twitter feed, and LessWrong.com, and found no real meat. I've searched PubMed and Google Scholar, and again found only pro-cryonics articles, with the exception of <a href=\"http://www.bmj.com/cgi/pdf_extract/283/6304/1449\">this 1981 BMJ article</a> which is I think more meant for humour value than serious argument.</p>\n<p>I've also emailed every expert I can find an email address for that has publically spoken against cryonics. Sadly I don't have email addresses for either Arthur W. Rowe or Peter Mazur, two giants of the cryobiology field who both have strongly anti-cryonics positions; I can only hope that blog posts like these might spur them into writing about the subject in depth rather than restricting themselves to rather brief and unsatisfactory remarks in interviews. (If they were to have a change of heart on the subject, they would have to choose between staying silent on their true opinions or being ejected from the Society for Cryobiology under a <a href=\"http://www.alcor.org/Library/html/coldwar.html\">1982 by-law</a>.) I mailed Michael Shermer, <a href=\"http://news.bbc.co.uk/1/hi/health/2268181.stm\">Steve Jones</a>, Quackwatch, and <a href=\"http://news.bbc.co.uk/1/hi/2133961.stm\">Professor David Pegg</a>. I told them (quite truthfully) that I had recently started talking to some people who were cryonics advocates, that they seemed persuasive but I wasn't an expert and didn't want to fall for a scam, and asked if there was anything they'd recommend I'd read on the subject to see the other side.</p>\n<p>The only one of these to reply was Michael Shermer. He recommended I read David Brin, Steve Harris and Gregory Benford. This is a pretty surprising reply. The latter two are cryonics advocates, and while Brin talks about a lot of possible problems, he agrees with cryonics advocates that it is technically feasable.</p>\n<p>I expanded my search to others who might be knowledgable: Society of Cryobiology fellows Professor Barry Fuller and Dr John G Baust, and computational neuroscience Professor Peter Dayan. I received one reply: Dayan was kind enough to reply very rapidly, sounding a cautionary note on how much we still don't know about the structure of memory and referring me to the literature on the subject, but was unable to help in my specific quest for technical anti-cryonics articles.</p>\n<p>In his 1994 paper <a href=\"http://www.merkle.com/cryo/techFeas.html\">The Molecular Repair of the Brain</a>, cryptology pioneer Professor Ralph Merkle remarks</p>\n<blockquote>Interestingly (and somewhat to the author's surprise) there are no published technical articles on cryonics that claim it won't work.</blockquote>\n<p>Sixteen years later, it seems that hasn't changed; in fact, as far as the issue of technical feasability goes it is starting to look as if on all the Earth, or at least all the Internet, <strong>there is not one person who has ever taken the time to read and understand cryonics claims in any detail, still considers it pseudoscience, and has written a paper, article or even a blog post to rebut anything that cryonics advocates actually say</strong>. In fact, the best of the comments on <a href=\"http://ciphergoth.livejournal.com/353258.html\">my first blog post on the subject</a> are already a higher standard than anything my searches have turned up.</p>\n<p>If you can find any articles that I've missed, please link to them in the comments. If you have any expertise in any relevant area, and you don't think that cryonics has scientific merit - or if you can find any claim made by prominent cryonics advocates that doesn't hold up - any paragraph in Ben Best's <a href=\"http://www.cryonics.org/reports/Scientific_Justification.pdf\">Scientific Justification of Cryonics Practice</a>, anything in the <a href=\"http://www.alcor.org/sciencefaq.htm\">Alcor Scientists&rsquo; Cryonics FAQ</a> or the <a href=\"http://cryonics.org/prod.html\">Cryonics Institute FAQ</a>, or anything in <a href=\"http://www.fhi.ox.ac.uk/selected_outputs/fohi_publications/brain_emulation_roadmap\">Whole Brain Emulation: A Roadmap</a> (which isn't directly about cryonics but is closely related) - then please, don't comment here to say so. Instead, write a paper or a blog post about it. If you don't have somewhere you're happy to post it, and if it's better than what's already out there, I'll be happy to host it here.</p>\n<p>Because as far as I can tell, if you want to write the best anti-cryonics article in the world, you have a very low bar to clear.</p>\n<p><strong>Related articles</strong>: Carl Schulman links to Robin Hanson's <a href=\"http://www.overcomingbias.com/2007/02/what_evidence_i.html\">What Evidence in Silence or Confusion?</a> on Overcoming Bias, which discusses what conclusions one can draw from this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZXaRHHLsxaTTQQsZb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 94, "baseScore": 113, "extendedScore": null, "score": 0.000185, "legacy": true, "legacyId": "2268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 113, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 326, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-08T00:02:50.262Z", "modifiedAt": null, "url": null, "title": "Epistemic Luck", "slug": "epistemic-luck", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:11.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8bWbNwiSGbGi9jXPS/epistemic-luck", "pageUrlRelative": "/posts/8bWbNwiSGbGi9jXPS/epistemic-luck", "linkUrl": "https://www.lesswrong.com/posts/8bWbNwiSGbGi9jXPS/epistemic-luck", "postedAtFormatted": "Monday, February 8th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Epistemic%20Luck&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEpistemic%20Luck%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bWbNwiSGbGi9jXPS%2Fepistemic-luck%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Epistemic%20Luck%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bWbNwiSGbGi9jXPS%2Fepistemic-luck", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8bWbNwiSGbGi9jXPS%2Fepistemic-luck", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 819, "htmlBody": "<p><em>Who we learn from and with can profoundly influence our beliefs. There's no obvious way to compensate.&nbsp; Is it time to panic?</em></p>\n<p>During one of my epistemology classes, my professor admitted (I can't recall the context) that his opinions on the topic would probably be different had he attended a different graduate school.</p>\n<p>What a peculiar thing for an epistemologist to admit!</p>\n<p>Of course, on the one hand, he's almost certainly right.&nbsp; Schools have their cultures, their traditional views, their favorite literature providers, their set of available teachers.&nbsp; These have a decided enough effect that I've heard \"X was a student of Y\" used to mean \"X holds views basically like Y's\".&nbsp; And everybody knows this.&nbsp; And people still show a distinct trend of agreeing with their teachers' views, even the most controversial - not an unbroken trend, but still an obvious one.&nbsp; So it's not at all unlikely that, yes, had the professor gone to a different graduate school, he'd believe something else about his subject, and he's not making a mistake in so acknowledging...</p>\n<p>But on the other hand... but... but...</p>\n<p>But how can he say that, and look so undubiously at the views he picked up this way?&nbsp; Surely the truth about knowledge and justification isn't correlated with which school you went to - even a little bit!&nbsp; Surely he knows that!<a id=\"more\"></a></p>\n<p>And he does - and so do I, and it doesn't stop it from happening.&nbsp; I even identified a quale associated with the inexorable slide towards a consensus position, which made for some interesting introspection, but averted no change of mind.&nbsp; Because what are you supposed to do - resolutely hold to whatever intuitions you walked in with, never mind the coaxing and arguing and ever-so-reasonable persuasions of the environment in which you are steeped?&nbsp; That won't do, and not only because it obviates the education.&nbsp; The truth isn't <em>anticorrelated </em>with the school you go to, either!</p>\n<p>Even if everyone collectively attempted this stubbornness only to the exact degree needed to remove the statistical connection between teachers' views and their students', it's still not truth-tracking.&nbsp; An analogy: suppose you give a standardized English language test, determine that Hispanics are doing disproportionately well on it, figure out that this is because many speak Romance languages and do well with Latinate words, and deflate Hispanic scores to even out the demographics of the test results.&nbsp; This might give you a racially balanced outcome, but on an individual level, it will unfairly hurt some monolingual Anglophone Hispanics, and help some Francophone test-takers - it will not do as much as you'd hope to improve the skill-tracking ability of the test.&nbsp; Similarly, flattening the impact of teaching on student views won't salvage truth-tracking of student views as though this trend never existed; it'll just yield the same high-level <em>statistics</em> you'd get if that bias weren't operating.</p>\n<p>Lots of biases still live in your head doing their thing even when you know about them.&nbsp; This one, though, puts you in an <em>awfully</em> weird epistemic situation.&nbsp; It's almost like the opposite of <a href=\"/lw/i4/belief_in_belief/\">belief in belief</a> - <em>disbelief</em> in belief.&nbsp; \"This is true, but my situation made me more prone than I should have been to believe it and my belief is therefore suspect.&nbsp; But dang, that argument my teacher explained to me sure was sound-looking!&nbsp; I must just be lucky - those poor saps with other teachers have it wrong!&nbsp; But of course I <em>would</em> think that...\"</p>\n<p>It <em>is</em> possible, to an extent, to reduce the risk here - you can surround yourself with cognitively diverse peers and teachers, even if only in unofficial capacities.&nbsp; But even then, who you spend the most time with, whom you get along with best, whose style of thought \"clicks\" most with yours, and - due to competing biases - whoever agrees with you <em>already </em>will have more of an effect than the others.&nbsp; In practice, you can't sit yourself in a <a href=\"/lw/3b/never_leave_your_room/\">controlled environment</a> and expose yourself to pure and perfect argument and evidence (without allowing accidental leanings to creep in via the order in which you read it, either).</p>\n<p>I'm not even sure if it's right to assign a higher confidence to beliefs that you happen to have maintained - absent special effort - in contravention of the general agreement.&nbsp; It seems to me that people have trains of thought that just seem more natural to them than others.&nbsp; (Was I the only one disconcerted by Eliezer announcing high confidence in Bayesianism in <a href=\"/lw/ul/my_bayesian_enlightenment/\">the same post</a> as a statement that he was probably \"born that way\"?)&nbsp; This isn't even a highly reliable way for you to learn things about <em>yourself</em>, let alone the rest of the world: unless there's a special reason your intuitions - and not those of people who think differently - should be truth-tracking, these beliefs are likely to represent where your brain just happens to clamp down really hard on something and resist group pressure and that inexorable slide.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xgpBASEThXPuKRhbS": 1, "5hpGj9nDLgokfghvR": 1, "fF9GEdWXKJ3z73TmB": 1, "5f5c37ee1b5cdee568cfb23b": 2, "sHbKQDqrSinRPcnBv": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8bWbNwiSGbGi9jXPS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 86, "baseScore": 102, "extendedScore": null, "score": 0.000171, "legacy": true, "legacyId": "2269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 102, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 133, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CqyJzDZWvGhhFJ7dY", "ZmQv4DFx6y4jFbhLy", "Ti3Z7eZtud32LhGZT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-09T19:27:22.781Z", "modifiedAt": null, "url": null, "title": "Common Errors in History", "slug": "common-errors-in-history", "viewCount": null, "lastCommentedAt": "2017-06-17T03:58:32.391Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bYxqxBxMKyFrNFyJk/common-errors-in-history", "pageUrlRelative": "/posts/bYxqxBxMKyFrNFyJk/common-errors-in-history", "linkUrl": "https://www.lesswrong.com/posts/bYxqxBxMKyFrNFyJk/common-errors-in-history", "postedAtFormatted": "Tuesday, February 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Common%20Errors%20in%20History&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommon%20Errors%20in%20History%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYxqxBxMKyFrNFyJk%2Fcommon-errors-in-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Common%20Errors%20in%20History%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYxqxBxMKyFrNFyJk%2Fcommon-errors-in-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbYxqxBxMKyFrNFyJk%2Fcommon-errors-in-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 402, "htmlBody": "<p>I bought a copy of Common Errors in History, which someone mentioned recently on LW.\u00a0 There were no copies on Amazon or other bookselling sites, but I found a copy on Ebay.\u00a0 No wonder it was hard to get - it's a 24-page pamphlet that was printed once, in 1945, by \"The Historical Association,\" London.</p>\n<p>I tried to find some common failures of rationality underlying the \"common errors\" listed.\u00a0 This is what I concluded:</p>\n<p><strong>English students in the mid-20th century learned a lot of history.</strong></p>\n<p>This booklet is full of statements such as, \"The facts relating to the Corn Laws [of 1815-1849] are more often than not mis-stated in school examination papers,\" and, \"The blockade of Brest and Toulon [during the Napoleonic wars] is usually misunderstood.\"\u00a0 My history lessons consisted primarily of repeatedly learning about the American Revolution and making turkeys or pilgrim hats out of colored cardboard.</p>\n<p><strong>The English sincerely apologize for their history</strong><strong>. </strong></p>\n<p>In other countries, textbook authors try to make their own countries look good.\u00a0 In England, that would seem gauche.\u00a0 The entries on \"Religion in the New England Colonies\", \"The Causes of the American War of Independence\", \"The First Chinese War, 1839-42\",\u00a0 \"Gladstone and the Turks\", and \"The Manchurian Crisis, 1931-32\" complain that British textbook accounts place all of the blame on Britain.</p>\n<p><strong>History is simplified in order to assign blame and credit.</strong></p>\n<p>In numerous of the 20 entries, notably \"The Dissolution of the Monasteries and Education\", \"Religion in the New England Colonies\", \"The Enclosure Movement\", \"The Causes of the American War of Independence\", \"The Great Trek\", \"The First Chinese War\", \"The Elementary Education Act\", and \"The Manchurian Crisis\", the tract alleges that standard accounts are simplified; and they appear to be simplified in ways that allow a simple causal summary, preferably with one person, side, or act of legislation to receive credit or blame.</p>\n<p>Not always.\u00a0 \"The Great Trek\" says that the Boers' depart is usually explained as due to their [blameworthy] indignation that the British had freed their slaves; whereas in fact they had a variety of different, equally blameworthy, reasons for leaving.\u00a0 And the entry on \"Bismarck's Alliances\" says that the textbook account is overly-complex in that it introduces a second treaty that did not exist.</p>\n<p>This is the only general principle I could extract from the book, so it may just be a statistical accident.</p>\n<p>\u00a0</p>\n<p>If anyone would like a copy of the book, send me an email at gmail.  But it's very boring.</p>\n<p>\u00a0</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bYxqxBxMKyFrNFyJk", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 2, "extendedScore": null, "score": 5.600593727966314e-07, "legacy": true, "legacyId": "2278", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I bought a copy of Common Errors in History, which someone mentioned recently on LW.&nbsp; There were no copies on Amazon or other bookselling sites, but I found a copy on Ebay.&nbsp; No wonder it was hard to get - it's a 24-page pamphlet that was printed once, in 1945, by \"The Historical Association,\" London.</p>\n<p>I tried to find some common failures of rationality underlying the \"common errors\" listed.&nbsp; This is what I concluded:</p>\n<p><strong id=\"English_students_in_the_mid_20th_century_learned_a_lot_of_history_\">English students in the mid-20th century learned a lot of history.</strong></p>\n<p>This booklet is full of statements such as, \"The facts relating to the Corn Laws [of 1815-1849] are more often than not mis-stated in school examination papers,\" and, \"The blockade of Brest and Toulon [during the Napoleonic wars] is usually misunderstood.\"&nbsp; My history lessons consisted primarily of repeatedly learning about the American Revolution and making turkeys or pilgrim hats out of colored cardboard.</p>\n<p><strong>The English sincerely apologize for their history</strong><strong>. </strong></p>\n<p>In other countries, textbook authors try to make their own countries look good.&nbsp; In England, that would seem gauche.&nbsp; The entries on \"Religion in the New England Colonies\", \"The Causes of the American War of Independence\", \"The First Chinese War, 1839-42\",&nbsp; \"Gladstone and the Turks\", and \"The Manchurian Crisis, 1931-32\" complain that British textbook accounts place all of the blame on Britain.</p>\n<p><strong id=\"History_is_simplified_in_order_to_assign_blame_and_credit_\">History is simplified in order to assign blame and credit.</strong></p>\n<p>In numerous of the 20 entries, notably \"The Dissolution of the Monasteries and Education\", \"Religion in the New England Colonies\", \"The Enclosure Movement\", \"The Causes of the American War of Independence\", \"The Great Trek\", \"The First Chinese War\", \"The Elementary Education Act\", and \"The Manchurian Crisis\", the tract alleges that standard accounts are simplified; and they appear to be simplified in ways that allow a simple causal summary, preferably with one person, side, or act of legislation to receive credit or blame.</p>\n<p>Not always.&nbsp; \"The Great Trek\" says that the Boers' depart is usually explained as due to their [blameworthy] indignation that the British had freed their slaves; whereas in fact they had a variety of different, equally blameworthy, reasons for leaving.&nbsp; And the entry on \"Bismarck's Alliances\" says that the textbook account is overly-complex in that it introduces a second treaty that did not exist.</p>\n<p>This is the only general principle I could extract from the book, so it may just be a statistical accident.</p>\n<p>&nbsp;</p>\n<p>If anyone would like a copy of the book, send me an email at gmail.  But it's very boring.</p>\n<p>&nbsp;</p>", "sections": [{"title": "English students in the mid-20th century learned a lot of history.", "anchor": "English_students_in_the_mid_20th_century_learned_a_lot_of_history_", "level": 1}, {"title": "History is simplified in order to assign blame and credit.", "anchor": "History_is_simplified_in_order_to_assign_blame_and_credit_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-09T20:09:06.208Z", "modifiedAt": null, "url": null, "title": "Shut Up and Divide?", "slug": "shut-up-and-divide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ea8pt2dsrS6D4P54F/shut-up-and-divide", "pageUrlRelative": "/posts/Ea8pt2dsrS6D4P54F/shut-up-and-divide", "linkUrl": "https://www.lesswrong.com/posts/Ea8pt2dsrS6D4P54F/shut-up-and-divide", "postedAtFormatted": "Tuesday, February 9th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shut%20Up%20and%20Divide%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShut%20Up%20and%20Divide%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEa8pt2dsrS6D4P54F%2Fshut-up-and-divide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shut%20Up%20and%20Divide%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEa8pt2dsrS6D4P54F%2Fshut-up-and-divide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEa8pt2dsrS6D4P54F%2Fshut-up-and-divide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 371, "htmlBody": "<p>During <a href=\"/lw/1pc/debunking_komponisto_on_amanda_knox_long/1l0r\">a recent discussion with komponisto</a> about why my fellow LWers are so interested in the Amanda Knox case, his answers made me realize that I had been asking the wrong question. After all, feeling interest or even outrage after seeing a possible case of injustice seems quite natural, so perhaps a better question to ask is why am <em>I</em> so <em>un</em>interested in the case.</p>\n<p>Reflecting upon that, it appears that I've been doing something like Eliezer's \"Shut Up and Multiply\", except in reverse. Both of us noticed the obvious craziness of&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Scope_insensitivity\">scope insensitivity</a>&nbsp;and tried to&nbsp;<a href=\"/lw/hp/feeling_rational/\">make our emotions work more rationally</a>. But whereas he decided to multiply his concern for individuals human beings by the population size to an enormous concern for humanity as a whole, I did the opposite. I noticed that my concern for humanity is limited, and therefore decided that it's crazy to care much about random individuals that I happen to come across. (Although I probably haven't consciously thought about it in this way until now.)</p>\n<p>The weird thing is that both of these emotional self-modification strategies seem to have worked, at least to a great extent. Eliezer has devoted his life to improving the lot of humanity, and I've managed to pass up news and discussions about Amanda Knox without a second thought. It can't be the case that both of these ways to change how our emotions work are the right thing to do, but the apparent symmetry between them seems hard to break.</p>\n<p>What ethical principles can we use to decide between \"Shut Up and Multiply\" and \"Shut Up and Divide\"? Why should we derive our values from our native emotional responses to seeing individual suffering, and not from the equally human paucity of response at seeing large portions of humanity suffer in aggregate? Or should we just keep our scope insensitivity,&nbsp;<a href=\"/lw/196/boredom_vs_scope_insensitivity/\">like our boredom</a>?</p>\n<p>And an interesting meta-question arises here as well: how much of what we think our values are, is actually the result of not thinking things through, and not realizing the implications and symmetries that exist? And if many of our values are just the result of cognitive errors or limitations, have we lived with them long enough that they've become an essential part of us?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7Gh5dSvySMNbA6niL": 1, "ZTRNmvQGgoYiymYnq": 1, "3ee9k6NJfcGzL6kMS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ea8pt2dsrS6D4P54F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 92, "baseScore": 106, "extendedScore": null, "score": 0.000175, "legacy": true, "legacyId": "2277", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 106, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 274, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqF8cHjJv43mvJJzx", "6S4Lf2tCMWAfbGtdt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-10T03:15:07.368Z", "modifiedAt": null, "url": null, "title": "The Craigslist Revolution: a real-world application of torture vs. dust specks OR How I learned to stop worrying and create one billion dollars out of nothing", "slug": "the-craigslist-revolution-a-real-world-application-of", "viewCount": null, "lastCommentedAt": "2019-01-09T04:37:19.918Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zp88RjceAbsBKgkWw/the-craigslist-revolution-a-real-world-application-of", "pageUrlRelative": "/posts/Zp88RjceAbsBKgkWw/the-craigslist-revolution-a-real-world-application-of", "linkUrl": "https://www.lesswrong.com/posts/Zp88RjceAbsBKgkWw/the-craigslist-revolution-a-real-world-application-of", "postedAtFormatted": "Wednesday, February 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Craigslist%20Revolution%3A%20a%20real-world%20application%20of%20torture%20vs.%20dust%20specks%20OR%20How%20I%20learned%20to%20stop%20worrying%20and%20create%20one%20billion%20dollars%20out%20of%20nothing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Craigslist%20Revolution%3A%20a%20real-world%20application%20of%20torture%20vs.%20dust%20specks%20OR%20How%20I%20learned%20to%20stop%20worrying%20and%20create%20one%20billion%20dollars%20out%20of%20nothing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZp88RjceAbsBKgkWw%2Fthe-craigslist-revolution-a-real-world-application-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Craigslist%20Revolution%3A%20a%20real-world%20application%20of%20torture%20vs.%20dust%20specks%20OR%20How%20I%20learned%20to%20stop%20worrying%20and%20create%20one%20billion%20dollars%20out%20of%20nothing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZp88RjceAbsBKgkWw%2Fthe-craigslist-revolution-a-real-world-application-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZp88RjceAbsBKgkWw%2Fthe-craigslist-revolution-a-real-world-application-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 625, "htmlBody": "<blockquote>\n<p><a href=\"/lw/1r9/shut_up_and_divide/1lll\">...this is the first crazy idea I've ever heard for generating a billion dollars out of nothing that could actually work. I mean, ever. &nbsp;-Eliezer Yudkowsky</a></p>\n</blockquote>\n<p>We can reasonably debate <a href=\"/lw/kn/torture_vs_dust_specks/\">torture vs. dust specks</a> when it is one person being tortured versus 3^^^3 people being subjected to motes of dust.</p>\n<p>However, there should be little debate when we are comparing the torture of one person to the minimal suffering of a mere millions of people. I propose a way to generate approximately one billion dollars for charity over five years: <em>The Craigslist Revolution</em>.</p>\n<p><a href=\"http://blogs.zdnet.com/BTL/?p=4082\">In 2006, Craigslist's CEO Jim Buckmaster said</a> that if enough users told them to \"raise revenue and plow it into charity\" that they would consider doing it. I have more recently emailed Craig Newmark and he indicated that they remain receptive to the idea if that's what the users want.</p>\n<p>A simple text advertising banner at the top of the Craigslist home or listing pages would generate enormous amounts of revenue. They could put a large \"X\" next to the ad, allowing you to permanently close it. There seems to be little objection to this idea. The optional banner is harmless, and a billion dollars could be enough to dramatically improve the lives of millions or make a serious impact in the causes we take seriously around here. As a moral calculus, the decision seems a no brainer. It's possible that some or many dollars would support bad charities, but the marginal impact of supporting some truly good charities makes the whole thing worthwhile.</p>\n<p>I don't have access to Craigslist's detailed traffic data, but I think one billion USD over five years is a reasonable estimate for a single optional banner ad. With 20 billion pageviews a month, a Google Adwords banner would bring in about 200 million dollars a year. Over five years that will be well over a billion dollars. With employees selling the advertising rather than Google, that number could very well be multiplied. An extremely low bound for the amount of additional revenue that could be trivially generated over five years would be 100 million.<a id=\"more\"></a></p>\n<p>I'm very open to other ideas, but I think the best way to assemble a critical mass of Craigslist users is via a Facebook fan page. Facebook makes it very easy to advertise Facebook pages so we can do viral marketing as well as paying Facebook to direct people to our page.</p>\n<p>50,000 users would surely count as a critical mass, meaning that each member of the Facebook page effectively created $20,000 for charity. I don't think there has been any time in history where a single click had the potential to do so much good, and the disbelief that this is possible is the main thing that our viral campaign would have to overcome. After the Facebook fan page got beyond a certain number of users, we could more aggressively take the campaign to Twitter and email.</p>\n<p>Are there any social media marketers in the house? The first step is deciding what to call the Facebook page; it's limited to 75 characters.</p>\n<p>It's time to shut up and multiply. I will match the first $250 donated towards the advertising budget for this, more next month depending on my personal finances. If anyone independently wealthy is reading this, $20,000 is probably enough to get the critical mass of users this week.</p>\n<p>I welcome all of your criticism, especially as far as the mechanics of actually making this happen. As far as how to&nbsp;optimally distribute money to charity, that is very much an unsolved problem, but I think it's one that we can mostly worry about when we get that far. I also expect Craig and Jim to take a leadership roll as far as the distribution of the money goes.</p>\n<p>Also see&nbsp;<a href=\"/lw/1r9/shut_up_and_divide/1ll3\">previous discussion</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zp88RjceAbsBKgkWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 53, "extendedScore": null, "score": 0.0001153750585598865, "legacy": true, "legacyId": "2247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 226, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-10T17:26:56.423Z", "modifiedAt": null, "url": null, "title": "My Fundamental Question About Omega", "slug": "my-fundamental-question-about-omega", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:39.421Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrHen", "createdAt": "2009-04-01T17:47:41.337Z", "isAdmin": false, "displayName": "MrHen"}, "userId": "HriC2mR9onYvGtwWr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6SSv2KC3mic5drr6A/my-fundamental-question-about-omega", "pageUrlRelative": "/posts/6SSv2KC3mic5drr6A/my-fundamental-question-about-omega", "linkUrl": "https://www.lesswrong.com/posts/6SSv2KC3mic5drr6A/my-fundamental-question-about-omega", "postedAtFormatted": "Wednesday, February 10th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20Fundamental%20Question%20About%20Omega&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20Fundamental%20Question%20About%20Omega%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SSv2KC3mic5drr6A%2Fmy-fundamental-question-about-omega%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20Fundamental%20Question%20About%20Omega%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SSv2KC3mic5drr6A%2Fmy-fundamental-question-about-omega", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6SSv2KC3mic5drr6A%2Fmy-fundamental-question-about-omega", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 566, "htmlBody": "<p>Omega has appeared to us inside of puzzles, games, and questions. The basic concept behind <a title=\"Wiki\" href=\"http://wiki.lesswrong.com/wiki/Omega\">Omega</a> is that it is (a) a perfect predictor and (b) not malevolent. The practical implications behind these points are that (a) it doesn't make mistakes and (b) you can trust its motives in the sense that it really, honestly doesn't care about you. This bugger is True Neutral and is good at it. And it doesn't lie.</p>\n<p>A quick peek at <a href=\"/tag/omega/\">Omega's presence on LessWrong</a> reveals <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's problem</a>&nbsp;and <a href=\"/lw/3l/counterfactual_mugging/\">Counterfactual Mugging</a> as the most prominent examples. For those that missed them, other articles include <a href=\"/lw/em/bead_jar_guesses/\">Bead Jars</a>&nbsp;and&nbsp;<a href=\"/lw/17h/the_lifespan_dilemma/\">The Lifespan Dilemma</a>.</p>\n<p>Counterfactual Mugging was the most annoying for me, however, because I thought the answer was completely obvious and apparently the answer <em>isn't</em>&nbsp;obvious. Instead of going around in circles with a complicated scenario I decided to find a simpler version that reveals what I consider to my the fundamental confusion about Omega.</p>\n<p>Suppose that Omega, as defined above, appears before you and says that it predicted you will give it $5. What do you do? If Omega is a perfect predictor, and it predicted you will give it $5... will you give it $5 dollars?</p>\n<p>The answer to this question is probably obvious but I am curious if we all end up with the <em>same</em>&nbsp;obvious answer.</p>\n<p><a id=\"more\"></a>The fundamental problem behind Omega is how to resolve a claim by a perfect predictor that includes a decision you and you alone are responsible for making. This invokes all sorts of assumptions about choice and free-will, but in terms of phrasing the question these assumptions do not matter. I care about how you will <em>act</em>. What <em>action</em>&nbsp;will you take? However you label the source of these actions is your prerogative. The question doesn't care how you got there; it cares about the answer.</p>\n<p>My answer is that you will give Omega $5. If you don't, Omega wouldn't have made the prediction. If Omega made the prediction AND you don't give $5 than the definition of Omega is flawed and we have to redefine Omega.</p>\n<p>A possible objection to the scenario is that the prediction itself is impossible to make. If Omega is a perfect predictor it follows that it would never make an impossible prediction and the prediction \"you will give Omega $5\" is impossible. This is invalid, however, as long as you can think of at least one scenario where you have a good reason to give Omega $5. Omega would show up in <em>that</em>&nbsp;scenario and ask for $5.</p>\n<p>If this scenario includes a long argument about why you should give it $5, so be it. If it means Omega gives you $10 in return, so be it. But it doesn't matter for the sake of the question. It matters for the <em>answer</em>, but the question doesn't need to include these details because the underlying problem is still the same. Omega made a prediction and now you need to act. All of the excuses and whining and arguing will eventually end with you handing Omega $5. Omega's prediction will have included all of this bickering.</p>\n<p>This question is essentially the same as saying, \"If you have a good reason to give Omega $5 then you will give Omega $5.\" It should be a completely uninteresting, obvious question. It holds some implications on other scenarios involving Omega but those are for another time. Those implications should have no bearing on the answer to this question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6SSv2KC3mic5drr6A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 33, "baseScore": 11, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "2134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 153, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6ddcsdA2c2XpNpE5x", "mg6jDEuQEjBGtibX7", "mXaPPjud9MuuboWgd", "9RCoE7jmmvGd5Zsh2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-11T00:38:43.590Z", "modifiedAt": null, "url": null, "title": "How Much Should We Care What the Founding Fathers Thought About Anything?", "slug": "how-much-should-we-care-what-the-founding-fathers-thought", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:22.044Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_J_Balan", "createdAt": "2009-06-21T18:40:19.064Z", "isAdmin": false, "displayName": "David_J_Balan"}, "userId": "LkRB9E4GWd7hRa7zt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EbrThdEbHi7fz63vq/how-much-should-we-care-what-the-founding-fathers-thought", "pageUrlRelative": "/posts/EbrThdEbHi7fz63vq/how-much-should-we-care-what-the-founding-fathers-thought", "linkUrl": "https://www.lesswrong.com/posts/EbrThdEbHi7fz63vq/how-much-should-we-care-what-the-founding-fathers-thought", "postedAtFormatted": "Thursday, February 11th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Much%20Should%20We%20Care%20What%20the%20Founding%20Fathers%20Thought%20About%20Anything%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Much%20Should%20We%20Care%20What%20the%20Founding%20Fathers%20Thought%20About%20Anything%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbrThdEbHi7fz63vq%2Fhow-much-should-we-care-what-the-founding-fathers-thought%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Much%20Should%20We%20Care%20What%20the%20Founding%20Fathers%20Thought%20About%20Anything%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbrThdEbHi7fz63vq%2Fhow-much-should-we-care-what-the-founding-fathers-thought", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEbrThdEbHi7fz63vq%2Fhow-much-should-we-care-what-the-founding-fathers-thought", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1109, "htmlBody": "<p>A while back I saw an interesting <a href=\"http://www.freerepublic.com/focus/news/1352357/posts\" target=\"_blank\">discussion</a> between U.S. Supreme Court Justices Stephen Breyer and Antonin Scalia. Scalia is well known for arguing that the way to deal with Constitutional questions is to use the plain meaning of the words in the Constitutional text as they would have been understood at the time and place they were written.* Any other approach, he argues, would amount to nothing more than an unelected judge taking his or her personal political and moral views and making them into the highest law of the land. In his view if a judge is not taking the answer out of the text, then that judge must be putting the answer into the text, and no judge should be allowed to do that.** One illustrative example that comes up in the exchange is the question of whether and when it's OK to cite foreign law in cases involving whether a particular punishment is \"Cruel and Unusual\" and hence unconstitutional. In Scalia's view, the right way to approach the question would be to try as best one could to figure out what was meant by the words \"cruel\" and \"unusual\" in 18th century England, and what contemporary foreign courts have to say cannot possibly inform that question. He also opposes (though somewhat less vigorously) the idea that decisions ought to take into account changes over time in what is considered cruel and unusual in America: he thinks that if people have updated their opinions about such matters, they are free to get their political representatives to pass new laws or to amend the Constitution***, but short of that it is simply not the judge's job to take that sort of thing into account.<br /><a id=\"more\"></a><br />I don't think it's an unfair caricature to describe Scalia's position as follows:</p>\n<ol>\n<li>It would be a bad thing for a bunch of unaccountable, unelected judges to have their own opinions be made the supreme law of the land.</li>\n<li>If there is no absolute rule for judges to stick to, then they end up in some sense making up the \"Constitution\" themselves.</li>\n<li>Therefore absolute Originalism is not only correct, but is the only position that distinguishes between \"judge\" and \"dictator.\"</li>\n</ol>\n<p>Claim #1 is true and Claim #2 is sort of true, but Claim #3 is clearly not true. What we're really faced with here is nothing more than a garden variety trade-off between two important values: trying to avoid disruptive or arbitrary rule by judges on the one hand and progress on the other. There simply is no absolute value at stake to which all other values must be subordinated. Does anyone really think that there is no real estate between \"live under the exact framework set up by a bunch of very flawed 18th century white dudes <em>forever</em>\" and \"have tyrannical judges just make up whatever the heck they feel like and call it the Constitution?\" It's worthwhile to argue about where on this spectrum to come down, but the idea that this problem can be made to go away as long as you have a good 18th century English dictionary is pretty delusional. <br /><br />This necessarily means that what judges do is not merely to \"interpret\" the Constitution. In this sense Scalia is right. The opponents of his position <em>aren't</em> really offering an alternative way to interpret the Constitution, and it's disingenuous of Breyer when he insists that that's what he's doing. That's why Breyer seems so unimpressive in that exchange. He can't come out and say that what he's really doing is casting about for guidance about how to deal with a hard question while not departing too much from what's written in the Constitution and in precedent, and that looking to see what some foreigners did makes sense as a (small) part of that effort. He has to pretend that he merely has a different method of interpretation, and Scalia rightly calls him on it. What I think Breyer is really saying, but can't come right out and say, is that what he's really doing is deciding how much to listen to the Constitution and how much to change it. This is <em>not</em> the same thing as saying \"take whatever the heck Stephen Breyer feels like saying and call it the Constitution,\" but he can't make that (correct) argument without conceding that he's not really exclusively an interpreter of a text. Though the parallel is not perfect. the exchange has something of a \"<a href=\"http://www.overcomingbias.com/2007/05/doubting_thomas.html\" target=\"_blank\">Doubting Thomas vs. Pious Pete</a>\" vibe.<br /><br />One implication of all this is that there's no good reason for the job of Constitutional arbiter to fall exclusively to judges. If what's really going on is not just the interpretation of the legal texts, but also a process by which some trusted, relatively non-political body gropes their way to the solutions to problems, then it may make sense to have a set of appointed \"Wise Elders,\" only some of whom are judges, chosen to perform this function in broad daylight. Then we could get rid of the silly system we have now where everyone sort of pretends that the qualifications for a Supreme Court Justice are primarily technical legal expertise****, and in which it becomes a scandal whenever someone digs up a comment that a nominee once made that suggests that their personal views about something or other might affect how they rule on the Court.<br /><br />*One could argue about whether this is possible, as there may have been some intentional \"constructive ambiguity\" in choosing those particular words in the first place.<br />**It is worth noting that Originalist arguments are generally used in support the positions of contemporary conservatives, so it's not always clear when such arguments are being made in good faith and when they're a fig leaf. <br />***The Constitution can be amended, but the conditions for doing so (which are of course set up by the Constitution) ensure that it hardly ever is, so this is mostly irrelevant.<br />****U.S. Supreme Court judges serve both as the highest regular court in the country and as the Constitutional arbiter. Technical legal expertise is of course central to the first function.</p>\n<p>&nbsp;</p>\n<p><strong>Update @11:03 AM:</strong> Some commenters have said that they don't think this is an appropriate topic for LW, so I've tried in the comments to explain better what I think the LW-relevant point is. A bunch of commenters objected to the Americocentric tone of the post, such as using the term \"we\" as if everyone on LW was an American. I will try to be careful to avoid doing that from now on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EbrThdEbHi7fz63vq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": -6, "extendedScore": null, "score": -8e-06, "legacy": true, "legacyId": "2283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2010-02-14T04:31:25.506Z", "modifiedAt": null, "url": null, "title": "Shock Levels are Point Estimates", "slug": "shock-levels-are-point-estimates", "viewCount": null, "lastCommentedAt": "2017-06-17T03:53:23.862Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Peter_de_Blanc", "createdAt": "2009-02-27T14:15:28.882Z", "isAdmin": false, "displayName": "Peter_de_Blanc"}, "userId": "vRvaAqR5tcjGEWaoC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rYoS9mdcypeQi3ZG8/shock-levels-are-point-estimates", "pageUrlRelative": "/posts/rYoS9mdcypeQi3ZG8/shock-levels-are-point-estimates", "linkUrl": "https://www.lesswrong.com/posts/rYoS9mdcypeQi3ZG8/shock-levels-are-point-estimates", "postedAtFormatted": "Sunday, February 14th 2010", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Shock%20Levels%20are%20Point%20Estimates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShock%20Levels%20are%20Point%20Estimates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYoS9mdcypeQi3ZG8%2Fshock-levels-are-point-estimates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Shock%20Levels%20are%20Point%20Estimates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYoS9mdcypeQi3ZG8%2Fshock-levels-are-point-estimates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrYoS9mdcypeQi3ZG8%2Fshock-levels-are-point-estimates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 541, "htmlBody": "<p>This is a post from my blog, <a title=\"Space and Games\" href=\"http://www.spaceandgames.com\">Space and Games</a>. Michael Vassar has requested that I repost it here. I thought about revising it to remove the mind projection fallacy, but instead I left it in for you to find.</p>\n<p>Eliezer Yudkowsky<sub>1999</sub> famously categorized beliefs about the future into discrete \"<a href=\"http://sl4.org/shocklevels.html\">shock levels</a>.\" Michael Anissimov later wrote a nice <a href=\"http://www.acceleratingfuture.com/michael/works/shocklevelanalysis.htm\">introduction</a> to future shock levels. Higher shock levels correspond to belief in more powerful and radical technologies, and are considered more correct than lower shock levels. Careful thinking and exposure to ideas will tend to increase one&rsquo;s shock level.</p>\n<p>If this is really true, and I think it is, shock levels are an example of human insanity. If you ask me to estimate some quantity, and track how my estimates change over time, you should expect it to look like a <a href=\"/lw/ii/conservation_of_expected_evidence/\">random walk</a> if I&rsquo;m being rational. Certainly I can&rsquo;t expect that my estimate will go up in the future. And yet shock levels mostly go up, not down.<a id=\"more\"></a></p>\n<p>I think this is because people model the future with <a href=\"http://en.wikipedia.org/wiki/Point_estimation\">point estimates</a> rather than probability distributions. If, when we try to picture the future, we actually imagine the <em>single </em>outcome which seems most likely, then our extrapolation will include every technology to which we assign a probability above 50%, and none of those that we assign a probability below 50%. Since most possible ideas will fail, an ignorant futurist should assign probabilities well below 50% to most future technologies. So an ignorant futurist&rsquo;s point estimate of the future will indeed be much less technologically advanced than that of a more knowledgeable futurist.</p>\n<p>For example, suppose we are considering four possible future technologies: molecular manufacturing (MM), faster-than-light travel (FTL), psychic powers (psi), and perpetual motion (PM). If we ask how likely these are to be developed in the next 100 years, the ignorant futurist might assign a 20% probability to each. A more knowledgeable futurist might assign a 70% probability to MM, 8% for FTL, and 1% for psi and PM. If we ask them to imagine a plethora of possible futures, their extrapolations might be, on average, equally radical and shocking. But if they instead generate point estimates, the ignorant futurist would round the 20% probabilities down to 0, and say that no new technologies will be invented. The knowledgeable futurist would say that we&rsquo;ll have MM, but no FTL, psi, or PM. And then we call the ignorant person &ldquo;shock level 0&Prime; and the knowledgeable person &ldquo;shock level 3.&rdquo;</p>\n<p>So future shock levels exist because people imagine a single future instead of a plethora of futures. If futurists imagined a plethora of futures, then ignorant futurists would assign a low probability to many possible technologies, but would also assign a relatively high probability to many impossible technologies, and there would be no simple relationship between a futurist&rsquo;s knowledge level and his or her expectation of the overall amount of technology that will exist in the future, although more knowledgeable futurists would be able to predict which specific technologies will exist. Shock levels would disappear.</p>\n<p>I do think that shock level 4 is an exception. SL4 has to do with the shocking implications of a single powerful technology (superhuman intelligence), rather than a sum of many technologies.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rYoS9mdcypeQi3ZG8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 5.612489975203245e-07, "legacy": true, "legacyId": "2292", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}