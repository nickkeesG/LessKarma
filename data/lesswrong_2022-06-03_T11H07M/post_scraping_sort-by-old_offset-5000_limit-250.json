{"results": [{"createdAt": null, "postedAt": "2011-11-04T16:26:57.437Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup", "slug": "meetup-fort-collins-colorado-meetup-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:21.414Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qR4LxgDng6g7PQJSb/meetup-fort-collins-colorado-meetup-0", "pageUrlRelative": "/posts/qR4LxgDng6g7PQJSb/meetup-fort-collins-colorado-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/qR4LxgDng6g7PQJSb/meetup-fort-collins-colorado-meetup-0", "postedAtFormatted": "Friday, November 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqR4LxgDng6g7PQJSb%2Fmeetup-fort-collins-colorado-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqR4LxgDng6g7PQJSb%2Fmeetup-fort-collins-colorado-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqR4LxgDng6g7PQJSb%2Fmeetup-fort-collins-colorado-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4n'>Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 November 2011 08:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Bean Cycle, 144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our weekly get together of interesting people you'd like to meet.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4n'>Fort Collins, Colorado Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qR4LxgDng6g7PQJSb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 7.943988060262804e-07, "legacy": true, "legacyId": "10783", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup\">Discussion article for the meetup : <a href=\"/meetups/4n\">Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 November 2011 08:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Bean Cycle, 144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our weekly get together of interesting people you'd like to meet.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/4n\">Fort Collins, Colorado Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-04T18:15:01.512Z", "modifiedAt": null, "url": null, "title": "On the fragility of values", "slug": "on-the-fragility-of-values", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:37.570Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eAv9j4XzJiEmBtzJv/on-the-fragility-of-values", "pageUrlRelative": "/posts/eAv9j4XzJiEmBtzJv/on-the-fragility-of-values", "linkUrl": "https://www.lesswrong.com/posts/eAv9j4XzJiEmBtzJv/on-the-fragility-of-values", "postedAtFormatted": "Friday, November 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20the%20fragility%20of%20values&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20the%20fragility%20of%20values%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeAv9j4XzJiEmBtzJv%2Fon-the-fragility-of-values%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20the%20fragility%20of%20values%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeAv9j4XzJiEmBtzJv%2Fon-the-fragility-of-values", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeAv9j4XzJiEmBtzJv%2Fon-the-fragility-of-values", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>Programming human values into an AI is often taken to be very hard because values are complex (no argument there) and fragile. I would agree that values are fragile in the construction; anything lost in the definition might doom us all. But once coded into a utility function, they are reasonably robust.</p>\n<p>As a toy model, let's say the friendly utility function U has a hundred valuable components - friendship, love, autonomy, etc... - assumed to have positive&nbsp;numeric&nbsp;values. Then to ensure that we don't lose any of these, U is defined as the minimum of all those hundred components.</p>\n<p>Now define V as U, except we forgot the autonomy term. This will result in a terrible world, without autonomy or independence, and there will be wailing and gnashing of teeth (or there would, except the AI won't let us do that). Values are indeed fragile in the definition.<a id=\"more\"></a></p>\n<p>However... A world in which V is maximised is a terrible world from the perspective of U as well. U will likely be zero in that world, as the V-maximising entity never bothers to move autonomy above zero. So in utility function space, V and U are actually quite far apart.</p>\n<p>Indeed we can add any small, bounded utility to W to U. Assume W is bounded between zero and one; then an AI that maximises W+U will never be more that one expected 'utiliton' away, according to U, from one that maximises U. So - assuming that one&nbsp;'utiliton'&nbsp;is small change for U - a world run by an W+U maximiser will be good.</p>\n<p>So&nbsp;once they're fully spelled out&nbsp;inside utility space, values are reasonably robust, it's in their initial definition that they're fragile.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eAv9j4XzJiEmBtzJv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 7.944365865163851e-07, "legacy": true, "legacyId": "10784", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-04T22:06:39.148Z", "modifiedAt": null, "url": null, "title": "[link] Back to the trees", "slug": "link-back-to-the-trees", "viewCount": null, "lastCommentedAt": "2019-07-01T07:29:44.406Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xNqeGssARAYwbNCgH/link-back-to-the-trees", "pageUrlRelative": "/posts/xNqeGssARAYwbNCgH/link-back-to-the-trees", "linkUrl": "https://www.lesswrong.com/posts/xNqeGssARAYwbNCgH/link-back-to-the-trees", "postedAtFormatted": "Friday, November 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Back%20to%20the%20trees&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Back%20to%20the%20trees%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNqeGssARAYwbNCgH%2Flink-back-to-the-trees%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Back%20to%20the%20trees%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNqeGssARAYwbNCgH%2Flink-back-to-the-trees", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxNqeGssARAYwbNCgH%2Flink-back-to-the-trees", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 694, "htmlBody": "<p>So we say we know evolution is an <a href=\"/lw/kr/an_alien_god/\">alien god</a>, which can do absolutely horrifying things to creatures. And surely we are aware that includes <em>us</em>, but how exactly does one internalize something like that? Something so at odds with default cultural intuitions. It may be just my mood tonight, but this short entry on the <a href=\"http://westhunt.wordpress.com/2011/11/04/back-to-the-trees/\">West Hunter</a> (thanks <a href=\"/lw/85b/link_loss_of_local_knowledge_affecting/\">Glados</a>) blog really grabbed my attention and in a few short paragraphs on a hypothesis regarding the Hobbits of Flores utterly changed how I grok Eliezer's old post.</p>\n<blockquote>\n<p>There is still doubt, but there seems to be a good chance that the Flores Hobbit was a member of a distinct hominid species, rather than some homo sap with a nasty case of microcephalic dwarfism.&nbsp;&nbsp; If this is the case, the Hobbits are likely descended from a small, Australopithecus-like population that managed to move from Africa to Indonesia without leaving any fossils in between, or from some ancient hominid (perhaps <em>homo erectus</em>) that managed to strand themselves on Flores and then shrank, as many large animals do when isolated on islands.</p>\n<p>Island dwarfing of a <em>homo erectus </em>population is the dominant idea right now.&nbsp; However, many proponents are really bothered by how small the Hobbit&rsquo;s brain was.&nbsp; <strong><em>At 400 cc, it was downright teeny, about the size of a chimpanzee&rsquo;s brain.</em>&nbsp;</strong> Most researchers seem to think that hominid brains naturally increase in size with time<strong>. <em>They also suspect that anyone with a brain this small couldn&rsquo;t be called sentient &ndash; and the idea of natural selection driving a population from sentience to nonsentience bothers them.</em></strong></p>\n<p><em><strong>They should get over it.</strong></em>&nbsp; Hominid brain volume has increased pretty rapidly over the past few million years, but the increase hasn&rsquo;t been monotonic.&nbsp; It&rsquo;s decreased about 10% over the past 25,000 years. Moreover, we know of examples where natural selection has caused drastic decreases in organismal complexity &ndash; for example, <em><strong>canine venereal sarcoma, which today is an infectious cancer, but was once a dog.</strong></em></p>\n</blockquote>\n<p>I have to break here to note that was the most awesome <a href=\"http://en.wikipedia.org/wiki/Canine_transmissible_venereal_tumor\">fact</a> I have learned in some time.</p>\n<blockquote>\n<p>There is a mechanism that might explain what happened on Flores &ndash; partial mutational meltdown.&nbsp; Classic mutational meltdown occurs when a population is too small for too long. Selection is inefficient in such a small population: alleles that decrease fitness by less than 1/N drift fairly freely, and can go to fixation.&nbsp; At the same time, favorable mutations, which are very rare, almost never occur.&nbsp; In such a situation, mutational load accumulates &ndash; likely further reducing population size &ndash; and the population spirals down into extinction. Since small population size and high genetic load increase vulnerability to disaster, some kind of environmental catastrophe usually nails such doomed, shrinking populations before they manage to die off from purely genetic causes.</p>\n<p>In principle, if the&nbsp; population is the right size and one adaptive function is considerably more complicated than others, presenting a bigger mutational target,&nbsp; you might see a population suffer a drastic decline in that function while continuing to exist. There is reason to think that intelligence is the most complex adaptation in hominids. More than half of all genes are expressed in the brain, and it seems that a given degree of inbreeding depression &ndash; say cousin marriage &ndash; depressesIQ more than other traits.</p>\n<p>Flores is not that big an island and the population density of homo-erectus type hunter-gatherers must have been low &ndash; certainly lower than that of contemporary hunter-gatherers, who have much more sophisticated tools.&nbsp; Thus the hobbit population was likely small.&nbsp; It may not have been possible to sustain a high-performing brain over the long haul in that situation.&nbsp; Given that their brains performed poorly &ndash; while the metabolic costs were as high as ever &ndash; selection would have shrunk their brains.&nbsp; Over hundreds of thousands of years, this could well have generated the chimp-sized brain we see in the LB1 skeleton.</p>\n<p><em>Of course, this could only have happened if there was an available ecological niche that did not require human-level intelligence.&nbsp; And there was such an opening:</em><em> <strong>Flores had no monkeys.</strong></em></p>\n</blockquote>\n<p>That last sentence just struck me with utter horror.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nZCb9BSnmXZXSNA2u": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xNqeGssARAYwbNCgH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 100, "baseScore": 127, "extendedScore": null, "score": 0.000261, "legacy": true, "legacyId": "10785", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 127, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pLRogvJLPPg6Mrvg4", "hf9ZP2wnPwZ5pcujv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-04T23:19:08.225Z", "modifiedAt": null, "url": null, "title": "A clever argument for buying lottery tickets", "slug": "a-clever-argument-for-buying-lottery-tickets", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gPTZjNsyRW94rcTBj/a-clever-argument-for-buying-lottery-tickets", "pageUrlRelative": "/posts/gPTZjNsyRW94rcTBj/a-clever-argument-for-buying-lottery-tickets", "linkUrl": "https://www.lesswrong.com/posts/gPTZjNsyRW94rcTBj/a-clever-argument-for-buying-lottery-tickets", "postedAtFormatted": "Friday, November 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20clever%20argument%20for%20buying%20lottery%20tickets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20clever%20argument%20for%20buying%20lottery%20tickets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPTZjNsyRW94rcTBj%2Fa-clever-argument-for-buying-lottery-tickets%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20clever%20argument%20for%20buying%20lottery%20tickets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPTZjNsyRW94rcTBj%2Fa-clever-argument-for-buying-lottery-tickets", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgPTZjNsyRW94rcTBj%2Fa-clever-argument-for-buying-lottery-tickets", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 650, "htmlBody": "<p>I use the phrase 'clever argument' deliberately: I have reached a conclusion that contradicts the usual wisdom around here, and want to check that I didn't make an elementary mistake somewhere.</p>\n<p>Consider a lottery ticket that costs $100 for a one-in-ten-thousand chance of winning a million dollars, expected value, $100. I can take this deal or leave it, and of course a realistic ticket actually costs 100+epsilon where epsilon covers the arranger's profit, which is a bad deal.</p>\n<p>But now consider this deal in terms of time. Suppose I've got a well-paid job in which it takes me an hour to earn that $100. Suppose further that I work 40 hours a week, 50 weeks a year, and that my living expenses are a modest $40k a year, making my yearly savings $160k. Then, with 4% interest on my $160k yearly, it would take me about 5.5 years to accumulate that million dollars, or 11000 hours. Also note that with these assumptions, once I have my million I don't need to work any more.</p>\n<p>It seems to me that, given the assumptions above, I could view the lottery deal as paying one hour of my life for a one-in-ten-thousand chance to win 11000 hours, expected value, 1.1 hours. (Note that leisure hours when young are probably worth more, since you'll be in better health to enjoy it; but this is not necessary to the argument.)</p>\n<p>Of course it is possible to adjust the numbers. For example, I could scrimp and save during my working years, and make my living expenses only 20k; in that case it would take me less than 5 years to accumulate the million, and the ticket goes back to being a bad deal. Alternatively, if I spend more than 40k a year, it takes longer to accumulate the million; in this case my standard of living drops when I retire to live off my 4% interest, but the lottery ticket becomes increasingly attractive in terms of hours of life.</p>\n<p>I think, and I could be mistaken, that the reason this works is that the rate at which I'm indifferent between money and time changes with my stock of money. Since I work for 8 hours a day at $100 an hour, we can reasonably conclude that I'm *roughly* indifferent between an hour and $100 at my current wealth. But I'm obviously not indifferent to the point that I'd work 24 hours a day for $2400, nor 0 hours a day for $0. Further, once I have accumulated my million dollars (or more generally, enough money to live off the interest), my indifference level becomes much higher - you'd have to offer me way more money per hour to get me to work. Notice that in this case I'm postulating a very sharp dropoff, in that I'm happy to work for $100 an hour until the moment my savings account hits seven digits, and then I am no longer willing to work at all; it seems possible that the argument no longer works if you allow a more gradual change in indifference, but on the other hand \"save to X dollars and then retire\" doesn't seem like a psychologically unrealistic plan either.</p>\n<p>Am I making any obvious mistakes? Of course it may well be the case that the actual lottery tickets for sale in the real world do not match the wages-and-savings situations of real people in such a way that they have positive expected value; that's an empirical question. But it does seem in-principle possible for an epsilon chance at one-over-epsilon dollars paid out right away to be of positive expected value after converting to expected hours of life, even though it's neutral in expected dollars. Am I mistaken?</p>\n<p>&nbsp;</p>\n<p>Edit: Wei Dai found the problem. Briefly, the 100 dollars added to my savings would cut more than 1.1 hours off the time I had to work at the end of the 5.5 years.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gPTZjNsyRW94rcTBj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 7.945429209630305e-07, "legacy": true, "legacyId": "10786", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-04T23:41:21.315Z", "modifiedAt": null, "url": null, "title": "Reforming rot13", "slug": "reforming-rot13", "viewCount": null, "lastCommentedAt": "2012-10-05T19:11:51.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gDe37MvsqTE35uNzt/reforming-rot13", "pageUrlRelative": "/posts/gDe37MvsqTE35uNzt/reforming-rot13", "linkUrl": "https://www.lesswrong.com/posts/gDe37MvsqTE35uNzt/reforming-rot13", "postedAtFormatted": "Friday, November 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reforming%20rot13&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReforming%20rot13%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDe37MvsqTE35uNzt%2Freforming-rot13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reforming%20rot13%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDe37MvsqTE35uNzt%2Freforming-rot13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgDe37MvsqTE35uNzt%2Freforming-rot13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 413, "htmlBody": "<p><a href=\"http://rot13.com\">rot13.com</a> is a service frequently used here to hide spoilers. I really hate it, though. If I had the time, I would build a simple, but much better alternative. Maybe somebody has more time to do that, so I'll share my rough specification:</p>\n<ul>\n<li>Dynamic. The encoding happens on the client automatically, whenever the content of the input textbox changes. No Submit key needed.</li>\n<li>Fully client-side. No server-side script, just a static page with Javascript.</li>\n<li>Decoded content can be reached through an encoded URL. Instead of the receiver pasting the ciphertext into the rot13.com textbox, the sender can provide an encoded URL, and the receiver simply clicks on the link. This is slightly less convenient for the sender, but is much more convenient for the (hopefully many) receivers.</li>\n<li>Many people can decrypt parts of rot13 ciphertexts without conscious effort. This defeats the purpose. The new service could provide some alternative encryption that is not a <a href=\"http://en.wikipedia.org/wiki/Substitution_cipher\">substitution cipher</a>. It should still be a <a href=\"http://en.wikipedia.org/wiki/Reciprocal_cipher\">reciprocal cipher</a>, that is a nice property of rot13.</li>\n<li>rot13 fails for non-ASCII characters. (rot13.com leaves them intact.) The alternative encryption should work for all Unicode strings.</li>\n<li>Strong, password-based encryption could also be added as an extra feature. I could send you a very long URL, and tell you a password on the phone, or only tell you the password at a later date.</li>\n</ul>\n<p><br />The only important design problem I don't know how best to solve is making the encryption work for Unicode, with the following three constraints: making it a reciprocal cypher, outputting visually nice strings, and making it map ASCII to ASCII. One possible solution is to drop the constraint that it is a reciprocal cypher. For this service it is probably not crucial anyway: the ciphertext can be base64, with some escape prefix distinguishing it from plaintext.</p>\n<p>After writing the above, I found this LW thread: <a href=\"/lw/2xk/does_anyone_else_find_rot13_spoilers_as_annoying/\">Does anyone else find ROT13 spoilers as annoying as I do?</a> There were several suggestions there, and two of the commenters, <a href=\"/lw/2xk/does_anyone_else_find_rot13_spoilers_as_annoying/2txp\">sketerpot</a> and <a href=\"/lw/2xk/does_anyone_else_find_rot13_spoilers_as_annoying/2yls\">LightningRose</a> even coded their own solutions to the spoiler problem. Each solution had some merit, and <a href=\"http://www.lightning-rose.com/projects/rot13.htm\">LightningRose's</a> in particular was far superior to the rot13.com site I used to use, but basically, they only dealt with the third point of my proposal.<br /><br />Is there anything like what I envision? Is anyone interested in building it? What changes or extra features would you like to see?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gDe37MvsqTE35uNzt", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 7.945506906532293e-07, "legacy": true, "legacyId": "10787", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ju2qFSS6qyr2LzjMQ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-11-04T23:41:21.315Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-05T01:58:23.281Z", "modifiedAt": null, "url": null, "title": "Tell me what you think of me", "slug": "tell-me-what-you-think-of-me", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:58.987Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zFj67rtrQ7HEaZ45F/tell-me-what-you-think-of-me", "pageUrlRelative": "/posts/zFj67rtrQ7HEaZ45F/tell-me-what-you-think-of-me", "linkUrl": "https://www.lesswrong.com/posts/zFj67rtrQ7HEaZ45F/tell-me-what-you-think-of-me", "postedAtFormatted": "Saturday, November 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tell%20me%20what%20you%20think%20of%20me&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATell%20me%20what%20you%20think%20of%20me%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFj67rtrQ7HEaZ45F%2Ftell-me-what-you-think-of-me%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tell%20me%20what%20you%20think%20of%20me%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFj67rtrQ7HEaZ45F%2Ftell-me-what-you-think-of-me", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFj67rtrQ7HEaZ45F%2Ftell-me-what-you-think-of-me", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>Time and time again, honest feedback has improved my life. I have sought it out on many specific occasions, but now I have a static, anonymous way for people to give me feedback &mdash; for any reason, at any time.</p>\n<p>You can give me feedback on my personality, my conduct, or the organization for which I work&nbsp;<a href=\"http://tinyurl.com/luke-feedback\"><strong>by following this link right here</strong></a>.</p>\n<p>Thank you. I apologize for making a discussion post that is all about me.</p>\n<p>I operate by <a href=\"http://tinyurl.com/Crockers-rules\">Crocker's Rules</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zFj67rtrQ7HEaZ45F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 21, "extendedScore": null, "score": 7.9459861390923e-07, "legacy": true, "legacyId": "10793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-05T11:06:42.308Z", "modifiedAt": null, "url": null, "title": "Rational Romantic Relationships, Part 1: Relationship Styles and Attraction Basics", "slug": "rational-romantic-relationships-part-1-relationship-styles", "viewCount": null, "lastCommentedAt": "2016-01-22T02:53:19.664Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JYckkCqhZPrdScjBx/rational-romantic-relationships-part-1-relationship-styles", "pageUrlRelative": "/posts/JYckkCqhZPrdScjBx/rational-romantic-relationships-part-1-relationship-styles", "linkUrl": "https://www.lesswrong.com/posts/JYckkCqhZPrdScjBx/rational-romantic-relationships-part-1-relationship-styles", "postedAtFormatted": "Saturday, November 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20Romantic%20Relationships%2C%20Part%201%3A%20Relationship%20Styles%20and%20Attraction%20Basics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20Romantic%20Relationships%2C%20Part%201%3A%20Relationship%20Styles%20and%20Attraction%20Basics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYckkCqhZPrdScjBx%2Frational-romantic-relationships-part-1-relationship-styles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20Romantic%20Relationships%2C%20Part%201%3A%20Relationship%20Styles%20and%20Attraction%20Basics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYckkCqhZPrdScjBx%2Frational-romantic-relationships-part-1-relationship-styles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJYckkCqhZPrdScjBx%2Frational-romantic-relationships-part-1-relationship-styles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5507, "htmlBody": "<p><small>Part of the Sequence: <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></small><small>. Co-authored with&nbsp;</small><span style=\"font-size: 11px;\"><a href=\"http://spirituality.mindamyers.com/\">Minda Myers</a>&nbsp;and</span><small>&nbsp;</small><span style=\"font-size: 11px;\"><a href=\"/user/HughRistik/\">Hugh Ristik</a></span><small>. Also see: <a href=\"/lw/79x/polyhacking/\">Polyhacking</a>.</small></p>\n<p>When things <a href=\"/lw/70u/rationality_lessons_learned_from_irrational/\">fell apart</a> between me (Luke) and my first girlfriend, I decided <em>that</em>&nbsp;kind of relationship wasn't ideal for me.</p>\n<p>I didn't like the jealous feelings that had arisen within me. I didn't like the desperate, codependent 'madness' that popular love songs <a href=\"http://www.azlyrics.com/lyrics/beyonceknowles/crazyinlove.html\">celebrate</a>. I had moral objections to the idea of owning somebody else's sexuality, and to the idea of somebody else owning <em>mine</em>. Some of my culture's scripts for what a man-woman relationship should look like didn't fit my own goals very well.</p>\n<p>I needed to <em>design</em>&nbsp;romantic relationships that made sense&nbsp;(<a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision-theoretically</a>) for me, rather than simply <em>falling</em>&nbsp;into whatever relationship model my culture happened to offer. (The ladies of&nbsp;<a style=\"font-style: italic;\" href=\"http://en.wikipedia.org/wiki/Sex_and_the_City#Series_overview\">Sex and the City</a>&nbsp;weren't&nbsp;too good with decision theory, but they certainly invested time figuring out which relationship styles worked for <em>them</em>.) For a while, this new approach led me into a series of short-lived flings. After that, I chose 4 months of contented celibacy. After that, <a href=\"http://en.wikipedia.org/wiki/Polyamory\">polyamory</a>. After that...</p>\n<p>Anyway, the results have been wonderful. Rationality and decision theory work for relationships, too!</p>\n<p>We humans <a href=\"/lw/1zu/compartmentalization_as_a_passive_phenomenon/\">compartmentalize by default</a>. Brains don't automatically enforce <a href=\"http://en.wikipedia.org/wiki/Belief_propagation\">belief propagation</a>, and aren't configured to do so. <a href=\"/lw/k5/cached_thoughts/\">Cached thoughts</a> and <a href=\"/lw/4e/cached_selves/\">cached selves</a>&nbsp;can remain even after one has applied the lessons of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core sequences</a> to particular parts of one's life. That's why it helps to <em>explicitly</em>&nbsp;examine what happens when you apply rationality to new areas of your life&nbsp;<span style=\"line-height: 14px; font-family: arial, sans-serif;\">&mdash;</span>&nbsp;from <a href=\"/lw/2as/diseased_thinking_dissolving_questions_about/\">disease</a> to <a href=\"/lw/bk/the_trouble_with_good/\">goodness</a> to&nbsp;<a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">morality</a>. Today, we apply rationality to <em>relationships</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4><a name=\"styles\"></a>Relationships Styles</h4>\n<p>When <a href=\"http://www.mindamyers.com/\">Minda</a> had her first relationship with a woman, she found that the cultural scripts for heterosexual relationships didn't work for a homosexual relationship style. For example, in heterosexual dating (in the USA) the man is expected to ask for the date, plan the date, and escalate sexual interaction. A woman expects that she will be pursued and not have to approach men, that on a date she should be passive and follow the man's lead, and that she shouldn't initiate sex herself.</p>\n<p>In the queer community, Minda quickly found that if she passively waited for a woman to hit on her, she'd be waiting all night! When she met her first girlfriend, <em>Minda</em> had to ask for the date. Minda writes:</p>\n<blockquote>\n<p>On dates, I didn't know if I should pay for the date or hold the door or what I was supposed to do! Each interaction required thought and negotiation that hadn't been necessary before. And this was really kind of neat. We had the opportunity to create a relationship that worked for us and represented us as unique and individual human beings. And when it came to sexual interactions, I found it easy to ask for and engage in exactly what I wanted. And I have since brought these practices into my relationships with men.&nbsp;</p>\n</blockquote>\n<p>But you don't need to have an 'alternative' relationship in order to decide you want to set aside some cultural scripts and design a relationship style that works for you. You can choose relationship styles that work for you <em>now</em>.</p>\n<p>With regard to which type(s) of romantic partner(s) you want, there are many possibilities.</p>\n<p>No partners:</p>\n<ul>\n<li><em>Asexuality</em>. Asexuals don't experience sexual attraction. They <a href=\"http://articles.cnn.com/2004-10-14/tech/asexual.study_1_sexuality-new-study-new-scientist?_s=PM:TECH\">comprise</a> perhaps 1% of the population,<sup>1</sup> and include <a href=\"http://en.wikipedia.org/wiki/Asexuality#Notable_asexuals\">notables</a> like Paul Erdos, Morrissey, and Janeane Garofalo. There is a network (<a href=\"http://en.wikipedia.org/wiki/Asexual_Visibility_and_Education_Network\">AVEN</a>) for asexuality awareness and acceptance.</li>\n<li><em>Celibacy</em>. Celibates feel sexual attraction, but abstain from sex. Some choose to abstain for medical, financial, psychological, or philosophical reasons. Others choose celibacy so they have more time to achieve other goals, as I (Luke) did for a time. Others are involuntarily celibate; perhaps they can't find or attract suitable mates. This problem can often be solved by <a href=\"/lw/5p6/how_and_why_to_granularize/#social\">learning and practicing social skills</a>.</li>\n</ul>\n<p>One partner:</p>\n<ul>\n<li><em style=\"font-style: italic;\">Monogamy</em>. Having one sexual partner at a time is a standard cultural script, and may be over-used due to the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Status_quo_bias\">status quo bias</a>. Long-term monogamy should not be done on the pretense that attraction and arousal for one's partner won't fade. It will.<sup>2</sup>&nbsp;Still, there may be many people for whom monogamy is optimal.&nbsp;</li>\n</ul>\n<p>Many partners:</p>\n<ul>\n<li><span style=\"font-style: normal;\"><em style=\"font-style: italic;\">Singlehood</em>. Singlehood can be a good way to get to know yourself and experience a variety of short-term partners. About 78% of college students have had at least one 'one-night stand', and most such encounters were preceded by alcohol or drug use.<sup>3</sup> Indeed, many young people today no longer go on 'dates' to get to know a potential partner. Instead, they meet each other at a social event, 'hook up', and </span><span style=\"font-style: normal;\"><em>then</em>&nbsp;go on dates (if the hookup went well).<sup>4</sup></span></li>\n<li><em>Friendship 'with benefits'</em>. Friends are often people you already enjoy and respect, and thus may also make excellent sexual partners. According to one study, 60% of undergraduates have been a 'friend with benefits' for someone at one time.<sup>5</sup></li>\n<li><em>Polyamory.</em><sup>6</sup>&nbsp;In a polyamorous relationship, partners are clear about their freedom to pursue multiple partners. Couples communicate their boundaries and make agreements about what is and isn't allowed. Polyamory often requires partners to <a href=\"http://books.google.com/books?id=SNCy0iqZMskC&amp;lpg=PT87&amp;vq=unlearning%20jealousy&amp;pg=PT87#v=onepage&amp;q&amp;f=false\">de-program jealousy</a>. In my experience, polyamory is <em>much</em> more common in the rationality community than in the general population.</li>\n</ul>\n<p><a href=\"/user/HughRistik/\">Hugh</a> points out that your limbic system may not agree (at least initially) with your cognitive choice of a relationship style. Some women say they want a long-term relationship but date 'bad boys' who are unlikely to become long-term mates. Someone may think they want polyamorous relationships but find it impossible to leave jealousy behind.<sup>7</sup></p>\n<p>&nbsp;</p>\n<h4><a name=\"science\"></a>The Science of Attraction</h4>\n<p>A key skillset required for having the relationships you want is that of&nbsp;<em>building and maintaining attraction in potential mates</em>.</p>\n<p>Guys seeking girls may wonder: Why do girls say they want \"nice guys\" but date only \"jerks\"? Girls seeking rationalist guys are at an advantage because the gender ratio lies in their favor, but they still might wonder: What can I do to attract the <em>best</em>&nbsp;mates?&nbsp;Those seeking same-sex partners may wonder how attraction can differ from heterosexual norms.</p>\n<p>How do you build and maintain attraction in others? A lot can be learned by <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying different things</a> and seeing what works. This is often better than polling people, because people's verbal reports about what attracts them don't always match their actual behavior.<sup>8</sup></p>\n<p>To get you started, the <a href=\"http://yudkowsky.net/rational/virtues\">virtues</a> of <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">scholarship</a> and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">empiricism</a> will serve you well.&nbsp;<a href=\"http://en.wikipedia.org/wiki/Social_psychology\">Social psychology</a> has a wealth of knowledge to offer on successful relationships.<sup>9</sup> For example, here are some things that, according to the latest research, will tend to make people more attracted to you:</p>\n<ul>\n<li><em>Proximity</em>&nbsp;and <em>familiarity</em>. Study after study shows that we tend to like those who live near us, partly due to availability,<sup>10</sup> and partly because repeated exposure to almost <em>anything</em>&nbsp;increases liking.<sup>11</sup>&nbsp;A Taiwanese man once demonstrated the power of proximity and repeated exposure when he wrote over 700 letters to his girlfriend, urging her to marry him. She married the mail carrier.<sup>12</sup></li>\n<li><em>Similarity</em>. We tend to like people who are similar to us.<sup>13</sup> We like people with faces similar to our own.<sup>14</sup> We are even more likely to marry someone with a similar-sounding name.<sup>15</sup> Similarity makes attraction endure longer.<sup>16&nbsp;</sup>Also, similar people are more likely to react to events the same way, thus reducing the odds of conflict.<sup>17</sup></li>\n<li><em>Physical attractiveness</em>. Both men and women prefer good-looking mates.<sup>18</sup>&nbsp;Partly, this is because the <a href=\"/lw/lj/the_halo_effect/\">halo effect</a>:&nbsp;we automatically assume that more attractive people are also healthier, happier, more sensitive, more successful, and more socially skilled (but not necessarily more honest or compassionate).<sup>19</sup>&nbsp;Some of these assumptions are correct: Attractive and well-dressed people <em>are</em>&nbsp;more likely to impress employers and succeed occupationally.<sup>20</sup>&nbsp;But isn't beauty relative? Some standards of beauty vary from culture to culture, but many are universal.<sup>21</sup> Men generally prefer women who exhibit signs of youth and fertility.<sup>22</sup>&nbsp;Women generally prefer men who (1) display possession of abundant resources,<sup>23</sup>&nbsp;(2) display high social status,<sup>24</sup> (3) exhibit a 'manly' face (large jaw, thick eyebrows, visible beard stubble)<sup>25</sup>&nbsp;and physique,<sup>26</sup>&nbsp;and (4) are tall.<sup>27</sup>&nbsp;Both genders generally prefer (1) <a href=\"http://en.wikipedia.org/wiki/Agreeableness\">agreeableness</a>, <a href=\"http://en.wikipedia.org/wiki/Conscientiousness\">conscientiousness</a>, and <a href=\"http://en.wikipedia.org/wiki/Extraversion_and_introversion\">extraversion</a>,<sup>28</sup> (2) 'average' and symmetrical faces with features that are neither unusually small or large,<sup>29</sup>&nbsp;(2) large smiles,<sup>30</sup> (3) pupil dilation,<sup>31</sup> and some other things (more on this later).</li>\n<li><em>Liking others</em>. Liking someone makes them more attracted to you.<sup>32</sup></li>\n<li><em>Arousing others</em>. Whether aroused by fright, exercise, stand-up comedy, or erotica, we are more likely to be attracted to an attractive person when we are generally aroused than when we are not generally aroused.<sup>33</sup> As David Myers writes, \"Adrenaline makes the heart grow fonder.\"<sup>34</sup> This may explain why rollercoasters and horror movies are such a popular date night choice.</li>\n</ul>\n<p>But this barely scratches the surface of attraction science. In a later post, we'll examine how attraction works in more detail, and draw up a science-supported game plan for building attraction in others.</p>\n<p>&nbsp;</p>\n<h4><a name=\"mean-variance\"></a>Attractiveness: Mean and Variance</h4>\n<p>Remember that increasing your <em>average</em>&nbsp;attractiveness (by appealing to more&nbsp;people) may not be an optimal strategy.</p>\n<p>Marketers know that it's often better to sacrifice broad appeal in order for a product to have very <em>strong</em>&nbsp;appeal to a <a href=\"http://en.wikipedia.org/wiki/Niche_market\">niche market</a>. <a href=\"http://web.archive.org/web/20110202095449/https://theappunto.com/\">The Appunto</a> doesn't appeal to most men, but it appeals strongly enough to <em>some</em>&nbsp;men that they are willing to pay the outrageous $200 price for it.</p>\n<p>Similarly, you may have the best success in dating if you appeal <em>very strongly</em>&nbsp;to <em>some</em>&nbsp;people, even if this makes you less appealing to <em>most</em>&nbsp;people&nbsp;<span style=\"line-height: 14px; font-family: arial, sans-serif; \">&mdash;</span>&nbsp;that is, if you adopt a niche marketing strategy in the dating world.<sup>35</sup></p>\n<p>As long as you can <em>find</em> those few people who find you <em>very</em>&nbsp;attractive, it won't matter (for dating) that <em>most</em>&nbsp;people aren't attracted to you. And because one can switch between niche appeal and broad appeal using fashion and behavior, you can simply use clothing and behavior with mainstream appeal during the day (to have general appeal in professional environments) and use alternative clothing and behavior when you're socializing (to have <em>strong</em>&nbsp;appeal to a small subset of people whom you've sought out).</p>\n<p>To visualize this point, consider two attraction strategies. Both strategies employ phenomena that are (almost) universally attractive, but the blue strategy aims to maximize the frequency of <em>somewhat</em> positive responses while the red strategy aims to maximize the frequency of <em>highly</em>&nbsp;positive responses. The red strategy (e.g. using&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/fashionable-mainstream-man.png\">mainstream fashion</a>) increases one's <em>mean attractiveness</em>, while the blue strategy (e.g. using&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/fashionable-goth-guy.png\">alternative fashion</a>) increases one's <em>attractiveness variance</em>. <a href=\"/user/HughRistik/\">Hugh Ristik</a> offers the following chart:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/attractiveness-mean-and-variance.png\" alt=\"\" /></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/death-guild-guy.jpg\">This goth guy</a>&nbsp;and I (Luke) can illustrate this phenomenon. I aim for mainstream appeal; he wears <a href=\"http://en.wikipedia.org/wiki/Gothic_fashion\">goth clothing</a> when socializing. My mainstream look turns off almost no one, and is attractive to most women, but doesn't get that many <em>strong</em>&nbsp;reactions right away unless I employ&nbsp;<em>other</em> high-variance strategies.<sup>36</sup> In contrast, I would bet the goth guy's alternative look turns off many people and is less attractive to most women than <em>my</em> look is, but has a higher frequency of <em>extremely</em>&nbsp;positive reactions in women.</p>\n<p>In one's professional life, it may be better to have broad appeal. But in dating, the <em>goal</em>&nbsp;is to find people who find you extremely attractive. The goth guy sacrifices his mean attractiveness to increase his attractiveness variance (and thus the frequency of <em>very</em>&nbsp;positive responses), and this works well for him in the dating scene.</p>\n<p>High-variance strategies like this are a good way to filter for people who are strongly attracted to you, and thus avoid wasting your time with potential mates who only feel lukewarm toward&nbsp;you.</p>\n<p>&nbsp;</p>\n<h4>Up next</h4>\n<p>In future posts we'll develop an action plan for using the science of attraction to create successful romantic relationships. We'll also explain how rationality helps with relationship maintenance<sup>37</sup> and relationship satisfaction.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Previous post: <a href=\"/lw/cu2/the_power_of_reinforcement/\">The Power of Reinforcement</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Notes</h4>\n<p><span style=\"font-size: 11px;\"><sup>1</sup>&nbsp;Bogaert (2004).</span></p>\n<p><span style=\"font-size: 11px; \"><sup>2</sup>&nbsp;About half of romantic relationships of all types end within a few years (Sprecher 1994; Kirkpatrick &amp; Davis 1994; Hill et al 1976), and even relationships that last exhibit diminishing attraction and arousal (Aron et al. 2006; Kurdek 2005; Miller et al. 2007). Note that even if attraction and arousal fades, romantic love can exist in long-term closed monogamy and it is associated with relationship satisfaction (Acevedo &amp; Aron, 2009).</span></p>\n<p style=\"font-size: small;\"><small><sup>3</sup>&nbsp;Paul et al. (2000); Grello et al. (2006).</small></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px; \"><sup>4</sup>&nbsp;Bogle (2008).</span></p>\n<p style=\"font-size: small;\"><small><sup>5</sup>&nbsp;Bisson &amp; Levine (2009).</small></p>\n<p><span style=\"font-size: 11px; \"><sup>6</sup>&nbsp;Two introductory books on the theory and practice of polyamory are: Easton &amp; Hardy (2009) and&nbsp;Taormino (2008).</span></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>7</sup>&nbsp;See work on 'conditional mating strategies' aka 'strategic pluralism' (Gangestad &amp; Simpson, 2000).</span></p>\n<p style=\"font-size: small;\"><small><sup>8</sup>&nbsp;Sprecher &amp; Felmlee (2008); Eastwick &amp; Finkel (2008). Likewise, there is a difference between what people publicly report as being the cause of a breakup, what they actually think caused a breakup, and what actually caused a breakup (Powell &amp; Fine, 2009). Also see <a href=\"/lw/5sk/inferring_our_desires/\">Inferring Our Desires</a>.</small></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>9</sup>&nbsp;For overviews of this research, see: Bradbury &amp; Karney (2010); Miller &amp; Perlman (2008); Vangelisti &amp; Perlman (2006); Sprecher et al. (2008); Weiten et al. (2011), chs. 8-12. For a history of personal relationships research, see Perlman &amp; Duck (2006).</span></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>10</sup>&nbsp;Goodfriend (2009).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>11</sup></span><span style=\"font-size: 11px;\">&nbsp;This is called the </span><span style=\"font-size: 11px;\"><a href=\"http://en.wikipedia.org/wiki/Mere_exposure_effect\">mere exposure effect</a></span><span style=\"font-size: 11px;\">. See Le (2009); Moreland &amp; Zajonc (1982); Nuttin (1987); Zajonc (1968, 2001); Moreland &amp; Beach (1992). The limits of this effect are explored in Bornstein (1989, 1999); Swap (1977).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>12</sup>&nbsp;Steinberg (1993).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>13</sup>&nbsp;Zajonc (1998); Devine (1995); Rosenbaum (1986); Surra et al. (2006); Morry (2007, 2009); Peplau &amp; Fingerhut (2007); Ledbetter et al. (2007); Montoya et al. (2008); Simpson &amp; Harris (1994).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>14</sup>&nbsp;DeBruine (2002, 2004); Bailenson et al. (2005).</span></p>\n<p><small><sup>15</sup> Jones et al. (2004).</small></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>16</sup>&nbsp;Byrne (1971); Ireland et al. (2011).</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>17</sup>&nbsp;Gonzaga (2009). For an overview of the research on self-disclosure, see Greene et al. (2006).</span></span></p>\n<p><small><sup>18</sup>&nbsp;Langlois et al. (2000); Walster et al. (1966); Feingold (1990); Woll (1986); Belot &amp; Francesconi (2006); Finkel &amp; Eastwick (2008); Neff (2009); Peretti &amp; Abplanalp (2004); Buss et al. (2001); Fehr (2009); Lee et al. (2008); Reis et al. (1980). This is also true for homosexuals: Peplau &amp; Spalding (2000).&nbsp;Even infants prefer attractive faces:&nbsp;</small><span style=\"font-size: 11px;\">Langlois et al. (1987);&nbsp;Langlois et al. (1990);&nbsp;Slater et al. (1998). Note that women report&nbsp;that the physical attractiveness is less important to their mate preferences than it actually is:&nbsp;</span><span style=\"font-size: 11px; \">Sprecher (1989).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>19</sup> Eagly et al. (1991); Feingold (1992a); Hatfield &amp; Sprecher (1986); Smith et al. (1999); Dion et al. (1972).</span></p>\n<p><small><sup>20</sup> Cash &amp; Janda (1984); Langlois et al. (2000); Solomon (1987).</small></p>\n<p><span style=\"font-size: 11px;\"><sup>21</sup>&nbsp;Cunningham et al. (1995); Cross &amp; Cross (1971); Jackson (1992); Jones (1996); Thakerar &amp; Iwawaki (1979).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>22</sup>&nbsp;Men certainly prefer youth (Buss 1989a; Kenrick &amp; Keefe 1992; Kenrick et al. 1996;&nbsp;Ben&nbsp;Hamida et al. 1998). Signs of fertility that men prefer include clear and smooth skin (Sugiyama 2005; Singh &amp; Bronstad 1997; Fink &amp; Neave 2005; Fink et al. 2008; Ford &amp; Beach 1951; Symons 1995), facial femininity (Cunningham 2009; Gangestad &amp; Scheyd 2005; Schaefer et al. 2006; Rhodes 2006), long legs (Fielding et al. 2008; Sorokowski &amp; Pawlowski 2008; Bertamini &amp; Bennett 2009; Swami et al. 2006), and a low waist-to-hip ratio (Singh 1993, 2000; Singh &amp; Young 1995; Jasienska et al. 2004; Singh &amp; Randall 2007; Connolly et al 2000; Furnham et al 1997; Franzoi &amp; Herzog 1987; Grabe &amp; Samson 2010). Even men blind from birth prefer a low waist-to-hip ratio (Karremans et al. 2010).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>23</sup>&nbsp;Buss et al. (1990); Buss &amp; Schmitt (1993); Khallad (2005); Gottschall et al. (2003); Gottschall et al. (2004); Kenrick et al. (1990); Gustavsson &amp; Johnsson (2008); Wiederman (1993); Badahdah &amp; Tiemann (2005); Marlowe (2004); Fisman et al. (2006); Asendorpf et al. (2010); Bokek-Cohen et al. (2007); Pettay et al. (2007); Goode (1996).</span></p>\n<p><span style=\"font-size: 11px; \"><sup>24</sup>&nbsp;Feingold (1990, 1992b).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>25</sup>&nbsp;Cunningham (2009); Cunningham et al. (1990).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>26</sup>&nbsp;Singh (1995); Martins et al. (2007).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>27</sup>&nbsp;Lynn &amp; Shurgot (1984); Ellis (1992); Gregor (1985); Kurzban &amp; Weeden (2005); Swami &amp; Furnham (2008). In contrast, men prefer women who are about 4.5 inches shorter than themselves: Gillis &amp; Avis (1980).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>28</sup>&nbsp;Figueredo et al. (2006).</span></p>\n<p><small><sup>29</sup> Langlois &amp; Roggman (1990); Rhodes et al. (1999); Singh (1995); Thornhill &amp; Gangestad (1994, 1999). We may have evolved to be attracted to symmetrical faces because they predict physical and mental health (</small><span style=\"font-size: 11px; \">Thornhill &amp; Moller, 1997).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>30</sup>&nbsp;Cunningham (2009).</span></p>\n<p><small><sup>31</sup>&nbsp;Cunningham (2009).</small></p>\n<p><span style=\"font-size: 11px; \"><sup>32</sup>&nbsp;This is called <a href=\"http://en.wikipedia.org/wiki/Reciprocal_liking\">reciprocal liking</a>. See Curtis &amp; Miller (1986); Aron et al (2006); Berscheid &amp; Walster (1978); Smith &amp; Caprariello (2009); Backman &amp; Secord (1959).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>33</sup>&nbsp;Carducci et al. (1978); Dermer &amp; Pszczynski (1978); White &amp; Knight (1984); Dutton &amp; Aron (1974).</span></p>\n<p><small><sup>34</sup> Myers (2010), p. 710.</small></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>35</sup>&nbsp;One example of a high-variance strategy for heterosexual men in the dating context is a bold opening line like \"You look familiar. Have we had sex?\" Most women will be turned off by such a line, but those who react positively are (by selection and/or by the confidence of the opening line) usually&nbsp;<em style=\"font-style: italic;\">very&nbsp;</em>attracted.&nbsp;</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>36</sup>&nbsp;In business, this is often said as \"not everyone is your customer\": <a href=\"http://www.drjeffcornwall.com/2010/10/everyone-is-not-your-potential.html\">1</a>, <a href=\"http://www.thesilverbulletforsmallbusiness.com/2011/04/small-business-marketing-tip-why-your-ideal-customer-isnt-everyone/\">2</a>, <a href=\"http://articles.mplans.com/not-everybody-is-your-customer/\">3</a>.</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>37</sup>&nbsp;For discussions of relationship maintenance in general, see: Ballard-Reisch &amp; Wiegel (1999); Dinda &amp; Baxter (1987); Haas &amp; Stafford (1998).</span></span></p>\n<p><span style=\"font-size: 11px;\">&nbsp;</span></p>\n<h4>References</h4>\n<p><small>Acevedo &amp; Aron (2009). <a href=\"https://1445081729018657037-a-1802744773732722657-s-sites.googlegroups.com/site/simingdong/Home/gpr13159.pdf?attachauth=ANoY7crX7vsV1CnXM9LZV2YzC3SNVKMbTJBaijisoM7FgAXymhq6oGUB5bB-hYxNA1m19PhaMXGYT1Bka_jOuZFSXhpAz78U37s0rUJl8XqKdmwd4NmeCF5fzoRBzRafjznA2BisONOQqyLqoE18cwBUYLcjSw3HiTaMDGG1uXvTxPu1z6WIjpG2jpBLcSODPDmzDYyPzqgcUHHNA6qYLbk4DcuM3udUsA%3D%3D&amp;attredirects=0\">Does a long-term relationship kill romantic love?</a> <em>Review of General Psychology, 13</em>: 59-65.</small></p>\n<p><small>Aron, Fisher, &amp; Strong (2006). Romantic love. In Vangelisti &amp; Perlman (eds.), <em>The Cambridge Handbook of Personal Relationships</em>. Cambridge University Press.</small></p>\n<p><small>Asendorpf, Penke, &amp; Back (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FAsendorpf-From-dating-to-mating-and-relating-Predictors-of-initial-and-long-term-outcomes-of-speed-dating-in-a-community-sample.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=From%20dating%20to%20mating%20and%20relating%3A%20Predictors%20of%20initial%20and%20long-term%20outcomes%20of%20speed%20dating%20in%20a%20community%20sample\">From dating to mating and relating: Predictors of initial and long-term outcomes of speed dating in a community sample</a>. <em>European Journal of Personality</em>.</small></p>\n<p><span style=\"font-size: 11px; \">Backman &amp; Secord (1959). The effect of perceived liking on interpersonal attraction. <em>Human Relations, 12</em>: 379-384.</span></p>\n<p><small>Badahdah &amp; Tiemann (2005). Mate selection criteria among Muslims living in America. <em>Evolution and Human Behavior, 26</em>: 432-440.</small></p>\n<p><small>Bailenson, Iyengar, &amp; Yee (2005). <a href=\"http://vhil.stanford.edu/pubs/2005/identity-capture.html\">Facial identity capture and presidential candidate preference</a>. Paper presented at the Annual Conference of the International Communication Association.</small></p>\n<p><span style=\"font-size: 11px;\">Ballard-Reisch &amp; Wiegel (1999). Communication processes in marital commitment: An integrative approach. In Adams &amp; Jones (eds.), <em>Handbook of interpersonal commitment and relationship stability</em>&nbsp;(pp. 407-424). Plenum.</span></p>\n<p><small>Belot &amp; Francesconi (2006). <a href=\"http://www.essex.ac.uk/economics/discussion-papers/papers-text/dp620.pdf\">Can anyone be 'the one'? Evidence on mate selection from speed dating</a>. Centre for Economic Policy Research.</small></p>\n<p><small>Ben&nbsp;Hamida, Mineka, &amp; Bailey (1998).&nbsp;Sex differences in perceived controllability of&nbsp;mate value: An evolutionary perspective. <em>Journal&nbsp;of Personality and Social Psychology, 75</em>:&nbsp;953&ndash;966.</small></p>\n<p><small>Berscheid &amp; Walster (1978). <em><a href=\"http://www.amazon.com/Interpersonal-Attraction-Ellen-Berscheid/dp/007554802X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Interpersonal Attraction</a></em>. Addison-Wesley.</small></p>\n<p><small>Bertamini &amp; Bennett (2009). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBertamini-The-effect-of-leg-length-on-perceived-attractiveness-of-simplified-stimuli.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20effect%20of%20leg%20length%20on%20perceived%20attractiveness%20of%20simplified%20stimuli\">The effect of leg length on perceived attractiveness of simplified stimuli</a>. <em>Journal of Social, Evolutionary, and Cultural Psychology, 3</em>: 233-250.</small></p>\n<p><small>Bogaert (2004). Asexuality: Prevalence and associated factors in a national probability sample. <em>Journal of Sex Research, 41</em>: 279-287.</small></p>\n<p><span style=\"font-size: 11px; \">Bogle (2008). <em><a href=\"http://www.amazon.com/Hooking-Up-Dating-Relationships-Campus/dp/0814799698/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hooking Up: Sex, dating, and relationships on campus</a></em>. New York University Press.</span></p>\n<p><small>Bokek-Cohen, Peres, &amp; Kanazawa (2007). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBokek-Cohen-Rational-choice-and-evolutionary-psychology-as-explanations-for-mate-selectivity.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Rational%20choice%20and%20evolutionary%20psychology%20as%20explanations%20for%20mate%20selectivity\">Rational choice and evolutionary psychology as explanations for mate selectivity</a>. <em>Journal of Social, Evolutionary, and Cultural Psychology, 2</em>: 42-55.</small></p>\n<p><small>Bornstein (1989). Exposure and affect: Overview and meta-analysis of research, 1968-1987. <em>Psychological Bulletin, 106</em>: 265-289.</small></p>\n<p><small>Bornstein (1999). Source amnesia, misattribution, and the power of unconscious perceptions and memories. <em>Psychoanalytic Psychology, 16</em>: 155-178.</small></p>\n<p><span style=\"font-size: 11px; \">Bradbury &amp; Karney (2010). <em><a href=\"http://www.amazon.com/Intimate-Relationships-Thomas-N-Bradbury/dp/0393979571/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Intimate Relationships</a></em>. W.W. Norton &amp; Company.</span></p>\n<p><small>Buss (1989). Sex differences in human mate preferences: Evolutionary hypotheses testing in 37 cultures. <em>Behavioral and Brain Sciences, 12</em>: 1-49.</small></p>\n<p><small>Buss &amp; Schmitt (1993). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBuss-Sexual-strategies-theory-An-evolutionary-perspective-on-human-mating.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Sexual%20strategies%20theory%3A%20An%20evolutionary%20perspective%20on%20human%20mating\">Sexual strategies theory: An evolutionary perspective on human mating</a>. <em>Psychological Review, 100</em>: 204-232.</small></p>\n<p><small>Buss, Abbott, Angleitner, Asherian, Biaggio, et al. (1990). International preferences in selecting mates: A study of 37 cultures. <em>Journal of Cross-Cultural Psychology, 21</em>: 5-47.</small></p>\n<p><small>Buss, Shackelford, Kirkpatrick, &amp; Larsen (2001). <a href=\"http://www.homepage.psy.utexas.edu/homepage/Group/BussLAB/pdffiles/half%20century%20of%20mate%20prefs-2001-jmf.pdf\">A half century of mate preeferences: The cultural evolution of values</a>. <em>Journal of Marriage and Family, 63</em>: 291-503.</small></p>\n<p><small>Byrne (1971). <em><a href=\"http://www.amazon.com/Attraction-Personality-Psychopathology-Psycho-pathology-Monographs/dp/0121486508/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Attraction Paradigm</a></em>. Academic Press.</small></p>\n<p><small>Carducci, Cosby, &amp; Ward (1978). Sexual arousal and interpersonal evaluations. <em>Journal of Experimental Social Psychology, 14</em>: 449-457.</small></p>\n<p><small>Cash &amp; Janda (1984). The eye of the beholder. <em>Psychology Today, November</em>: 46-52.</small></p>\n<p><small>Connolly, Mealey, &amp; Slaughter (2000). The development of waist-to-hip ratio preferences. <em>Perspectives in Human Biology, 5</em>: 19-29.</small></p>\n<p><small>Cross &amp; Cross (1971). Age, sex, race, and the perception of facial beauty. <em>Developmental Psychology, 5</em>: 433-439.</small></p>\n<p><small>Cunningham, Roberts, Wu, Barbee, &amp; Druen (1995). \"Their ideas of beauty are, on the whole, the same as ours\": Consistency and variability in the cross-cultural perception of female attractiveness. <em>Journal of Personality and Social Psychology, 68</em>: 261-279.</small></p>\n<p><small>Cunningham (2009). Physical Attractiveness, Defining Characteristics. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1237-1242). Sage Reference.</small></p>\n<p><small>Curtis &amp; Miller (1986). Believing another likes or dislikes you: Behaviors making the beliefs come true. <em>Journal of Personality and Social Psychology, 51</em>: 284-290.</small></p>\n<p><small>DeBruine (2002). <a href=\"http://rspb.royalsocietypublishing.org/content/269/1498/1307.full.pdf\">Facial resemblance enhances trust</a>. <em>Proceedings of the Royal Society of London, 269</em>: 1307-1312.</small></p>\n<p><small>DeBruine (2004). <a href=\"http://rspb.royalsocietypublishing.org/content/271/1552/2085.full.pdf\">Facial resemblance increases the attractiveness of same-sex faces more than other-sex faces</a>. <em>Proceedings of the Royal Society of London B, 271</em>: 2085-2090.</small></p>\n<p><small>Dermer &amp; Pszczynski (1978). <a href=\"https://pantherfile.uwm.edu/dermer/public/vita/dermer_erotica.pdf\">Effects of erotica upon men's loving and liking responses for women they love</a>. <em>Journal of Personality and Social Psychology, 36</em>: 1302-1309.</small></p>\n<p><small>Devine (1995). Prejudice and outgroup perception. In Teser (ed.), <em>Advanced Social Psychology.</em>&nbsp;McGraw-Hill.</small></p>\n<p><span style=\"font-size: 11px;\">Dinda &amp; Baxter (1987). Strategies for maintaining and repairing marital relationships. <em>Journal of Social and Personal Relationships, 4</em>: 143-158.</span></p>\n<p><span style=\"font-size: 11px; \">Dion, Berscheid, &amp; Walster (1972). What is beautiful is good. <em>Journal of Personality and Social Psychology, 24</em>: 285-290.</span></p>\n<p><span style=\"font-size: 11px; \">Dutton &amp; Aron (1974). Some evidence for heightened sexual attraction under conditions of high anxiety. <em>Journal of Personality and Social Psychology, 30</em>: 510-517.</span></p>\n<p><small>Eagly, Ashmore, Makhijani, &amp; Kennedy (1991). What is beautiful is good, but...: A meta-analytic review of research on the physical attractiveness stereotype. <em>Psychological Bulletin, 110</em>: 109-128.</small></p>\n<p><small>Easton &amp; Hardy (2009).&nbsp;<em style=\"font-style: italic; \"><a href=\"http://www.amazon.com/Ethical-Slut-Practical-Relationships-Adventures/dp/1587613379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Ethical Slut: A Practical Guide to Polyamory, Open Relationships &amp; Other Adventures, 2nd edition</a></em>. The Celestial Arts.</small></p>\n<p><span style=\"font-size: 11px;\">Eastwick &amp; Finkel (2008).&nbsp;<a href=\"http://www.eastwick.motives.com/EastwickFinkel2008JPSP.pdf\">Sex differences in mate preferences revisited: Do people know what they initially desire in a romantic partner?</a> Journal of Personality and Social Psychology, 94: 245-264.</span></p>\n<p><small>Eldridge (2009). Conflict patterns. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of human relationships: Vol. 1</em>&nbsp;(pp. 307-310). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Ellis (1992). The evolution of sexual attraction: Evaluative mechanisms in women. In Barkow, Cosmides, &amp; Tooby (eds.), <em>The Adapted Mind: Evolutionary psychology and the generation of culture</em>&nbsp;(pp. 267-288). Oxford University Press.</span></p>\n<p><small>Fehr (2009). Friendship formation and development. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 1</em>&nbsp;(pp. 706-10). Sage Reference.</small></p>\n<p><small>Feingold (1990). Gender differences in effects of physical attractiveness on romantic attraction: A comparison across five research paradigms. <em>Journal of Personality and Social Psychology, 59</em>: 981-993.</small></p>\n<p><small>Feingold (1992a). Good-looking people are not what we think. <em>Psychological Bulletin, 111</em>: 304-341.</small></p>\n<p><span style=\"font-size: 11px; \">Feingold (1992b). Gender differences in mate selection preferences: A test of the parental investment model. <em>Psychological Bulletin, 116</em>: 429-256.</span></p>\n<p><small>Figueredo, Sefcek, &amp; Jones (2006). <a href=\"http://www.u.arizona.edu/~ajf/pdf/Figueredo,%20Sefcek,%20%26%20Jones%202006.pdf\">The ideal romantic partner personality</a>. <em>Personality and Individual Differences, 41</em>: 431-441.</small></p>\n<p><small>Fielding, Scholling, Adab, Cheng, Lao et al. (2008). Are longer legs associated with enhanced fertility in Chinese women? <em>Evolution and Human Behavior, 29</em>: 434-443.</small></p>\n<p><small>Fink &amp; Neave (2005). The biology of facial beauty. <em>Internal Journal of Cosmetic Science, 27</em>: 317-325.</small></p>\n<p><small>Fink, Matts, Klingenberg, Kuntze, Weege, &amp; Grammar (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FFink-Visual-attention-to-variation-in-female-skin-color-distribution.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Visual%20attention%20to%20variation%20in%20female%20skin%20color%20distribution\">Visual attention to variation in female skin color distribution</a>. <em>Journal of Cosmetic Dermatology, 7</em>: 155-161.</small></p>\n<p><small>Finkel &amp; Eastwick (2008). Speed-dating. <em>Current Directions in Psychological Science, 17</em>: 193-197.</small></p>\n<p><small>Fisman, Iyengar, Kamenica, &amp; Simonson (2006). Gender differences in mate selection: Evidence from a speed dating experiment. <em>The Quarterly Journal of Economics, 121</em>: 673-697.</small></p>\n<p><small>Ford &amp; Beach (1951). <em><a href=\"http://www.amazon.com/Patterns-Sexual-Behavior-Clellan-Stearns/dp/B000OEXAMM/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Patterns of Sexual Behavior</a></em>. Harper &amp; Row.</small></p>\n<p><small>Franzoi &amp; Herzog (1987). Judging personal attractiveness: What body aspects do we use? <em>Personality and Social Psychology Bulletin, 13</em>: 19-33.</small></p>\n<p><small>Furnham, Tan, &amp; McManus (1997). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FFurnham-Waist-to-hip-ratio-and-preferences-for-body-shape-A-replication-and-extension.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Waist-to-hip%20ratio%20and%20preferences%20for%20body%20shape%3A%20A%20replication%20and%20extension\">Waist-to-hip ratio and preferences for body shape: A replication and extension</a>. <em>Personality and Individual Differences, 22</em>: 539-549.</small></p>\n<p><span style=\"font-size: 11px; \">Gangestad &amp; Simpson (2000). <a href=\"http://smg.media.mit.edu/classes/Identity2004/EvolutionOfHumanMating.pdf\">The evolution of human mating: Trade-offs and strategic pluralism</a>. Behavioral and Brain Sciences, 23: 573-644.</span></p>\n<p><small>Gangestad &amp; Scheyd (2005). The evolution of human physical attractiveness. <em>Annual Review of Anthropology, 34</em>: 523-548.</small></p>\n<p><span style=\"font-size: 11px;\">Gillis &amp; Avis (1980).</span></p>\n<p><small>Gonzaga (2009). Similarity in ongoing relationships. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1496-1499). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Goode (1996). Gender and courtship entitlement: Responses to personal ads. <em>Sex Roles, 34</em>: 141-169.</span></p>\n<p><small>Goodfriend (2009). Proximity and attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1297-1299). Sage Reference.</small></p>\n<p><small>Gottschall, Berkey, Cawson, Drown, Fleischner, et al. (2003). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGottschall-Patterns-of-characterization-in-folktales-across-geographic-regions-and-levels-of-cultural-complexity.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Patterns%20of%20characterization%20in%20folktales%20across%20geographic%20regions%20and%20levels%20of%20cultural%20complexity%3A%20Literature%20as%20a%20neglec...\">Patterns of characterization in folktales across geographic regions and levels of cultural complexity: Literature as a neglected source of quantitative data</a>. <em>Human Nature, 14</em>: 365-382.</small></p>\n<p><small>Gottschall, Martin, Quish, &amp; Rea (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGottschall-Sex-differences-in-mate-choice-criteria-are-reflected-in-folktales-from-around-the-world-and-in-historical-European-literature.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Sex%20differences%20in%20mate%20choice%20criteria%20are%20reflected%20in%20folktales%20from%20around%20the%20world%20and%20in%20historical%20European%20literature\">Sex differences in mate choice criteria are reflected in folktales from around the world and in historical European literature</a>. <em>Evolution and Human Behavior, 25</em>: 102-112.</small></p>\n<p><small>Grabe &amp; Samson (2010).&nbsp;Sexual Cues Emanating From the Anchorette Chair: Implications for Perceived Professionalism, Fitness for Beat, and Memory for News.&nbsp;<em>Communication Research, December 14</em>.</small></p>\n<p><small>Greene, Derlega, Mathews (2006). Self-disclosure in personal relationships. In Vangelisti &amp; Perlman (eds.),&nbsp;<em style=\"font-style: italic;\">The Cambridge Handbook of Personal Relationships&nbsp;</em>(pp. 409-428). Cambridge University Press.</small></p>\n<p><span style=\"font-size: 11px; \">Gregor (1985). <em><a href=\"http://www.amazon.com/Anxious-Pleasures-Sexual-Amazonian-People/dp/0226307433/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anxious Pleasures: The sexual lives of an Amazonian people</a></em>. University of Chicago Press.</span></p>\n<p><span style=\"font-size: 11px; \">Grello, Welsh, &amp; Harper (2006). No strings attached: The nature of casual sex in college students. <em>Journal of Sex Research, 43</em>: 255-267.</span></p>\n<p><small>Gustavsson &amp; Johnsson (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGustavvsson-Mixed-support-for-sexual-selection-theories-of-mate-preferences-in-the-Swedish-population.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Mixed%20support%20for%20sexual%20selection%20theories%20of%20mate%20preferences%20in%20the%20Swedish%20population\">Mixed support for sexual selection theories of mate preferences in the Swedish population</a>. <em>Evolutionary Psychology, 6</em>: 454-470.</small></p>\n<p><span style=\"font-size: 11px;\">Haas &amp; Stafford (1998). An initial examination of maintenance behaviors in gay and lesbian relationships. <em>Journal of Social and Personal Relationships, 15</em>: 846-855.</span></p>\n<p><small>Hatfield &amp; Sprecher (1986). <em><a href=\"http://www.amazon.com/Mirror-Importance-Everyday-Sexual-Behavior/dp/0887061249/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Mirror, mirror... The importance of looks in everyday life</a></em>. State University of New York Press.</small></p>\n<p><small>Hill, Rubin, &amp; Peplau (1976). Breakups before marriage: The end of 103 affairs. <em>Journal of Social Issues, 32</em>: 147-168.</small></p>\n<p><span style=\"font-size: 11px;\">Ireland, Slatcher, Eastwick, Scissors, Finkel, &amp; Pennebaker (2011). <a href=\"http://people.tamu.edu/~eastwick/Ireland2011_PSci.pdf\">Language style matching predicts relationship initiation and stability</a>. Psychological Science, 22: 39-44.</span></p>\n<p><small>Jackson (1992). <a href=\"http://www.amazon.com/Physical-Appearance-Gender-Sociobiological-Sociocultural/dp/0791408248/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Physical appearance and gender: Sociobiological and sociocultural perspectives</a>. State University of New York Press.</small></p>\n<p><small>Jasienska, Ziomkiewicz, Ellison, Lipson, &amp; Thune (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FJasienska-Large-breasts-and-narrow-waists-indicate-high-reproductive-potential-in-women.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Large%20breasts%20and%20narrow%20waists%20indicate%20high%20reproductive%20potential%20in%20women\">Large breasts and narrow waists indicate high reproductive potential in women</a>. <em>Proceedings of the Royal Society of London, B, 271</em>: 1213-1217.</small></p>\n<p><small>Jones (1996). <em><a href=\"http://www.amazon.com/Physical-Attractiveness-Theory-Sexual-Selection/dp/0915703408/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Physical attractiveness and the theory of sexual selection</a></em>. University of Michigan Press.</small></p>\n<p><small>Jones, Pelham, Carvallo, &amp; Mirenberg (2004). <a href=\"http://new.dixie.edu/humanities/File/Count%20the%20Js.pdf\">How do I love thee? Let me count the Js: Implicit egotism and interpersonal attraction</a>. <em>Journal of Personality and Social Psychology, 87</em>: 665-683.</small></p>\n<p><small>Karremans, Frankenhuis, &amp; Arons (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FKarremans-Blind-men-prefer-a-low-waist-to-hip-ratio.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Blind%20men%20prefer%20a%20low%20waist-to-hip%20ratio\">Blind men prefer a low waist-to-hip ratio</a>. <em>Evolution and Human Behavior, 31</em>: 182-186.</small></p>\n<p><small>Kenrick, Sadalla, Groth, &amp; Trost (1990). Evolution, traits, and the stages of human courtship: Qualifying the parental investment model. <em>Journal of Personality, 58</em>: 97-116.</small></p>\n<p><small>Kenrick, Keefe, Gabrielidis, &amp; Cornelius (1996). Adolescents' age preferences for dating partners: Support for an evolutionary model of life-history strategies. <em>Child Development, 67</em>: 1499-1511.</small></p>\n<p><small>Kenrick &amp; Keefe (1992). Age preferences in mates reflect sex differences in reproductive strategies. <em>Behaivoral and Brain Sciences, 15</em>: 75-133.</small></p>\n<p><small>Khallad (2005). Mate selection in Jordan: Effects of sex, socio-economic status, and culture. <em>Journal of Social and Personal Relationships, 22</em>: 155-168.</small></p>\n<p><small>Kirkpatrick &amp; Davis (1994). Attachment style, gender, and relationship stability: A longitudinal analysis. <em>Journal of Personality and Social Psychology, 66</em>: 502-512.</small></p>\n<p><small>Kurdek (2005). What do we know about gay and lesbian couples? <em>Current Directions in Psychological Science, 14</em>: 251-254.&nbsp;</small></p>\n<p><span style=\"font-size: 11px;\">Kurzban &amp; Weeden (2005).&nbsp;<a href=\"http://www.unc.edu/courses/2006spring/spcl/091p/016/hurrydate.pdf\">HurryDate: Mate preferences in action</a>.&nbsp;<em>Evolution and Human Behavior, 26</em>: 227-244.</span></p>\n<p><small>Langlois &amp; Roggman (1990). <a href=\"http://homepage.psy.utexas.edu/homePage/Group/LangloisLAB/PDFs/Langlois.PS.1990.pdf\">Attractive faces are only average</a>. <em>Psychological Science, 1</em>: 115-121.</small></p>\n<p><small>Langlois, Roggman, &amp; Reiser-Danner (1990). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FLanglois-Infants-differential-social-responses-to-attractive-and-unattractive-faces.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Infants'%20differential%20social%20responses%20to%20attractive%20and%20unattractive%20faces\">Infants' differential social responses to attractive and unattractive faces</a>. <em>Developmental Psychology, 26</em>: 153-159.</small></p>\n<p><small>Langlois, Roggman, Casey, Ritter, Riser-Danner, &amp; Jenkins (1987). Infant preferences for attractive faces: Rudiments of a stereotype? <em>Developmental Psychology, 23</em>: 363-369.</small></p>\n<p><small>Langlois, Kalakanis, Rubenstein, Larson, Hallam, &amp; Smoot (2000). <a href=\"http://homepage.psy.utexas.edu/Homepage/Group/LangloisLAB/meta.PDF\">Maxims or myths of beauty? A meta-analysis and theoretical review</a>. <em>Psychological Bulletin, 126</em>: 390-423.</small></p>\n<p><small>Le (2009). Familiarity principle of attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 1</em>&nbsp;(pp. 596-597). Sage Reference.</small></p>\n<p><small>Ledbetter, Griffin, &amp; Sparks (2007). <a href=\"http://web.ics.purdue.edu/~sparks/Friends%20Forever.pdf\">Forecasting 'friends forever': A longitudinal investigation of sustained closeness between friends</a>. <em>Personal Relationships, 14</em>: 343-350.</small></p>\n<p><span style=\"font-size: 11px; \">Lee, Loewenstein, Ariely, Hong, &amp; Young (2008). If I'm not hot, are you hot or not? Physical-attractiveness evaluations and dating preferences as a function of one's own attractiveness. <em>Psychological Science, 19</em>: 669-577.</span></p>\n<p><small>Lynn &amp; Shurgot (1984). Responses to lonely hearts advertisements: Effects of reported physical attractiveness, physique, and coloration. <em>Personal and Social Psychology Bulletin, 10</em>: 349-357.</small></p>\n<p><small>Marlowe (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FLanglois-Infants-differential-social-responses-to-attractive-and-unattractive-faces.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Mate%20preferences%20among%20Hadza%20hunter-gatherers\">Mate preferences among Hadza hunter-gatherers</a>. <em>Human Nature, 4</em>: 365-376.</small></p>\n<p><small>Martins, Tiggermann, &amp; Kirkbride (2007). Those speedos become them: The role of self-objectification in gay and heterosexual men's body image. <em>Personality and Social Psychology Bulletin, 33</em>: 634-647.</small></p>\n<p><small>Miller &amp; Perlman (2008). <em><a href=\"http://www.amazon.com/Intimate-Relationships-Rowland-Miller/dp/0073370185/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Intimate Relationships, 5th edition</a></em>. McGraw-Hill.</small></p>\n<p><small>Montoya, Horton, &amp; Kirchner (2008).&nbsp;Is actual similarity necessary for&nbsp;attraction? A meta-analysis of&nbsp;actual and perceived similarity.&nbsp;Journal of Social and Personal Relationships, 25: 889-922.</small></p>\n<p><small>Moreland &amp; Beach (1992). Exposure effects in the classroom: The development of affinity among students. <em>Journal of Experimental Social Psychology, 28</em>: 255-276.</small></p>\n<p><small>Moreland &amp; Zajonc (1982). <a href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/23882/1/0000121.pdf\">Exposure effects in person perception: Familiarity, similarity, and attraction</a>. <em>Journal of Experimental Social Psychology, 18</em>: 395-415.</small></p>\n<p><small>Morry (2007). The attraction-similarity hypothesis among cross-sex friends: Relationship satisfactions, perceived similarities, and self-serving perception. <em>Journal of Social and Personal Relationships, 24</em>: 117-138.</small></p>\n<p><small>Morry (2009). Similarity principle in attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1500-1504.</small></p>\n<p><small>Myers (2010). <em><a href=\"http://www.amazon.com/Psychology-David-G-Myers/dp/1429215976/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology, 9th edition</a></em>. Worth Publishers.</small></p>\n<p><small>Neff (2009). Physical attractiveness, role in relationships. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1242-1245). Sage Reference.</small></p>\n<p><small>Nuttin (1987). Affective consequences of mere ownership: The name letter effect in twelve European languages. <em>European Journal of Social Psychology, 17</em>: 381-402.</small></p>\n<p><small>Paul, Wenzel, &amp; Harvey (2000). 'Hookups': Characteristics and correlates of college students' spontaneous and anonymous sexual experiences. <em>Journal of Sex Research, 37</em>: 76-88.</small></p>\n<p><small>Peplau &amp; Fingerhut (2007). <a href=\"http://www.casamariposa.org/Prop8/Attachments/PX1245.pdf\">The close relationships of lesbians and gay men</a>. <em>Annual Review of Psychology, 58</em>: 405-424.</small></p>\n<p><small>Peplau &amp; Spalding (2000). The close relationships of lesbians, gay men, and bisexuals. In Hendrick &amp; Hendrick (eds.), <em>Close relationships: A Sourcebook</em>. Sage.</small></p>\n<p><small>Peretti &amp; Abplanalp (2004). Chemistry in the college dating process: Structure and function. <em>Social Behavior and Personality, 32</em>: 147-154.</small></p>\n<p><small>Perlman &amp; Duck (2006).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Perlman-Duck-The-seven-seas-of-the-study-of-personal-relationships.pdf\">The seven seas of the study of personal&nbsp;relationships: From &ldquo;the thousand&nbsp;islands&rdquo; to interconnected waterways</a>. In&nbsp;Vangelisti &amp; Perlman (eds.), <em>The Cambridge Handbook of Personal Relationships</em>&nbsp;(pp. 11-34). Cambridge University Press.</small></p>\n<p><small>Pettay, Helle, Jokela, &amp; Lummaa (2007). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FPettay-Natural-selection-on-female-life-history-traits-in-relation-to-socio-economic-class-in-pre-industrial-human-populations.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Natural%20selection%20on%20female%20life-history%20traits%20in%20relation%20to%20socio-economic%20class%20in%20pre-industrial%20human%20populations\">Natural selection on female life-history traits in relation to socio-economic class in pre-industrial human populations</a>. <em>Plos ONE, July</em>: 1-9.</small></p>\n<p><small>Powell &amp; Fine (2009). Dissolution of relationships, causes. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships: Vol. 1</em>&nbsp;(pp. 436-440). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Reis, Nezlek, &amp; Wheeler (1980). Physical attractiveness in social interaction. <em>Journal of Personality and Social Psychology, 38</em>: 604-617.</span></p>\n<p><small>Rhodes, Sumich, &amp; Byatt (1999). <a href=\"http://academic.udayton.edu/psy49305/Articles/Rhodes%20(1999).pdf\">Are average facial configurations attractive only because of their symmetry?</a> <em>Psychological Science, 10</em>: 52-58.</small></p>\n<p><small>Rhodes (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FRhodes-The-evolutionary-psychology-of-facial-beauty.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20evolutionary%20psychology%20of%20facial%20beauty\">The evolutionary psychology of facial beauty</a>. <em>Annual Review of Psychology, 57</em>: 199-226.</small></p>\n<p><small>Rosenbaum (1986). <a href=\"http://www.debralieberman.com/downloads/courses/625/rosenbaum_1986a.pdf\">The repulsion hypothesis: On the nondevelopment of relationships</a>. <em>Journal of Personality and Social Psychology, 51</em>: 1156-1166.</small></p>\n<p><small>Schaefer, Fink, Grammar, Mitteroecker, Gunz, &amp; Bookstein (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSchafer-et-al-Female-appearance-Facial-and-bodily-attractiveness-as-shape.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Female%20appearance%3A%20Facial%20and%20bodily%20attractiveness%20as%20shape\">Female appearance: Facial and bodily attractiveness as shape</a>. Psychology Science, 48: 187-205.</small></p>\n<p><span style=\"font-size: 11px; \">Simpson &amp; Harris (1994). Interpersonal attraction. In Weber &amp; Harvey (eds.), <em>Perspective on close relationships</em>&nbsp;(pp. 45-66). Allyn &amp; Bacon.</span></p>\n<p><small>Singh (1993). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSingh-Adaptive-significance-of-waist-to-hip-ratio-and-female-physical-attractiveness.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Adaptive%20significance%20of%20waist-to-hip%20ratio%20and%20female%20physical%20attractiveness\">Adaptive significance of waist-to-hip ratio and female physical attractiveness</a>. Journal of Personality and Social Psychology, 65: 293-307.</small></p>\n<p><small>Singh (1995). Female health, attractiveness, and desirability for relationships: Role of breast asymmetry and waist-to-hip ratio. <em>Ethology and Sociobiology, 16</em>: 465-481.</small></p>\n<p><small>Singh (2000). Waist-to-hip ratio: An indicator of female mate value. <em>International Research Center for Japanese Studies, International Symposium 16</em>: 79-99.</small></p>\n<p><small>Singh &amp; Bronstad (1997). Sex differences in the anatomical locations of human body scarification and tattooing as a function of pathogen prevalence. <em>Evolution and Human Behavior, 18</em>: 403-416.</small></p>\n<p><small>Singh &amp; Young (1995). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSingh-Body-weight-waist-to-hip-ratio-breasts-and-hips-Role-in-judgments-of-female-attractiveness-and-desirability-for-relationships.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Body%20weight%2C%20waist-to-hip%20ratio%2C%20breasts%2C%20and%20hips%3A%20Role%20in%20judgments%20of%20female%20attractiveness%20and%20desirability%20for%20relations...\">Body weight, waist-to-hip ratio, breasts, and hips: Role in judgments of female attractiveness and desirability for relationships</a>. <em>Ethology and Sociobiology, 16</em>: 483-507.</small></p>\n<p><small>Singh &amp; Randall (2007). Beauty is in the eye of the plastic surgeon: Waist-to-hip ratio (WHR) and women's attractiveness. <em>Personality and Individual Differences, 43</em>: 329-340.&nbsp;</small></p>\n<p><small>Slater, Von der Schulenburg, Brown, Badenoch, Butterworth, Parsons, &amp; Samuels (1998). Newborn infants prefer attractive faces. <em>Infant Behavior and Development, 21</em>: 345-354.</small></p>\n<p><small>Smith &amp; Caprariello (2009). Liking. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 2</em>&nbsp;(pp. 978-982). Sage Reference.</small></p>\n<p><small>Smith, McIntosh, &amp; Bazzini (1999). <a href=\"http://jrscience.wcp.muohio.edu/humans_web_04/beauty/film.pdf\">Are the beautiful good in Hollywood? An investigation of the beauty-and-goodness stereotype on film</a>. <em>Basic and Applied Social Psychology, 21</em>: 69-80.</small></p>\n<p><small>Solomon (1987). Standard issue. <em>Psychology Today, November</em>: 30-31.</small></p>\n<p><small>Sorokowski &amp; Pawlowski (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSorokowski-Adaptive-preferences-for-leg-length-in-a-potential-partner.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Adaptive%20preferences%20for%20leg%20length%20in%20a%20potential%20partner\">Adaptive preferences for leg length in a potential partner</a>. <em>Evolution and Human Behavior, 29</em>: 86-91.</small></p>\n<p><small>Sprecher (1989). The importance to males and females of physical attractiveness, earning potential, and expressiveness in initial attraction. <em>Sex Roles, 21</em>: 591-607.</small></p>\n<p><small>Sprecher (1994). Two studies on the breakup of dating and relationships. <em>Personal Relationships, 1</em>: 199-222.</small></p>\n<p><span style=\"font-size: 11px; \">Sprecher, Wenzel, &amp; Harvey, eds. (2008). <em><a href=\"http://www.amazon.com/Handbook-Relationship-Initiation-Susan-Sprecher/dp/0805861602/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Handbook of Relationship Initiation</a></em>. Psychology Press.</span></p>\n<p><small>Steinberg (1993). Astonishing love stories (from an earlier United Press International report). <em>Games, February</em>: 47.</small></p>\n<p><small>Sugiyama (2005). Physical attractiveness in adaptationist perspective. In Buss (ed.),<em> The handbook of evolutionary psychology</em> (pp. 292-342). Wiley.</small></p>\n<p><small>Surra, Gray, Boettcher, Cottle, &amp; West (2006).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Surra-et-al-From-Courtship-to-Universal-Properties-Research-on-Dating-and-Mate-Selection.pdf\">From Courtship to Universal Properties: Research on Dating and Mate Selection, 1950 to 2003</a>. In Vangelisti &amp; Perlman (eds.), <em>Cambridge Handbook of Personal Relationships</em>. Cambridge University Press.</small></p>\n<p><small>Swami, Einon, &amp; Furnham (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSwami-The-leg-to-body-ratio-as-a-human-aesthetic-criterion.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20leg-to-body%20ratio%20as%20a%20human%20aesthetic%20criterion\">The leg-to-body ratio as a human aesthetic criterion</a>. <em>Body Image, 3</em>: 317-323.</small></p>\n<p><span style=\"font-size: 11px;\">Swami &amp; Furnham (2008).</span></p>\n<p><small>Swap (1977).&nbsp;Interpersonal Attraction and Repeated Exposure to Rewarders and Punishers. Personality and Social Psychology Bulletin, 3: 248&ndash;251.</small></p>\n<p><small>Symons (1995). Beauty is in the adaptations of the beholder: The evolutionary psychology of human female sexual attractiveness. In Abramson &amp; Pinkerton (eds.), <em>Sexual nature, sexual culture</em> (pp. 80-118). University of Chicago Press.</small></p>\n<p><small>Taormino (2008).&nbsp;<a style=\"font-style: italic; \" href=\"http://www.amazon.com/Opening-Up-Creating-Sustaining-Relationships/dp/157344295X/\">Opening Up: A Guide to Creating and Sustaining Open Relationships</a>. Cleis Press.</small></p>\n<p><small>Thakerar &amp; Iwawaki (1979). Cross-cultural comparisons in interpersonal attraction of females toward males. <em>Journal of Social Psychology, 108</em>: 121-122.</small></p>\n<p><small>Thornhill &amp; Gangestad (1994). Human fluctuating asymmetry and sexual behavior. <em>Psychological Science, 5</em>: 292-302.</small></p>\n<p><span style=\"font-size: 11px; \">Thornhill &amp; Gangestad (1999). The scent of symmetry: A human sex pheromone that signals fitness? <em>Evolution and Human Behavior, 20</em>: 175-201.</span></p>\n<p><span style=\"font-size: 11px; \">Thornhill &amp; Moller (1997). The relative importance of size and asymmetry in sexual selection. <em>Behavioral Ecology, 9</em>: 546-551.</span></p>\n<p><small>Vangelisti &amp; Perlman (2006). <em><a href=\"http://www.amazon.com/Cambridge-Handbook-Relationships-Handbooks-Psychology/dp/0521826179/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Cambridge Handbook of Personal Relationships</a></em>. Cambridge University Press.</small></p>\n<p><small>Walster, Aronson, Abrahams, &amp; Rottman (1966). <a href=\"http://www2.hawaii.edu/~elaineh/13.pdf\">Importance of physical attractiveness in dating behavior</a>. <em>Journal of Personality and Social Psychology, 4</em>: 508-516.</small></p>\n<p><small>Weiten, Dunn, &amp; Hammer (2011). <em><a href=\"http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life: Adjustment in the 21st Century, 10th edition</a></em>. Wadsworth Publishing.</small></p>\n<p><small>White &amp; Knight (1984). Misattribution of arousal and attraction: Effects of salience of explanations for arousal. <em>Journal of Experimental Social Psychology, 20</em>: 55-64.</small></p>\n<p><small>Wiederman (1993). Evolved gender differences in mate preferences: Evidence from personal advertisements. <em>Ethology and Sociobiology, 14</em>: 331-352.</small></p>\n<p><small>Woll (1986). So many to choose from: Decision strategies in videodating. <em>Journal of Social and Personal Relationships, 3</em>: 43-52.</small></p>\n<p><small>Zajonc (1968). Attitudinal effects of mere exposure. <em>Journal of Personality and Social Psychology, 9</em>: 1-27.</small></p>\n<p><small>Zajonc (1998). Emotions. In Gilbert, Fiske, &amp; Lindzey (eds.), <em>Handbook of Social Psychology, 4th edition.</em>&nbsp;McGraw Hill.</small></p>\n<p><small>Zajonc (2001). <a href=\"http://www.debralieberman.com/downloads/courses/625/Zajonc_2001.pdf\">Mere exposure: A gateway to the subliminal</a>. <em>Current Directions in Psychological Science, 10</em>: 224-228.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JYckkCqhZPrdScjBx", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 116, "baseScore": 61, "extendedScore": null, "score": 0.00012, "legacy": true, "legacyId": "7902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 62, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Part of the Sequence: <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a></small><small>. Co-authored with&nbsp;</small><span style=\"font-size: 11px;\"><a href=\"http://spirituality.mindamyers.com/\">Minda Myers</a>&nbsp;and</span><small>&nbsp;</small><span style=\"font-size: 11px;\"><a href=\"/user/HughRistik/\">Hugh Ristik</a></span><small>. Also see: <a href=\"/lw/79x/polyhacking/\">Polyhacking</a>.</small></p>\n<p>When things <a href=\"/lw/70u/rationality_lessons_learned_from_irrational/\">fell apart</a> between me (Luke) and my first girlfriend, I decided <em>that</em>&nbsp;kind of relationship wasn't ideal for me.</p>\n<p>I didn't like the jealous feelings that had arisen within me. I didn't like the desperate, codependent 'madness' that popular love songs <a href=\"http://www.azlyrics.com/lyrics/beyonceknowles/crazyinlove.html\">celebrate</a>. I had moral objections to the idea of owning somebody else's sexuality, and to the idea of somebody else owning <em>mine</em>. Some of my culture's scripts for what a man-woman relationship should look like didn't fit my own goals very well.</p>\n<p>I needed to <em>design</em>&nbsp;romantic relationships that made sense&nbsp;(<a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">decision-theoretically</a>) for me, rather than simply <em>falling</em>&nbsp;into whatever relationship model my culture happened to offer. (The ladies of&nbsp;<a style=\"font-style: italic;\" href=\"http://en.wikipedia.org/wiki/Sex_and_the_City#Series_overview\">Sex and the City</a>&nbsp;weren't&nbsp;too good with decision theory, but they certainly invested time figuring out which relationship styles worked for <em>them</em>.) For a while, this new approach led me into a series of short-lived flings. After that, I chose 4 months of contented celibacy. After that, <a href=\"http://en.wikipedia.org/wiki/Polyamory\">polyamory</a>. After that...</p>\n<p>Anyway, the results have been wonderful. Rationality and decision theory work for relationships, too!</p>\n<p>We humans <a href=\"/lw/1zu/compartmentalization_as_a_passive_phenomenon/\">compartmentalize by default</a>. Brains don't automatically enforce <a href=\"http://en.wikipedia.org/wiki/Belief_propagation\">belief propagation</a>, and aren't configured to do so. <a href=\"/lw/k5/cached_thoughts/\">Cached thoughts</a> and <a href=\"/lw/4e/cached_selves/\">cached selves</a>&nbsp;can remain even after one has applied the lessons of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core sequences</a> to particular parts of one's life. That's why it helps to <em>explicitly</em>&nbsp;examine what happens when you apply rationality to new areas of your life&nbsp;<span style=\"line-height: 14px; font-family: arial, sans-serif;\">\u2014</span>&nbsp;from <a href=\"/lw/2as/diseased_thinking_dissolving_questions_about/\">disease</a> to <a href=\"/lw/bk/the_trouble_with_good/\">goodness</a> to&nbsp;<a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">morality</a>. Today, we apply rationality to <em>relationships</em>.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4 id=\"Relationships_Styles\"><a name=\"styles\"></a>Relationships Styles</h4>\n<p>When <a href=\"http://www.mindamyers.com/\">Minda</a> had her first relationship with a woman, she found that the cultural scripts for heterosexual relationships didn't work for a homosexual relationship style. For example, in heterosexual dating (in the USA) the man is expected to ask for the date, plan the date, and escalate sexual interaction. A woman expects that she will be pursued and not have to approach men, that on a date she should be passive and follow the man's lead, and that she shouldn't initiate sex herself.</p>\n<p>In the queer community, Minda quickly found that if she passively waited for a woman to hit on her, she'd be waiting all night! When she met her first girlfriend, <em>Minda</em> had to ask for the date. Minda writes:</p>\n<blockquote>\n<p>On dates, I didn't know if I should pay for the date or hold the door or what I was supposed to do! Each interaction required thought and negotiation that hadn't been necessary before. And this was really kind of neat. We had the opportunity to create a relationship that worked for us and represented us as unique and individual human beings. And when it came to sexual interactions, I found it easy to ask for and engage in exactly what I wanted. And I have since brought these practices into my relationships with men.&nbsp;</p>\n</blockquote>\n<p>But you don't need to have an 'alternative' relationship in order to decide you want to set aside some cultural scripts and design a relationship style that works for you. You can choose relationship styles that work for you <em>now</em>.</p>\n<p>With regard to which type(s) of romantic partner(s) you want, there are many possibilities.</p>\n<p>No partners:</p>\n<ul>\n<li><em>Asexuality</em>. Asexuals don't experience sexual attraction. They <a href=\"http://articles.cnn.com/2004-10-14/tech/asexual.study_1_sexuality-new-study-new-scientist?_s=PM:TECH\">comprise</a> perhaps 1% of the population,<sup>1</sup> and include <a href=\"http://en.wikipedia.org/wiki/Asexuality#Notable_asexuals\">notables</a> like Paul Erdos, Morrissey, and Janeane Garofalo. There is a network (<a href=\"http://en.wikipedia.org/wiki/Asexual_Visibility_and_Education_Network\">AVEN</a>) for asexuality awareness and acceptance.</li>\n<li><em>Celibacy</em>. Celibates feel sexual attraction, but abstain from sex. Some choose to abstain for medical, financial, psychological, or philosophical reasons. Others choose celibacy so they have more time to achieve other goals, as I (Luke) did for a time. Others are involuntarily celibate; perhaps they can't find or attract suitable mates. This problem can often be solved by <a href=\"/lw/5p6/how_and_why_to_granularize/#social\">learning and practicing social skills</a>.</li>\n</ul>\n<p>One partner:</p>\n<ul>\n<li><em style=\"font-style: italic;\">Monogamy</em>. Having one sexual partner at a time is a standard cultural script, and may be over-used due to the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Status_quo_bias\">status quo bias</a>. Long-term monogamy should not be done on the pretense that attraction and arousal for one's partner won't fade. It will.<sup>2</sup>&nbsp;Still, there may be many people for whom monogamy is optimal.&nbsp;</li>\n</ul>\n<p>Many partners:</p>\n<ul>\n<li><span style=\"font-style: normal;\"><em style=\"font-style: italic;\">Singlehood</em>. Singlehood can be a good way to get to know yourself and experience a variety of short-term partners. About 78% of college students have had at least one 'one-night stand', and most such encounters were preceded by alcohol or drug use.<sup>3</sup> Indeed, many young people today no longer go on 'dates' to get to know a potential partner. Instead, they meet each other at a social event, 'hook up', and </span><span style=\"font-style: normal;\"><em>then</em>&nbsp;go on dates (if the hookup went well).<sup>4</sup></span></li>\n<li><em>Friendship 'with benefits'</em>. Friends are often people you already enjoy and respect, and thus may also make excellent sexual partners. According to one study, 60% of undergraduates have been a 'friend with benefits' for someone at one time.<sup>5</sup></li>\n<li><em>Polyamory.</em><sup>6</sup>&nbsp;In a polyamorous relationship, partners are clear about their freedom to pursue multiple partners. Couples communicate their boundaries and make agreements about what is and isn't allowed. Polyamory often requires partners to <a href=\"http://books.google.com/books?id=SNCy0iqZMskC&amp;lpg=PT87&amp;vq=unlearning%20jealousy&amp;pg=PT87#v=onepage&amp;q&amp;f=false\">de-program jealousy</a>. In my experience, polyamory is <em>much</em> more common in the rationality community than in the general population.</li>\n</ul>\n<p><a href=\"/user/HughRistik/\">Hugh</a> points out that your limbic system may not agree (at least initially) with your cognitive choice of a relationship style. Some women say they want a long-term relationship but date 'bad boys' who are unlikely to become long-term mates. Someone may think they want polyamorous relationships but find it impossible to leave jealousy behind.<sup>7</sup></p>\n<p>&nbsp;</p>\n<h4 id=\"The_Science_of_Attraction\"><a name=\"science\"></a>The Science of Attraction</h4>\n<p>A key skillset required for having the relationships you want is that of&nbsp;<em>building and maintaining attraction in potential mates</em>.</p>\n<p>Guys seeking girls may wonder: Why do girls say they want \"nice guys\" but date only \"jerks\"? Girls seeking rationalist guys are at an advantage because the gender ratio lies in their favor, but they still might wonder: What can I do to attract the <em>best</em>&nbsp;mates?&nbsp;Those seeking same-sex partners may wonder how attraction can differ from heterosexual norms.</p>\n<p>How do you build and maintain attraction in others? A lot can be learned by <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying different things</a> and seeing what works. This is often better than polling people, because people's verbal reports about what attracts them don't always match their actual behavior.<sup>8</sup></p>\n<p>To get you started, the <a href=\"http://yudkowsky.net/rational/virtues\">virtues</a> of <a href=\"/lw/3m3/the_neglected_virtue_of_scholarship/\">scholarship</a> and <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">empiricism</a> will serve you well.&nbsp;<a href=\"http://en.wikipedia.org/wiki/Social_psychology\">Social psychology</a> has a wealth of knowledge to offer on successful relationships.<sup>9</sup> For example, here are some things that, according to the latest research, will tend to make people more attracted to you:</p>\n<ul>\n<li><em>Proximity</em>&nbsp;and <em>familiarity</em>. Study after study shows that we tend to like those who live near us, partly due to availability,<sup>10</sup> and partly because repeated exposure to almost <em>anything</em>&nbsp;increases liking.<sup>11</sup>&nbsp;A Taiwanese man once demonstrated the power of proximity and repeated exposure when he wrote over 700 letters to his girlfriend, urging her to marry him. She married the mail carrier.<sup>12</sup></li>\n<li><em>Similarity</em>. We tend to like people who are similar to us.<sup>13</sup> We like people with faces similar to our own.<sup>14</sup> We are even more likely to marry someone with a similar-sounding name.<sup>15</sup> Similarity makes attraction endure longer.<sup>16&nbsp;</sup>Also, similar people are more likely to react to events the same way, thus reducing the odds of conflict.<sup>17</sup></li>\n<li><em>Physical attractiveness</em>. Both men and women prefer good-looking mates.<sup>18</sup>&nbsp;Partly, this is because the <a href=\"/lw/lj/the_halo_effect/\">halo effect</a>:&nbsp;we automatically assume that more attractive people are also healthier, happier, more sensitive, more successful, and more socially skilled (but not necessarily more honest or compassionate).<sup>19</sup>&nbsp;Some of these assumptions are correct: Attractive and well-dressed people <em>are</em>&nbsp;more likely to impress employers and succeed occupationally.<sup>20</sup>&nbsp;But isn't beauty relative? Some standards of beauty vary from culture to culture, but many are universal.<sup>21</sup> Men generally prefer women who exhibit signs of youth and fertility.<sup>22</sup>&nbsp;Women generally prefer men who (1) display possession of abundant resources,<sup>23</sup>&nbsp;(2) display high social status,<sup>24</sup> (3) exhibit a 'manly' face (large jaw, thick eyebrows, visible beard stubble)<sup>25</sup>&nbsp;and physique,<sup>26</sup>&nbsp;and (4) are tall.<sup>27</sup>&nbsp;Both genders generally prefer (1) <a href=\"http://en.wikipedia.org/wiki/Agreeableness\">agreeableness</a>, <a href=\"http://en.wikipedia.org/wiki/Conscientiousness\">conscientiousness</a>, and <a href=\"http://en.wikipedia.org/wiki/Extraversion_and_introversion\">extraversion</a>,<sup>28</sup> (2) 'average' and symmetrical faces with features that are neither unusually small or large,<sup>29</sup>&nbsp;(2) large smiles,<sup>30</sup> (3) pupil dilation,<sup>31</sup> and some other things (more on this later).</li>\n<li><em>Liking others</em>. Liking someone makes them more attracted to you.<sup>32</sup></li>\n<li><em>Arousing others</em>. Whether aroused by fright, exercise, stand-up comedy, or erotica, we are more likely to be attracted to an attractive person when we are generally aroused than when we are not generally aroused.<sup>33</sup> As David Myers writes, \"Adrenaline makes the heart grow fonder.\"<sup>34</sup> This may explain why rollercoasters and horror movies are such a popular date night choice.</li>\n</ul>\n<p>But this barely scratches the surface of attraction science. In a later post, we'll examine how attraction works in more detail, and draw up a science-supported game plan for building attraction in others.</p>\n<p>&nbsp;</p>\n<h4 id=\"Attractiveness__Mean_and_Variance\"><a name=\"mean-variance\"></a>Attractiveness: Mean and Variance</h4>\n<p>Remember that increasing your <em>average</em>&nbsp;attractiveness (by appealing to more&nbsp;people) may not be an optimal strategy.</p>\n<p>Marketers know that it's often better to sacrifice broad appeal in order for a product to have very <em>strong</em>&nbsp;appeal to a <a href=\"http://en.wikipedia.org/wiki/Niche_market\">niche market</a>. <a href=\"http://web.archive.org/web/20110202095449/https://theappunto.com/\">The Appunto</a> doesn't appeal to most men, but it appeals strongly enough to <em>some</em>&nbsp;men that they are willing to pay the outrageous $200 price for it.</p>\n<p>Similarly, you may have the best success in dating if you appeal <em>very strongly</em>&nbsp;to <em>some</em>&nbsp;people, even if this makes you less appealing to <em>most</em>&nbsp;people&nbsp;<span style=\"line-height: 14px; font-family: arial, sans-serif; \">\u2014</span>&nbsp;that is, if you adopt a niche marketing strategy in the dating world.<sup>35</sup></p>\n<p>As long as you can <em>find</em> those few people who find you <em>very</em>&nbsp;attractive, it won't matter (for dating) that <em>most</em>&nbsp;people aren't attracted to you. And because one can switch between niche appeal and broad appeal using fashion and behavior, you can simply use clothing and behavior with mainstream appeal during the day (to have general appeal in professional environments) and use alternative clothing and behavior when you're socializing (to have <em>strong</em>&nbsp;appeal to a small subset of people whom you've sought out).</p>\n<p>To visualize this point, consider two attraction strategies. Both strategies employ phenomena that are (almost) universally attractive, but the blue strategy aims to maximize the frequency of <em>somewhat</em> positive responses while the red strategy aims to maximize the frequency of <em>highly</em>&nbsp;positive responses. The red strategy (e.g. using&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/fashionable-mainstream-man.png\">mainstream fashion</a>) increases one's <em>mean attractiveness</em>, while the blue strategy (e.g. using&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/fashionable-goth-guy.png\">alternative fashion</a>) increases one's <em>attractiveness variance</em>. <a href=\"/user/HughRistik/\">Hugh Ristik</a> offers the following chart:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/attractiveness-mean-and-variance.png\" alt=\"\"></p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/07/death-guild-guy.jpg\">This goth guy</a>&nbsp;and I (Luke) can illustrate this phenomenon. I aim for mainstream appeal; he wears <a href=\"http://en.wikipedia.org/wiki/Gothic_fashion\">goth clothing</a> when socializing. My mainstream look turns off almost no one, and is attractive to most women, but doesn't get that many <em>strong</em>&nbsp;reactions right away unless I employ&nbsp;<em>other</em> high-variance strategies.<sup>36</sup> In contrast, I would bet the goth guy's alternative look turns off many people and is less attractive to most women than <em>my</em> look is, but has a higher frequency of <em>extremely</em>&nbsp;positive reactions in women.</p>\n<p>In one's professional life, it may be better to have broad appeal. But in dating, the <em>goal</em>&nbsp;is to find people who find you extremely attractive. The goth guy sacrifices his mean attractiveness to increase his attractiveness variance (and thus the frequency of <em>very</em>&nbsp;positive responses), and this works well for him in the dating scene.</p>\n<p>High-variance strategies like this are a good way to filter for people who are strongly attracted to you, and thus avoid wasting your time with potential mates who only feel lukewarm toward&nbsp;you.</p>\n<p>&nbsp;</p>\n<h4 id=\"Up_next\">Up next</h4>\n<p>In future posts we'll develop an action plan for using the science of attraction to create successful romantic relationships. We'll also explain how rationality helps with relationship maintenance<sup>37</sup> and relationship satisfaction.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Previous post: <a href=\"/lw/cu2/the_power_of_reinforcement/\">The Power of Reinforcement</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Notes\">Notes</h4>\n<p><span style=\"font-size: 11px;\"><sup>1</sup>&nbsp;Bogaert (2004).</span></p>\n<p><span style=\"font-size: 11px; \"><sup>2</sup>&nbsp;About half of romantic relationships of all types end within a few years (Sprecher 1994; Kirkpatrick &amp; Davis 1994; Hill et al 1976), and even relationships that last exhibit diminishing attraction and arousal (Aron et al. 2006; Kurdek 2005; Miller et al. 2007). Note that even if attraction and arousal fades, romantic love can exist in long-term closed monogamy and it is associated with relationship satisfaction (Acevedo &amp; Aron, 2009).</span></p>\n<p style=\"font-size: small;\"><small><sup>3</sup>&nbsp;Paul et al. (2000); Grello et al. (2006).</small></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px; \"><sup>4</sup>&nbsp;Bogle (2008).</span></p>\n<p style=\"font-size: small;\"><small><sup>5</sup>&nbsp;Bisson &amp; Levine (2009).</small></p>\n<p><span style=\"font-size: 11px; \"><sup>6</sup>&nbsp;Two introductory books on the theory and practice of polyamory are: Easton &amp; Hardy (2009) and&nbsp;Taormino (2008).</span></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>7</sup>&nbsp;See work on 'conditional mating strategies' aka 'strategic pluralism' (Gangestad &amp; Simpson, 2000).</span></p>\n<p style=\"font-size: small;\"><small><sup>8</sup>&nbsp;Sprecher &amp; Felmlee (2008); Eastwick &amp; Finkel (2008). Likewise, there is a difference between what people publicly report as being the cause of a breakup, what they actually think caused a breakup, and what actually caused a breakup (Powell &amp; Fine, 2009). Also see <a href=\"/lw/5sk/inferring_our_desires/\">Inferring Our Desires</a>.</small></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>9</sup>&nbsp;For overviews of this research, see: Bradbury &amp; Karney (2010); Miller &amp; Perlman (2008); Vangelisti &amp; Perlman (2006); Sprecher et al. (2008); Weiten et al. (2011), chs. 8-12. For a history of personal relationships research, see Perlman &amp; Duck (2006).</span></p>\n<p style=\"font-size: small;\"><span style=\"font-size: 11px;\"><sup>10</sup>&nbsp;Goodfriend (2009).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>11</sup></span><span style=\"font-size: 11px;\">&nbsp;This is called the </span><span style=\"font-size: 11px;\"><a href=\"http://en.wikipedia.org/wiki/Mere_exposure_effect\">mere exposure effect</a></span><span style=\"font-size: 11px;\">. See Le (2009); Moreland &amp; Zajonc (1982); Nuttin (1987); Zajonc (1968, 2001); Moreland &amp; Beach (1992). The limits of this effect are explored in Bornstein (1989, 1999); Swap (1977).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>12</sup>&nbsp;Steinberg (1993).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>13</sup>&nbsp;Zajonc (1998); Devine (1995); Rosenbaum (1986); Surra et al. (2006); Morry (2007, 2009); Peplau &amp; Fingerhut (2007); Ledbetter et al. (2007); Montoya et al. (2008); Simpson &amp; Harris (1994).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>14</sup>&nbsp;DeBruine (2002, 2004); Bailenson et al. (2005).</span></p>\n<p><small><sup>15</sup> Jones et al. (2004).</small></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>16</sup>&nbsp;Byrne (1971); Ireland et al. (2011).</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>17</sup>&nbsp;Gonzaga (2009). For an overview of the research on self-disclosure, see Greene et al. (2006).</span></span></p>\n<p><small><sup>18</sup>&nbsp;Langlois et al. (2000); Walster et al. (1966); Feingold (1990); Woll (1986); Belot &amp; Francesconi (2006); Finkel &amp; Eastwick (2008); Neff (2009); Peretti &amp; Abplanalp (2004); Buss et al. (2001); Fehr (2009); Lee et al. (2008); Reis et al. (1980). This is also true for homosexuals: Peplau &amp; Spalding (2000).&nbsp;Even infants prefer attractive faces:&nbsp;</small><span style=\"font-size: 11px;\">Langlois et al. (1987);&nbsp;Langlois et al. (1990);&nbsp;Slater et al. (1998). Note that women report&nbsp;that the physical attractiveness is less important to their mate preferences than it actually is:&nbsp;</span><span style=\"font-size: 11px; \">Sprecher (1989).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>19</sup> Eagly et al. (1991); Feingold (1992a); Hatfield &amp; Sprecher (1986); Smith et al. (1999); Dion et al. (1972).</span></p>\n<p><small><sup>20</sup> Cash &amp; Janda (1984); Langlois et al. (2000); Solomon (1987).</small></p>\n<p><span style=\"font-size: 11px;\"><sup>21</sup>&nbsp;Cunningham et al. (1995); Cross &amp; Cross (1971); Jackson (1992); Jones (1996); Thakerar &amp; Iwawaki (1979).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>22</sup>&nbsp;Men certainly prefer youth (Buss 1989a; Kenrick &amp; Keefe 1992; Kenrick et al. 1996;&nbsp;Ben&nbsp;Hamida et al. 1998). Signs of fertility that men prefer include clear and smooth skin (Sugiyama 2005; Singh &amp; Bronstad 1997; Fink &amp; Neave 2005; Fink et al. 2008; Ford &amp; Beach 1951; Symons 1995), facial femininity (Cunningham 2009; Gangestad &amp; Scheyd 2005; Schaefer et al. 2006; Rhodes 2006), long legs (Fielding et al. 2008; Sorokowski &amp; Pawlowski 2008; Bertamini &amp; Bennett 2009; Swami et al. 2006), and a low waist-to-hip ratio (Singh 1993, 2000; Singh &amp; Young 1995; Jasienska et al. 2004; Singh &amp; Randall 2007; Connolly et al 2000; Furnham et al 1997; Franzoi &amp; Herzog 1987; Grabe &amp; Samson 2010). Even men blind from birth prefer a low waist-to-hip ratio (Karremans et al. 2010).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>23</sup>&nbsp;Buss et al. (1990); Buss &amp; Schmitt (1993); Khallad (2005); Gottschall et al. (2003); Gottschall et al. (2004); Kenrick et al. (1990); Gustavsson &amp; Johnsson (2008); Wiederman (1993); Badahdah &amp; Tiemann (2005); Marlowe (2004); Fisman et al. (2006); Asendorpf et al. (2010); Bokek-Cohen et al. (2007); Pettay et al. (2007); Goode (1996).</span></p>\n<p><span style=\"font-size: 11px; \"><sup>24</sup>&nbsp;Feingold (1990, 1992b).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>25</sup>&nbsp;Cunningham (2009); Cunningham et al. (1990).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>26</sup>&nbsp;Singh (1995); Martins et al. (2007).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>27</sup>&nbsp;Lynn &amp; Shurgot (1984); Ellis (1992); Gregor (1985); Kurzban &amp; Weeden (2005); Swami &amp; Furnham (2008). In contrast, men prefer women who are about 4.5 inches shorter than themselves: Gillis &amp; Avis (1980).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>28</sup>&nbsp;Figueredo et al. (2006).</span></p>\n<p><small><sup>29</sup> Langlois &amp; Roggman (1990); Rhodes et al. (1999); Singh (1995); Thornhill &amp; Gangestad (1994, 1999). We may have evolved to be attracted to symmetrical faces because they predict physical and mental health (</small><span style=\"font-size: 11px; \">Thornhill &amp; Moller, 1997).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>30</sup>&nbsp;Cunningham (2009).</span></p>\n<p><small><sup>31</sup>&nbsp;Cunningham (2009).</small></p>\n<p><span style=\"font-size: 11px; \"><sup>32</sup>&nbsp;This is called <a href=\"http://en.wikipedia.org/wiki/Reciprocal_liking\">reciprocal liking</a>. See Curtis &amp; Miller (1986); Aron et al (2006); Berscheid &amp; Walster (1978); Smith &amp; Caprariello (2009); Backman &amp; Secord (1959).</span></p>\n<p><span style=\"font-size: 11px;\"><sup>33</sup>&nbsp;Carducci et al. (1978); Dermer &amp; Pszczynski (1978); White &amp; Knight (1984); Dutton &amp; Aron (1974).</span></p>\n<p><small><sup>34</sup> Myers (2010), p. 710.</small></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>35</sup>&nbsp;One example of a high-variance strategy for heterosexual men in the dating context is a bold opening line like \"You look familiar. Have we had sex?\" Most women will be turned off by such a line, but those who react positively are (by selection and/or by the confidence of the opening line) usually&nbsp;<em style=\"font-style: italic;\">very&nbsp;</em>attracted.&nbsp;</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>36</sup>&nbsp;In business, this is often said as \"not everyone is your customer\": <a href=\"http://www.drjeffcornwall.com/2010/10/everyone-is-not-your-potential.html\">1</a>, <a href=\"http://www.thesilverbulletforsmallbusiness.com/2011/04/small-business-marketing-tip-why-your-ideal-customer-isnt-everyone/\">2</a>, <a href=\"http://articles.mplans.com/not-everybody-is-your-customer/\">3</a>.</span></span></p>\n<p><span style=\"font-size: xx-small;\"><span style=\"font-size: 11px;\"><sup>37</sup>&nbsp;For discussions of relationship maintenance in general, see: Ballard-Reisch &amp; Wiegel (1999); Dinda &amp; Baxter (1987); Haas &amp; Stafford (1998).</span></span></p>\n<p><span style=\"font-size: 11px;\">&nbsp;</span></p>\n<h4 id=\"References\">References</h4>\n<p><small>Acevedo &amp; Aron (2009). <a href=\"https://1445081729018657037-a-1802744773732722657-s-sites.googlegroups.com/site/simingdong/Home/gpr13159.pdf?attachauth=ANoY7crX7vsV1CnXM9LZV2YzC3SNVKMbTJBaijisoM7FgAXymhq6oGUB5bB-hYxNA1m19PhaMXGYT1Bka_jOuZFSXhpAz78U37s0rUJl8XqKdmwd4NmeCF5fzoRBzRafjznA2BisONOQqyLqoE18cwBUYLcjSw3HiTaMDGG1uXvTxPu1z6WIjpG2jpBLcSODPDmzDYyPzqgcUHHNA6qYLbk4DcuM3udUsA%3D%3D&amp;attredirects=0\">Does a long-term relationship kill romantic love?</a> <em>Review of General Psychology, 13</em>: 59-65.</small></p>\n<p><small>Aron, Fisher, &amp; Strong (2006). Romantic love. In Vangelisti &amp; Perlman (eds.), <em>The Cambridge Handbook of Personal Relationships</em>. Cambridge University Press.</small></p>\n<p><small>Asendorpf, Penke, &amp; Back (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FAsendorpf-From-dating-to-mating-and-relating-Predictors-of-initial-and-long-term-outcomes-of-speed-dating-in-a-community-sample.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=From%20dating%20to%20mating%20and%20relating%3A%20Predictors%20of%20initial%20and%20long-term%20outcomes%20of%20speed%20dating%20in%20a%20community%20sample\">From dating to mating and relating: Predictors of initial and long-term outcomes of speed dating in a community sample</a>. <em>European Journal of Personality</em>.</small></p>\n<p><span style=\"font-size: 11px; \">Backman &amp; Secord (1959). The effect of perceived liking on interpersonal attraction. <em>Human Relations, 12</em>: 379-384.</span></p>\n<p><small>Badahdah &amp; Tiemann (2005). Mate selection criteria among Muslims living in America. <em>Evolution and Human Behavior, 26</em>: 432-440.</small></p>\n<p><small>Bailenson, Iyengar, &amp; Yee (2005). <a href=\"http://vhil.stanford.edu/pubs/2005/identity-capture.html\">Facial identity capture and presidential candidate preference</a>. Paper presented at the Annual Conference of the International Communication Association.</small></p>\n<p><span style=\"font-size: 11px;\">Ballard-Reisch &amp; Wiegel (1999). Communication processes in marital commitment: An integrative approach. In Adams &amp; Jones (eds.), <em>Handbook of interpersonal commitment and relationship stability</em>&nbsp;(pp. 407-424). Plenum.</span></p>\n<p><small>Belot &amp; Francesconi (2006). <a href=\"http://www.essex.ac.uk/economics/discussion-papers/papers-text/dp620.pdf\">Can anyone be 'the one'? Evidence on mate selection from speed dating</a>. Centre for Economic Policy Research.</small></p>\n<p><small>Ben&nbsp;Hamida, Mineka, &amp; Bailey (1998).&nbsp;Sex differences in perceived controllability of&nbsp;mate value: An evolutionary perspective. <em>Journal&nbsp;of Personality and Social Psychology, 75</em>:&nbsp;953\u2013966.</small></p>\n<p><small>Berscheid &amp; Walster (1978). <em><a href=\"http://www.amazon.com/Interpersonal-Attraction-Ellen-Berscheid/dp/007554802X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Interpersonal Attraction</a></em>. Addison-Wesley.</small></p>\n<p><small>Bertamini &amp; Bennett (2009). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBertamini-The-effect-of-leg-length-on-perceived-attractiveness-of-simplified-stimuli.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20effect%20of%20leg%20length%20on%20perceived%20attractiveness%20of%20simplified%20stimuli\">The effect of leg length on perceived attractiveness of simplified stimuli</a>. <em>Journal of Social, Evolutionary, and Cultural Psychology, 3</em>: 233-250.</small></p>\n<p><small>Bogaert (2004). Asexuality: Prevalence and associated factors in a national probability sample. <em>Journal of Sex Research, 41</em>: 279-287.</small></p>\n<p><span style=\"font-size: 11px; \">Bogle (2008). <em><a href=\"http://www.amazon.com/Hooking-Up-Dating-Relationships-Campus/dp/0814799698/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hooking Up: Sex, dating, and relationships on campus</a></em>. New York University Press.</span></p>\n<p><small>Bokek-Cohen, Peres, &amp; Kanazawa (2007). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBokek-Cohen-Rational-choice-and-evolutionary-psychology-as-explanations-for-mate-selectivity.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Rational%20choice%20and%20evolutionary%20psychology%20as%20explanations%20for%20mate%20selectivity\">Rational choice and evolutionary psychology as explanations for mate selectivity</a>. <em>Journal of Social, Evolutionary, and Cultural Psychology, 2</em>: 42-55.</small></p>\n<p><small>Bornstein (1989). Exposure and affect: Overview and meta-analysis of research, 1968-1987. <em>Psychological Bulletin, 106</em>: 265-289.</small></p>\n<p><small>Bornstein (1999). Source amnesia, misattribution, and the power of unconscious perceptions and memories. <em>Psychoanalytic Psychology, 16</em>: 155-178.</small></p>\n<p><span style=\"font-size: 11px; \">Bradbury &amp; Karney (2010). <em><a href=\"http://www.amazon.com/Intimate-Relationships-Thomas-N-Bradbury/dp/0393979571/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Intimate Relationships</a></em>. W.W. Norton &amp; Company.</span></p>\n<p><small>Buss (1989). Sex differences in human mate preferences: Evolutionary hypotheses testing in 37 cultures. <em>Behavioral and Brain Sciences, 12</em>: 1-49.</small></p>\n<p><small>Buss &amp; Schmitt (1993). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FBuss-Sexual-strategies-theory-An-evolutionary-perspective-on-human-mating.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Sexual%20strategies%20theory%3A%20An%20evolutionary%20perspective%20on%20human%20mating\">Sexual strategies theory: An evolutionary perspective on human mating</a>. <em>Psychological Review, 100</em>: 204-232.</small></p>\n<p><small>Buss, Abbott, Angleitner, Asherian, Biaggio, et al. (1990). International preferences in selecting mates: A study of 37 cultures. <em>Journal of Cross-Cultural Psychology, 21</em>: 5-47.</small></p>\n<p><small>Buss, Shackelford, Kirkpatrick, &amp; Larsen (2001). <a href=\"http://www.homepage.psy.utexas.edu/homepage/Group/BussLAB/pdffiles/half%20century%20of%20mate%20prefs-2001-jmf.pdf\">A half century of mate preeferences: The cultural evolution of values</a>. <em>Journal of Marriage and Family, 63</em>: 291-503.</small></p>\n<p><small>Byrne (1971). <em><a href=\"http://www.amazon.com/Attraction-Personality-Psychopathology-Psycho-pathology-Monographs/dp/0121486508/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Attraction Paradigm</a></em>. Academic Press.</small></p>\n<p><small>Carducci, Cosby, &amp; Ward (1978). Sexual arousal and interpersonal evaluations. <em>Journal of Experimental Social Psychology, 14</em>: 449-457.</small></p>\n<p><small>Cash &amp; Janda (1984). The eye of the beholder. <em>Psychology Today, November</em>: 46-52.</small></p>\n<p><small>Connolly, Mealey, &amp; Slaughter (2000). The development of waist-to-hip ratio preferences. <em>Perspectives in Human Biology, 5</em>: 19-29.</small></p>\n<p><small>Cross &amp; Cross (1971). Age, sex, race, and the perception of facial beauty. <em>Developmental Psychology, 5</em>: 433-439.</small></p>\n<p><small>Cunningham, Roberts, Wu, Barbee, &amp; Druen (1995). \"Their ideas of beauty are, on the whole, the same as ours\": Consistency and variability in the cross-cultural perception of female attractiveness. <em>Journal of Personality and Social Psychology, 68</em>: 261-279.</small></p>\n<p><small>Cunningham (2009). Physical Attractiveness, Defining Characteristics. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1237-1242). Sage Reference.</small></p>\n<p><small>Curtis &amp; Miller (1986). Believing another likes or dislikes you: Behaviors making the beliefs come true. <em>Journal of Personality and Social Psychology, 51</em>: 284-290.</small></p>\n<p><small>DeBruine (2002). <a href=\"http://rspb.royalsocietypublishing.org/content/269/1498/1307.full.pdf\">Facial resemblance enhances trust</a>. <em>Proceedings of the Royal Society of London, 269</em>: 1307-1312.</small></p>\n<p><small>DeBruine (2004). <a href=\"http://rspb.royalsocietypublishing.org/content/271/1552/2085.full.pdf\">Facial resemblance increases the attractiveness of same-sex faces more than other-sex faces</a>. <em>Proceedings of the Royal Society of London B, 271</em>: 2085-2090.</small></p>\n<p><small>Dermer &amp; Pszczynski (1978). <a href=\"https://pantherfile.uwm.edu/dermer/public/vita/dermer_erotica.pdf\">Effects of erotica upon men's loving and liking responses for women they love</a>. <em>Journal of Personality and Social Psychology, 36</em>: 1302-1309.</small></p>\n<p><small>Devine (1995). Prejudice and outgroup perception. In Teser (ed.), <em>Advanced Social Psychology.</em>&nbsp;McGraw-Hill.</small></p>\n<p><span style=\"font-size: 11px;\">Dinda &amp; Baxter (1987). Strategies for maintaining and repairing marital relationships. <em>Journal of Social and Personal Relationships, 4</em>: 143-158.</span></p>\n<p><span style=\"font-size: 11px; \">Dion, Berscheid, &amp; Walster (1972). What is beautiful is good. <em>Journal of Personality and Social Psychology, 24</em>: 285-290.</span></p>\n<p><span style=\"font-size: 11px; \">Dutton &amp; Aron (1974). Some evidence for heightened sexual attraction under conditions of high anxiety. <em>Journal of Personality and Social Psychology, 30</em>: 510-517.</span></p>\n<p><small>Eagly, Ashmore, Makhijani, &amp; Kennedy (1991). What is beautiful is good, but...: A meta-analytic review of research on the physical attractiveness stereotype. <em>Psychological Bulletin, 110</em>: 109-128.</small></p>\n<p><small>Easton &amp; Hardy (2009).&nbsp;<em style=\"font-style: italic; \"><a href=\"http://www.amazon.com/Ethical-Slut-Practical-Relationships-Adventures/dp/1587613379/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Ethical Slut: A Practical Guide to Polyamory, Open Relationships &amp; Other Adventures, 2nd edition</a></em>. The Celestial Arts.</small></p>\n<p><span style=\"font-size: 11px;\">Eastwick &amp; Finkel (2008).&nbsp;<a href=\"http://www.eastwick.motives.com/EastwickFinkel2008JPSP.pdf\">Sex differences in mate preferences revisited: Do people know what they initially desire in a romantic partner?</a> Journal of Personality and Social Psychology, 94: 245-264.</span></p>\n<p><small>Eldridge (2009). Conflict patterns. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of human relationships: Vol. 1</em>&nbsp;(pp. 307-310). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Ellis (1992). The evolution of sexual attraction: Evaluative mechanisms in women. In Barkow, Cosmides, &amp; Tooby (eds.), <em>The Adapted Mind: Evolutionary psychology and the generation of culture</em>&nbsp;(pp. 267-288). Oxford University Press.</span></p>\n<p><small>Fehr (2009). Friendship formation and development. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 1</em>&nbsp;(pp. 706-10). Sage Reference.</small></p>\n<p><small>Feingold (1990). Gender differences in effects of physical attractiveness on romantic attraction: A comparison across five research paradigms. <em>Journal of Personality and Social Psychology, 59</em>: 981-993.</small></p>\n<p><small>Feingold (1992a). Good-looking people are not what we think. <em>Psychological Bulletin, 111</em>: 304-341.</small></p>\n<p><span style=\"font-size: 11px; \">Feingold (1992b). Gender differences in mate selection preferences: A test of the parental investment model. <em>Psychological Bulletin, 116</em>: 429-256.</span></p>\n<p><small>Figueredo, Sefcek, &amp; Jones (2006). <a href=\"http://www.u.arizona.edu/~ajf/pdf/Figueredo,%20Sefcek,%20%26%20Jones%202006.pdf\">The ideal romantic partner personality</a>. <em>Personality and Individual Differences, 41</em>: 431-441.</small></p>\n<p><small>Fielding, Scholling, Adab, Cheng, Lao et al. (2008). Are longer legs associated with enhanced fertility in Chinese women? <em>Evolution and Human Behavior, 29</em>: 434-443.</small></p>\n<p><small>Fink &amp; Neave (2005). The biology of facial beauty. <em>Internal Journal of Cosmetic Science, 27</em>: 317-325.</small></p>\n<p><small>Fink, Matts, Klingenberg, Kuntze, Weege, &amp; Grammar (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FFink-Visual-attention-to-variation-in-female-skin-color-distribution.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Visual%20attention%20to%20variation%20in%20female%20skin%20color%20distribution\">Visual attention to variation in female skin color distribution</a>. <em>Journal of Cosmetic Dermatology, 7</em>: 155-161.</small></p>\n<p><small>Finkel &amp; Eastwick (2008). Speed-dating. <em>Current Directions in Psychological Science, 17</em>: 193-197.</small></p>\n<p><small>Fisman, Iyengar, Kamenica, &amp; Simonson (2006). Gender differences in mate selection: Evidence from a speed dating experiment. <em>The Quarterly Journal of Economics, 121</em>: 673-697.</small></p>\n<p><small>Ford &amp; Beach (1951). <em><a href=\"http://www.amazon.com/Patterns-Sexual-Behavior-Clellan-Stearns/dp/B000OEXAMM/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Patterns of Sexual Behavior</a></em>. Harper &amp; Row.</small></p>\n<p><small>Franzoi &amp; Herzog (1987). Judging personal attractiveness: What body aspects do we use? <em>Personality and Social Psychology Bulletin, 13</em>: 19-33.</small></p>\n<p><small>Furnham, Tan, &amp; McManus (1997). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FFurnham-Waist-to-hip-ratio-and-preferences-for-body-shape-A-replication-and-extension.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Waist-to-hip%20ratio%20and%20preferences%20for%20body%20shape%3A%20A%20replication%20and%20extension\">Waist-to-hip ratio and preferences for body shape: A replication and extension</a>. <em>Personality and Individual Differences, 22</em>: 539-549.</small></p>\n<p><span style=\"font-size: 11px; \">Gangestad &amp; Simpson (2000). <a href=\"http://smg.media.mit.edu/classes/Identity2004/EvolutionOfHumanMating.pdf\">The evolution of human mating: Trade-offs and strategic pluralism</a>. Behavioral and Brain Sciences, 23: 573-644.</span></p>\n<p><small>Gangestad &amp; Scheyd (2005). The evolution of human physical attractiveness. <em>Annual Review of Anthropology, 34</em>: 523-548.</small></p>\n<p><span style=\"font-size: 11px;\">Gillis &amp; Avis (1980).</span></p>\n<p><small>Gonzaga (2009). Similarity in ongoing relationships. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1496-1499). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Goode (1996). Gender and courtship entitlement: Responses to personal ads. <em>Sex Roles, 34</em>: 141-169.</span></p>\n<p><small>Goodfriend (2009). Proximity and attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1297-1299). Sage Reference.</small></p>\n<p><small>Gottschall, Berkey, Cawson, Drown, Fleischner, et al. (2003). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGottschall-Patterns-of-characterization-in-folktales-across-geographic-regions-and-levels-of-cultural-complexity.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Patterns%20of%20characterization%20in%20folktales%20across%20geographic%20regions%20and%20levels%20of%20cultural%20complexity%3A%20Literature%20as%20a%20neglec...\">Patterns of characterization in folktales across geographic regions and levels of cultural complexity: Literature as a neglected source of quantitative data</a>. <em>Human Nature, 14</em>: 365-382.</small></p>\n<p><small>Gottschall, Martin, Quish, &amp; Rea (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGottschall-Sex-differences-in-mate-choice-criteria-are-reflected-in-folktales-from-around-the-world-and-in-historical-European-literature.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Sex%20differences%20in%20mate%20choice%20criteria%20are%20reflected%20in%20folktales%20from%20around%20the%20world%20and%20in%20historical%20European%20literature\">Sex differences in mate choice criteria are reflected in folktales from around the world and in historical European literature</a>. <em>Evolution and Human Behavior, 25</em>: 102-112.</small></p>\n<p><small>Grabe &amp; Samson (2010).&nbsp;Sexual Cues Emanating From the Anchorette Chair: Implications for Perceived Professionalism, Fitness for Beat, and Memory for News.&nbsp;<em>Communication Research, December 14</em>.</small></p>\n<p><small>Greene, Derlega, Mathews (2006). Self-disclosure in personal relationships. In Vangelisti &amp; Perlman (eds.),&nbsp;<em style=\"font-style: italic;\">The Cambridge Handbook of Personal Relationships&nbsp;</em>(pp. 409-428). Cambridge University Press.</small></p>\n<p><span style=\"font-size: 11px; \">Gregor (1985). <em><a href=\"http://www.amazon.com/Anxious-Pleasures-Sexual-Amazonian-People/dp/0226307433/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anxious Pleasures: The sexual lives of an Amazonian people</a></em>. University of Chicago Press.</span></p>\n<p><span style=\"font-size: 11px; \">Grello, Welsh, &amp; Harper (2006). No strings attached: The nature of casual sex in college students. <em>Journal of Sex Research, 43</em>: 255-267.</span></p>\n<p><small>Gustavsson &amp; Johnsson (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FGustavvsson-Mixed-support-for-sexual-selection-theories-of-mate-preferences-in-the-Swedish-population.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Mixed%20support%20for%20sexual%20selection%20theories%20of%20mate%20preferences%20in%20the%20Swedish%20population\">Mixed support for sexual selection theories of mate preferences in the Swedish population</a>. <em>Evolutionary Psychology, 6</em>: 454-470.</small></p>\n<p><span style=\"font-size: 11px;\">Haas &amp; Stafford (1998). An initial examination of maintenance behaviors in gay and lesbian relationships. <em>Journal of Social and Personal Relationships, 15</em>: 846-855.</span></p>\n<p><small>Hatfield &amp; Sprecher (1986). <em><a href=\"http://www.amazon.com/Mirror-Importance-Everyday-Sexual-Behavior/dp/0887061249/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Mirror, mirror... The importance of looks in everyday life</a></em>. State University of New York Press.</small></p>\n<p><small>Hill, Rubin, &amp; Peplau (1976). Breakups before marriage: The end of 103 affairs. <em>Journal of Social Issues, 32</em>: 147-168.</small></p>\n<p><span style=\"font-size: 11px;\">Ireland, Slatcher, Eastwick, Scissors, Finkel, &amp; Pennebaker (2011). <a href=\"http://people.tamu.edu/~eastwick/Ireland2011_PSci.pdf\">Language style matching predicts relationship initiation and stability</a>. Psychological Science, 22: 39-44.</span></p>\n<p><small>Jackson (1992). <a href=\"http://www.amazon.com/Physical-Appearance-Gender-Sociobiological-Sociocultural/dp/0791408248/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Physical appearance and gender: Sociobiological and sociocultural perspectives</a>. State University of New York Press.</small></p>\n<p><small>Jasienska, Ziomkiewicz, Ellison, Lipson, &amp; Thune (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FJasienska-Large-breasts-and-narrow-waists-indicate-high-reproductive-potential-in-women.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Large%20breasts%20and%20narrow%20waists%20indicate%20high%20reproductive%20potential%20in%20women\">Large breasts and narrow waists indicate high reproductive potential in women</a>. <em>Proceedings of the Royal Society of London, B, 271</em>: 1213-1217.</small></p>\n<p><small>Jones (1996). <em><a href=\"http://www.amazon.com/Physical-Attractiveness-Theory-Sexual-Selection/dp/0915703408/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Physical attractiveness and the theory of sexual selection</a></em>. University of Michigan Press.</small></p>\n<p><small>Jones, Pelham, Carvallo, &amp; Mirenberg (2004). <a href=\"http://new.dixie.edu/humanities/File/Count%20the%20Js.pdf\">How do I love thee? Let me count the Js: Implicit egotism and interpersonal attraction</a>. <em>Journal of Personality and Social Psychology, 87</em>: 665-683.</small></p>\n<p><small>Karremans, Frankenhuis, &amp; Arons (2010). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FKarremans-Blind-men-prefer-a-low-waist-to-hip-ratio.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Blind%20men%20prefer%20a%20low%20waist-to-hip%20ratio\">Blind men prefer a low waist-to-hip ratio</a>. <em>Evolution and Human Behavior, 31</em>: 182-186.</small></p>\n<p><small>Kenrick, Sadalla, Groth, &amp; Trost (1990). Evolution, traits, and the stages of human courtship: Qualifying the parental investment model. <em>Journal of Personality, 58</em>: 97-116.</small></p>\n<p><small>Kenrick, Keefe, Gabrielidis, &amp; Cornelius (1996). Adolescents' age preferences for dating partners: Support for an evolutionary model of life-history strategies. <em>Child Development, 67</em>: 1499-1511.</small></p>\n<p><small>Kenrick &amp; Keefe (1992). Age preferences in mates reflect sex differences in reproductive strategies. <em>Behaivoral and Brain Sciences, 15</em>: 75-133.</small></p>\n<p><small>Khallad (2005). Mate selection in Jordan: Effects of sex, socio-economic status, and culture. <em>Journal of Social and Personal Relationships, 22</em>: 155-168.</small></p>\n<p><small>Kirkpatrick &amp; Davis (1994). Attachment style, gender, and relationship stability: A longitudinal analysis. <em>Journal of Personality and Social Psychology, 66</em>: 502-512.</small></p>\n<p><small>Kurdek (2005). What do we know about gay and lesbian couples? <em>Current Directions in Psychological Science, 14</em>: 251-254.&nbsp;</small></p>\n<p><span style=\"font-size: 11px;\">Kurzban &amp; Weeden (2005).&nbsp;<a href=\"http://www.unc.edu/courses/2006spring/spcl/091p/016/hurrydate.pdf\">HurryDate: Mate preferences in action</a>.&nbsp;<em>Evolution and Human Behavior, 26</em>: 227-244.</span></p>\n<p><small>Langlois &amp; Roggman (1990). <a href=\"http://homepage.psy.utexas.edu/homePage/Group/LangloisLAB/PDFs/Langlois.PS.1990.pdf\">Attractive faces are only average</a>. <em>Psychological Science, 1</em>: 115-121.</small></p>\n<p><small>Langlois, Roggman, &amp; Reiser-Danner (1990). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FLanglois-Infants-differential-social-responses-to-attractive-and-unattractive-faces.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Infants'%20differential%20social%20responses%20to%20attractive%20and%20unattractive%20faces\">Infants' differential social responses to attractive and unattractive faces</a>. <em>Developmental Psychology, 26</em>: 153-159.</small></p>\n<p><small>Langlois, Roggman, Casey, Ritter, Riser-Danner, &amp; Jenkins (1987). Infant preferences for attractive faces: Rudiments of a stereotype? <em>Developmental Psychology, 23</em>: 363-369.</small></p>\n<p><small>Langlois, Kalakanis, Rubenstein, Larson, Hallam, &amp; Smoot (2000). <a href=\"http://homepage.psy.utexas.edu/Homepage/Group/LangloisLAB/meta.PDF\">Maxims or myths of beauty? A meta-analysis and theoretical review</a>. <em>Psychological Bulletin, 126</em>: 390-423.</small></p>\n<p><small>Le (2009). Familiarity principle of attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 1</em>&nbsp;(pp. 596-597). Sage Reference.</small></p>\n<p><small>Ledbetter, Griffin, &amp; Sparks (2007). <a href=\"http://web.ics.purdue.edu/~sparks/Friends%20Forever.pdf\">Forecasting 'friends forever': A longitudinal investigation of sustained closeness between friends</a>. <em>Personal Relationships, 14</em>: 343-350.</small></p>\n<p><span style=\"font-size: 11px; \">Lee, Loewenstein, Ariely, Hong, &amp; Young (2008). If I'm not hot, are you hot or not? Physical-attractiveness evaluations and dating preferences as a function of one's own attractiveness. <em>Psychological Science, 19</em>: 669-577.</span></p>\n<p><small>Lynn &amp; Shurgot (1984). Responses to lonely hearts advertisements: Effects of reported physical attractiveness, physique, and coloration. <em>Personal and Social Psychology Bulletin, 10</em>: 349-357.</small></p>\n<p><small>Marlowe (2004). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FLanglois-Infants-differential-social-responses-to-attractive-and-unattractive-faces.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Mate%20preferences%20among%20Hadza%20hunter-gatherers\">Mate preferences among Hadza hunter-gatherers</a>. <em>Human Nature, 4</em>: 365-376.</small></p>\n<p><small>Martins, Tiggermann, &amp; Kirkbride (2007). Those speedos become them: The role of self-objectification in gay and heterosexual men's body image. <em>Personality and Social Psychology Bulletin, 33</em>: 634-647.</small></p>\n<p><small>Miller &amp; Perlman (2008). <em><a href=\"http://www.amazon.com/Intimate-Relationships-Rowland-Miller/dp/0073370185/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Intimate Relationships, 5th edition</a></em>. McGraw-Hill.</small></p>\n<p><small>Montoya, Horton, &amp; Kirchner (2008).&nbsp;Is actual similarity necessary for&nbsp;attraction? A meta-analysis of&nbsp;actual and perceived similarity.&nbsp;Journal of Social and Personal Relationships, 25: 889-922.</small></p>\n<p><small>Moreland &amp; Beach (1992). Exposure effects in the classroom: The development of affinity among students. <em>Journal of Experimental Social Psychology, 28</em>: 255-276.</small></p>\n<p><small>Moreland &amp; Zajonc (1982). <a href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/23882/1/0000121.pdf\">Exposure effects in person perception: Familiarity, similarity, and attraction</a>. <em>Journal of Experimental Social Psychology, 18</em>: 395-415.</small></p>\n<p><small>Morry (2007). The attraction-similarity hypothesis among cross-sex friends: Relationship satisfactions, perceived similarities, and self-serving perception. <em>Journal of Social and Personal Relationships, 24</em>: 117-138.</small></p>\n<p><small>Morry (2009). Similarity principle in attraction. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1500-1504.</small></p>\n<p><small>Myers (2010). <em><a href=\"http://www.amazon.com/Psychology-David-G-Myers/dp/1429215976/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology, 9th edition</a></em>. Worth Publishers.</small></p>\n<p><small>Neff (2009). Physical attractiveness, role in relationships. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 3</em>&nbsp;(pp. 1242-1245). Sage Reference.</small></p>\n<p><small>Nuttin (1987). Affective consequences of mere ownership: The name letter effect in twelve European languages. <em>European Journal of Social Psychology, 17</em>: 381-402.</small></p>\n<p><small>Paul, Wenzel, &amp; Harvey (2000). 'Hookups': Characteristics and correlates of college students' spontaneous and anonymous sexual experiences. <em>Journal of Sex Research, 37</em>: 76-88.</small></p>\n<p><small>Peplau &amp; Fingerhut (2007). <a href=\"http://www.casamariposa.org/Prop8/Attachments/PX1245.pdf\">The close relationships of lesbians and gay men</a>. <em>Annual Review of Psychology, 58</em>: 405-424.</small></p>\n<p><small>Peplau &amp; Spalding (2000). The close relationships of lesbians, gay men, and bisexuals. In Hendrick &amp; Hendrick (eds.), <em>Close relationships: A Sourcebook</em>. Sage.</small></p>\n<p><small>Peretti &amp; Abplanalp (2004). Chemistry in the college dating process: Structure and function. <em>Social Behavior and Personality, 32</em>: 147-154.</small></p>\n<p><small>Perlman &amp; Duck (2006).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Perlman-Duck-The-seven-seas-of-the-study-of-personal-relationships.pdf\">The seven seas of the study of personal&nbsp;relationships: From \u201cthe thousand&nbsp;islands\u201d to interconnected waterways</a>. In&nbsp;Vangelisti &amp; Perlman (eds.), <em>The Cambridge Handbook of Personal Relationships</em>&nbsp;(pp. 11-34). Cambridge University Press.</small></p>\n<p><small>Pettay, Helle, Jokela, &amp; Lummaa (2007). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FPettay-Natural-selection-on-female-life-history-traits-in-relation-to-socio-economic-class-in-pre-industrial-human-populations.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Natural%20selection%20on%20female%20life-history%20traits%20in%20relation%20to%20socio-economic%20class%20in%20pre-industrial%20human%20populations\">Natural selection on female life-history traits in relation to socio-economic class in pre-industrial human populations</a>. <em>Plos ONE, July</em>: 1-9.</small></p>\n<p><small>Powell &amp; Fine (2009). Dissolution of relationships, causes. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships: Vol. 1</em>&nbsp;(pp. 436-440). Sage Reference.</small></p>\n<p><span style=\"font-size: 11px; \">Reis, Nezlek, &amp; Wheeler (1980). Physical attractiveness in social interaction. <em>Journal of Personality and Social Psychology, 38</em>: 604-617.</span></p>\n<p><small>Rhodes, Sumich, &amp; Byatt (1999). <a href=\"http://academic.udayton.edu/psy49305/Articles/Rhodes%20(1999).pdf\">Are average facial configurations attractive only because of their symmetry?</a> <em>Psychological Science, 10</em>: 52-58.</small></p>\n<p><small>Rhodes (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FRhodes-The-evolutionary-psychology-of-facial-beauty.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20evolutionary%20psychology%20of%20facial%20beauty\">The evolutionary psychology of facial beauty</a>. <em>Annual Review of Psychology, 57</em>: 199-226.</small></p>\n<p><small>Rosenbaum (1986). <a href=\"http://www.debralieberman.com/downloads/courses/625/rosenbaum_1986a.pdf\">The repulsion hypothesis: On the nondevelopment of relationships</a>. <em>Journal of Personality and Social Psychology, 51</em>: 1156-1166.</small></p>\n<p><small>Schaefer, Fink, Grammar, Mitteroecker, Gunz, &amp; Bookstein (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSchafer-et-al-Female-appearance-Facial-and-bodily-attractiveness-as-shape.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Female%20appearance%3A%20Facial%20and%20bodily%20attractiveness%20as%20shape\">Female appearance: Facial and bodily attractiveness as shape</a>. Psychology Science, 48: 187-205.</small></p>\n<p><span style=\"font-size: 11px; \">Simpson &amp; Harris (1994). Interpersonal attraction. In Weber &amp; Harvey (eds.), <em>Perspective on close relationships</em>&nbsp;(pp. 45-66). Allyn &amp; Bacon.</span></p>\n<p><small>Singh (1993). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSingh-Adaptive-significance-of-waist-to-hip-ratio-and-female-physical-attractiveness.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Adaptive%20significance%20of%20waist-to-hip%20ratio%20and%20female%20physical%20attractiveness\">Adaptive significance of waist-to-hip ratio and female physical attractiveness</a>. Journal of Personality and Social Psychology, 65: 293-307.</small></p>\n<p><small>Singh (1995). Female health, attractiveness, and desirability for relationships: Role of breast asymmetry and waist-to-hip ratio. <em>Ethology and Sociobiology, 16</em>: 465-481.</small></p>\n<p><small>Singh (2000). Waist-to-hip ratio: An indicator of female mate value. <em>International Research Center for Japanese Studies, International Symposium 16</em>: 79-99.</small></p>\n<p><small>Singh &amp; Bronstad (1997). Sex differences in the anatomical locations of human body scarification and tattooing as a function of pathogen prevalence. <em>Evolution and Human Behavior, 18</em>: 403-416.</small></p>\n<p><small>Singh &amp; Young (1995). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSingh-Body-weight-waist-to-hip-ratio-breasts-and-hips-Role-in-judgments-of-female-attractiveness-and-desirability-for-relationships.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Body%20weight%2C%20waist-to-hip%20ratio%2C%20breasts%2C%20and%20hips%3A%20Role%20in%20judgments%20of%20female%20attractiveness%20and%20desirability%20for%20relations...\">Body weight, waist-to-hip ratio, breasts, and hips: Role in judgments of female attractiveness and desirability for relationships</a>. <em>Ethology and Sociobiology, 16</em>: 483-507.</small></p>\n<p><small>Singh &amp; Randall (2007). Beauty is in the eye of the plastic surgeon: Waist-to-hip ratio (WHR) and women's attractiveness. <em>Personality and Individual Differences, 43</em>: 329-340.&nbsp;</small></p>\n<p><small>Slater, Von der Schulenburg, Brown, Badenoch, Butterworth, Parsons, &amp; Samuels (1998). Newborn infants prefer attractive faces. <em>Infant Behavior and Development, 21</em>: 345-354.</small></p>\n<p><small>Smith &amp; Caprariello (2009). Liking. In Reis &amp; Sprecher (eds.), <em>Encyclopedia of Human Relationships, Vol. 2</em>&nbsp;(pp. 978-982). Sage Reference.</small></p>\n<p><small>Smith, McIntosh, &amp; Bazzini (1999). <a href=\"http://jrscience.wcp.muohio.edu/humans_web_04/beauty/film.pdf\">Are the beautiful good in Hollywood? An investigation of the beauty-and-goodness stereotype on film</a>. <em>Basic and Applied Social Psychology, 21</em>: 69-80.</small></p>\n<p><small>Solomon (1987). Standard issue. <em>Psychology Today, November</em>: 30-31.</small></p>\n<p><small>Sorokowski &amp; Pawlowski (2008). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSorokowski-Adaptive-preferences-for-leg-length-in-a-potential-partner.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=Adaptive%20preferences%20for%20leg%20length%20in%20a%20potential%20partner\">Adaptive preferences for leg length in a potential partner</a>. <em>Evolution and Human Behavior, 29</em>: 86-91.</small></p>\n<p><small>Sprecher (1989). The importance to males and females of physical attractiveness, earning potential, and expressiveness in initial attraction. <em>Sex Roles, 21</em>: 591-607.</small></p>\n<p><small>Sprecher (1994). Two studies on the breakup of dating and relationships. <em>Personal Relationships, 1</em>: 199-222.</small></p>\n<p><span style=\"font-size: 11px; \">Sprecher, Wenzel, &amp; Harvey, eds. (2008). <em><a href=\"http://www.amazon.com/Handbook-Relationship-Initiation-Susan-Sprecher/dp/0805861602/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Handbook of Relationship Initiation</a></em>. Psychology Press.</span></p>\n<p><small>Steinberg (1993). Astonishing love stories (from an earlier United Press International report). <em>Games, February</em>: 47.</small></p>\n<p><small>Sugiyama (2005). Physical attractiveness in adaptationist perspective. In Buss (ed.),<em> The handbook of evolutionary psychology</em> (pp. 292-342). Wiley.</small></p>\n<p><small>Surra, Gray, Boettcher, Cottle, &amp; West (2006).&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/06/Surra-et-al-From-Courtship-to-Universal-Properties-Research-on-Dating-and-Mate-Selection.pdf\">From Courtship to Universal Properties: Research on Dating and Mate Selection, 1950 to 2003</a>. In Vangelisti &amp; Perlman (eds.), <em>Cambridge Handbook of Personal Relationships</em>. Cambridge University Press.</small></p>\n<p><small>Swami, Einon, &amp; Furnham (2006). <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Flw%2F5bw%2Fyour_evolved_intuitions%2F&amp;v=1&amp;libid=1309141282434&amp;out=http%3A%2F%2Fcommonsenseatheism.com%2Fwp-content%2Fuploads%2F2011%2F05%2FSwami-The-leg-to-body-ratio-as-a-human-aesthetic-criterion.pdf&amp;ref=http%3A%2F%2Fwww.google.com%2Fcse%3Fcx%3D015839050583929870010%253A-802ptn4igi%26cof%3DFORID%253A11%26ie%3DUTF-8%26q%3Dlukeprog%2B%2522waist%2Bto%2Bhip%2Bratio%2522%26sa%3DSearch%26siteurl%3Dlesswrong.com%252F%26toJSONString%3Dtrue%26ad%3Dw9%26num%3D10%26rurl%3Dhttp%253A%252F%252Flesswrong.com%252Fsearch%252Fresults%253Fcx%253D015839050583929870010%25253A-802ptn4igi%2526cof%253DFORID%25253A11%2526ie%253DUTF-8%2526q%253Dlukeprog%252B%252522waist%252Bto%252Bhip%252Bratio%252522%2526sa%253DSearch%2526siteurl%253Dlesswrong.com%25252F&amp;title=Your%20Evolved%20Intuitions%20-%20Less%20Wrong&amp;txt=The%20leg-to-body%20ratio%20as%20a%20human%20aesthetic%20criterion\">The leg-to-body ratio as a human aesthetic criterion</a>. <em>Body Image, 3</em>: 317-323.</small></p>\n<p><span style=\"font-size: 11px;\">Swami &amp; Furnham (2008).</span></p>\n<p><small>Swap (1977).&nbsp;Interpersonal Attraction and Repeated Exposure to Rewarders and Punishers. Personality and Social Psychology Bulletin, 3: 248\u2013251.</small></p>\n<p><small>Symons (1995). Beauty is in the adaptations of the beholder: The evolutionary psychology of human female sexual attractiveness. In Abramson &amp; Pinkerton (eds.), <em>Sexual nature, sexual culture</em> (pp. 80-118). University of Chicago Press.</small></p>\n<p><small>Taormino (2008).&nbsp;<a style=\"font-style: italic; \" href=\"http://www.amazon.com/Opening-Up-Creating-Sustaining-Relationships/dp/157344295X/\">Opening Up: A Guide to Creating and Sustaining Open Relationships</a>. Cleis Press.</small></p>\n<p><small>Thakerar &amp; Iwawaki (1979). Cross-cultural comparisons in interpersonal attraction of females toward males. <em>Journal of Social Psychology, 108</em>: 121-122.</small></p>\n<p><small>Thornhill &amp; Gangestad (1994). Human fluctuating asymmetry and sexual behavior. <em>Psychological Science, 5</em>: 292-302.</small></p>\n<p><span style=\"font-size: 11px; \">Thornhill &amp; Gangestad (1999). The scent of symmetry: A human sex pheromone that signals fitness? <em>Evolution and Human Behavior, 20</em>: 175-201.</span></p>\n<p><span style=\"font-size: 11px; \">Thornhill &amp; Moller (1997). The relative importance of size and asymmetry in sexual selection. <em>Behavioral Ecology, 9</em>: 546-551.</span></p>\n<p><small>Vangelisti &amp; Perlman (2006). <em><a href=\"http://www.amazon.com/Cambridge-Handbook-Relationships-Handbooks-Psychology/dp/0521826179/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">The Cambridge Handbook of Personal Relationships</a></em>. Cambridge University Press.</small></p>\n<p><small>Walster, Aronson, Abrahams, &amp; Rottman (1966). <a href=\"http://www2.hawaii.edu/~elaineh/13.pdf\">Importance of physical attractiveness in dating behavior</a>. <em>Journal of Personality and Social Psychology, 4</em>: 508-516.</small></p>\n<p><small>Weiten, Dunn, &amp; Hammer (2011). <em><a href=\"http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Psychology Applied to Modern Life: Adjustment in the 21st Century, 10th edition</a></em>. Wadsworth Publishing.</small></p>\n<p><small>White &amp; Knight (1984). Misattribution of arousal and attraction: Effects of salience of explanations for arousal. <em>Journal of Experimental Social Psychology, 20</em>: 55-64.</small></p>\n<p><small>Wiederman (1993). Evolved gender differences in mate preferences: Evidence from personal advertisements. <em>Ethology and Sociobiology, 14</em>: 331-352.</small></p>\n<p><small>Woll (1986). So many to choose from: Decision strategies in videodating. <em>Journal of Social and Personal Relationships, 3</em>: 43-52.</small></p>\n<p><small>Zajonc (1968). Attitudinal effects of mere exposure. <em>Journal of Personality and Social Psychology, 9</em>: 1-27.</small></p>\n<p><small>Zajonc (1998). Emotions. In Gilbert, Fiske, &amp; Lindzey (eds.), <em>Handbook of Social Psychology, 4th edition.</em>&nbsp;McGraw Hill.</small></p>\n<p><small>Zajonc (2001). <a href=\"http://www.debralieberman.com/downloads/courses/625/Zajonc_2001.pdf\">Mere exposure: A gateway to the subliminal</a>. <em>Current Directions in Psychological Science, 10</em>: 224-228.</small></p>", "sections": [{"title": "Relationships Styles", "anchor": "Relationships_Styles", "level": 1}, {"title": "The Science of Attraction", "anchor": "The_Science_of_Attraction", "level": 1}, {"title": "Attractiveness: Mean and Variance", "anchor": "Attractiveness__Mean_and_Variance", "level": 1}, {"title": "Up next", "anchor": "Up_next", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1537 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1554, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kLR5H4pbaBjzZxLv6", "x8Fp9NMgDWbuMpizA", "kD8uzcmjKwSaTHnQJ", "2MD3NMLBPCqPfnfre", "BHYBdijDcAKQ6e45Z", "895quRDaK6gR2rM82", "M2LWXsJxKS626QNEA", "2YPbdHgcjt7g5ZaFN", "Zmfo388RA9oky3KYe", "64FdKLwmea8MCLWkE", "a7n8GdKiAZRX86T5A", "ACGeaAk6KButv2xwQ", "GGn8MBiY8Xz6NdNdH", "2G7AH92pHyj3nC32T"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-11-05T11:06:42.308Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-05T13:31:38.564Z", "modifiedAt": null, "url": null, "title": "Anthropic Decision Theory V: Linking and ADT", "slug": "anthropic-decision-theory-v-linking-and-adt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:19.968Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wQd3rLLLNWYRgjaYg/anthropic-decision-theory-v-linking-and-adt", "pageUrlRelative": "/posts/wQd3rLLLNWYRgjaYg/anthropic-decision-theory-v-linking-and-adt", "linkUrl": "https://www.lesswrong.com/posts/wQd3rLLLNWYRgjaYg/anthropic-decision-theory-v-linking-and-adt", "postedAtFormatted": "Saturday, November 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20Decision%20Theory%20V%3A%20Linking%20and%20ADT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20Decision%20Theory%20V%3A%20Linking%20and%20ADT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQd3rLLLNWYRgjaYg%2Fanthropic-decision-theory-v-linking-and-adt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20Decision%20Theory%20V%3A%20Linking%20and%20ADT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQd3rLLLNWYRgjaYg%2Fanthropic-decision-theory-v-linking-and-adt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwQd3rLLLNWYRgjaYg%2Fanthropic-decision-theory-v-linking-and-adt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1020, "htmlBody": "<p>A near-final version of my Anthropic Decision Theory&nbsp;<a href=\"http://arxiv.org/abs/1110.6437\">paper</a>&nbsp;is available on the arXiv. Since anthropics problems have been discussed quite a bit on this list, I'll be presenting its arguments and results in this, subsequent, and previous posts&nbsp;<a href=\"/r/discussion/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">1</a>&nbsp;<a href=\"/r/discussion/lw/892/anthropic_decision_theory_ii_selfindication/\">2</a>&nbsp;<a href=\"/r/discussion/lw/89q/anthropic_decision_theory_iii_solving_selfless/\">3</a>&nbsp;<a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">4</a>&nbsp;<a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">5</a>&nbsp;<a href=\"/lw/8bw/anthropic_decision_theory_vi_applying_adt_to/\">6</a>.</p>\n<p>Now that we've seen what the 'correct' decision is for various Sleeping Beauty Problems, let's see a decision theory that reaches the same conclusions.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2>Linked decisions</h2>\n<p class=\"MsoNormal\">Identical copies of Sleeping Beauty will make the same decision when faced with same situations (technically true until quantum and chaotic effects cause a divergence between them, but most decision processes will not be sensitive to random noise like this). Similarly, Sleeping Beauty and the random man on the street will make the same decision when confronted with a twenty pound note: they will pick it up. However, while we could say that the first situation is linked, the second is coincidental: were Sleeping Beauty to refrain from picking up the note, the man on the street would not so refrain, while her copy would.</p>\n<p class=\"MsoNormal\">The above statement brings up subtle issues of causality and counterfactuals, a deep philosophical debate. To sidestep it entirely, let us recast the problem in programming terms, seeing the agent's decision process as a deterministic algorithm. If agent &alpha; is an agent that follows an automated decision algorithm A, then if A knows its own source code (by quining for instance), it might have a line saying something like:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">Module M: If B is another algorithm, belonging to agent &beta;, identical with A ('yourself'), assume A and B will have identical outputs on identical inputs, and base your decision on this.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">This could lead, for example, to &alpha; and &beta; cooperating in a symmetric Prisoner's Dilemma. And there is no problem with A believing the above assumption, as it is entirely true: identical deterministic algorithms on the same input do produce the same outputs. With this in mind, we give an informal definition of a linked decision as:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Linked decisions</strong>: Agent &alpha;'s decisions are linked with agent &beta;'s, if both can prove they will both make the same decision, even after taking into account the fact they know they are linked.</p>\n<p class=\"MsoNormal\">An example of agents that are <em>not</em> linked would be two agents &alpha; and &beta;, running identical algorithms A and B on identical data, except that A has module M while B doesn't. Then A's module might correctly deduce that they will output the same decision, but only if A disregards the difference between them, i.e.module M. So A can 'know' they will output the same decision, but if it acts on that knowledge, it makes it incorrect. If A and B both had module M, then they could both act on the knowledge and it would remain correct.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2>ADT</h2>\n<p class=\"MsoNormal\">Given the above definition, anthropic decision theory (ADT) can be simply stated as:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Anthropic Decision Theory</strong> (ADT): An agent should first find all the decisions linked with their own. Then they should maximise expected utility, acting as if they simultaneously controlled the outcomes of all linked decisions, and using the objective (non-anthropic) probabilities of the various worlds.</p>\n<p class=\"MsoNormal\">ADT is similar to SSA in that it makes use of reference classes. However, SSA needs to have the reference class information established separately before it can calculate probabilities, and different reference classes give very different results. In contrast, the reference class for ADT is part of the definition. It is not the class of identical or similar agents; instead, it is the class of linked decisions which (by definition) is the class of decisions that the agent can prove are linked. Hence the whole procedure is perfectly deterministic, and known for a given agent.</p>\n<p class=\"MsoNormal\">It can be seen that ADT obeys all the axioms in the Sleeping Beauty problems, so must reach the <a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">same</a> <a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">conclusions</a> as there.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2>Linking non-identical agents&nbsp;</h2>\n<p class=\"MsoNormal\">Now, module M is enough when the agents/algorithms are strictly identical, but fails when they differ slightly. For instance, imagine a variant of the selfless Sleeping Beauty problem where the two agents aren't exactly identical in tails world. The first agent has the same utility as before, while the second agent has some personal displeasure in engaging in trade -- if she buys the coupon, she will suffer a single -&pound;0.05 penalty for doing so.</p>\n<p class=\"MsoNormal\">Then if the coupon is priced at &pound;0.60, something quite interesting happens. If the agents do not believe they are linked, they will refuse the offer: their expected returns are 0.5(-0.6 + (1-0.6)) = -0.1 and -0.1-0.05=-0.15 respectively. If however they believe their decisions are linked, they will calculate the expected return from buying the coupon as 0.5 (-0.60 + 2(1-0.60)) = 0.1 and 0.1-0.05 = 0.05 respectively. Since these are positive, they will buy the coupon: meaning their assumption that they were linked was actually correct!</p>\n<p class=\"MsoNormal\">If the coupon is priced at &pound;0.66, things change. If the two agents assume their decisions are linked, then they will calculate their expected return from buying the coupon as 0.5(-0.66 + 2(1-0.66))= 0.01 and 0.01-0.05=-0.04 respectively. The first agent will buy, and the second will not -- they were wrong to assume they were linked</p>\n<p class=\"MsoNormal\">A more general module that gives this kind of behaviour is:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">Module N: Let H be the hypothesis that the decision of A ('myself') and those of algorithm B are linked. I will then compute what each of us will decide if we were both to accept H. If our ultimate decisions are indeed the same, and if the other agent also has a module N, then I will accept H.</p>\n<p class=\"MsoNormal\">The module N gives correct behaviour. It only triggers if the agents can prove that accepting H will ensure that H is true -- and then N makes them accept H, hence making H true.</p>\n<p class=\"MsoNormal\">For the coupon priced at &pound;0.60, it will correctly tell them they are linked, and they will both buy it. For the coupon priced at &pound;0.66, it will not trigger, and both will refuse to buy it -- though they reach the same decision, they will not have done so if they had assumed they were linked. For a coupon priced above &pound;2/3, module N will correctly tell them are linked again, and they will both refuse to buy it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wQd3rLLLNWYRgjaYg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.948411385720344e-07, "legacy": true, "legacyId": "10778", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "kmryZRz5r9bjsug9e", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "solving-selfishness-for-udt", "canonicalPrevPostSlug": "anthropic-decision-theory-vi-applying-adt-to-common", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A near-final version of my Anthropic Decision Theory&nbsp;<a href=\"http://arxiv.org/abs/1110.6437\">paper</a>&nbsp;is available on the arXiv. Since anthropics problems have been discussed quite a bit on this list, I'll be presenting its arguments and results in this, subsequent, and previous posts&nbsp;<a href=\"/r/discussion/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">1</a>&nbsp;<a href=\"/r/discussion/lw/892/anthropic_decision_theory_ii_selfindication/\">2</a>&nbsp;<a href=\"/r/discussion/lw/89q/anthropic_decision_theory_iii_solving_selfless/\">3</a>&nbsp;<a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">4</a>&nbsp;<a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">5</a>&nbsp;<a href=\"/lw/8bw/anthropic_decision_theory_vi_applying_adt_to/\">6</a>.</p>\n<p>Now that we've seen what the 'correct' decision is for various Sleeping Beauty Problems, let's see a decision theory that reaches the same conclusions.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2 id=\"Linked_decisions\">Linked decisions</h2>\n<p class=\"MsoNormal\">Identical copies of Sleeping Beauty will make the same decision when faced with same situations (technically true until quantum and chaotic effects cause a divergence between them, but most decision processes will not be sensitive to random noise like this). Similarly, Sleeping Beauty and the random man on the street will make the same decision when confronted with a twenty pound note: they will pick it up. However, while we could say that the first situation is linked, the second is coincidental: were Sleeping Beauty to refrain from picking up the note, the man on the street would not so refrain, while her copy would.</p>\n<p class=\"MsoNormal\">The above statement brings up subtle issues of causality and counterfactuals, a deep philosophical debate. To sidestep it entirely, let us recast the problem in programming terms, seeing the agent's decision process as a deterministic algorithm. If agent \u03b1 is an agent that follows an automated decision algorithm A, then if A knows its own source code (by quining for instance), it might have a line saying something like:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">Module M: If B is another algorithm, belonging to agent \u03b2, identical with A ('yourself'), assume A and B will have identical outputs on identical inputs, and base your decision on this.<a id=\"more\"></a></p>\n<p class=\"MsoNormal\">This could lead, for example, to \u03b1 and \u03b2 cooperating in a symmetric Prisoner's Dilemma. And there is no problem with A believing the above assumption, as it is entirely true: identical deterministic algorithms on the same input do produce the same outputs. With this in mind, we give an informal definition of a linked decision as:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Linked decisions</strong>: Agent \u03b1's decisions are linked with agent \u03b2's, if both can prove they will both make the same decision, even after taking into account the fact they know they are linked.</p>\n<p class=\"MsoNormal\">An example of agents that are <em>not</em> linked would be two agents \u03b1 and \u03b2, running identical algorithms A and B on identical data, except that A has module M while B doesn't. Then A's module might correctly deduce that they will output the same decision, but only if A disregards the difference between them, i.e.module M. So A can 'know' they will output the same decision, but if it acts on that knowledge, it makes it incorrect. If A and B both had module M, then they could both act on the knowledge and it would remain correct.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2 id=\"ADT\">ADT</h2>\n<p class=\"MsoNormal\">Given the above definition, anthropic decision theory (ADT) can be simply stated as:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\"><strong>Anthropic Decision Theory</strong> (ADT): An agent should first find all the decisions linked with their own. Then they should maximise expected utility, acting as if they simultaneously controlled the outcomes of all linked decisions, and using the objective (non-anthropic) probabilities of the various worlds.</p>\n<p class=\"MsoNormal\">ADT is similar to SSA in that it makes use of reference classes. However, SSA needs to have the reference class information established separately before it can calculate probabilities, and different reference classes give very different results. In contrast, the reference class for ADT is part of the definition. It is not the class of identical or similar agents; instead, it is the class of linked decisions which (by definition) is the class of decisions that the agent can prove are linked. Hence the whole procedure is perfectly deterministic, and known for a given agent.</p>\n<p class=\"MsoNormal\">It can be seen that ADT obeys all the axioms in the Sleeping Beauty problems, so must reach the <a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">same</a> <a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">conclusions</a> as there.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<h2 id=\"Linking_non_identical_agents_\">Linking non-identical agents&nbsp;</h2>\n<p class=\"MsoNormal\">Now, module M is enough when the agents/algorithms are strictly identical, but fails when they differ slightly. For instance, imagine a variant of the selfless Sleeping Beauty problem where the two agents aren't exactly identical in tails world. The first agent has the same utility as before, while the second agent has some personal displeasure in engaging in trade -- if she buys the coupon, she will suffer a single -\u00a30.05 penalty for doing so.</p>\n<p class=\"MsoNormal\">Then if the coupon is priced at \u00a30.60, something quite interesting happens. If the agents do not believe they are linked, they will refuse the offer: their expected returns are 0.5(-0.6 + (1-0.6)) = -0.1 and -0.1-0.05=-0.15 respectively. If however they believe their decisions are linked, they will calculate the expected return from buying the coupon as 0.5 (-0.60 + 2(1-0.60)) = 0.1 and 0.1-0.05 = 0.05 respectively. Since these are positive, they will buy the coupon: meaning their assumption that they were linked was actually correct!</p>\n<p class=\"MsoNormal\">If the coupon is priced at \u00a30.66, things change. If the two agents assume their decisions are linked, then they will calculate their expected return from buying the coupon as 0.5(-0.66 + 2(1-0.66))= 0.01 and 0.01-0.05=-0.04 respectively. The first agent will buy, and the second will not -- they were wrong to assume they were linked</p>\n<p class=\"MsoNormal\">A more general module that gives this kind of behaviour is:</p>\n<p class=\"MsoNormal\" style=\"padding-left: 30px;\">Module N: Let H be the hypothesis that the decision of A ('myself') and those of algorithm B are linked. I will then compute what each of us will decide if we were both to accept H. If our ultimate decisions are indeed the same, and if the other agent also has a module N, then I will accept H.</p>\n<p class=\"MsoNormal\">The module N gives correct behaviour. It only triggers if the agents can prove that accepting H will ensure that H is true -- and then N makes them accept H, hence making H true.</p>\n<p class=\"MsoNormal\">For the coupon priced at \u00a30.60, it will correctly tell them they are linked, and they will both buy it. For the coupon priced at \u00a30.66, it will not trigger, and both will refuse to buy it -- though they reach the same decision, they will not have done so if they had assumed they were linked. For a coupon priced above \u00a32/3, module N will correctly tell them are linked again, and they will both refuse to buy it.</p>", "sections": [{"title": "Linked decisions", "anchor": "Linked_decisions", "level": 1}, {"title": "ADT", "anchor": "ADT", "level": 1}, {"title": "Linking non-identical agents\u00a0", "anchor": "Linking_non_identical_agents_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["svhbnSdxW3XmFXXTK", "G9scbpNrCxfJZQmYu", "zqm2nTqjAKg6EyxE3", "LXHsiHahr2eFQ4Hsp", "wQd3rLLLNWYRgjaYg", "NpPEJT2CCKjeo59Ps"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-05T17:53:47.332Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Hidden Complexity of Wishes", "slug": "seq-rerun-the-hidden-complexity-of-wishes", "viewCount": null, "lastCommentedAt": "2011-11-06T09:01:51.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qm3R4JwTztekWSN5X/seq-rerun-the-hidden-complexity-of-wishes", "pageUrlRelative": "/posts/qm3R4JwTztekWSN5X/seq-rerun-the-hidden-complexity-of-wishes", "linkUrl": "https://www.lesswrong.com/posts/qm3R4JwTztekWSN5X/seq-rerun-the-hidden-complexity-of-wishes", "postedAtFormatted": "Saturday, November 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Hidden%20Complexity%20of%20Wishes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Hidden%20Complexity%20of%20Wishes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm3R4JwTztekWSN5X%2Fseq-rerun-the-hidden-complexity-of-wishes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Hidden%20Complexity%20of%20Wishes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm3R4JwTztekWSN5X%2Fseq-rerun-the-hidden-complexity-of-wishes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqm3R4JwTztekWSN5X%2Fseq-rerun-the-hidden-complexity-of-wishes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 203, "htmlBody": "<p>Today's post, <a href=\"/lw/ld/the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a> was originally published on 24 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#The_Hidden_Complexity_of_Wishes\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are a lot of things that humans care about. Therefore, the wishes that we make (as if to a genie) are enormously more complicated than we would intuitively suspect. In order to safely ask a powerful, intelligent being to do something for you, that being must share your entire decision criterion, or else the outcome will likely be horrible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8ba/seq_rerun_leaky_generalizations/\">Leaky Generalizations</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qm3R4JwTztekWSN5X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.949328805568583e-07, "legacy": true, "legacyId": "10797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ARaTpNX62uaL86j6", "2TAfmGBZeRjRDLiMc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-06T00:14:52.216Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lost Purposes", "slug": "seq-rerun-lost-purposes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:19.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FGxYL6tzCuiyjWXu5/seq-rerun-lost-purposes", "pageUrlRelative": "/posts/FGxYL6tzCuiyjWXu5/seq-rerun-lost-purposes", "linkUrl": "https://www.lesswrong.com/posts/FGxYL6tzCuiyjWXu5/seq-rerun-lost-purposes", "postedAtFormatted": "Sunday, November 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lost%20Purposes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lost%20Purposes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGxYL6tzCuiyjWXu5%2Fseq-rerun-lost-purposes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lost%20Purposes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGxYL6tzCuiyjWXu5%2Fseq-rerun-lost-purposes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGxYL6tzCuiyjWXu5%2Fseq-rerun-lost-purposes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 233, "htmlBody": "<p>Today's post, <a href=\"/lw/le/lost_purposes/\">Lost Purposes</a> was originally published on 25 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Lost_Purposes\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is possible for the various steps in a complex plan to become valued in and of themselves, rather than as steps to achieve some desired goal. It is especially easy if the plan is being executed by a complex organization, where each group or individual in the organization is only evaluated by whether or not they carry out their assigned step. When this process is carried to its extreme, we get Soviet shoe factories manufacturing tiny shoes to increase their production quotas, and the No Child Left Behind Act.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8bx/seq_rerun_the_hidden_complexity_of_wishes/\">The Hidden Complexity of Wishes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FGxYL6tzCuiyjWXu5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 7.950662791444721e-07, "legacy": true, "legacyId": "10799", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sP2Hg6uPwpfp3jZJN", "qm3R4JwTztekWSN5X", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-06T11:50:21.148Z", "modifiedAt": null, "url": null, "title": "Anthropic Decision Theory VI: Applying ADT to common anthropic problems", "slug": "anthropic-decision-theory-vi-applying-adt-to-common", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:37.848Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NpPEJT2CCKjeo59Ps/anthropic-decision-theory-vi-applying-adt-to-common", "pageUrlRelative": "/posts/NpPEJT2CCKjeo59Ps/anthropic-decision-theory-vi-applying-adt-to-common", "linkUrl": "https://www.lesswrong.com/posts/NpPEJT2CCKjeo59Ps/anthropic-decision-theory-vi-applying-adt-to-common", "postedAtFormatted": "Sunday, November 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anthropic%20Decision%20Theory%20VI%3A%20Applying%20ADT%20to%20common%20anthropic%20problems&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnthropic%20Decision%20Theory%20VI%3A%20Applying%20ADT%20to%20common%20anthropic%20problems%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpPEJT2CCKjeo59Ps%2Fanthropic-decision-theory-vi-applying-adt-to-common%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anthropic%20Decision%20Theory%20VI%3A%20Applying%20ADT%20to%20common%20anthropic%20problems%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpPEJT2CCKjeo59Ps%2Fanthropic-decision-theory-vi-applying-adt-to-common", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNpPEJT2CCKjeo59Ps%2Fanthropic-decision-theory-vi-applying-adt-to-common", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2371, "htmlBody": "<p>A near-final version of my Anthropic Decision Theory&nbsp;<a href=\"http://arxiv.org/abs/1110.6437\">paper</a>&nbsp;is available on the arXiv. Since anthropics problems have been discussed quite a bit on this list, I'll be presenting its arguments and results in this and previous posts&nbsp;<a href=\"/r/discussion/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">1</a>&nbsp;<a href=\"/r/discussion/lw/892/anthropic_decision_theory_ii_selfindication/\">2</a>&nbsp;<a href=\"/r/discussion/lw/89q/anthropic_decision_theory_iii_solving_selfless/\">3</a>&nbsp;<a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">4</a>&nbsp;<a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">5</a>&nbsp;<a href=\"/lw/8bw/anthropic_decision_theory_vi_applying_adt_to/\">6</a>.</p>\n<p>Having presented ADT previously, I'll round off this mini-sequence by showing how it behaves with common anthropic problems, such as the Presumptuous Philosopher, Adam and Eve problem, and the Doomsday argument.</p>\n<h2>The Presumptuous Philosopher</h2>\n<p class=\"MsoNoSpacing\">The Presumptuous Philosopher was introduced by Nick Bostrom as a way of pointing out the absurdities in SIA. In the setup, the universe either has a trillion observers, or a trillion trillion trillion observers, and physics is indifferent as to which one is correct. Some physicists are preparing to do an experiment to determine the correct universe, until a presumptuous philosopher runs up to them, claiming that his SIA probability makes the larger one nearly certainly the correct one. In fact, he will accept bets at a trillion trillion to one odds that he is in the larger universe, repeatedly defying even strong experimental evidence with his SIA probability correction.</p>\n<p class=\"MsoNoSpacing\">What does ADT have to say about this problem? Implicitly, when the problem is discussed, the philosopher is understood to be selfish towards any putative other copies of himself (similarly, Sleeping Beauty is often implicitly assumed to be selfless, which may explain the diverge of intuitions that people have on the two problems). Are there necessarily other similar copies? Well, in order to use SIA, the philosopher must believe that there is nothing blocking the creation of presumptuous philosophers in the larger universe; for if there was, the odds would shift away from the larger universe (in the extreme case when only one presumptuous philosopher is allowed in any universe, SIA finds them equi-probable). So the expected number of presumptuous philosophers in the larger universe is a trillion trillion times greater than the expected number in the small universe.<a id=\"more\"></a></p>\n<p class=\"MsoNoSpacing\">Now if the philosopher is indeed selfish towards his copies, then ADT reduces to SSA-type behaviour: the philosopher will correctly deduce that in the larger universe, the other trillion trillion philosophers or so will have their decision linked with his. However, he doesn&rsquo;t care about them: any benefit that accrue to them are not of his concern, and so if he correctly guesses that he resides in the larger universe, he will accrue a single benefit. Hence there will be no paradox: he will bet at 1:1 odds of residing in either the larger or the smaller universe.</p>\n<p class=\"MsoNoSpacing\">If the philosopher is an altruistic total utilitarian, on the other hand, he will accept bets at odds of a trillion trillion to one of residing in the larger universe. But this no longer counter-intuitive (or at least, no more counter-intuitive than maximising expect utility with very small probabilities): the other presumptuous philosophers will make the same bet, so in the larger universe, their total profit and loss will be multiplied by a trillion trillion. And since the philosopher is altruistic, the impact on his own utility is multiplied by a trillion trillion in the large universe, making his bets rational.</p>\n<p class=\"MsoNoSpacing\">At this point, it might be fair to ask what would happen if some of the philosophers were altruistic while others were selfish. How would the two interact; would the selfless philosopher be incorrectly believing his own decision was somehow &lsquo;benefiting&rsquo; the selfish ones? Not at all. The decisions of the selfless and selfish philosophers are not linked: they both use ADT, but because they have very different utilities, they cannot prove that their decisions are linked. Which is fortunate, because they aren&rsquo;t.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2>Adam and Eve</h2>\n<p class=\"MsoNoSpacing\">The presumptuous philosopher thought experiment was designed to show up problems with SIA reasoning. Another thought experiment by the same author as designed to show problems with SSA reasoning.</p>\n<p class=\"MsoNoSpacing\">In this thought experiment, Adam and Eve, the first two humans, have the opportunity to breed or not to breed. If they breed, they will produce trillions of trillions of descendants. Under SSA odds, the probability of being Adam or Eve in a universe with trillions of trillion humans is tiny, while the corresponding probability in a universe with just two observers is one. Therefore Adam and Eve should conclude that whatever they do, it is tremendously unlikely that they will succeed in breeding. Nick Bostrom them proceeds to draw many amusing consequences from this &lsquo;future affecting&rsquo; paradox, such as the couple forming the firm intention of having sex (and hence risking pregnancy) if an edible animal doesn&rsquo;t wander through the entrance of their cave in the next few minutes. There seems to be something very wrong with the reasoning, but it is a quite natural consequence of SSA.</p>\n<p class=\"MsoNoSpacing\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_8bw_0.png\" alt=\"\" width=\"648\" height=\"531\" /></p>\n<p class=\"MsoNoSpacing\">What does ADT have to say on the problem? This depends on whether the decisions of Adam and Eve and their (potential) descendants are linked. There is prima facia no reason for this to be the case; in any case, how could potential future descendants make the decision as to whether Adam and Eve should breed? One way of imagining this is if each human is born fully rational, aware of the possible world they is born into, but in ignorance as to their identity and position in that world. Call this the ignorant rational baby stage. They can then make a decision as to what they would do, conditional upon discovering their identity. They may then decide or not to stick to their decision. Hence we can distinguish several scenarios:</p>\n<ul>\n<li>These agents have no &lsquo;ignorant rational baby stage&rsquo;, and do not take it into account.</li>\n<li>These agents have an &lsquo;ignorant rational baby stage&rsquo;, but do not allow precommitments.</li>\n<li>These agents have an &lsquo;ignorant rational baby stage&rsquo;, but do allow precommitments.</li>\n<li>These agents have no &lsquo;ignorant rational baby stage&rsquo;, but require themselves to follow the hypothetical precommitments they would have made had they had such a stage.</li>\n</ul>\n<p>To make this into a decision problem, assume all agents are selfish, and know they will be confronted by a choice between a coupon C<sub>1</sub> that pays out &pound;1 to Adam and Eve if they have no descendants and C<sub>2</sub> that pays out &pound;1 to Adam and Eve if they have (trillions of trillions of) descendants. Assume Adam and Eve will have sex exactly once, and the (objective) chance of them having a successful pregnancy is 50%. Now each agent must decide on the relative values of the two coupons.</p>\n<p class=\"MsoNoSpacing\">Obviously in situation 1, the decisions of Adam and Eve and their descendants are not linked, and ADT means that Adam and Eve will value C<sub>1</sub> compared with C<sub>2</sub> according to their objective estimates of having descendants, i.e. they will value then equally. There is no SSA-like paradox here. Their eventual descendants will also value the coupons as equally worthless, as they will never derive any value from them.</p>\n<p class=\"MsoNoSpacing\">Now assume there is a &lsquo;ignorant rational baby stage&rsquo;. During this stage, the decisions of all agents are linked, as they have the same information, the same (selfish) preferences, and they all know this. Each rational baby can then reason:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">&ldquo;If I am in the world with no descendants, then I am Adam or Eve, and the C<sub>1</sub> coupon is worth &pound;1 to me (and C<sub>2</sub> is worthless). If, on the other hand, I am in the world with trillions of trillions of descendants, there is only two chances in 2+10<sup>24</sup> of me being Adam or Eve, so I value the C<sub>2</sub> coupon at &pound;2/(2+10<sup>24</sup>) (and C<sub>1</sub> is worthless). These worlds are equiprobable. So I would value C<sub>1</sub> as being 1+0.5 x 10<sup>24</sup> times more valuable than C<sub>2</sub>.&rdquo;</p>\n<p class=\"MsoNoSpacing\">So the rational babies in situations 2 and 3 would take C<sub>1</sub> as much more valuable than C<sub>2</sub>, if the deal was proposed to them immediately. Since there are no precommitments in situation 2, once the rational babies discover who they actually are, they would revert to situation 1 and take them as equally valuable. If precommitments are allowed, then the rational babies would further reason:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">&ldquo;The strategy &lsquo;upon discovering I am Adam or Eve, take C<sub>1</sub>&rsquo; nets me an expected &pound;1/2, while the strategy &lsquo;upon discovering I am Adam or Eve, take C<sub>2</sub>&rsquo; nets me an expected &pound;1/(2+10<sup>24</sup>), because it is very unlikely that I would actually discover that I am Adam and Eve. Hence the strategy &lsquo;upon discovering I am Adam or Eve, accept trades between C<sub>1</sub> and C<sub>2</sub> at 2:(2+10<sup>24</sup>) ratios&rsquo; is neutral in expected utility, and so I will now precommit to accepting any trade at any ratios slightly better than this.&rdquo;</p>\n<p class=\"MsoNoSpacing\">So in situation 3, even after discovering that they are Adam or Eve, they will continue to accept deals at ratios that would seem to imply that they believe in the SSA odds, i.e. that they are nearly certain to not have descendants. But it is a lot less of a paradox now; it simply arises because there was a time when they were uncertain as to what their actual position was, and the effects of this uncertainty were &lsquo;locked in&rsquo; by their precommitment.</p>\n<p class=\"MsoNoSpacing\">Situation 4 is very interesting. By construction, it reproduces the seemingly paradoxical behaviour, but here there was never a rational baby stage where the behaviour made sense. Why would any agent follow such a behaviour? Well, mainly because it allows trade between agents who might not otherwise be able to agree on a &lsquo;fair&rsquo; distribution of goods. If all agents agree to the division that they would have wanted had they been ignorant of their identity (a &lsquo;rawlsian veil of ignorance&rsquo; situation), then they can trade between each other without threats or bargaining in these simple cases.</p>\n<p class=\"MsoNoSpacing\">If the agents are simply altruistic average utilitarians, then Adam and Eve would accept SSA odds in all four situations; things that benefit them specifically are weighted more highly in a universe with few people. So the varying behaviour above is a feature of selfishness, not of SSA-type behaviour, and it seems precommitments become very important in the selfish case. This certainly merits further study. Temporal consistency is a virtue, but does it extend to situations like this, where the agent makes binding decisions before knowing their identity? Certainly <em>if</em> the agent had to make a decision immediately, and if there were anyone around to profit from temporal inconsistencies, the agent should remain consistent, which means following precommitments. However this is not entirely obvious that it should still be the case if there were no-one to exploit the inconsistency.</p>\n<p class=\"MsoNoSpacing\">This is not, incidentally, a problem only of ADT - SIA has similar problem under the creation of &lsquo;irrelevant&rsquo; selfless agents who don&rsquo;t yet know who they are, while SSA has problems under the creation of agents who don&rsquo;t yet know what reference class they are in.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2>The Doomsday argument</h2>\n<p class=\"MsoNoSpacing\">Closely related to the Adam and Eve paradox, though discovered first, is the Doomsday argument. Based on SSA&rsquo;s preference for &lsquo;smaller&rsquo; universes, it implies that there is a high probability of the human race becoming extinct within a few generations - at the very least, a much higher probability than objective factors would imply.</p>\n<p class=\"MsoNoSpacing\">Under SIA, the argument goes away, so it would seem that ADT must behave oddly: depending on the selfishness and selflessness of the agents, they would give different probabilities to the extinction of the human race. This is not the case, however. Recall that under ADT, decisions matter, not probabilities. And agents that are selfish or average utilitarians would not be directly concerned with the extinction of the human race, so would not act in bets to prevent this.</p>\n<p class=\"MsoNoSpacing\">This is not a specious point - there are ways of constructing the doomsday argument in ADT, but they all rely on odd agents who are selfish with respect to their own generation but selfless with respect to the future survival of the human race. This lacks the potency of the original formulation: having somewhat odd agents behaving in a somewhat odd fashion is not very surprising. For the moment, until a better version is produced, we should simply say that the doomsday argument is not present in ADT.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2>Sleeping Anti-Beauty</h2>\n<p class=\"MsoNoSpacing\"><a href=\"/lw/4e0/sleeping_antibeauty_and_the_presumptuous/\">Sleeping Anti-Beauty</a> is a thought experiment similar to the Sleeping Beauty experiment, but with one important caveat: the two copies in the tails world hate each other. This works best if the two copies are duplicates, rather than the same person at different times. One could imagine, for instance, that a week after the experiment, all copies of Sleeping Beauty are awakened and made to fight to the death - maybe they live in a civilization that prohibits more than one copy of an agent from existing. The single copy in the heads world will be left unmolested, as she has nobody to fight.</p>\n<p class=\"MsoNoSpacing\">That means that the Sleeping Beauties in the tail world are in a zero sum game; any gain for one is a loss for the other, and vice-versa. Actually, we need a few more assumptions for this to be true: the Sleeping Beauties have to be entirely selfish apart from their rivalry, and they do not get offered any goods for immediate consumption. Given all these assumptions, what does ADT have to say about their decisions?</p>\n<p class=\"MsoNoSpacing\">As usual, all existent copies of Sleeping Beauty are offered a coupon that pays out &pound;1 if the coin fell tails, and asked how much she would be willing to give for that. ADT reasoning proceeds for each agent as follows:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">&ldquo;In the heads world, if I pay &pound;x, all that happens is that I lose &pound;x. In the tails, world, if I pay &pound;x, I gain &pound;(1-x). However my other hated copy will make the same decision, and also gain &pound;(1-x). This causes me the same amount of loss as the gain of &pound;(1-x) does, so I gain nothing at all in the tails world, whatever &pound;x is. So I value the coupon precisely at zero: I would not pay any amount to get it.&rdquo;</p>\n<p class=\"MsoNoSpacing\">In this, and other similar decisions, Sleeping Beauty would act as if she had an absolute certainty of being in the heads world, offering infinity to one odds of this being the case, as she cannot realise any gains - or losses! - in the tails world.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\">It should be noted that selfish Sleeping Beauty can be correctly modelled by seeing it as a 50-50 mix of selfless Sleeping Beauty and Sleeping Anti-Beauty.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PbShukhzpLsWpGXkM": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NpPEJT2CCKjeo59Ps", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.953098367422192e-07, "legacy": true, "legacyId": "10796", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "kmryZRz5r9bjsug9e", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "anthropic-decision-theory-v-linking-and-adt", "canonicalPrevPostSlug": "anthropic-decision-theory-v-linking-and-adt", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A near-final version of my Anthropic Decision Theory&nbsp;<a href=\"http://arxiv.org/abs/1110.6437\">paper</a>&nbsp;is available on the arXiv. Since anthropics problems have been discussed quite a bit on this list, I'll be presenting its arguments and results in this and previous posts&nbsp;<a href=\"/r/discussion/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">1</a>&nbsp;<a href=\"/r/discussion/lw/892/anthropic_decision_theory_ii_selfindication/\">2</a>&nbsp;<a href=\"/r/discussion/lw/89q/anthropic_decision_theory_iii_solving_selfless/\">3</a>&nbsp;<a href=\"/r/discussion/lw/8aw/anthropic_decision_theory_iv_solving_selfish_and/\">4</a>&nbsp;<a href=\"/r/discussion/lw/8be/anthropic_decision_theory_v_linking_and_adt/\">5</a>&nbsp;<a href=\"/lw/8bw/anthropic_decision_theory_vi_applying_adt_to/\">6</a>.</p>\n<p>Having presented ADT previously, I'll round off this mini-sequence by showing how it behaves with common anthropic problems, such as the Presumptuous Philosopher, Adam and Eve problem, and the Doomsday argument.</p>\n<h2 id=\"The_Presumptuous_Philosopher\">The Presumptuous Philosopher</h2>\n<p class=\"MsoNoSpacing\">The Presumptuous Philosopher was introduced by Nick Bostrom as a way of pointing out the absurdities in SIA. In the setup, the universe either has a trillion observers, or a trillion trillion trillion observers, and physics is indifferent as to which one is correct. Some physicists are preparing to do an experiment to determine the correct universe, until a presumptuous philosopher runs up to them, claiming that his SIA probability makes the larger one nearly certainly the correct one. In fact, he will accept bets at a trillion trillion to one odds that he is in the larger universe, repeatedly defying even strong experimental evidence with his SIA probability correction.</p>\n<p class=\"MsoNoSpacing\">What does ADT have to say about this problem? Implicitly, when the problem is discussed, the philosopher is understood to be selfish towards any putative other copies of himself (similarly, Sleeping Beauty is often implicitly assumed to be selfless, which may explain the diverge of intuitions that people have on the two problems). Are there necessarily other similar copies? Well, in order to use SIA, the philosopher must believe that there is nothing blocking the creation of presumptuous philosophers in the larger universe; for if there was, the odds would shift away from the larger universe (in the extreme case when only one presumptuous philosopher is allowed in any universe, SIA finds them equi-probable). So the expected number of presumptuous philosophers in the larger universe is a trillion trillion times greater than the expected number in the small universe.<a id=\"more\"></a></p>\n<p class=\"MsoNoSpacing\">Now if the philosopher is indeed selfish towards his copies, then ADT reduces to SSA-type behaviour: the philosopher will correctly deduce that in the larger universe, the other trillion trillion philosophers or so will have their decision linked with his. However, he doesn\u2019t care about them: any benefit that accrue to them are not of his concern, and so if he correctly guesses that he resides in the larger universe, he will accrue a single benefit. Hence there will be no paradox: he will bet at 1:1 odds of residing in either the larger or the smaller universe.</p>\n<p class=\"MsoNoSpacing\">If the philosopher is an altruistic total utilitarian, on the other hand, he will accept bets at odds of a trillion trillion to one of residing in the larger universe. But this no longer counter-intuitive (or at least, no more counter-intuitive than maximising expect utility with very small probabilities): the other presumptuous philosophers will make the same bet, so in the larger universe, their total profit and loss will be multiplied by a trillion trillion. And since the philosopher is altruistic, the impact on his own utility is multiplied by a trillion trillion in the large universe, making his bets rational.</p>\n<p class=\"MsoNoSpacing\">At this point, it might be fair to ask what would happen if some of the philosophers were altruistic while others were selfish. How would the two interact; would the selfless philosopher be incorrectly believing his own decision was somehow \u2018benefiting\u2019 the selfish ones? Not at all. The decisions of the selfless and selfish philosophers are not linked: they both use ADT, but because they have very different utilities, they cannot prove that their decisions are linked. Which is fortunate, because they aren\u2019t.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2 id=\"Adam_and_Eve\">Adam and Eve</h2>\n<p class=\"MsoNoSpacing\">The presumptuous philosopher thought experiment was designed to show up problems with SIA reasoning. Another thought experiment by the same author as designed to show problems with SSA reasoning.</p>\n<p class=\"MsoNoSpacing\">In this thought experiment, Adam and Eve, the first two humans, have the opportunity to breed or not to breed. If they breed, they will produce trillions of trillions of descendants. Under SSA odds, the probability of being Adam or Eve in a universe with trillions of trillion humans is tiny, while the corresponding probability in a universe with just two observers is one. Therefore Adam and Eve should conclude that whatever they do, it is tremendously unlikely that they will succeed in breeding. Nick Bostrom them proceeds to draw many amusing consequences from this \u2018future affecting\u2019 paradox, such as the couple forming the firm intention of having sex (and hence risking pregnancy) if an edible animal doesn\u2019t wander through the entrance of their cave in the next few minutes. There seems to be something very wrong with the reasoning, but it is a quite natural consequence of SSA.</p>\n<p class=\"MsoNoSpacing\"><img style=\"vertical-align: middle;\" src=\"http://images.lesswrong.com/t3_8bw_0.png\" alt=\"\" width=\"648\" height=\"531\"></p>\n<p class=\"MsoNoSpacing\">What does ADT have to say on the problem? This depends on whether the decisions of Adam and Eve and their (potential) descendants are linked. There is prima facia no reason for this to be the case; in any case, how could potential future descendants make the decision as to whether Adam and Eve should breed? One way of imagining this is if each human is born fully rational, aware of the possible world they is born into, but in ignorance as to their identity and position in that world. Call this the ignorant rational baby stage. They can then make a decision as to what they would do, conditional upon discovering their identity. They may then decide or not to stick to their decision. Hence we can distinguish several scenarios:</p>\n<ul>\n<li>These agents have no \u2018ignorant rational baby stage\u2019, and do not take it into account.</li>\n<li>These agents have an \u2018ignorant rational baby stage\u2019, but do not allow precommitments.</li>\n<li>These agents have an \u2018ignorant rational baby stage\u2019, but do allow precommitments.</li>\n<li>These agents have no \u2018ignorant rational baby stage\u2019, but require themselves to follow the hypothetical precommitments they would have made had they had such a stage.</li>\n</ul>\n<p>To make this into a decision problem, assume all agents are selfish, and know they will be confronted by a choice between a coupon C<sub>1</sub> that pays out \u00a31 to Adam and Eve if they have no descendants and C<sub>2</sub> that pays out \u00a31 to Adam and Eve if they have (trillions of trillions of) descendants. Assume Adam and Eve will have sex exactly once, and the (objective) chance of them having a successful pregnancy is 50%. Now each agent must decide on the relative values of the two coupons.</p>\n<p class=\"MsoNoSpacing\">Obviously in situation 1, the decisions of Adam and Eve and their descendants are not linked, and ADT means that Adam and Eve will value C<sub>1</sub> compared with C<sub>2</sub> according to their objective estimates of having descendants, i.e. they will value then equally. There is no SSA-like paradox here. Their eventual descendants will also value the coupons as equally worthless, as they will never derive any value from them.</p>\n<p class=\"MsoNoSpacing\">Now assume there is a \u2018ignorant rational baby stage\u2019. During this stage, the decisions of all agents are linked, as they have the same information, the same (selfish) preferences, and they all know this. Each rational baby can then reason:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">\u201cIf I am in the world with no descendants, then I am Adam or Eve, and the C<sub>1</sub> coupon is worth \u00a31 to me (and C<sub>2</sub> is worthless). If, on the other hand, I am in the world with trillions of trillions of descendants, there is only two chances in 2+10<sup>24</sup> of me being Adam or Eve, so I value the C<sub>2</sub> coupon at \u00a32/(2+10<sup>24</sup>) (and C<sub>1</sub> is worthless). These worlds are equiprobable. So I would value C<sub>1</sub> as being 1+0.5 x 10<sup>24</sup> times more valuable than C<sub>2</sub>.\u201d</p>\n<p class=\"MsoNoSpacing\">So the rational babies in situations 2 and 3 would take C<sub>1</sub> as much more valuable than C<sub>2</sub>, if the deal was proposed to them immediately. Since there are no precommitments in situation 2, once the rational babies discover who they actually are, they would revert to situation 1 and take them as equally valuable. If precommitments are allowed, then the rational babies would further reason:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">\u201cThe strategy \u2018upon discovering I am Adam or Eve, take C<sub>1</sub>\u2019 nets me an expected \u00a31/2, while the strategy \u2018upon discovering I am Adam or Eve, take C<sub>2</sub>\u2019 nets me an expected \u00a31/(2+10<sup>24</sup>), because it is very unlikely that I would actually discover that I am Adam and Eve. Hence the strategy \u2018upon discovering I am Adam or Eve, accept trades between C<sub>1</sub> and C<sub>2</sub> at 2:(2+10<sup>24</sup>) ratios\u2019 is neutral in expected utility, and so I will now precommit to accepting any trade at any ratios slightly better than this.\u201d</p>\n<p class=\"MsoNoSpacing\">So in situation 3, even after discovering that they are Adam or Eve, they will continue to accept deals at ratios that would seem to imply that they believe in the SSA odds, i.e. that they are nearly certain to not have descendants. But it is a lot less of a paradox now; it simply arises because there was a time when they were uncertain as to what their actual position was, and the effects of this uncertainty were \u2018locked in\u2019 by their precommitment.</p>\n<p class=\"MsoNoSpacing\">Situation 4 is very interesting. By construction, it reproduces the seemingly paradoxical behaviour, but here there was never a rational baby stage where the behaviour made sense. Why would any agent follow such a behaviour? Well, mainly because it allows trade between agents who might not otherwise be able to agree on a \u2018fair\u2019 distribution of goods. If all agents agree to the division that they would have wanted had they been ignorant of their identity (a \u2018rawlsian veil of ignorance\u2019 situation), then they can trade between each other without threats or bargaining in these simple cases.</p>\n<p class=\"MsoNoSpacing\">If the agents are simply altruistic average utilitarians, then Adam and Eve would accept SSA odds in all four situations; things that benefit them specifically are weighted more highly in a universe with few people. So the varying behaviour above is a feature of selfishness, not of SSA-type behaviour, and it seems precommitments become very important in the selfish case. This certainly merits further study. Temporal consistency is a virtue, but does it extend to situations like this, where the agent makes binding decisions before knowing their identity? Certainly <em>if</em> the agent had to make a decision immediately, and if there were anyone around to profit from temporal inconsistencies, the agent should remain consistent, which means following precommitments. However this is not entirely obvious that it should still be the case if there were no-one to exploit the inconsistency.</p>\n<p class=\"MsoNoSpacing\">This is not, incidentally, a problem only of ADT - SIA has similar problem under the creation of \u2018irrelevant\u2019 selfless agents who don\u2019t yet know who they are, while SSA has problems under the creation of agents who don\u2019t yet know what reference class they are in.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2 id=\"The_Doomsday_argument\">The Doomsday argument</h2>\n<p class=\"MsoNoSpacing\">Closely related to the Adam and Eve paradox, though discovered first, is the Doomsday argument. Based on SSA\u2019s preference for \u2018smaller\u2019 universes, it implies that there is a high probability of the human race becoming extinct within a few generations - at the very least, a much higher probability than objective factors would imply.</p>\n<p class=\"MsoNoSpacing\">Under SIA, the argument goes away, so it would seem that ADT must behave oddly: depending on the selfishness and selflessness of the agents, they would give different probabilities to the extinction of the human race. This is not the case, however. Recall that under ADT, decisions matter, not probabilities. And agents that are selfish or average utilitarians would not be directly concerned with the extinction of the human race, so would not act in bets to prevent this.</p>\n<p class=\"MsoNoSpacing\">This is not a specious point - there are ways of constructing the doomsday argument in ADT, but they all rely on odd agents who are selfish with respect to their own generation but selfless with respect to the future survival of the human race. This lacks the potency of the original formulation: having somewhat odd agents behaving in a somewhat odd fashion is not very surprising. For the moment, until a better version is produced, we should simply say that the doomsday argument is not present in ADT.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<h2 id=\"Sleeping_Anti_Beauty\">Sleeping Anti-Beauty</h2>\n<p class=\"MsoNoSpacing\"><a href=\"/lw/4e0/sleeping_antibeauty_and_the_presumptuous/\">Sleeping Anti-Beauty</a> is a thought experiment similar to the Sleeping Beauty experiment, but with one important caveat: the two copies in the tails world hate each other. This works best if the two copies are duplicates, rather than the same person at different times. One could imagine, for instance, that a week after the experiment, all copies of Sleeping Beauty are awakened and made to fight to the death - maybe they live in a civilization that prohibits more than one copy of an agent from existing. The single copy in the heads world will be left unmolested, as she has nobody to fight.</p>\n<p class=\"MsoNoSpacing\">That means that the Sleeping Beauties in the tail world are in a zero sum game; any gain for one is a loss for the other, and vice-versa. Actually, we need a few more assumptions for this to be true: the Sleeping Beauties have to be entirely selfish apart from their rivalry, and they do not get offered any goods for immediate consumption. Given all these assumptions, what does ADT have to say about their decisions?</p>\n<p class=\"MsoNoSpacing\">As usual, all existent copies of Sleeping Beauty are offered a coupon that pays out \u00a31 if the coin fell tails, and asked how much she would be willing to give for that. ADT reasoning proceeds for each agent as follows:</p>\n<p class=\"MsoNoSpacing\" style=\"padding-left: 30px;\">\u201cIn the heads world, if I pay \u00a3x, all that happens is that I lose \u00a3x. In the tails, world, if I pay \u00a3x, I gain \u00a3(1-x). However my other hated copy will make the same decision, and also gain \u00a3(1-x). This causes me the same amount of loss as the gain of \u00a3(1-x) does, so I gain nothing at all in the tails world, whatever \u00a3x is. So I value the coupon precisely at zero: I would not pay any amount to get it.\u201d</p>\n<p class=\"MsoNoSpacing\">In this, and other similar decisions, Sleeping Beauty would act as if she had an absolute certainty of being in the heads world, offering infinity to one odds of this being the case, as she cannot realise any gains - or losses! - in the tails world.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\">It should be noted that selfish Sleeping Beauty can be correctly modelled by seeing it as a 50-50 mix of selfless Sleeping Beauty and Sleeping Anti-Beauty.</p>\n<p>&nbsp;</p>", "sections": [{"title": "The Presumptuous Philosopher", "anchor": "The_Presumptuous_Philosopher", "level": 1}, {"title": "Adam and Eve", "anchor": "Adam_and_Eve", "level": 1}, {"title": "The Doomsday argument", "anchor": "The_Doomsday_argument", "level": 1}, {"title": "Sleeping Anti-Beauty", "anchor": "Sleeping_Anti_Beauty", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["svhbnSdxW3XmFXXTK", "G9scbpNrCxfJZQmYu", "zqm2nTqjAKg6EyxE3", "LXHsiHahr2eFQ4Hsp", "wQd3rLLLNWYRgjaYg", "NpPEJT2CCKjeo59Ps", "QKc9Kctw6kxJP9amc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-06T13:37:29.702Z", "modifiedAt": null, "url": null, "title": "Less Wrong and non-native English speakers", "slug": "less-wrong-and-non-native-english-speakers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.396Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kilobug", "createdAt": "2011-09-02T14:37:51.213Z", "isAdmin": false, "displayName": "kilobug"}, "userId": "7BQMuDSmLE2XRq2ph", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2RJRZtnbCafK2r4cb/less-wrong-and-non-native-english-speakers", "pageUrlRelative": "/posts/2RJRZtnbCafK2r4cb/less-wrong-and-non-native-english-speakers", "linkUrl": "https://www.lesswrong.com/posts/2RJRZtnbCafK2r4cb/less-wrong-and-non-native-english-speakers", "postedAtFormatted": "Sunday, November 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20and%20non-native%20English%20speakers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20and%20non-native%20English%20speakers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RJRZtnbCafK2r4cb%2Fless-wrong-and-non-native-english-speakers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20and%20non-native%20English%20speakers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RJRZtnbCafK2r4cb%2Fless-wrong-and-non-native-english-speakers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RJRZtnbCafK2r4cb%2Fless-wrong-and-non-native-english-speakers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 700, "htmlBody": "<p>Hello Less Wrongers.</p>\n<p>I'm still relatively new to the LW community, but I would like to share with you a few comments and ideas for making LW a better place for non-native English speakers.</p>\n<p>There are two classes of people among non-native English speakers (of course, those boundaries are fuzzy) :</p>\n<ol>\n<li>People who, like me, are relatively fluent in English, but not who don't have the same fluency at English as natives do.</li>\n<li>People who don't speak much English at all.</li>\n</ol>\n<p>The problems are of course different between 1. and 2., but yet I can see ways to improve things to both categories.</p>\n<h2>Moderately fluent English speakers<br /></h2>\n<p>Being a member of 1., here are my feelings after a few months of lurking and then trying to participate a bit in LW, from my own French pov :</p>\n<ol>\n<li>LW is still quite US-centric in many ways. That's not much of a problem, at least for me who is used to dealing with US citizen from IRC or other Internet places, but it still something to keep in mind. Political question of <a href=\"/lw/89a/2011_less_wrong_census_survey/\">Yvain's census/survey</a> (please Yvain don't take that personally, overall you're doing a great and useful thing with that survey, so thanks to you), your&nbsp; is a clear example of that, but it's much more general.</li>\n<li>Writing an article on LW is not easy for non-native English speakers. I tried twice, and twice I got many remarks about my English skills. I don't take them badly, thanks for those who took the time to point to my mistakes and explain them so I can improve, but still, it feels like it's harder to participate.</li>\n</ol>\n<p>I don't have any magical solution from 1., except for anyone to try to be more careful when stating things which are culture-dependant, but it's part of the most general problem of <a href=\"/lw/kg/expecting_short_inferential_distances\">excepting short inferential distances</a>.</p>\n<p>For 2., I'm wondering if it would be possible to have some LW to volunteer to review articles done by non-native English speakers, and improve the English quality, before the article is published to LW in general. Do you think the idea is good overall ? Would any of you volunteer to do that ? If so, it would be nice to include a paragraph about it, or at least a link to a page explaining the modality (how to submit an article to that team, ...), on the <a href=\"/lw/2ku/welcome_to_less_wrong_2010/\">Welcome to Less Wrong</a> page.</p>\n<h2>Non-English speakers</h2>\n<p>I don't think non-English speakers (or people with only basic English skills) can reasonably participate on LW itself, of course. But there are ways to still be able to offer them ways to become stronger, I'm thinking about translation.</p>\n<p>Right now I'm helping Adrien with the <a href=\"http://www.fanfiction.net/u/2842070/AdrienH\">French translation of HP:MoR</a>. There are also attempts to translate some parts of the Sequences into other languages. In the mirror way of the \"having native English speaker to help correct the English of non-native\", us the non-native can help by participating to the various translation efforts. But that give raise to several questions :</p>\n<ol>\n<li>What are the legal issues about translating HP:MoR and Sequences ? Since Eliezer is linking to the translations of HP:MoR from his fanfiction.net page, I guess he approves of them. But what of the Sequences ? It would be nice to have some official stance from him and other people writing in the Sequences to know what they feel about translations. I'm not a fan of the copyright system in general, but I still would consider an utter lack of respect to someone to translate his work against his will.</li>\n<li>How can the team be coordinated, and how can newcomers to Less Wrong know that efforts are underway and requiring help ? Once again, I think it would be nice to have some page (maybe on the wiki ?) with the undergoing efforts, who is participating in which, how to contact them, and have a link to it from the Welcome to Less Wrong page.</li>\n</ol>\n<p>Any opinion on those suggestions ? Any volunteer for joining some of the teams ? Anyone from \"the staff\" who could answer about the legal issues, and about the opportunity of including those pointers in the \"Welcome to Less Wrong\" page ?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2RJRZtnbCafK2r4cb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 36, "extendedScore": null, "score": 7.953473697513838e-07, "legacy": true, "legacyId": "10800", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Hello Less Wrongers.</p>\n<p>I'm still relatively new to the LW community, but I would like to share with you a few comments and ideas for making LW a better place for non-native English speakers.</p>\n<p>There are two classes of people among non-native English speakers (of course, those boundaries are fuzzy) :</p>\n<ol>\n<li>People who, like me, are relatively fluent in English, but not who don't have the same fluency at English as natives do.</li>\n<li>People who don't speak much English at all.</li>\n</ol>\n<p>The problems are of course different between 1. and 2., but yet I can see ways to improve things to both categories.</p>\n<h2 id=\"Moderately_fluent_English_speakers\">Moderately fluent English speakers<br></h2>\n<p>Being a member of 1., here are my feelings after a few months of lurking and then trying to participate a bit in LW, from my own French pov :</p>\n<ol>\n<li>LW is still quite US-centric in many ways. That's not much of a problem, at least for me who is used to dealing with US citizen from IRC or other Internet places, but it still something to keep in mind. Political question of <a href=\"/lw/89a/2011_less_wrong_census_survey/\">Yvain's census/survey</a> (please Yvain don't take that personally, overall you're doing a great and useful thing with that survey, so thanks to you), your&nbsp; is a clear example of that, but it's much more general.</li>\n<li>Writing an article on LW is not easy for non-native English speakers. I tried twice, and twice I got many remarks about my English skills. I don't take them badly, thanks for those who took the time to point to my mistakes and explain them so I can improve, but still, it feels like it's harder to participate.</li>\n</ol>\n<p>I don't have any magical solution from 1., except for anyone to try to be more careful when stating things which are culture-dependant, but it's part of the most general problem of <a href=\"/lw/kg/expecting_short_inferential_distances\">excepting short inferential distances</a>.</p>\n<p>For 2., I'm wondering if it would be possible to have some LW to volunteer to review articles done by non-native English speakers, and improve the English quality, before the article is published to LW in general. Do you think the idea is good overall ? Would any of you volunteer to do that ? If so, it would be nice to include a paragraph about it, or at least a link to a page explaining the modality (how to submit an article to that team, ...), on the <a href=\"/lw/2ku/welcome_to_less_wrong_2010/\">Welcome to Less Wrong</a> page.</p>\n<h2 id=\"Non_English_speakers\">Non-English speakers</h2>\n<p>I don't think non-English speakers (or people with only basic English skills) can reasonably participate on LW itself, of course. But there are ways to still be able to offer them ways to become stronger, I'm thinking about translation.</p>\n<p>Right now I'm helping Adrien with the <a href=\"http://www.fanfiction.net/u/2842070/AdrienH\">French translation of HP:MoR</a>. There are also attempts to translate some parts of the Sequences into other languages. In the mirror way of the \"having native English speaker to help correct the English of non-native\", us the non-native can help by participating to the various translation efforts. But that give raise to several questions :</p>\n<ol>\n<li>What are the legal issues about translating HP:MoR and Sequences ? Since Eliezer is linking to the translations of HP:MoR from his fanfiction.net page, I guess he approves of them. But what of the Sequences ? It would be nice to have some official stance from him and other people writing in the Sequences to know what they feel about translations. I'm not a fan of the copyright system in general, but I still would consider an utter lack of respect to someone to translate his work against his will.</li>\n<li>How can the team be coordinated, and how can newcomers to Less Wrong know that efforts are underway and requiring help ? Once again, I think it would be nice to have some page (maybe on the wiki ?) with the undergoing efforts, who is participating in which, how to contact them, and have a link to it from the Welcome to Less Wrong page.</li>\n</ol>\n<p>Any opinion on those suggestions ? Any volunteer for joining some of the teams ? Anyone from \"the staff\" who could answer about the legal issues, and about the opportunity of including those pointers in the \"Welcome to Less Wrong\" page ?</p>", "sections": [{"title": "Moderately fluent English speakers", "anchor": "Moderately_fluent_English_speakers", "level": 1}, {"title": "Non-English speakers", "anchor": "Non_English_speakers", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["miHttwTgajY2sjY3L", "HLqWn5LASfhhArZ7w", "hoh3ysTRDXJmcWjEH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T02:20:45.773Z", "modifiedAt": null, "url": null, "title": "Should You Make a Complete Map of Every Thought You Think?", "slug": "should-you-make-a-complete-map-of-every-thought-you-think", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ucnEJF27nSgDvYiho/should-you-make-a-complete-map-of-every-thought-you-think", "pageUrlRelative": "/posts/ucnEJF27nSgDvYiho/should-you-make-a-complete-map-of-every-thought-you-think", "linkUrl": "https://www.lesswrong.com/posts/ucnEJF27nSgDvYiho/should-you-make-a-complete-map-of-every-thought-you-think", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20You%20Make%20a%20Complete%20Map%20of%20Every%20Thought%20You%20Think%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20You%20Make%20a%20Complete%20Map%20of%20Every%20Thought%20You%20Think%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucnEJF27nSgDvYiho%2Fshould-you-make-a-complete-map-of-every-thought-you-think%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20You%20Make%20a%20Complete%20Map%20of%20Every%20Thought%20You%20Think%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucnEJF27nSgDvYiho%2Fshould-you-make-a-complete-map-of-every-thought-you-think", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FucnEJF27nSgDvYiho%2Fshould-you-make-a-complete-map-of-every-thought-you-think", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 383, "htmlBody": "<h6><strong>Related to: </strong><a href=\"/lw/1xh/living_luminously/\">Living Luminously</a></h6>\n<p><a href=\"http://www.speakeasy.org/~lion/nb/\">Well? Should you?</a></p>\n<p>Linked is a treatise on exactly this concept. If the effects of recording and classifying every thought pan out like the author says they'll pan out... well, read a (limited) excerpt (from the Introduction), and I'll let you decide whether it's worth your time.</p>\n<blockquote>\n<p>If you do the things described in this book, you will be&nbsp;<em>IMMOBILIZED</em>&nbsp;for the duration of your commitment.The immobilization will come on gradually, but steadily. In the end, you will be incapable of going somewhere without your cache of notes, and will always want a pen and paper w/ you. When you do not have pen and paper, you will rely on complex memory pegging devices, described in \"The Memory Book''. You will NEVER BE WITHOUT RECORD, and you will ALWAYS RECORD.</p>\n<p>YOU MAY ALSO ARTICULATE. Your thoughts will be clearer to you than they have ever been before. You will see things you have never seen before. When someone shows you one corner, you'll have the other 3 in mind. This is both good and bad. It means you will have the right information at the right time in the right place. It also means you may have trouble shutting up. Your mileage may vary.</p>\n<p>You will not only be immobilized in the arena of action, but you will also be immobilized in the arena of thought. This appears to be contradictory, but it's not really. When you are writing down your thoughts, you are making them clear to yourself, but when you revise your thoughts, it requires a lot of work - you have to update old ideas to point to new ideas. This discourages a lot of new thinking. There is also a \"structural integrity'' to your old thoughts that will resist change. You may actively not-think certain things, because it would demand a lot of note keeping work. (Thus the notion that notebooks are best applied to things that are not changing.)</p>\n</blockquote>\n<p>The full text is written in a stream-of-consciousness style, which is why I hesitated to post this topic in the first place. But there are probably note-taking junkies, or luminosity junkies, or otherwise interested folk amongst LW. So why not?</p>\n<p>(Incidentally I'm reminded of Buckminster Fuller's&nbsp;<a href=\"http://en.wikipedia.org/wiki/Dymaxion_Chronofile\">Dymaxion Chronofile</a>. I wonder how he managed it, or what benefits/costs it wrought?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ucnEJF27nSgDvYiho", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 1, "extendedScore": null, "score": 7.956148400495926e-07, "legacy": true, "legacyId": "10812", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9o3Cjjem7AbmmZfBs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T03:38:09.383Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Purpose and Pragmatism", "slug": "seq-rerun-purpose-and-pragmatism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mF5aiRKTmCjkDmpJd/seq-rerun-purpose-and-pragmatism", "pageUrlRelative": "/posts/mF5aiRKTmCjkDmpJd/seq-rerun-purpose-and-pragmatism", "linkUrl": "https://www.lesswrong.com/posts/mF5aiRKTmCjkDmpJd/seq-rerun-purpose-and-pragmatism", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Purpose%20and%20Pragmatism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Purpose%20and%20Pragmatism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmF5aiRKTmCjkDmpJd%2Fseq-rerun-purpose-and-pragmatism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Purpose%20and%20Pragmatism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmF5aiRKTmCjkDmpJd%2Fseq-rerun-purpose-and-pragmatism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmF5aiRKTmCjkDmpJd%2Fseq-rerun-purpose-and-pragmatism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/lf/purpose_and_pragmatism/\">Purpose and Pragmatism</a> was originally published on 26 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Purpose_and_Pragmatism\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is easier to get trapped in a mistake of cognition if you have no practical purpose for your thoughts. Although pragmatic usefulness is not the same thing as truth, there is a deep connection between the two.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8bz/seq_rerun_lost_purposes\">Lost Purposes</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mF5aiRKTmCjkDmpJd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 7.956419697433039e-07, "legacy": true, "legacyId": "10815", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AZfBrZfBu8Aa2FK9D", "FGxYL6tzCuiyjWXu5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T04:58:05.074Z", "modifiedAt": null, "url": null, "title": "Q&A with new Executive Director of Singularity Institute", "slug": "q-and-a-with-new-executive-director-of-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:58.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G65tLdGma8Xgh3p7L/q-and-a-with-new-executive-director-of-singularity-institute", "pageUrlRelative": "/posts/G65tLdGma8Xgh3p7L/q-and-a-with-new-executive-director-of-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/G65tLdGma8Xgh3p7L/q-and-a-with-new-executive-director-of-singularity-institute", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20new%20Executive%20Director%20of%20Singularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20new%20Executive%20Director%20of%20Singularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG65tLdGma8Xgh3p7L%2Fq-and-a-with-new-executive-director-of-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20new%20Executive%20Director%20of%20Singularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG65tLdGma8Xgh3p7L%2Fq-and-a-with-new-executive-director-of-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG65tLdGma8Xgh3p7L%2Fq-and-a-with-new-executive-director-of-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p>Today I was appointed the new Executive Director of Singularity Institute.</p>\n<p>Because I care about transparency, one of my first projects as an intern was to begin work on the organization's first <a href=\"http://intelligence.org/blog/2011/08/26/singularity-institute-strategic-plan-2011/\">Strategic Plan</a>. I researched how to write a strategic plan, tracked down the strategic plans of similar organizations, and met with each staff member, progressively iterating the document until it was something everyone could get behind.</p>\n<p>I quickly learned why there isn't more of this kind of thing: transparency is a lot of work! 100+ hours of work later, plus dozens of hours from others, and the&nbsp;strategic&nbsp;plan was finally finished and ratified by the board. It doesn't accomplish much by itself, but it's one important stepping stone in building an organization that is more productive, more trusted, and more likely to <a href=\"http://intelligence.org/overview/whyworktowardthesingularity\">help solve the world's biggest problems</a>.</p>\n<p>I spent two months as a researcher, and was then appointed Executive Director.</p>\n<p>In further pursuit of transparency, I'd like to answer (on video) submitted questions from the Less Wrong community <a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">just as Eliezer did two years ago</a>.</p>\n<p>&nbsp;</p>\n<p><strong>The Rules</strong></p>\n<p>1) One question per comment (to allow voting to carry more information about people's preferences).</p>\n<p>2) Try to be as clear and concise as possible. If your question can't be condensed into one paragraph, you should probably ask in a separate post. Make sure you have an actual question somewhere in there (you can bold it to make it easier to scan).</p>\n<p>3) I will generally answer the top-voted questions, but will skip some of them. I will tend to select <strong>questions about Singularity Institute as an organization</strong>, not about the technical details of some bit of research. You can read some of the details of the Friendly AI research program in my&nbsp;<a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">interview with Michael Anissimov</a>.</p>\n<p>4) If you reference certain things that are online in your question, provide a link.</p>\n<p>5) This thread will be open to questions and votes for 7 days, at which time I will decide which questions to begin recording video responses for.</p>\n<p>&nbsp;</p>\n<p>I might respond to certain questions within the comments thread and not on video; for example, when there is a one-word answer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DigEmY3RrF3XL5cwe": 1, "NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G65tLdGma8Xgh3p7L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 33, "extendedScore": null, "score": 7.956699897051172e-07, "legacy": true, "legacyId": "10803", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 182, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qyix5Z5YPSGYxf7GG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T13:37:54.826Z", "modifiedAt": null, "url": null, "title": "Felicifia: a Utilitarianism Forum", "slug": "felicifia-a-utilitarianism-forum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:33.959Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2NEv7k9Qc8e9mpDwY/felicifia-a-utilitarianism-forum", "pageUrlRelative": "/posts/2NEv7k9Qc8e9mpDwY/felicifia-a-utilitarianism-forum", "linkUrl": "https://www.lesswrong.com/posts/2NEv7k9Qc8e9mpDwY/felicifia-a-utilitarianism-forum", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Felicifia%3A%20a%20Utilitarianism%20Forum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFelicifia%3A%20a%20Utilitarianism%20Forum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NEv7k9Qc8e9mpDwY%2Ffelicifia-a-utilitarianism-forum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Felicifia%3A%20a%20Utilitarianism%20Forum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NEv7k9Qc8e9mpDwY%2Ffelicifia-a-utilitarianism-forum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2NEv7k9Qc8e9mpDwY%2Ffelicifia-a-utilitarianism-forum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>Utilitarianism seems to be a common theme on this site. I suggest checking out <a href=\"http://felicifia.org\">felicifia.org</a><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 22px; text-align: justify;\">,</span>&nbsp;a Utilitarianism forum. That is all.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2NEv7k9Qc8e9mpDwY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 13, "extendedScore": null, "score": 7.958520956463982e-07, "legacy": true, "legacyId": "10772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T14:05:08.985Z", "modifiedAt": null, "url": null, "title": "Biointelligence Explosion", "slug": "biointelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:25.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EMGybRcbJhhBs8GwS/biointelligence-explosion", "pageUrlRelative": "/posts/EMGybRcbJhhBs8GwS/biointelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/EMGybRcbJhhBs8GwS/biointelligence-explosion", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Biointelligence%20Explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABiointelligence%20Explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMGybRcbJhhBs8GwS%2Fbiointelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Biointelligence%20Explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMGybRcbJhhBs8GwS%2Fbiointelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMGybRcbJhhBs8GwS%2Fbiointelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 17, "htmlBody": "<p><a href=\"http://biointelligence-explosion.com/\">http://biointelligence-explosion.com/</a></p>\n<p>- Site put together by David Pearce</p>\n<p>The content and choice of domain name should be of interest.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EMGybRcbJhhBs8GwS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 6, "extendedScore": null, "score": 1.1e-05, "legacy": true, "legacyId": "10804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T14:14:15.721Z", "modifiedAt": null, "url": null, "title": "Singularity Institute mentioned on Franco-German TV", "slug": "singularity-institute-mentioned-on-franco-german-tv", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.250Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5toz9mT5aDBPXpsTT/singularity-institute-mentioned-on-franco-german-tv", "pageUrlRelative": "/posts/5toz9mT5aDBPXpsTT/singularity-institute-mentioned-on-franco-german-tv", "linkUrl": "https://www.lesswrong.com/posts/5toz9mT5aDBPXpsTT/singularity-institute-mentioned-on-franco-german-tv", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20mentioned%20on%20Franco-German%20TV&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20mentioned%20on%20Franco-German%20TV%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5toz9mT5aDBPXpsTT%2Fsingularity-institute-mentioned-on-franco-german-tv%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20mentioned%20on%20Franco-German%20TV%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5toz9mT5aDBPXpsTT%2Fsingularity-institute-mentioned-on-franco-german-tv", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5toz9mT5aDBPXpsTT%2Fsingularity-institute-mentioned-on-franco-german-tv", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 269, "htmlBody": "<p>The following is a clipping of a <a href=\"http://www.arte.tv/de/suche/4135792.html\">documentary about transhumanism</a> that I recorded when it aired on <a href=\"http://en.wikipedia.org/wiki/Arte\">Arte</a>, September 22 2011.</p>\n<p>At the beginning and end of the video Luke Muehlhauser and Michael Anissimov give a short commentary.</p>\n<p><strong><span style=\"color: #ff0000;\">Download here: <a href=\"http://kruel.co/arte-siai.flv\">German</a>, <a href=\"http://kruel.co/arte-siai-french.flv\">French</a></span></strong> (ask for HD download link). Should play with <a href=\"http://www.videolan.org/vlc/\">VLC player</a>.</p>\n<p>\n<object width=\"580\" height=\"326\" data=\"http://vimeo.com/moogaloop.swf?clip_id=31730148&amp;server=vimeo.com&amp;show_title=0&amp;show_byline=0&amp;show_portrait=0&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowfullscreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://vimeo.com/moogaloop.swf?clip_id=31730148&amp;server=vimeo.com&amp;show_title=0&amp;show_byline=0&amp;show_portrait=0&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\" />\n</object>\n</p>\n<p>Sadly, the people who produced the show seemed to be somewhat confused about the agenda of the Singularity Institute. At one point they seem to be saying that the SIAI believes into \"the good in the machines\", adding <em>\"how naive!\"</em>, while the next sentence talks about how the SIAI tries to figure out how to make machines respect humans.</p>\n<p>Here is the original part of the clip that I am talking about:</p>\n<blockquote>\n<p>In San Francisco glaubt eine Vereinigung ehrenamtlicher junger Wissenschaftler dennoch an das Gute im Roboter. Wie naiv! Hier im Singularity Institute, dass Kontakte zu den gro&szlig;en Unis wie Oxford hat, zerbricht man sich den Kopf dar&uuml;ber, wie man zuk&uuml;nftigen Formen k&uuml;nstlicher Intelligenz beibringt, den Menschen zu respektieren. <br /><br />Die Forscher kombinieren Daten aus Informatik und psychologischen Studien. <a href=\"http://www.youtube.com/watch?v=0A9pGhwQbS0\" target=\"_blank\">Ihr Ziel</a>: Eine Not-to-do-Liste, die jedes Unternehmen bekommt, das an k&uuml;nstlicher Intelligenz arbeitet.</p>\n</blockquote>\n<p>My translation:</p>\n<blockquote>\n<p>In San Francisco however, a society of young voluntary scientists believes in the good in robots. How naive! Here at the Singularity Institute, which has a connection to big universities like Oxford, they think about how to teach future artificial intelligences to respect humans.</p>\n</blockquote>\n<p>I am a native German speaker by the way, maybe someone else who speaks German can make more sense of it (and is willing to translate the whole clip).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5toz9mT5aDBPXpsTT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 7.958650139676975e-07, "legacy": true, "legacyId": "10817", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T19:00:17.009Z", "modifiedAt": null, "url": null, "title": "Meetup : Ottawa Meetup - Computational Complexity", "slug": "meetup-ottawa-meetup-computational-complexity", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Eagxb9kWsEeSJ2KgC/meetup-ottawa-meetup-computational-complexity", "pageUrlRelative": "/posts/Eagxb9kWsEeSJ2KgC/meetup-ottawa-meetup-computational-complexity", "linkUrl": "https://www.lesswrong.com/posts/Eagxb9kWsEeSJ2KgC/meetup-ottawa-meetup-computational-complexity", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ottawa%20Meetup%20-%20Computational%20Complexity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ottawa%20Meetup%20-%20Computational%20Complexity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEagxb9kWsEeSJ2KgC%2Fmeetup-ottawa-meetup-computational-complexity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ottawa%20Meetup%20-%20Computational%20Complexity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEagxb9kWsEeSJ2KgC%2Fmeetup-ottawa-meetup-computational-complexity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEagxb9kWsEeSJ2KgC%2Fmeetup-ottawa-meetup-computational-complexity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 64, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4o'>Ottawa Meetup - Computational Complexity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 November 2011 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Elgin & Gladstone, Ottawa Ontario Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Today, we'll discuss the first three chapters of Scott Aaronson's  <a href=\"http://eccc.hpi-web.de/report/2011/108/\" rel=\"nofollow\">Why Philosophers Should Care About Computational Complexity</a>.</p>\n\n<p>There is also a high probability of playing some <a href=\"http://en.wikipedia.org/wiki/Set_%28game%29\" rel=\"nofollow\">Set</a>.</p>\n\n<p>Join our <a href=\"http://groups.google.com/group/less-wrong-ottawa?pli=1\" rel=\"nofollow\">Google Group</a> for specifics!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4o'>Ottawa Meetup - Computational Complexity</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Eagxb9kWsEeSJ2KgC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.959653406628993e-07, "legacy": true, "legacyId": "10818", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Meetup___Computational_Complexity\">Discussion article for the meetup : <a href=\"/meetups/4o\">Ottawa Meetup - Computational Complexity</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 November 2011 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Elgin &amp; Gladstone, Ottawa Ontario Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Today, we'll discuss the first three chapters of Scott Aaronson's  <a href=\"http://eccc.hpi-web.de/report/2011/108/\" rel=\"nofollow\">Why Philosophers Should Care About Computational Complexity</a>.</p>\n\n<p>There is also a high probability of playing some <a href=\"http://en.wikipedia.org/wiki/Set_%28game%29\" rel=\"nofollow\">Set</a>.</p>\n\n<p>Join our <a href=\"http://groups.google.com/group/less-wrong-ottawa?pli=1\" rel=\"nofollow\">Google Group</a> for specifics!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Meetup___Computational_Complexity1\">Discussion article for the meetup : <a href=\"/meetups/4o\">Ottawa Meetup - Computational Complexity</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ottawa Meetup - Computational Complexity", "anchor": "Discussion_article_for_the_meetup___Ottawa_Meetup___Computational_Complexity", "level": 1}, {"title": "Discussion article for the meetup : Ottawa Meetup - Computational Complexity", "anchor": "Discussion_article_for_the_meetup___Ottawa_Meetup___Computational_Complexity1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T19:49:31.112Z", "modifiedAt": null, "url": null, "title": "Spencer Greenberg's TEDx talk: \"Improve Your Life with Probability\"", "slug": "spencer-greenberg-s-tedx-talk-improve-your-life-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xn2PvMcr9FmcQFDiu/spencer-greenberg-s-tedx-talk-improve-your-life-with", "pageUrlRelative": "/posts/Xn2PvMcr9FmcQFDiu/spencer-greenberg-s-tedx-talk-improve-your-life-with", "linkUrl": "https://www.lesswrong.com/posts/Xn2PvMcr9FmcQFDiu/spencer-greenberg-s-tedx-talk-improve-your-life-with", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spencer%20Greenberg's%20TEDx%20talk%3A%20%22Improve%20Your%20Life%20with%20Probability%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpencer%20Greenberg's%20TEDx%20talk%3A%20%22Improve%20Your%20Life%20with%20Probability%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXn2PvMcr9FmcQFDiu%2Fspencer-greenberg-s-tedx-talk-improve-your-life-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spencer%20Greenberg's%20TEDx%20talk%3A%20%22Improve%20Your%20Life%20with%20Probability%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXn2PvMcr9FmcQFDiu%2Fspencer-greenberg-s-tedx-talk-improve-your-life-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXn2PvMcr9FmcQFDiu%2Fspencer-greenberg-s-tedx-talk-improve-your-life-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11, "htmlBody": "<p><a href=\"http://www.youtube.com/watch?v=GZ69g8LtZc0\">Video link</a>.</p>\n<p>This kind of material is regularly featured on Spencer's&nbsp;<a href=\"http://www.spencergreenberg.com/\">blog</a>, too.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xn2PvMcr9FmcQFDiu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 7.959826129144016e-07, "legacy": true, "legacyId": "10819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T23:05:21.928Z", "modifiedAt": null, "url": null, "title": "SI and Social Business", "slug": "si-and-social-business-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Roy", "createdAt": "2010-02-13T02:57:14.500Z", "isAdmin": false, "displayName": "Nick_Roy"}, "userId": "LouYAbtgk7xDBDhZA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8GdaLHtNghGDEYy6C/si-and-social-business-0", "pageUrlRelative": "/posts/8GdaLHtNghGDEYy6C/si-and-social-business-0", "linkUrl": "https://www.lesswrong.com/posts/8GdaLHtNghGDEYy6C/si-and-social-business-0", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%20and%20Social%20Business&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%20and%20Social%20Business%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GdaLHtNghGDEYy6C%2Fsi-and-social-business-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%20and%20Social%20Business%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GdaLHtNghGDEYy6C%2Fsi-and-social-business-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8GdaLHtNghGDEYy6C%2Fsi-and-social-business-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 174, "htmlBody": "<p>I asked this question for the [Q&amp;A](http://lesswrong.com/r/discussion/lw/8c3/qa_with_new_executive_director_of_singularity/):</p>\n<p>&gt;Non-profit organizations like SI need robust, sustainable resource strategies. Donations and grants are not reliable. According to my university Social Entrepreneurship course, <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Social_business\">social businesses</a> are the best resource strategy available. The Singularity Summit is a profitable and expanding example of a social business. Is SI planning on creating more social businesses (either related or unrelated to the organization's mission) to address long-term funding needs?</p>\n<p>I also recently asked this of [Luke](http://lesswrong.com/user/lukeprog/) for his [feedback post](http://lesswrong.com/r/discussion/lw/8bt/tell_me_what_you_think_of_me/) before the Q&amp;A was up, and he mentioned in his response that SI is continuing to grow the Summit brand in a multifarious manner. Luke also asked me for additional social business ideas, citing a lack of staff working on the issue.</p>\n<p>Less Wrong's collective intelligence trumps my own, so I'm fielding it to you. I do have a few ideas, but I'll [hold off on proposing solutions](http://lesswrong.com/lw/ka/hold_off_on_proposing_solutions/) at first. I find that this is a fascinating and difficult thought experiment in addition to its usefulness both for SI and as practice in recognizing opportunities.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8GdaLHtNghGDEYy6C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "10821", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T23:08:29.433Z", "modifiedAt": null, "url": null, "title": "SI and Social Business", "slug": "si-and-social-business-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Roy", "createdAt": "2010-02-13T02:57:14.500Z", "isAdmin": false, "displayName": "Nick_Roy"}, "userId": "LouYAbtgk7xDBDhZA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/m6YWmz4tq2oE54Ttn/si-and-social-business-2", "pageUrlRelative": "/posts/m6YWmz4tq2oE54Ttn/si-and-social-business-2", "linkUrl": "https://www.lesswrong.com/posts/m6YWmz4tq2oE54Ttn/si-and-social-business-2", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%20and%20Social%20Business&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%20and%20Social%20Business%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm6YWmz4tq2oE54Ttn%2Fsi-and-social-business-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%20and%20Social%20Business%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm6YWmz4tq2oE54Ttn%2Fsi-and-social-business-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fm6YWmz4tq2oE54Ttn%2Fsi-and-social-business-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<div id=\"entry_t3_8cl\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div id=\"body_t1_56yl\" class=\"comment-content \">\n<div class=\"md\">\n<p>I asked this question for the <a href=\"../lw/8c3/qa_with_new_executive_director_of_singularity/\">Q&amp;A</a>:</p>\n<blockquote>\n<p>Non-profit organizations like SI need robust, sustainable resource strategies. Donations and grants are not reliable. According to my university Social Entrepreneurship course, social businesses are the best resource strategy available. The Singularity Summit is a profitable and expanding example of a social business. Is SI planning on creating more social businesses (either related or unrelated to the organization's mission) to address long-term funding needs?</p>\n</blockquote>\n<p>I also recently asked this of <a href=\"../../../user/lukeprog/\">Luke</a> for his <a href=\"../lw/8bt/tell_me_what_you_think_of_me/\">feedback post</a> before the Q&amp;A was up, and he mentioned in his response that SI is continuing to grow the Summit brand in a multifarious manner. Luke also asked me for additional social business ideas, citing a lack of staff working on the issue.</p>\n<p>Less Wrong's collective intelligence trumps my own, so I'm fielding it to you. I do have a few ideas, but I'll <a href=\"../../../lw/ka/hold_off_on_proposing_solutions/\">hold off on proposing solutions</a> at first. I find that this is a fascinating and difficult thought experiment in addition to its usefulness both for SI and as practice in recognizing opportunities.</p>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "m6YWmz4tq2oE54Ttn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "10822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G65tLdGma8Xgh3p7L", "zFj67rtrQ7HEaZ45F", "uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T23:25:53.204Z", "modifiedAt": null, "url": null, "title": "SI and Social Business", "slug": "si-and-social-business", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nick_Roy", "createdAt": "2010-02-13T02:57:14.500Z", "isAdmin": false, "displayName": "Nick_Roy"}, "userId": "LouYAbtgk7xDBDhZA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cGEDjNitzjMxHGhkx/si-and-social-business", "pageUrlRelative": "/posts/cGEDjNitzjMxHGhkx/si-and-social-business", "linkUrl": "https://www.lesswrong.com/posts/cGEDjNitzjMxHGhkx/si-and-social-business", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%20and%20Social%20Business&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%20and%20Social%20Business%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcGEDjNitzjMxHGhkx%2Fsi-and-social-business%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%20and%20Social%20Business%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcGEDjNitzjMxHGhkx%2Fsi-and-social-business", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcGEDjNitzjMxHGhkx%2Fsi-and-social-business", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<div id=\"body_t1_56yp\" class=\"comment-content \">\n<div class=\"md\">\n<p>I asked this question for the <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity\">Q&amp;A</a>:</p>\n<blockquote>\n<p>Non-profit organizations like SI need robust, sustainable resource strategies. Donations and grants are not reliable. According to my university Social Entrepreneurship course, <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Social_business\">social businesses</a> are the best resource strategy available. The Singularity Summit is a profitable and expanding example of a social business. Is SI planning on creating more social businesses (either related or unrelated to the organization's mission) to address long-term funding needs?</p>\n</blockquote>\n<p>I also recently asked this of <a href=\"/user/lukeprog\">Luke</a> for his <a href=\"/lw/8bt/tell_me_what_you_think_of_me\">feedback post</a> before the Q&amp;A was up, and he mentioned in his response that SI is continuing to grow the Summit brand in a multifarious manner. Luke also asked me for additional social business ideas, citing a lack of staff working on the issue.</p>\n<p>Less Wrong's collective intelligence trumps my own, so I'm fielding it to you. I do have a few ideas, but I'll <a href=\"/lw/ka/hold_off_on_proposing_solutions\">hold off on proposing solutions</a> at first. I find that this is a fascinating and difficult thought experiment in addition to its usefulness both for SI and as practice in recognizing opportunities.</p>\n<strong>Edited to add: I posted my own ideas concerning SI and social business in the comments. What are yours? Also, addressing some valid points made in the comments, what are some other innovative ways to fund SI?</strong></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cGEDjNitzjMxHGhkx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 7.960585253553048e-07, "legacy": true, "legacyId": "10824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G65tLdGma8Xgh3p7L", "zFj67rtrQ7HEaZ45F", "uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-07T23:39:30.527Z", "modifiedAt": null, "url": null, "title": "Any thoughts on how to locate job opportunities in Europe for US Citizen?", "slug": "any-thoughts-on-how-to-locate-job-opportunities-in-europe", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.863Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zuZa3kCKzKn3z8ybs/any-thoughts-on-how-to-locate-job-opportunities-in-europe", "pageUrlRelative": "/posts/zuZa3kCKzKn3z8ybs/any-thoughts-on-how-to-locate-job-opportunities-in-europe", "linkUrl": "https://www.lesswrong.com/posts/zuZa3kCKzKn3z8ybs/any-thoughts-on-how-to-locate-job-opportunities-in-europe", "postedAtFormatted": "Monday, November 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Any%20thoughts%20on%20how%20to%20locate%20job%20opportunities%20in%20Europe%20for%20US%20Citizen%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAny%20thoughts%20on%20how%20to%20locate%20job%20opportunities%20in%20Europe%20for%20US%20Citizen%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuZa3kCKzKn3z8ybs%2Fany-thoughts-on-how-to-locate-job-opportunities-in-europe%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Any%20thoughts%20on%20how%20to%20locate%20job%20opportunities%20in%20Europe%20for%20US%20Citizen%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuZa3kCKzKn3z8ybs%2Fany-thoughts-on-how-to-locate-job-opportunities-in-europe", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuZa3kCKzKn3z8ybs%2Fany-thoughts-on-how-to-locate-job-opportunities-in-europe", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 288, "htmlBody": "<p>I am a graduate student in the U.S. nearing the completion of my master's degree in applied mathematics. I am looking for jobs at the master's degree level that involve working on large data sets, doing machine learning, scientific computing, etc. I don't want to be a software developer <em>per se, </em>but I don't mind doing software development among other scientific tasks.</p>\n<p>For comparison, I used to work at MIT Lincoln Laboratory as an assistant radar analyst. It was a very very good mix of theoretical work, algorithmic development, and plain software development. I'm looking for similar institutions in Europe that are looking to hire full-time employees at the master's degree level. I've considered CERN (and may still apply there) but it appears to be either too much purely experimental physics or plain software development for me.</p>\n<p>How do I locate these kinds of institutions, and what processes should I undertake to find such a job in Europe? What kinds of things should I look for in terms of job security, visa issues (I am only a US citizen), etc? I have worked for an extended time in Paris once before and loved living there (it was as a visiting research assistant as part of my current grad program). I'm looking to replicate that experience but in a full-time, non-student position.<br /><br />Note: I've already done the standard many hours of Googling to find obvious job search web pages and this has proved extremely unsuccessful. My university career services office also said they do not support overseas job searching beyond whatever is listed in their web recruiting interface. All of the jobs that are overseas in that interface are banking-related, which is a field I want to avoid.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zuZa3kCKzKn3z8ybs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -1, "extendedScore": null, "score": 7.96063305042897e-07, "legacy": true, "legacyId": "10825", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T01:41:18.679Z", "modifiedAt": null, "url": null, "title": "LW November Diplomacy", "slug": "lw-november-diplomacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.279Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pqCmXhfFtqo3PKiB9/lw-november-diplomacy", "pageUrlRelative": "/posts/pqCmXhfFtqo3PKiB9/lw-november-diplomacy", "linkUrl": "https://www.lesswrong.com/posts/pqCmXhfFtqo3PKiB9/lw-november-diplomacy", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20November%20Diplomacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20November%20Diplomacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqCmXhfFtqo3PKiB9%2Flw-november-diplomacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20November%20Diplomacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqCmXhfFtqo3PKiB9%2Flw-november-diplomacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpqCmXhfFtqo3PKiB9%2Flw-november-diplomacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 170, "htmlBody": "<p>The <a href=\"http://webdiplomacy.net/board.php?gameID=66933\">LW September Diplomacy game</a> <a href=\"/r/discussion/lw/7ec/lw_september_webdiplomacy_games_starting_soon/56z3\">has come to an end</a>. Congratulations to <a href=\"/user/Prismattic/\">Prismattic</a>, <a href=\"/user/prase\">prase</a>, and <a href=\"/user/GuySrinivasan/\">GuySrinivasan</a>! It's time to see if we can start up another game.</p>\n<p>If you would like to get in on a Diplomacy game with other LW users,  please respond with a comment expressing your desire (between 1 and 0)  to play in a game based on its turn length: 24, 36, 48, or 72 hours.  (You don't need to make 0s explicit.) For example, if you can't play in a  24 hour game, could play in a 36 hour game or 72 hour game, but really  want to play in a 48 hour game, then you might comment with something  like \"(0, .8, 1, .6)\" or \"24: 1; 36: .8; 72: .6\". Please express your  interest by midnight on <strong>11/13</strong> so we can get any games started on the 14th.</p>\n<p>If November isn't a good month for you, but you'd like to make sure I  make another Diplomacy thread in December, leave a comment to that  effect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pqCmXhfFtqo3PKiB9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 7.961060453162209e-07, "legacy": true, "legacyId": "10830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T03:10:34.668Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Affect Heuristic", "slug": "seq-rerun-the-affect-heuristic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:20.237Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NXqFEHtibXBCNEfSp/seq-rerun-the-affect-heuristic", "pageUrlRelative": "/posts/NXqFEHtibXBCNEfSp/seq-rerun-the-affect-heuristic", "linkUrl": "https://www.lesswrong.com/posts/NXqFEHtibXBCNEfSp/seq-rerun-the-affect-heuristic", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Affect%20Heuristic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Affect%20Heuristic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXqFEHtibXBCNEfSp%2Fseq-rerun-the-affect-heuristic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Affect%20Heuristic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXqFEHtibXBCNEfSp%2Fseq-rerun-the-affect-heuristic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNXqFEHtibXBCNEfSp%2Fseq-rerun-the-affect-heuristic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"/lw/lg/the_affect_heuristic/\">The Affect Heuristic</a> was originally published on 27 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Positive and negative emotional impressions exert a greater effect on many decisions than does rational analysis.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8cf/seq_rerun_purpose_and_pragmatism/\">Purpose and Pragmatism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NXqFEHtibXBCNEfSp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 7.96137371320833e-07, "legacy": true, "legacyId": "10834", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kow8xRzpfkoY7pa69", "mF5aiRKTmCjkDmpJd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T09:37:41.875Z", "modifiedAt": null, "url": null, "title": "Query the LessWrong Hivemind", "slug": "query-the-lesswrong-hivemind", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.525Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ky5mhm5KMPRT3wyNn/query-the-lesswrong-hivemind", "pageUrlRelative": "/posts/ky5mhm5KMPRT3wyNn/query-the-lesswrong-hivemind", "linkUrl": "https://www.lesswrong.com/posts/ky5mhm5KMPRT3wyNn/query-the-lesswrong-hivemind", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Query%20the%20LessWrong%20Hivemind&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuery%20the%20LessWrong%20Hivemind%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fky5mhm5KMPRT3wyNn%2Fquery-the-lesswrong-hivemind%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Query%20the%20LessWrong%20Hivemind%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fky5mhm5KMPRT3wyNn%2Fquery-the-lesswrong-hivemind", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fky5mhm5KMPRT3wyNn%2Fquery-the-lesswrong-hivemind", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 351, "htmlBody": "<p>Often, there are questions you want to know the answers to. You want other people's opinions, because knowing the answer isn't worth the time you'd have to spend to find it, or you're unsure whether your answer is right.</p>\n<p>LW seems like a good place to ask these questions because the people here are pretty rational. So, in this thread: <strong>You post a top-level comment with some question. Other people reply to your comment with their answers</strong>. You upvote answers that you agree with and questions whose answers you'd like to know.</p>\n<p>&nbsp;</p>\n<p>A few (mostly obvious) guidelines:</p>\n<p>For questions:</p>\n<ul>\n<li>Your question should probably be in one of the following forms: \n<ul>\n<li>Asking for the probability some proposition is true.</li>\n<li>Asking for a confidence interval.</li>\n</ul>\n</li>\n<li>Be specific. Don't ask when the singularity will happen unless you define 'singularity' to reasonable precision.</li>\n<li>If you have several questions, post each separately, unless they're strongly related.</li>\n</ul>\n<p>For answers:</p>\n<ul>\n<li>Give what the question asks for, be it a probability or a confidence interval or something else. Try to give numbers.</li>\n<li><strong>Give some indication of how good your map is</strong>, i.e why is your answer that? If you want, give links.</li>\n<li>If you think you know the answer to your own question, you can post it.</li>\n<li>If you want to, give more information. For instance, if someone asks whether it's a good idea to brush their teeth, you can include info about flossing.</li>\n<li>If you've researched something well but don't feel like typing up a long justification of your opinions, that's fine. Rather give your opinion without detailed arguments than give nothing at all. You can always flesh your answer out later, or never.</li>\n</ul>\n<p>&nbsp;</p>\n<p>This thread is primarily for getting the hivemind's opinions on things, not for debating probabilities of propositions. Debating is also okay, though, especially since it will help question-posters to make up their minds.</p>\n<p>Don't be too squeamish about breaking the question-answer format.</p>\n<p>This is a followup to <a href=\"/r/discussion/lw/8ad/open_thread_november_2011/55oz\">my comment</a> in the open thread.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ky5mhm5KMPRT3wyNn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 7.962732476033727e-07, "legacy": true, "legacyId": "10836", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 90, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T09:46:02.546Z", "modifiedAt": null, "url": null, "title": "Low legibility of Cognitive Reflection Test dramatically improves performance?", "slug": "low-legibility-of-cognitive-reflection-test-dramatically", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.646Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "uzalud", "createdAt": "2011-07-16T12:16:46.510Z", "isAdmin": false, "displayName": "uzalud"}, "userId": "LtYeek58ACZWdRy8n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9HkCYH4XbkMZWKu7Y/low-legibility-of-cognitive-reflection-test-dramatically", "pageUrlRelative": "/posts/9HkCYH4XbkMZWKu7Y/low-legibility-of-cognitive-reflection-test-dramatically", "linkUrl": "https://www.lesswrong.com/posts/9HkCYH4XbkMZWKu7Y/low-legibility-of-cognitive-reflection-test-dramatically", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Low%20legibility%20of%20Cognitive%20Reflection%20Test%20dramatically%20improves%20performance%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALow%20legibility%20of%20Cognitive%20Reflection%20Test%20dramatically%20improves%20performance%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9HkCYH4XbkMZWKu7Y%2Flow-legibility-of-cognitive-reflection-test-dramatically%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Low%20legibility%20of%20Cognitive%20Reflection%20Test%20dramatically%20improves%20performance%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9HkCYH4XbkMZWKu7Y%2Flow-legibility-of-cognitive-reflection-test-dramatically", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9HkCYH4XbkMZWKu7Y%2Flow-legibility-of-cognitive-reflection-test-dramatically", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>I'm reading Kahneman's Thinking, Fast and Slow and I've stopped on this:</p>\n<blockquote>\n<p>90% of the students who saw the CRT in normal font made at least one mistake in the test, but the proportion dropped to 35% when the font was barely legible. You read this correctly: performance was better with the bad font.</p>\n</blockquote>\n<p>This seems like an important finding, but I can't find references in the book (Kindle) or on the Web. Does anybody know any real evidence for this claim? <em>EDIT: I found <a href=\"http://pages.stern.nyu.edu/~aalter/intuitive.pdf\">the original paper</a></em></p>\n<p>Do you think that people could behave rationally with such a simple intervention?</p>\n<p><a href=\"http://whilecharliesleeps.blogspot.com/2006/03/take-cognitive-reflection-test.html\">simple intro to CRT</a></p>\n<p><em>EDIT: fixed spelling in title</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9HkCYH4XbkMZWKu7Y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 16, "extendedScore": null, "score": 7.962761768486899e-07, "legacy": true, "legacyId": "10837", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T21:41:01.852Z", "modifiedAt": null, "url": null, "title": "Looking for Article", "slug": "looking-for-article", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.989Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cody", "createdAt": "2011-11-03T05:47:09.337Z", "isAdmin": false, "displayName": "Cody"}, "userId": "76WhKjGvL2de2jC4P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/drdkcHzCNbFGuiDy3/looking-for-article", "pageUrlRelative": "/posts/drdkcHzCNbFGuiDy3/looking-for-article", "linkUrl": "https://www.lesswrong.com/posts/drdkcHzCNbFGuiDy3/looking-for-article", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20Article&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20Article%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrdkcHzCNbFGuiDy3%2Flooking-for-article%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20Article%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrdkcHzCNbFGuiDy3%2Flooking-for-article", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdrdkcHzCNbFGuiDy3%2Flooking-for-article", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 30, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">I can't find the article that references the study about bosses having their subordinates write ideas down instead of announcing them in turn. &nbsp;Does anybody remember which post it's in?&nbsp;</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "drdkcHzCNbFGuiDy3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 7.965272420319501e-07, "legacy": true, "legacyId": "10839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-08T22:30:47.029Z", "modifiedAt": null, "url": null, "title": "Science of human dominance?", "slug": "science-of-human-dominance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M5SPNt36fYY565iip/science-of-human-dominance", "pageUrlRelative": "/posts/M5SPNt36fYY565iip/science-of-human-dominance", "linkUrl": "https://www.lesswrong.com/posts/M5SPNt36fYY565iip/science-of-human-dominance", "postedAtFormatted": "Tuesday, November 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%20of%20human%20dominance%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%20of%20human%20dominance%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5SPNt36fYY565iip%2Fscience-of-human-dominance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%20of%20human%20dominance%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5SPNt36fYY565iip%2Fscience-of-human-dominance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM5SPNt36fYY565iip%2Fscience-of-human-dominance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p>I'm trying to do some research related to human dominance including social signaling and how dominance is both successfully and unsuccessfully challenged. &nbsp;Ideally I'd like to find what the common factors are rather than having it be too particular to one community or another. &nbsp;Unfortunately everything I can find on the topic is either about dominance behavior of other primates or is ad-hoc self-help advice by self-proclaimed gurus of social power.</p>\n<p>Can anyone point me in the direction of the <em>science</em>&nbsp;of <em>human</em>&nbsp;dominance behavior?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M5SPNt36fYY565iip", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 7.965445473246294e-07, "legacy": true, "legacyId": "10840", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-09T00:49:58.184Z", "modifiedAt": null, "url": null, "title": "Michael Lewis on Kahneman and Tversky! [link]", "slug": "michael-lewis-on-kahneman-and-tversky-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:21.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fzv4zcjXLeHPP22Aw/michael-lewis-on-kahneman-and-tversky-link", "pageUrlRelative": "/posts/fzv4zcjXLeHPP22Aw/michael-lewis-on-kahneman-and-tversky-link", "linkUrl": "https://www.lesswrong.com/posts/fzv4zcjXLeHPP22Aw/michael-lewis-on-kahneman-and-tversky-link", "postedAtFormatted": "Wednesday, November 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Michael%20Lewis%20on%20Kahneman%20and%20Tversky!%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMichael%20Lewis%20on%20Kahneman%20and%20Tversky!%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffzv4zcjXLeHPP22Aw%2Fmichael-lewis-on-kahneman-and-tversky-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Michael%20Lewis%20on%20Kahneman%20and%20Tversky!%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffzv4zcjXLeHPP22Aw%2Fmichael-lewis-on-kahneman-and-tversky-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffzv4zcjXLeHPP22Aw%2Fmichael-lewis-on-kahneman-and-tversky-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.vanityfair.com/business/features/2011/12/michael-lewis-201112.print\">http://www.vanityfair.com/business/features/2011/12/michael-lewis-201112.print</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fzv4zcjXLeHPP22Aw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 7.965936105023434e-07, "legacy": true, "legacyId": "10841", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-09T02:59:52.728Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Evaluability (And Cheap Holiday Shopping)", "slug": "seq-rerun-evaluability-and-cheap-holiday-shopping", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:21.935Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wfg9AaLSoPAEsF8EY/seq-rerun-evaluability-and-cheap-holiday-shopping", "pageUrlRelative": "/posts/Wfg9AaLSoPAEsF8EY/seq-rerun-evaluability-and-cheap-holiday-shopping", "linkUrl": "https://www.lesswrong.com/posts/Wfg9AaLSoPAEsF8EY/seq-rerun-evaluability-and-cheap-holiday-shopping", "postedAtFormatted": "Wednesday, November 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Evaluability%20(And%20Cheap%20Holiday%20Shopping)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Evaluability%20(And%20Cheap%20Holiday%20Shopping)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfg9AaLSoPAEsF8EY%2Fseq-rerun-evaluability-and-cheap-holiday-shopping%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Evaluability%20(And%20Cheap%20Holiday%20Shopping)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfg9AaLSoPAEsF8EY%2Fseq-rerun-evaluability-and-cheap-holiday-shopping", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWfg9AaLSoPAEsF8EY%2Fseq-rerun-evaluability-and-cheap-holiday-shopping", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/lh/evaluability_and_cheap_holiday_shopping/\">Evaluability (And Cheap Holiday Shopping)</a> was originally published on 28 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Evaluability_.28And_Cheap_Holiday_Shopping.29\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It's difficult for humans to evaluate an option except in comparison to other options. Poor decisions result when a poor category for comparison is used. Includes an application for cheap gift-shopping.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8cy/seq_rerun_the_affect_heuristic/#comments\">The Affect Heuristic</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wfg9AaLSoPAEsF8EY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.96639249208084e-07, "legacy": true, "legacyId": "10848", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3T6p93Mut7G8qdkAs", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-09T10:08:00.548Z", "modifiedAt": null, "url": null, "title": "Why would an AI try to figure out its goals?", "slug": "why-would-an-ai-try-to-figure-out-its-goals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.707Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n5Kqe527prQoJDEB6/why-would-an-ai-try-to-figure-out-its-goals", "pageUrlRelative": "/posts/n5Kqe527prQoJDEB6/why-would-an-ai-try-to-figure-out-its-goals", "linkUrl": "https://www.lesswrong.com/posts/n5Kqe527prQoJDEB6/why-would-an-ai-try-to-figure-out-its-goals", "postedAtFormatted": "Wednesday, November 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20would%20an%20AI%20try%20to%20figure%20out%20its%20goals%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20would%20an%20AI%20try%20to%20figure%20out%20its%20goals%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Kqe527prQoJDEB6%2Fwhy-would-an-ai-try-to-figure-out-its-goals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20would%20an%20AI%20try%20to%20figure%20out%20its%20goals%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Kqe527prQoJDEB6%2Fwhy-would-an-ai-try-to-figure-out-its-goals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn5Kqe527prQoJDEB6%2Fwhy-would-an-ai-try-to-figure-out-its-goals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<blockquote>\n<p>\"So how can it ensure that future self-modi\ufb01cations will accomplish its current objectives? For one thing, it has to make those objectives clear to itself. If its objectives are only implicit in the structure of a complex circuit or program, then future modi\ufb01cations are unlikely to preserve them. Systems will therefore be motivated to re\ufb02ect on their goals and to make them explicit.\" --&nbsp;Stephen M. Omohundro, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">The Basic AI Drives</a></p>\n</blockquote>\n<blockquote>\n<p>This AI becomes able to improve itself in a haphazard way, makes various changes that are net improvements but may introduce value drift, and then gets smart enough to do guaranteed self-improvement, at which point its values freeze (forever). -- Eliezer Yudkowsky, <a href=\"/lw/wp/what_i_think_if_not_why/\">What I Think, If Not Why</a></p>\n</blockquote>\n<p>I have stopped understanding why these quotes are correct. Help!</p>\n<p>More specifically, if you design an AI using \"shallow insights\" without an explicit goal-directed architecture - some program that \"just happens\" to make intelligent decisions that can be viewed by us as fulfilling certain goals - then it has no particular reason to stabilize its goals. Isn't that anthropomorphizing? We humans don't exhibit a lot of goal-directed behavior, but we do have a verbal concept of \"goals\", so the verbal phantom of \"figuring out our true goals\" sounds meaningful to us. But why would AIs behave the same way if they don't think verbally? It looks more likely to me that an AI that acts semi-haphazardly may well continue doing so even after amassing a lot of computing power. Or is there some more compelling argument that I'm missing?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n5Kqe527prQoJDEB6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 21, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "10851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["z3kYdw54htktqt9Jb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-09T12:38:30.910Z", "modifiedAt": null, "url": null, "title": "What visionary project would you fund?", "slug": "what-visionary-project-would-you-fund", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.619Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gtwhEaZuJfX5uXtP2/what-visionary-project-would-you-fund", "pageUrlRelative": "/posts/gtwhEaZuJfX5uXtP2/what-visionary-project-would-you-fund", "linkUrl": "https://www.lesswrong.com/posts/gtwhEaZuJfX5uXtP2/what-visionary-project-would-you-fund", "postedAtFormatted": "Wednesday, November 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20visionary%20project%20would%20you%20fund%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20visionary%20project%20would%20you%20fund%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtwhEaZuJfX5uXtP2%2Fwhat-visionary-project-would-you-fund%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20visionary%20project%20would%20you%20fund%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtwhEaZuJfX5uXtP2%2Fwhat-visionary-project-would-you-fund", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgtwhEaZuJfX5uXtP2%2Fwhat-visionary-project-would-you-fund", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 69, "htmlBody": "<p>\n<p>I have just received a survey questionnaire regarding future directions in EU (European Union) research funding, and thought it would be interesting to see how LessWrong would answer the main question:</p>\n<p><em>Imagine that EU funding is available for one ambitious, visionary project extending beyond 2020.</em></p>\n<p>\n<ul>\n<li><em>What kind of research challenges should such a project address in your area?</em></li>\n<li><em>What would be the most urgent research tasks?</em></li>\n</ul>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gtwhEaZuJfX5uXtP2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 7.968425880270008e-07, "legacy": true, "legacyId": "10852", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 71, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-09T16:25:00.975Z", "modifiedAt": null, "url": null, "title": "[link] I Was Wrong, and So Are You ", "slug": "link-i-was-wrong-and-so-are-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:26.016Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uL6fiWsG8J4DymAYf/link-i-was-wrong-and-so-are-you", "pageUrlRelative": "/posts/uL6fiWsG8J4DymAYf/link-i-was-wrong-and-so-are-you", "linkUrl": "https://www.lesswrong.com/posts/uL6fiWsG8J4DymAYf/link-i-was-wrong-and-so-are-you", "postedAtFormatted": "Wednesday, November 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20I%20Was%20Wrong%2C%20and%20So%20Are%20You%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20I%20Was%20Wrong%2C%20and%20So%20Are%20You%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuL6fiWsG8J4DymAYf%2Flink-i-was-wrong-and-so-are-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20I%20Was%20Wrong%2C%20and%20So%20Are%20You%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuL6fiWsG8J4DymAYf%2Flink-i-was-wrong-and-so-are-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuL6fiWsG8J4DymAYf%2Flink-i-was-wrong-and-so-are-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 887, "htmlBody": "<p>A article in <a href=\"http://www.theatlantic.com/magazine/archive/2011/12/i-was-wrong-and-so-are-you/8713/\">the Atlantic</a>, linked to by someone on the unofficial LW <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_IRC_Chatroom\">IRC channel</a> caught my eye. Nothing all that new for LessWrong readers, but still it is good to see any mention of such biases in mainstream media.</p>\n<blockquote>\n<h2 class=\"headline\">I Was Wrong, and So Are You</h2>\n<h5 class=\"blurb\">A libertarian economist retracts a swipe at the left&mdash;after discovering that our political leanings leave us more biased than we think.</h5>\n<p>...</p>\n<p>Y<span style=\"text-transform: uppercase;\">ou may have </span>noticed that several of the statements we analyzed implicitly challenge positions held by the left, while none specifically challenges conservative or libertarian positions. A great deal of research shows that people are more likely to heed information that supports their prior positions, and discard or discount contrary information. Suppose that on some public issue, Anne favors position A, and Burt favors position B. Anne is more likely than Burt to agree with statements that support A, and to disagree with statements that support B, because doing so simplifies her case for favoring A. Otherwise, she would have to make a concession to the opposing side. <em><strong>Psychologists would count this tendency as a manifestation of &ldquo;myside bias,&rdquo; or &ldquo;confirmation bias.&rdquo; </strong></em></p>\n<p>Buturovic and I openly acknowledged that the set of eight statements was biased. But these were the statements we had available to us. And as we explained in the paper, some of them&mdash;including those on professional licensing, standard of living, monopoly, and trade&mdash;did not appear to fit neatly into a partisan debate. Yet even on those, respondents on the left fared worst. What&rsquo;s more, in separate research, Buturovic found that the respondents themselves either had difficulty classifying some of the statements on an ideological scale, or simply believed those statements were not,<em> </em>prima facie, ideological. So while we thought the results were probably exaggerated because of the bias in the survey, we nonetheless felt that they were telling.</p>\n<p>Buturovic and I largely refrained from replying to the criticism (much of which focused on myside bias) that followed publication of the article. Instead, we planned a second survey that would balance the first one by including questions that would challenge conservative and/or libertarian positions.</p>\n<p>...</p>\n<p>Buturovic began putting all 17 questions to a new group of respondents last December. I eagerly awaited the results, hoping that the conservatives and especially the libertarians (my side!) would exhibit less myside bias. Buturovic was more detached. She e-mailed me the results, and commented that conservatives and libertarians did not do well on the new questions. After a hard look, I realized that they had bombed on the questions that challenged their position. A full tabulation of all 17 questions showed that no group clearly out-stupids the others. They appear about equally stupid when faced with proper challenges to their position.</p>\n<p><em><strong>Writing up these results was, for me, a gloomy task&mdash;I expected critics to gloat and point fingers.</strong></em> In May, we published another paper in <em>Econ Journal Watch</em>, saying in the title that the new results &ldquo;Vitiate Prior Evidence of the Left Being Worse.&rdquo; More than 30 percent of my libertarian compatriots (and more than 40 percent of conservatives), for instance, disagreed with the statement &ldquo;A dollar means more to a poor person than it does to a rich person&rdquo;&mdash;c&rsquo;mon, people!&mdash;versus just 4 percent among progressives. Seventy-eight percent of libertarians believed gun-control laws fail to reduce people&rsquo;s access to guns. Overall, on the nine new items, the respondents on the left did much better than the conservatives and libertarians. Some of the new questions challenge (or falsely reassure) conservative and not libertarian positions, and vice versa. Consistently, the more a statement challenged a group&rsquo;s position, the worse the group did.</p>\n<p>The reaction to the new paper was quieter than I expected. Jonathan Chait, who had knocked the first paper, wrote a forgiving notice on his <em>New Republic</em> blog: &ldquo;Insult Retractions: A (Very) Occasional Feature.&rdquo; Matthew Yglesias, writing at ThinkProgress, summed up the takeaway: &ldquo;Basically, there&rsquo;s a lot of confirmation bias out there.&rdquo; Nothing illustrates that point better than my confidence in the claims of the first paper, especially as distilled in my <em>Wall Street Journal </em>op-ed.</p>\n<p><em><strong>Shouldn&rsquo;t a college professor have known better?</strong></em></p>\n</blockquote>\n<p>I break here to comment that I don't see why we would expect this to be so given the reality of academia.</p>\n<blockquote>\n<p><em><strong> </strong></em>Perhaps.<em><strong> </strong><strong>But adjusting for bias and groupthink is not so easy, </strong></em>as indicated by one of the major conclusions developed by Buturovic and sustained in our joint papers.<em><strong> </strong><strong>Education had very little impact on responses, we found; survey respondents who&rsquo;d gone to college did only slightly less badly than those who hadn&rsquo;t. </strong>Among members of less-educated groups, brighter people tend to respond more frequently to online surveys, so it&rsquo;s likely that our sample of non-college-educated respondents is more enlightened than the larger group they represent.</em> Still, <em><strong>the fact that a college education showed almost no effect&mdash;at least for those inclined to take such a survey&mdash;strongly suggests that the classroom is no great corrective for myside bias.</strong></em> At least when it comes to public-policy issues, the corrective value of professional academic experience might be doubted as well.</p>\n<p>Discourse affords some opportunity to challenge the judgments of others and to revise our own. Yet inevitably, somewhere in the process, we place what faith we have.</p>\n</blockquote>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uL6fiWsG8J4DymAYf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 25, "extendedScore": null, "score": 7.969222078643505e-07, "legacy": true, "legacyId": "10856", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 96, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T00:20:53.331Z", "modifiedAt": null, "url": null, "title": "A question on rationality.", "slug": "a-question-on-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.240Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D227", "createdAt": "2011-07-20T16:22:50.660Z", "isAdmin": false, "displayName": "D227"}, "userId": "MZNDKaBencf7N9kGt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Szapw4bZY7sHYXSRf/a-question-on-rationality", "pageUrlRelative": "/posts/Szapw4bZY7sHYXSRf/a-question-on-rationality", "linkUrl": "https://www.lesswrong.com/posts/Szapw4bZY7sHYXSRf/a-question-on-rationality", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20question%20on%20rationality.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20question%20on%20rationality.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzapw4bZY7sHYXSRf%2Fa-question-on-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20question%20on%20rationality.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzapw4bZY7sHYXSRf%2Fa-question-on-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSzapw4bZY7sHYXSRf%2Fa-question-on-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p><span style=\"font-family: Verdana, sans-serif;\">My long runs on Saturdays give me time to ponder the various material at lesswrong. &nbsp;Recently my attention has been kept busy pondering a question about rationality that I have not yet resolved and would like to present to lesswrong as a discussion. &nbsp;I will try to be as&nbsp;succinct&nbsp;as possible, please correct me if I make any logical&nbsp;fallacies.</span></p>\n<p><span style=\"font-family: Verdana, sans-serif;\"><br /></span></p>\n<blockquote>\n<p style=\"text-align: -webkit-auto;\"><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><span style=\"font-family: Verdana, sans-serif; \">Instrumental rationality is defined as t</span><span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">he art of choosing actions that steer the future toward outcomes ranked higher in your</span><span>&nbsp;</span><span>preferences/values/goals (PVGs)</span></span></span></p>\n</blockquote>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \"><span style=\"font-family: Verdana, sans-serif; \">&nbsp;</span>Here are my questions:</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \"><span style=\"font-family: Verdana, sans-serif; \">1. If rationality is the function of achieving our preferences/values/goals, what is the function of choosing our PVGs to begin with,</span><span style=\"font-family: Verdana, sans-serif; \">&nbsp;</span><strong style=\"font-family: Verdana, sans-serif; \">if we could choose &nbsp;our preferences</strong><span style=\"font-family: Verdana, sans-serif;\">? &nbsp;In other words, is there an \"inherent rationality\" absence of preference or values? &nbsp;It seems as if the definition of instrumental rationality is saying that if you have a PVG, that there is a rational way to achieve it, but there is not necessarily rational PVGs. &nbsp;</span></p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \"><span style=\"font-family: Verdana, sans-serif;\"><br /></span></p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \"><span style=\"font-family: Verdana, sans-serif;\">2.If the answer is no, there is no \"inherent rationality\" absence of a PVG, then what would preclude the&nbsp;possibility&nbsp;that a perfect rationalist, given enough time and resources, will eventually become a perfectly self interested entity with only one overall goal which is to&nbsp;perpetuate&nbsp;his existence, at the&nbsp;sacrifice&nbsp;of everything and everyone else?</span></p>\n<blockquote>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Suppose a superintelligence visits Bob and grants him the power to edit his own code. &nbsp;Bob can now edit or choose his own preferences/values/goals. &nbsp;</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Bob is a perfect rationalist.</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Bob is genetically&nbsp;predisposed&nbsp;to abuse alcohol, as such he rationally did everything he could to keep alcohol off his mind. &nbsp;</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Now, Bob no longer has to do this, he simply goes into his own code and deletes this code/PVG/meme for alcohol abuse.</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Bob continues to cull his code of \"inefficient\" PVGs. &nbsp;&nbsp;</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">Soon Bob only has one goal, the most important goal, self preservation.&nbsp;</p>\n</blockquote>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">3. Is it rational for Bob, having these powers, to rid himself of humanity, and&nbsp;rewrite&nbsp;his code to only want to support one meme, that is the meme to ensure his existence. &nbsp;Everything he will do will goes to support this meme. &nbsp;He will drop all his relationships, his hobbies, all his wants and desires into concentrate on a single objective. &nbsp;How does Bob not become a monster superintelligence hell bent on using all the energy in the universe for his own selfish reasons?</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">&nbsp;</p>\n<p style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: white; background-position: initial initial; background-repeat: initial initial; \">I have not resolved any of these questions yet, and look forward to any responses I may receive. &nbsp;I am very perplexed at Bob's situation. &nbsp;If there are some sequences that would help me better understand my questions please suggest them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Szapw4bZY7sHYXSRf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 7.970895328497202e-07, "legacy": true, "legacyId": "10857", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T04:29:47.914Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Unbounded Scales, Huge Jury Awards, & Futurism", "slug": "seq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:22.830Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D2adfyjbgPCYz8ua6/seq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "pageUrlRelative": "/posts/D2adfyjbgPCYz8ua6/seq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "linkUrl": "https://www.lesswrong.com/posts/D2adfyjbgPCYz8ua6/seq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Unbounded%20Scales%2C%20Huge%20Jury%20Awards%2C%20%26%20Futurism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Unbounded%20Scales%2C%20Huge%20Jury%20Awards%2C%20%26%20Futurism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2adfyjbgPCYz8ua6%2Fseq-rerun-unbounded-scales-huge-jury-awards-and-futurism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Unbounded%20Scales%2C%20Huge%20Jury%20Awards%2C%20%26%20Futurism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2adfyjbgPCYz8ua6%2Fseq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD2adfyjbgPCYz8ua6%2Fseq-rerun-unbounded-scales-huge-jury-awards-and-futurism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/li/unbounded_scales_huge_jury_awards_futurism/\">Unbounded Scales, Huge Jury Awards, &amp; Futurism</a> was originally published on 29 November 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Unbounded_Scales.2C_Huge_Jury_Awards.2C_.26_Futurism\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Without a metric for comparison, estimates of, e.g., what sorts of punitive damages should be awarded, or when some future advance will happen, vary widely simply due to the lack of a scale.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/8dc/seq_rerun_evaluability_and_cheap_holiday_shopping/\">Evaluability (And Cheap Holiday Shopping)</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D2adfyjbgPCYz8ua6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 7.971770782026537e-07, "legacy": true, "legacyId": "10864", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5u5THLyRkTpPHiaG5", "Wfg9AaLSoPAEsF8EY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T10:29:37.863Z", "modifiedAt": null, "url": null, "title": "Slightly known unknowns", "slug": "slightly-known-unknowns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:27.725Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h86e5mYuBFh5xPifx/slightly-known-unknowns", "pageUrlRelative": "/posts/h86e5mYuBFh5xPifx/slightly-known-unknowns", "linkUrl": "https://www.lesswrong.com/posts/h86e5mYuBFh5xPifx/slightly-known-unknowns", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Slightly%20known%20unknowns&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASlightly%20known%20unknowns%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh86e5mYuBFh5xPifx%2Fslightly-known-unknowns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Slightly%20known%20unknowns%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh86e5mYuBFh5xPifx%2Fslightly-known-unknowns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh86e5mYuBFh5xPifx%2Fslightly-known-unknowns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Inspired by <a href=\"/r/discussion/lw/8dg/what_visionary_project_would_you_fund/\">What visionary project would you fund?</a>, I'm wondering about whether there are known blank areas in our knowledge which might turn up surprising knowledge.</p>\n<p>Once upon a time, the sun was a mystery. People had a pretty good idea of its mass, and how much chemical energy would be needed to keep it shining. I don't remember how long the sun could last running on chemical energy, but it didn't seem plausible that it could be so new.</p>\n<p>It turned out that chemical energy wasn't the only possibility.</p>\n<p>I don't get the impression that there was wide appreciation that most of the world was being ignored by scientists because the calculations were too difficult-- until the calculations were made easier.</p>\n<p>Do people here expect to be surprised by whatever it takes to understand qualia?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h86e5mYuBFh5xPifx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 7.973036673404767e-07, "legacy": true, "legacyId": "10866", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gtwhEaZuJfX5uXtP2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T10:49:55.502Z", "modifiedAt": null, "url": null, "title": "Is the SIAI interview series available on youtube? ", "slug": "is-the-siai-interview-series-available-on-youtube", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CCQfRQDyLWJS3tyGX/is-the-siai-interview-series-available-on-youtube", "pageUrlRelative": "/posts/CCQfRQDyLWJS3tyGX/is-the-siai-interview-series-available-on-youtube", "linkUrl": "https://www.lesswrong.com/posts/CCQfRQDyLWJS3tyGX/is-the-siai-interview-series-available-on-youtube", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20the%20SIAI%20interview%20series%20available%20on%20youtube%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20the%20SIAI%20interview%20series%20available%20on%20youtube%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCQfRQDyLWJS3tyGX%2Fis-the-siai-interview-series-available-on-youtube%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20the%20SIAI%20interview%20series%20available%20on%20youtube%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCQfRQDyLWJS3tyGX%2Fis-the-siai-interview-series-available-on-youtube", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCCQfRQDyLWJS3tyGX%2Fis-the-siai-interview-series-available-on-youtube", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>Or some other site? And if not, why not?</p>\n<p>I'm especially interested in higher quality versions, as well as in a easy to share (embedding) format with a possiblity of people commenting them. At least there is convenient way to download them. Watching the interviews with Peter Norvig, Vernor Vinge and <a href=\"http://intelligence.org/media/interviews/peterthiel\">Peter Thiel</a>, I found the current way a bit buggy, full screen option didn't work for example.</p>\n<p>People searching for say interviews with X person on youtube might find these and later head out to the SIAI website or watch the other videos on the channel. More clicks are good right? I'm asking because there are some <a href=\"http://www.youtube.com/watch?v=5zMAiGcDHG8\">modified versions</a> of the interviews on the site but I didn't find the original ones there.</p>\n<p>Yes I know there are ways around this, but why should they put <a href=\"http://wiki.lesswrong.com/wiki/Trivial_inconvenience\">trivial inconveniences</a> in people's way? It should be more user friendly.</p>\n<p><br /><strong>Edit:</strong> Also why haven't the <a href=\"http://intelligence.org/media/presentations\">presentations</a> been updated since oh <em><strong>2007</strong></em>?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CCQfRQDyLWJS3tyGX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 7.973108078178202e-07, "legacy": true, "legacyId": "10867", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T13:27:34.207Z", "modifiedAt": null, "url": null, "title": "No Basic AI Drives", "slug": "no-basic-ai-drives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:24.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RQWMDhhRPEf5AXtg3/no-basic-ai-drives", "pageUrlRelative": "/posts/RQWMDhhRPEf5AXtg3/no-basic-ai-drives", "linkUrl": "https://www.lesswrong.com/posts/RQWMDhhRPEf5AXtg3/no-basic-ai-drives", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20Basic%20AI%20Drives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20Basic%20AI%20Drives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQWMDhhRPEf5AXtg3%2Fno-basic-ai-drives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20Basic%20AI%20Drives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQWMDhhRPEf5AXtg3%2Fno-basic-ai-drives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRQWMDhhRPEf5AXtg3%2Fno-basic-ai-drives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 579, "htmlBody": "<p>People who think that <a href=\"http://kruel.co/2011/07/21/why-i-am-skeptical-of-risks-from-ai-2/\">risks from AI</a> is the <a href=\"http://en.wikipedia.org/wiki/Existential_risk\">category of dangers</a> that is most likely to be the cause of a loss of all human value in the  universe often argue that artificial general intelligence tends to  undergo <a href=\"lw/we/recursive_selfimprovement/\">recursive self-improvement</a>.  The reason for doing so is that intelligence is maximally  instrumentally useful in the realization of almost any terminal goal an  AI might be equipped with. They believe that intelligence is an <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">universal instrumental value</a>. This sounds convincing, so let's accept it as given.</p>\n<p>What kind of instrumental value is <em>general intelligence</em>, what is it good for? Personally I try to see <em>general intelligence</em> purely as a potential. It allows an agent to achieve its goals.</p>\n<p>The question that is not asked is why an <em>artificial</em> agent would tap the full potential of its general intelligence rather  than only use the amount it is \"told\" to use, where would the incentive  to do more come from?</p>\n<p>If you deprived a human infant of all its  evolutionary drives (e.g. to avoid pain, seek nutrition, status and - <em> later on</em> - sex), would it just grow into an adult that might try to become  rich or rule a country? No, it would have no incentive to do so. Even  though such a \"<a href=\"http://en.wikipedia.org/wiki/The_Blank_Slate\">blank slate</a>\" would have the same potential for general intelligence, it wouldn't use it.</p>\n<p>Say  you came up with the most basic template for general intelligence that  works given limited resources. If you wanted to apply this potential to  improve your template, would this be a sufficient condition for it to take over  the world? I don't think so. If you didn't explicitly told it to do so,  why would it?</p>\n<p>The crux of the matter is that a goal isn't enough  to enable the full potential of general intelligence, you also need to  explicitly define <em>how</em> to achieve that goal. General  intelligence does not imply recursive self-improvement, just the  potential to do so, but not the incentive. The incentive has to be  given, it is not implied by general intelligence.</p>\n<p>For the same reasons that I don't think that an AGI will be automatically <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">friendly</a>, I don't think that it will automatically undergo recursive self-improvement. Maximizing <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">expected utility</a> is, just like friendliness, something that needs to be explicitly defined, otherwise there will be no incentive to do so.</p>\n<p>For example, in what sense would it be wrong for a general intelligence to <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">maximize paperclips</a> in the universe by waiting for them to arise <a href=\"http://en.wikipedia.org/wiki/Boltzmann_brain\">due to random fluctuations out of a state of chaos</a>? It is not inherently stupid to desire that, there is no law of nature that prohibits certain goals.</p>\n<p>Why would an generally intelligent artificial&nbsp;agent care about <em>how</em> to reach its goals if the preferred way is undefined? It is not <em>intelligent</em> to do something as quickly or effectively as possible if doing so is  not desired. And an artificial agent doesn't desire anything that it  isn't made to desire.</p>\n<p>There exists an interesting&nbsp;idiom stating that <em>the journey is the reward</em>.  Humans know that it takes a journey to reach a goal and that the  journey can be a goal in and of itself. For an artificial agent there is  no difference between a goal and how to reach it. If you told it to  reach Africa but not how, it might as well wait until it reaches Africa  by means of <a href=\"http://en.wikipedia.org/wiki/Continental_drift\">continental drift</a>. Would that be stupid? Only for humans, the AI has infinite patience, it just doesn't care about any implicit connotations.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RQWMDhhRPEf5AXtg3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -2, "extendedScore": null, "score": 7.973662794842422e-07, "legacy": true, "legacyId": "10868", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBadX7rwdcRFzGuju"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T15:10:28.480Z", "modifiedAt": null, "url": null, "title": "PredictionBook: A Short Note", "slug": "predictionbook-a-short-note", "viewCount": null, "lastCommentedAt": "2021-12-27T20:45:46.224Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMF9dmiTfdBDfHAhC/predictionbook-a-short-note", "pageUrlRelative": "/posts/KMF9dmiTfdBDfHAhC/predictionbook-a-short-note", "linkUrl": "https://www.lesswrong.com/posts/KMF9dmiTfdBDfHAhC/predictionbook-a-short-note", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PredictionBook%3A%20A%20Short%20Note&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APredictionBook%3A%20A%20Short%20Note%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMF9dmiTfdBDfHAhC%2Fpredictionbook-a-short-note%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PredictionBook%3A%20A%20Short%20Note%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMF9dmiTfdBDfHAhC%2Fpredictionbook-a-short-note", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMF9dmiTfdBDfHAhC%2Fpredictionbook-a-short-note", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 655, "htmlBody": "<p>&nbsp;</p>\n<p><strong>New Updates</strong></p>\n<p><a href=\"http://predictionbook.com/\">PredictionBook</a> has been updated and the speed improvements are massive. I was considering abandoning it because navigating the site was so slow (especially on my <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Android_%28operating_system%29\">Android</a> smartphone), but now <a href=\"http://predictionbook.com/predictions/4390\">I'm here to stay</a>. Since there is no news feed on the website itself, it seemed appropriate to announce the update here. Also, the layout has changed slightly (for the better IMO) and the probability assignments have been made more intuitive (which many of the newcomers were having trouble with). The updates, along with the (slowly) growing user base, make now a better time then ever to join. Thanks <a href=\"http://trikeapps.com/\">TrikeApps</a> and <a rel=\"nofollow\" href=\"https://github.com/ivankozik\">Ivan</a> <a rel=\"nofollow\" href=\"http://ludios.org/\">Kozik</a>!</p>\n<p><strong>My Experience So Far</strong></p>\n<p>I was one of those that was convinced to give PredictionBook a shot based on <a href=\"http://www.gwern.net/index\">gwern</a>'s <a href=\"/lw/7z9/1001_predictionbook_nights/\">article</a>. It has only been a month, so I am using up my willpower preventing myself from drawing any firm conclusions about my <a href=\"http://plato.stanford.edu/entries/scientific-underdetermination/#HolUndVerIde\">web of belief</a> or sanity from such a small sample size. Ultimately, my goal is to <a href=\"http://rationalpoker.com/2011/04/21/this-is-what-5-feels-like/\">know what 5% feels like</a> and I already believe I have made a small step towards doing so. Even if PredictionBook turns out to be a failure for improving my calibration to the extent that I can intuitively feel what my <a href=\"http://plato.stanford.edu/entries/probability-interpret/#SubPro\">degree of belief in a proposition</a> is in percentage terms, it would still be worth it for the simple reason that I now have an inkling about how poorly calibrated I am. Finally, on at least three occasions, I have found myself trying to cook up some kind of rationalization in order to put off doing some task that I probably should be doing, but have felt additional pressure not to give in to <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">akrasia</a> because \"my prediction will be judged false\".</p>\n<p><strong>Admonition/Guilt Trip</strong></p>\n<p>Honestly, PredictionBook is a rationalist's dream tool. How would you even know if you are actually becoming less wrong <em>without</em> tracking your predictions? <a href=\"http://predictionbook.com/predictions/4239\">Eliezer Yudkowsky</a>, <a href=\"http://predictionbook.com/predictions/4238\">Robin Hanson</a>, <a href=\"http://predictionbook.com/predictions/4241\">Lukeprog</a>, <a href=\"http://predictionbook.com/predictions/4251\">Yvain</a>, <a href=\"http://predictionbook.com/predictions/4252\">Alicorn</a>, <a href=\"http://predictionbook.com/predictions/4253\">Phil Goetz</a>, <a href=\"http://predictionbook.com/predictions/4254\">wedrifid</a>, <a href=\"http://predictionbook.com/predictions/4255\">Anna Salamon</a>, <a href=\"http://predictionbook.com/predictions/4256\">Wei Dai</a>, and <a href=\"http://predictionbook.com/predictions/4257\">cousin_it</a> where the hell are you? Do you doubt the potential of PredictionBook to improve your calibration and diminish akrasia or is rationality really about \"<a href=\"http://www.cato-unbound.org/archives/july-2011-whats-wrong-with-expert-predictions/\">affiliating with an ideology or signaling one&rsquo;s authority</a>\" and not about having the map reflect the territory (yes, I realize this is a false dichotomy, but it makes for better rhetoric)?</p>\n<p><strong>The Future of PredictionBook</strong></p>\n<p>I would like to see a few changes in the way PredictionBook works and a few new features. Sometimes when you are entering dates into the date field it shows an incorrect number of minutes/days/years (but seems to fix itself once the prediction is submitted). This seems like a minor nitpick that should be easily fixable. Another minor change would be to make it more intuitive how the judging buttons work, since some of the newer members try pushing them instead of assigning a probability to the statement in the prediction.</p>\n<p>It would be nice to have a measure of the user's calibration that can be compared across time, so that the user can easily determine if their calibration is getting better or worse. This could be something like a measure of how closely an <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Ordinary_least_squares\">OLS</a> line of the user's predictions matches the <em>line-of-perfect-calibration</em> in the graph on the user's page. Also, I would like to see some of the community conventions written down in an FAQ-like document (specifically how conditional predictions should be judged), perhaps after they have been discussed.</p>\n<p>I should add that the <a href=\"https://github.com/tricycle/predictionbook\">source code</a> is now available on <a href=\"https://github.com/\">github</a>, but it is written in <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Ruby_%28programming_language%29\">Ruby</a> (which I don't have any experience with yet). If there are any Ruby programmers on the <a href=\"http://groups.google.com/group/lw-public-goods-team\">Less Wrong Public Goods Team</a>, this might be something worth considering for a project.</p>\n<p><em>Note: Now that I called some people out, my probability estimates concerning who will make predictions on PredictionBook will have to be adjusted.</em></p>\n<p>EDIT: There already exists a measure of calibration somewhat like the one I hinted at above; it's called a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Brier_score\">Brier score</a>.</p>\n<p>EDIT 2: <a href=\"http://predictionbook.com/users/lukeprog\">lukeprog</a> has joined the PredictionBook community.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1, "8daMDi9NEShyLqxth": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMF9dmiTfdBDfHAhC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 30, "extendedScore": null, "score": 7.974023225632861e-07, "legacy": true, "legacyId": "10869", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p><strong id=\"New_Updates\">New Updates</strong></p>\n<p><a href=\"http://predictionbook.com/\">PredictionBook</a> has been updated and the speed improvements are massive. I was considering abandoning it because navigating the site was so slow (especially on my <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Android_%28operating_system%29\">Android</a> smartphone), but now <a href=\"http://predictionbook.com/predictions/4390\">I'm here to stay</a>. Since there is no news feed on the website itself, it seemed appropriate to announce the update here. Also, the layout has changed slightly (for the better IMO) and the probability assignments have been made more intuitive (which many of the newcomers were having trouble with). The updates, along with the (slowly) growing user base, make now a better time then ever to join. Thanks <a href=\"http://trikeapps.com/\">TrikeApps</a> and <a rel=\"nofollow\" href=\"https://github.com/ivankozik\">Ivan</a> <a rel=\"nofollow\" href=\"http://ludios.org/\">Kozik</a>!</p>\n<p><strong id=\"My_Experience_So_Far\">My Experience So Far</strong></p>\n<p>I was one of those that was convinced to give PredictionBook a shot based on <a href=\"http://www.gwern.net/index\">gwern</a>'s <a href=\"/lw/7z9/1001_predictionbook_nights/\">article</a>. It has only been a month, so I am using up my willpower preventing myself from drawing any firm conclusions about my <a href=\"http://plato.stanford.edu/entries/scientific-underdetermination/#HolUndVerIde\">web of belief</a> or sanity from such a small sample size. Ultimately, my goal is to <a href=\"http://rationalpoker.com/2011/04/21/this-is-what-5-feels-like/\">know what 5% feels like</a> and I already believe I have made a small step towards doing so. Even if PredictionBook turns out to be a failure for improving my calibration to the extent that I can intuitively feel what my <a href=\"http://plato.stanford.edu/entries/probability-interpret/#SubPro\">degree of belief in a proposition</a> is in percentage terms, it would still be worth it for the simple reason that I now have an inkling about how poorly calibrated I am. Finally, on at least three occasions, I have found myself trying to cook up some kind of rationalization in order to put off doing some task that I probably should be doing, but have felt additional pressure not to give in to <a href=\"http://wiki.lesswrong.com/wiki/Akrasia\">akrasia</a> because \"my prediction will be judged false\".</p>\n<p><strong id=\"Admonition_Guilt_Trip\">Admonition/Guilt Trip</strong></p>\n<p>Honestly, PredictionBook is a rationalist's dream tool. How would you even know if you are actually becoming less wrong <em>without</em> tracking your predictions? <a href=\"http://predictionbook.com/predictions/4239\">Eliezer Yudkowsky</a>, <a href=\"http://predictionbook.com/predictions/4238\">Robin Hanson</a>, <a href=\"http://predictionbook.com/predictions/4241\">Lukeprog</a>, <a href=\"http://predictionbook.com/predictions/4251\">Yvain</a>, <a href=\"http://predictionbook.com/predictions/4252\">Alicorn</a>, <a href=\"http://predictionbook.com/predictions/4253\">Phil Goetz</a>, <a href=\"http://predictionbook.com/predictions/4254\">wedrifid</a>, <a href=\"http://predictionbook.com/predictions/4255\">Anna Salamon</a>, <a href=\"http://predictionbook.com/predictions/4256\">Wei Dai</a>, and <a href=\"http://predictionbook.com/predictions/4257\">cousin_it</a> where the hell are you? Do you doubt the potential of PredictionBook to improve your calibration and diminish akrasia or is rationality really about \"<a href=\"http://www.cato-unbound.org/archives/july-2011-whats-wrong-with-expert-predictions/\">affiliating with an ideology or signaling one\u2019s authority</a>\" and not about having the map reflect the territory (yes, I realize this is a false dichotomy, but it makes for better rhetoric)?</p>\n<p><strong id=\"The_Future_of_PredictionBook\">The Future of PredictionBook</strong></p>\n<p>I would like to see a few changes in the way PredictionBook works and a few new features. Sometimes when you are entering dates into the date field it shows an incorrect number of minutes/days/years (but seems to fix itself once the prediction is submitted). This seems like a minor nitpick that should be easily fixable. Another minor change would be to make it more intuitive how the judging buttons work, since some of the newer members try pushing them instead of assigning a probability to the statement in the prediction.</p>\n<p>It would be nice to have a measure of the user's calibration that can be compared across time, so that the user can easily determine if their calibration is getting better or worse. This could be something like a measure of how closely an <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Ordinary_least_squares\">OLS</a> line of the user's predictions matches the <em>line-of-perfect-calibration</em> in the graph on the user's page. Also, I would like to see some of the community conventions written down in an FAQ-like document (specifically how conditional predictions should be judged), perhaps after they have been discussed.</p>\n<p>I should add that the <a href=\"https://github.com/tricycle/predictionbook\">source code</a> is now available on <a href=\"https://github.com/\">github</a>, but it is written in <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Ruby_%28programming_language%29\">Ruby</a> (which I don't have any experience with yet). If there are any Ruby programmers on the <a href=\"http://groups.google.com/group/lw-public-goods-team\">Less Wrong Public Goods Team</a>, this might be something worth considering for a project.</p>\n<p><em>Note: Now that I called some people out, my probability estimates concerning who will make predictions on PredictionBook will have to be adjusted.</em></p>\n<p>EDIT: There already exists a measure of calibration somewhat like the one I hinted at above; it's called a <a href=\"https://secure.wikimedia.org/wikipedia/en/wiki/Brier_score\">Brier score</a>.</p>\n<p>EDIT 2: <a href=\"http://predictionbook.com/users/lukeprog\">lukeprog</a> has joined the PredictionBook community.</p>\n<p>&nbsp;</p>", "sections": [{"title": "New Updates", "anchor": "New_Updates", "level": 1}, {"title": "My Experience So Far", "anchor": "My_Experience_So_Far", "level": 1}, {"title": "Admonition/Guilt Trip", "anchor": "Admonition_Guilt_Trip", "level": 1}, {"title": "The Future of PredictionBook", "anchor": "The_Future_of_PredictionBook", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "38 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yE4Fdx4kYmQchBCek"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T16:29:21.585Z", "modifiedAt": null, "url": null, "title": "Philanthropy Promotion: Bolder Giving", "slug": "philanthropy-promotion-bolder-giving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BTmnATZSyL6Amn3Z9/philanthropy-promotion-bolder-giving", "pageUrlRelative": "/posts/BTmnATZSyL6Amn3Z9/philanthropy-promotion-bolder-giving", "linkUrl": "https://www.lesswrong.com/posts/BTmnATZSyL6Amn3Z9/philanthropy-promotion-bolder-giving", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philanthropy%20Promotion%3A%20Bolder%20Giving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilanthropy%20Promotion%3A%20Bolder%20Giving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTmnATZSyL6Amn3Z9%2Fphilanthropy-promotion-bolder-giving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philanthropy%20Promotion%3A%20Bolder%20Giving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTmnATZSyL6Amn3Z9%2Fphilanthropy-promotion-bolder-giving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTmnATZSyL6Amn3Z9%2Fphilanthropy-promotion-bolder-giving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<p>If you're trying to do as much good as possible, earning money to give away is a strong contender.&nbsp; Convincing other people to follow your example multiplies your impact.&nbsp; <a href=\"/user/juliawise/\">Julia</a> and <a href=\"http://www.jefftk.com/\">I</a> have a <a href=\"http://boldergiving.org/stories.php?story=Julia_Wise_97\">profile</a> on the philanthropy promotion website <a href=\"http://boldergiving.org\">Bolder Giving</a>.&nbsp; Much of the site consists of <a href=\"http://boldergiving.org/stories.php\">stories of people giving</a>.&nbsp; I was talking to one of the co-founders, <a href=\"http://boldergiving.org/staff.php?story=Anne_Ellinger\">Anne Ellinger</a>, and she would be interested in hearing from you if you give away a substantial fraction of your income or wealth, especially if you have been giving 20%+ of your income for 3+ years and are open to having your story posted on their site.&nbsp; I would be happy to introduce you, or you could <a href=\"http://boldergiving.org/contact.php\">write them</a> yourself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BTmnATZSyL6Amn3Z9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 7.974302555329587e-07, "legacy": true, "legacyId": "10870", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T17:14:01.171Z", "modifiedAt": null, "url": null, "title": "[Draft] Poker With Lennier", "slug": "draft-poker-with-lennier", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.377Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N3KmoJXav4wx3JHNG/draft-poker-with-lennier", "pageUrlRelative": "/posts/N3KmoJXav4wx3JHNG/draft-poker-with-lennier", "linkUrl": "https://www.lesswrong.com/posts/N3KmoJXav4wx3JHNG/draft-poker-with-lennier", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BDraft%5D%20Poker%20With%20Lennier&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BDraft%5D%20Poker%20With%20Lennier%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3KmoJXav4wx3JHNG%2Fdraft-poker-with-lennier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BDraft%5D%20Poker%20With%20Lennier%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3KmoJXav4wx3JHNG%2Fdraft-poker-with-lennier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3KmoJXav4wx3JHNG%2Fdraft-poker-with-lennier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1614, "htmlBody": "<p class=\"p1\">In J. Michael Straczynski's science fiction TV show <em>Babylon 5</em>, there's a character named Lennier. He's pretty Spock-like: he's a long-lived alien who avoids displaying emotion and feels superior to humans in intellect and wisdom. He's sworn to always speak the truth. In one episode, he and another character, the corrupt and rakish Ambassador Mollari, are chatting. Mollari is bored. But then Lennier mentions that he's spent decades studying probability. Mollari perks up, and offers to introduce him to this game the humans call <em>poker.<a id=\"more\"></a></em></p>\n<p class=\"p1\">Later, we see Mollari, Lennier, and some others playing poker. Lennier squints at his hand and remarks, \"Interesting. The odds of this combination are 5000:1, against.\" Everybody considers this revelation for a moment, then folds, conceding the hand. Mollari is exasperated, and tells him to stop doing that. Because Lennier is essentially announcing that he has a good hand, Lennier's winning far fewer chips than he should.</p>\n<p class=\"p1\">The other poker players, and the audience, are picturing Lennier as having a hand something like this:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/3c.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3h.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3s.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\" /></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is a four of a kind, the second-best hand in most poker games. The odds against being dealt a four of a kind in a hand of five cards are 4164:1--one might, in a moment of excitement, round that up to an even five thousand.</p>\n<p class=\"p1\">We the audience are meant to have a hearty chuckle over how theory doesn't translate into practice. But! We never get to see Lennier's cards, which means we get to picture whatever we want. I choose to believe, and I urge you to do so as well, that Lennier had this hand:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/ac.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/2c.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/5d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\" /></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is one of the worst hands possible in poker: ace-high. It loses to almost everything. By causing everyone else to fold, Lennier won a hand he probably would otherwise have lost. He knew exactly what he was doing.</p>\n<p class=\"p1\">\"Wait,\" I hear you say. \"Lennier is sworn to always tell the truth. How could he ever make a verbal bluff in a poker game?\" Well. Let's consider a few different ways we can interpret the phrase \"the odds of this combination.\"</p>\n<p class=\"p1\">First of all, the specific two hands I've given above are <em>equally likely </em>to be dealt. Any specific set of five cards is just as likely to show up as any other. There are 2,598,960 distinct hands of poker, all created equal, so the odds against any particular one showing up are 2598959:1. That's all the hands of poker but one, lined up against that one. Throw a ball, then notice which blade of grass it crushes. The odds against it crushing that blade of grass would have seemed nigh-impossible if you tried to predict it ahead of time. Lennier would have been fully justified in gaping in astonishment at his hand, and announcing that the odds were millions-to-one against. But if he does that, he's obligated to spend every waking moment in a perpetual state of amazement at everything that happens.</p>\n<p class=\"p1\">That's not how we normally talk about poker hands. Instead, we describe them as falling into relevant categories. There are 624 different four-of-a-kind hands, so the odds of getting a four of a kind, any four of a kind plus any other card, are 624 times better than the odds of getting one specific hand. There are 502,860 ace-high hands, which makes the odds against getting an ace-high hand, any ace high hand, 2096099:502860, which reduces to a mere 4.2:1. Throw a ball into the air: you can almost guarantee it's going to crush <em>some </em>blade of grass or other. Since all similar blades of grass on the lawn fall into the same category, we're not surprised when one in particular gets crushed.</p>\n<p class=\"p1\">But Lennier's an alien, and more importantly, a novice to poker, and <em>more </em>importantly, a dirty rotten sneak. He's under no obligation to lump poker hands into the same categories as a human poker player does. He's also fully capable of noticing that his cards have the Fibbonaci relation: when placed in ascending order with the ace counting as 1, they form a sequence such that each card N+2 is the sum of card N and card N+1. Furthermore, the first two cards and the second two cards each share a suit! The odds against such a combination are 5076:1. When he gives the odds as 5000:1, he's committed no sin other than a little rounding (down!). We're surprised if the ball lands right on the particular patch of grass we remember once burying a goldfish under, while a stranger who didn't know about the goldfish wouldn't find this spot remarkable.</p>\n<p class=\"p1\">\"But!\" you cry. \"I still feel like I'll be more surprised to be dealt a four of a kind than a two-striped-Fibbonaci-hand. Is that irrational?\" Not exactly. The poker hand categories are relevant to probabilistic analysis in one important way: the hand you got may not actually be random. Maybe the dealer is crooked. Maybe this is a dream. If you get dealt a four of a kind, the odds of either being true rise significantly. But we can't say this possibility makes the four of a kind less probable. Indeed it makes it <em>more probable</em>. A nonrandom process is equally likely to give you a four of a kind as an ace high. So if we allow for the possibility of a crooked dealer, all of our previous probability estimates were wrong. Rather than talking about this suddenly murky idea of the probability of a hand, we should talk about the <em>suspiciousness </em>of a hand. We can calculate it as follows.</p>\n<p class=\"p1\">First divide the set of poker hands into the relevant categories. We've got straight flush, four of a kind, full house, flush, three of a kind, two pairs, one pair, high card, and then separate categories for when they're topped by an ace (the straight flush becomes a royal flush, high card becomes ace high, etc.) That's sixteen categories. This is a culturally-dependent count. In some poker circles, it might make more sense to subdivide pairs into \"jacks-or-better\" and \"tens-or-worse,\" and in Hypothetical Minbari Poker, Fibbonaci hands are important. Whatever natural categories are in your head are the appropriate ones. If we knew the dealer thought as you did and was crooked and in complete control of what was dealt, but we had no other information on motives or behavior, the probability of being dealt any one of those categories would be 1/16. On the other hand, suppose, as we assumed earlier, we knew the deal was perfectly random. Then the probability of getting a non-ace four of a kind is about 1/4512, while the probability of an ace high is about 1/5. We'll calculate the <em>suspiciousness </em>of each hand by dividing the first figure (1/16 in both cases) by the second. So the suspiciousness of the four of a kind is about (1/16)/(1/4512) = 4512/16=282, while the suspiciousness of the ace high is (1/16)/(1/5)=5/16. In general, this kind of calculation is called a likelihood ratio.</p>\n<p class=\"p1\">Here's what you can do with this number. Suppose you currently believe that the odds against the dealer being crooked are 100:1. Once you see your hand, you can multiply that 1 by the suspiciousness of the hand. So if you're dealt a four of a kind, your new odds are now 100:282 against, which reduce to 141:50, or roughly 3:2, in <em>favor </em>of the dealer being crooked. On the other hand, if you're dealt ace high, your new odds are 100:(5/16), which reduce to 320:1. After seeing such an ordinary hand, you're now more confident the hands are random. Every time you see another hand, you can do the same calculation again on your current beliefs. Each time, you'll be performing a Bayesian Update, which is a sacred sacrament.</p>\n<p class=\"p1\">\"Hold on,\" you say, and at this point I'm starting to suspect you just enjoy interrupting. \"What if I <em>want </em>to notice patterns in my poker hands? Can I quantify the <em>remarkability </em>of a hand?\" Absolutely. We could say that a pattern is worth remarking on if its length is low relative to the number of cards it's talking about. The four of a kind hand can be described as \"Four threes and the eight of hearts,\" which is much quicker than \"The four of clubs, the four of diamonds, the four of spades, the four of hearts, and the eight of hearts.\" Here we see that Lennier's hand does okay in one respect: \"One,two,Fibbonaci\" is a little shorter than \"One, two, three, five, eight\". The suit pattern doesn't save any space, so it doesn't actually count as remarkable. And this is still subjective: alien cultures wouldn't know what \"Fibbonaci\" meant, and they might not even have a name for that particular relation. For the hand to be <em>objectively </em>remarkable, you'd have to be able to describe it succinctly even when including a definition of the term. That's not possible in this case, although if you had an eight-card hand following the pattern it would be. And the odds of getting an <em>objectively remarkable </em>hand by pure chance are always low, no matter how good at spotting patterns you are, because there's not enough room in language to describe the majority of n-card hands more succinctly than you could just the list the hand. This type of analysis is a useful scientific principle, referred to as <em>minimum message length, </em>a generalized and formalized Occam's Razor.</p>\n<p class=\"p1\">So what are the odds of a combination? It depends on what you're trying to accomplish! There is no gap between proper theory and proper practice, because theory is only coherent when it is instrumental. And that's how I know that, whatever J. Michael Straczynski might think, Lennier won with a bad hand.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1, "RLQumypPQGPYg9t6G": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N3KmoJXav4wx3JHNG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 29, "extendedScore": null, "score": 7.974459737245094e-07, "legacy": true, "legacyId": "10871", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T19:25:14.904Z", "modifiedAt": null, "url": null, "title": "Maximizing Cost-effectiveness via Critical Inquiry", "slug": "maximizing-cost-effectiveness-via-critical-inquiry", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.763Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HoldenKarnofsky", "createdAt": "2009-12-30T00:19:32.818Z", "isAdmin": false, "displayName": "HoldenKarnofsky"}, "userId": "kdeMdATaSc2MZKmdH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QQo9N3WhZpL3ewii6/maximizing-cost-effectiveness-via-critical-inquiry", "pageUrlRelative": "/posts/QQo9N3WhZpL3ewii6/maximizing-cost-effectiveness-via-critical-inquiry", "linkUrl": "https://www.lesswrong.com/posts/QQo9N3WhZpL3ewii6/maximizing-cost-effectiveness-via-critical-inquiry", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Maximizing%20Cost-effectiveness%20via%20Critical%20Inquiry&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMaximizing%20Cost-effectiveness%20via%20Critical%20Inquiry%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQo9N3WhZpL3ewii6%2Fmaximizing-cost-effectiveness-via-critical-inquiry%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Maximizing%20Cost-effectiveness%20via%20Critical%20Inquiry%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQo9N3WhZpL3ewii6%2Fmaximizing-cost-effectiveness-via-critical-inquiry", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQQo9N3WhZpL3ewii6%2Fmaximizing-cost-effectiveness-via-critical-inquiry", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2227, "htmlBody": "<p>&nbsp;</p>\n<p><em>I am cross-posting this <a href=\"http://blog.givewell.org\">GiveWell Blog</a> post, a followup to an&nbsp;<a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">earlier cross-post I made</a>. Here I provide a slightly more fleshed-out model that helps clarify the implications of Bayesian adjustments to cost-effectiveness estimates. It illustrates how it can be rational to take a \"threshold\" approach to cost-effectiveness, asking that actions/donations meet a minimum bar for estimated cost-effectiveness but otherwise focusing on robustness of evidence rather than magnitude of estimated impact.</em></p>\n<p>&nbsp;</p>\n<p>We've recently been writing about the shortcomings of formal cost-effectiveness estimation (i.e., trying to estimate how much good, as measured in lives saved, <a href=\"http://www.givewell.org/international/technical/additional/DALY\">DALYs</a> or other units, is accomplished per dollar spent). After <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">conceptually arguing that cost-effectiveness estimates can't be taken literally when they are not robust</a>, we <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\">found major problems in one of the most prominent sources of cost-effectiveness estimates for aid</a>, and <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">generalized from these problems to discuss major hurdles to usefulness faced by the endeavor of formal cost-effectiveness estimation</a>.</p>\n<p>Despite these misgivings, we would be determined to make cost-effectiveness estimates work, if we thought this were the only way to figure out how to allocate resources for maximal impact. But we don't. This post argues that <strong>when information quality is poor, the best way to maximize cost-effectiveness is to examine charities from as many different angles as possible</strong> - looking for ways in which their stories can be checked against reality - and support the charities that have a combination of <em>reasonably high estimated cost-effectiveness</em> and <em>maximally robust evidence</em>. This is the approach GiveWell has taken since our inception, and it is more similar to investigative journalism or early-stage research (other domains in which people look for surprising but valid claims in low-information environments) than to formal estimation of numerical quantities.</p>\n<p>The rest of this post</p>\n<ul>\n<li>Conceptually illustrates (using the mathematical framework laid out <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">previously</a>) the value of examining charities from different angles when seeking to maximize cost-effectiveness. </li>\n<li>Discusses how this conceptual approach matches the approach GiveWell has taken since inception.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p><strong>Conceptual illustration</strong></p>\n<p>I <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">previously laid out a framework</a> for making a \"Bayesian adjustment\" to a cost-effectiveness estimate. I stated (and posted the mathematical argument) that when considering a given cost-effectiveness estimate, one must also consider one's <em>prior distribution</em> (i.e., what is predicted for the value of one's actions by other life experience and evidence) and the <em>variance of the estimate error around the cost-effectiveness estimate</em> (i.e., how much room for error the estimate has). This section works off of that framework to illustrate the potential importance of examining charities from multiple angles - relative to formally estimating their cost-effectiveness - in low-information environments.</p>\n<p>I don't wish to present this illustration either as official GiveWell analysis or as \"the reason\" that we believe what we do. This is more of an illustration/explication of my views than a justification; GiveWell has implicitly (and intuitively) operated consistent with the conclusions of this analysis, long before we had a way of formalizing these conclusions or the model behind them. Furthermore, while the conclusions are broadly shared by GiveWell staff, the formal illustration of them should only be attributed to me.</p>\n<p><em>The model</em></p>\n<p>Suppose that:</p>\n<ul>\n<li>Your <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a> over the \"good accomplished per $1000 given to a charity\" is normally distributed with mean 0 and standard deviation 1 (denoted from this point on as N(0,1)). Note that I'm not saying that you believe the average donation has zero effectiveness; I'm just denoting whatever you believe about the impact of your donations in units of standard deviations, such that 0 represents the impact your $1000 has when given to an \"average\" charity and 1 represents the impact your $1000 has when given to \"a charity one standard deviation better than average\" (top 16% of charities). </li>\n<li>You are considering a particular charity, and your back-of-the-envelope initial estimate of the good accomplished by $1000 given to this charity is represented by X. It is a very rough estimate and could easily be completely wrong: specifically, it has a normally distributed \"estimate error\" with mean 0 (the estimate is as likely to be too optimistic as too pessimistic) and standard deviation X (so 16% of the time, the actual impact of your $1000 will be 0 or \"average\").* Thus, your estimate is denoted as N(X,X).</li>\n</ul>\n<p><em>The implications</em></p>\n<p><strong>I use \"initial estimate\" to refer to the formal cost-effectiveness estimate you create for a charity</strong> - along the lines of the <a href=\"http://www.dcp2.org\">DCP2</a> estimates or <a href=\"http://www.beguide.org\">Back of the Envelope Guide</a> estimates. <strong>I use \"final estimate\" to refer to the cost-effectiveness you should expect, after considering your initial estimate and making adjustments for the key other factors:</strong> your prior distribution and the \"estimate error\" variance around the initial estimate. The following chart illustrates the relationship between your initial estimate and final estimate based on the above assumptions.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea1.png\" alt=\"\" /></p>\n<p>Note that there is an inflection point (X=1), past which point your final estimate <em>falls</em> as your initial estimate <em>rises</em>. <strong>With such a rough estimate, the <em>maximum value</em> of your final estimate is 0.5 no matter how high your initial estimate says the value is. In fact, once your initial estimate goes \"too high\" the <em>final</em> estimated cost-effectiveness <em>falls</em>.</strong></p>\n<p>This is in some ways a counterintuitive result. A couple of ways of thinking about it:</p>\n<ul>\n<li>Informally: estimates that are \"too high,\" to the point where they go beyond what seems easily plausible, seem - by this very fact - more uncertain and more likely to have something wrong with them. Again, this point applies to very rough back-of-the-envelope style estimates, not to more precise and consistently obtained estimates. </li>\n<li>Formally: in this model, the higher your estimate of cost-effectiveness goes, the higher the error around that estimate is (both are represented by X), and thus the less information is contained in this estimate in a way that is likely to shift you away from your prior. This will be an unreasonable model for some situations, but I believe it is a reasonable model when discussing very rough (\"back-of-the-envelope\" style) estimates of good accomplished by disparate charities. The key component of this model is that of holding the \"probability that the right cost-effectiveness estimate is actually 'zero' [average]\" constant. Thus, an estimate of 1 has a 67% confidence interval of 0-2; an estimate of 1000 has a 67% confidence interval of 0-2000; the former is a more concentrated probability distribution. </li>\n</ul>\n<p>Now suppose that you make another, <em>independent</em> estimate of the good accomplished by your $1000, for the same charity. Suppose that this estimate is equally rough and comes to the same conclusion: it again has a value of X and a standard deviation of X. So you have two separate, independent \"initial estimates\" of good accomplished, and both are N(X,X). Properly combining these two estimates into one yields an estimate with the same average (X) but less \"estimate error\" (standard deviation = X/sqrt(2)). Now the relationship between X and adjusted expected value changes:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea2.png\" alt=\"\" /></p>\n<p>Now you have a higher maximum (for the final estimated good accomplished) <em>and</em> a later inflection point - higher estimates can be taken more seriously. But it's still the case that \"too high\" initial estimates lead to lower final estimates.</p>\n<p>The following charts show what happens if you manage to collect even more independent cost-effectiveness estimates, each one as rough as the others, each one with the same midpoint as the others (i.e., each is N(X,X)).</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea3.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea4.png\" alt=\"\" /></p>\n<p>The pattern here is that <strong>when you have many independent estimates, the key figure is X, or \"how good\" your estimates say the charity is. But when you have very few independent estimates, the key figure is K - how many different independent estimates you have.</strong> More broadly - <strong>when information quality is good, you should focus on quantifying your different options; when it isn't, you should focus on raising information quality.</strong></p>\n<p>A few other notes:</p>\n<ul>\n<li>The full calculations behind the above charts are available <a href=\"http://blog.givewell.org/attachments/cea_source.xls\">here (XLS)</a>. We also provide <a href=\"http://blog.givewell.org/attachments/cea_source_alt.xls\">another Excel file</a> that is identical except that it assumes a variance for each estimate of X/2, rather than X. This places \"0\" just inside your 95% confidence interval for the \"correct\" version of your estimate. While the inflection points are later and higher, the basic picture is the same. </li>\n<li><strong>It is important to have a cost-effectiveness estimate.</strong> If the initial estimate is too low, then regardless of evidence quality, the charity isn't a good one. In addition, very high initial estimates can imply higher potential gains to further investigation. However, \"the higher the initial estimate of cost-effectiveness, the better\" is <em>not&nbsp;</em>strictly true.</li>\n<li><em>Independence</em> of estimates is key to the above analysis. In my view, different formal estimates of cost-effectiveness are likely to be <em>very</em> far from independent because they will tend to use the same background data and assumptions and will tend to make the same simplifications that are inherent to cost-effectiveness estimation (see previous discussion of these simplifications <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">here</a> and <a href=\"http://blog.givewell.org/2010/03/19/cost-effectiveness-estimates-inside-the-sausage-factory/\">here</a>).</li>\n<li>\n<p>Instead, when I think about how to improve the robustness of evidence and thus reduce the variance of \"estimate error,\" I think about <em>examining a charity from different angles</em> - asking critical questions and looking for places where reality may or may not match the basic narrative being presented. As one collects more data points that support a charity's basic narrative (and weren't known to do so prior to investigation), the variance of the estimate falls, which is the same thing that happens when one collects more independent estimates. (Though it doesn't fall <em>as much</em> with each new data point as it would with one of the idealized \"fully independent cost-effectiveness estimates\" discussed above.)</p>\n</li>\n<li>The specific assumption of a normal distribution isn't crucial to the above analysis. I believe (based mostly on a conversation with <a href=\"http://blog.givewell.org/2010/06/03/my-donation-for-2009-guest-post-from-dario-amodei/\">Dario Amodei</a>) that for most commonly occurring distribution types, if you hold the \"probability of 0 or less\" constant, then as the midpoint of the \"estimate/estimate error\" distribution approaches infinity the distribution becomes approximately constant (and non-negligible) over the area where the prior probability is non-negligible, resulting in a negligible effect of the estimate on the prior.\n<p>While other distributions may involve later/higher inflection points than normal distributions, the general point that there is a threshold past which higher initial estimates no longer translate to higher final estimates holds for many distributions.</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong>The GiveWell approach</strong></p>\n<p>Since the beginning of our project, GiveWell has focused on maximizing the amount of good accomplished per dollar donated. Our <a href=\"http://givewell.org/about/progress#Originalbusinessplanpublished472007\">original business plan</a> (written in 2007 before we had raised any funding or gone full-time) lays out \"ideal metrics\" for charities such as</p>\n<p>&nbsp;</p>\n<blockquote>number of people whose jobs produce the income necessary to give them and their families a relatively comfortable lifestyle (including health, nourishment, relatively clean and comfortable shelter, some leisure time, and some room in the budget for luxuries), but would have been unemployed or working completely non-sustaining jobs without the charity&rsquo;s activities, per dollar per year. (Systematic differences in family size would complicate this.)</blockquote>\n<p>Early on, we weren't sure of whether we would find good enough information to quantify these sorts of things. After some experience, we came to the view that most cost-effectiveness analysis in the world of charity is extraordinarily rough, and we then began using a <a href=\"http://givewell.org/international/technical/criteria/cost-effectiveness#Howcosteffectiveiscosteffective\">threshold approach</a>, preferring charities whose cost-effectiveness is above a certain level but not distinguishing past that level. This approach is conceptually in line with the above analysis.</p>\n<p>It has been <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/#comment-236570\">remarked</a> that \"GiveWell takes a deliberately critical stance when evaluating any intervention type or charity.\" This is true, and in line with how the above analysis implies one should maximize cost-effectiveness. We generally investigate charities whose estimated cost-effectiveness is quite high in the scheme of things, and so for these charities the most important input into their <em>actual</em> cost-effectiveness is the robustness of their case and the number of factors in their favor. We critically examine these charities' claims and look for places in which they may turn out not to match reality; when we investigate these and find confirmation rather than refutation of charities' claims, we are finding new data points that support what they're saying. We're thus doing something conceptually similar to \"increasing K\" according to the model above. We've <a href=\"http://blog.givewell.org/2011/10/18/what-it-takes-to-evaluate-impact/\">recently written</a> about all the different angles we examine when strongly recommending a charity.</p>\n<p>We hope that the content we've published over the years, including recent content on cost-effectiveness (see the first paragraph of this post), has made it clear why we think we are in fact in a low-information environment, and why, therefore, the best approach is the one we've taken, which is more similar to investigative journalism or early-stage research (other domains in which people look for surprising but valid claims in low-information environments) than to formal estimation of numerical quantities.</p>\n<p>As long as the impacts of charities remain relatively poorly understood, we feel that focusing on <em>robustness of evidence</em> holds more promise than focusing on <em>quantification of impact</em>.</p>\n<p>*<em>This implies that the variance of your estimate error depends on the estimate itself. I think this is a reasonable thing to suppose in the scenario under discussion. Estimating cost-effectiveness for different charities is likely to involve using quite disparate frameworks, and the value of your estimate does contain information about the possible size of the estimate error. In our model, what stays constant across back-of-the-envelope estimates is the probability that the \"right estimate\" would be 0; this seems reasonable to me.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"EeSkeTcT4wtW2fWsL": 1, "xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QQo9N3WhZpL3ewii6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 7.974919931587852e-07, "legacy": true, "legacyId": "10854", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p><em>I am cross-posting this <a href=\"http://blog.givewell.org\">GiveWell Blog</a> post, a followup to an&nbsp;<a href=\"/lw/745/why_we_cant_take_expected_value_estimates/\">earlier cross-post I made</a>. Here I provide a slightly more fleshed-out model that helps clarify the implications of Bayesian adjustments to cost-effectiveness estimates. It illustrates how it can be rational to take a \"threshold\" approach to cost-effectiveness, asking that actions/donations meet a minimum bar for estimated cost-effectiveness but otherwise focusing on robustness of evidence rather than magnitude of estimated impact.</em></p>\n<p>&nbsp;</p>\n<p>We've recently been writing about the shortcomings of formal cost-effectiveness estimation (i.e., trying to estimate how much good, as measured in lives saved, <a href=\"http://www.givewell.org/international/technical/additional/DALY\">DALYs</a> or other units, is accomplished per dollar spent). After <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">conceptually arguing that cost-effectiveness estimates can't be taken literally when they are not robust</a>, we <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/\">found major problems in one of the most prominent sources of cost-effectiveness estimates for aid</a>, and <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">generalized from these problems to discuss major hurdles to usefulness faced by the endeavor of formal cost-effectiveness estimation</a>.</p>\n<p>Despite these misgivings, we would be determined to make cost-effectiveness estimates work, if we thought this were the only way to figure out how to allocate resources for maximal impact. But we don't. This post argues that <strong>when information quality is poor, the best way to maximize cost-effectiveness is to examine charities from as many different angles as possible</strong> - looking for ways in which their stories can be checked against reality - and support the charities that have a combination of <em>reasonably high estimated cost-effectiveness</em> and <em>maximally robust evidence</em>. This is the approach GiveWell has taken since our inception, and it is more similar to investigative journalism or early-stage research (other domains in which people look for surprising but valid claims in low-information environments) than to formal estimation of numerical quantities.</p>\n<p>The rest of this post</p>\n<ul>\n<li>Conceptually illustrates (using the mathematical framework laid out <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">previously</a>) the value of examining charities from different angles when seeking to maximize cost-effectiveness. </li>\n<li>Discusses how this conceptual approach matches the approach GiveWell has taken since inception.</li>\n</ul>\n<p><a id=\"more\"></a></p>\n<p><strong id=\"Conceptual_illustration\">Conceptual illustration</strong></p>\n<p>I <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">previously laid out a framework</a> for making a \"Bayesian adjustment\" to a cost-effectiveness estimate. I stated (and posted the mathematical argument) that when considering a given cost-effectiveness estimate, one must also consider one's <em>prior distribution</em> (i.e., what is predicted for the value of one's actions by other life experience and evidence) and the <em>variance of the estimate error around the cost-effectiveness estimate</em> (i.e., how much room for error the estimate has). This section works off of that framework to illustrate the potential importance of examining charities from multiple angles - relative to formally estimating their cost-effectiveness - in low-information environments.</p>\n<p>I don't wish to present this illustration either as official GiveWell analysis or as \"the reason\" that we believe what we do. This is more of an illustration/explication of my views than a justification; GiveWell has implicitly (and intuitively) operated consistent with the conclusions of this analysis, long before we had a way of formalizing these conclusions or the model behind them. Furthermore, while the conclusions are broadly shared by GiveWell staff, the formal illustration of them should only be attributed to me.</p>\n<p><em>The model</em></p>\n<p>Suppose that:</p>\n<ul>\n<li>Your <a href=\"http://en.wikipedia.org/wiki/Prior_probability\">prior</a> over the \"good accomplished per $1000 given to a charity\" is normally distributed with mean 0 and standard deviation 1 (denoted from this point on as N(0,1)). Note that I'm not saying that you believe the average donation has zero effectiveness; I'm just denoting whatever you believe about the impact of your donations in units of standard deviations, such that 0 represents the impact your $1000 has when given to an \"average\" charity and 1 represents the impact your $1000 has when given to \"a charity one standard deviation better than average\" (top 16% of charities). </li>\n<li>You are considering a particular charity, and your back-of-the-envelope initial estimate of the good accomplished by $1000 given to this charity is represented by X. It is a very rough estimate and could easily be completely wrong: specifically, it has a normally distributed \"estimate error\" with mean 0 (the estimate is as likely to be too optimistic as too pessimistic) and standard deviation X (so 16% of the time, the actual impact of your $1000 will be 0 or \"average\").* Thus, your estimate is denoted as N(X,X).</li>\n</ul>\n<p><em>The implications</em></p>\n<p><strong>I use \"initial estimate\" to refer to the formal cost-effectiveness estimate you create for a charity</strong> - along the lines of the <a href=\"http://www.dcp2.org\">DCP2</a> estimates or <a href=\"http://www.beguide.org\">Back of the Envelope Guide</a> estimates. <strong>I use \"final estimate\" to refer to the cost-effectiveness you should expect, after considering your initial estimate and making adjustments for the key other factors:</strong> your prior distribution and the \"estimate error\" variance around the initial estimate. The following chart illustrates the relationship between your initial estimate and final estimate based on the above assumptions.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea1.png\" alt=\"\"></p>\n<p>Note that there is an inflection point (X=1), past which point your final estimate <em>falls</em> as your initial estimate <em>rises</em>. <strong>With such a rough estimate, the <em>maximum value</em> of your final estimate is 0.5 no matter how high your initial estimate says the value is. In fact, once your initial estimate goes \"too high\" the <em>final</em> estimated cost-effectiveness <em>falls</em>.</strong></p>\n<p>This is in some ways a counterintuitive result. A couple of ways of thinking about it:</p>\n<ul>\n<li>Informally: estimates that are \"too high,\" to the point where they go beyond what seems easily plausible, seem - by this very fact - more uncertain and more likely to have something wrong with them. Again, this point applies to very rough back-of-the-envelope style estimates, not to more precise and consistently obtained estimates. </li>\n<li>Formally: in this model, the higher your estimate of cost-effectiveness goes, the higher the error around that estimate is (both are represented by X), and thus the less information is contained in this estimate in a way that is likely to shift you away from your prior. This will be an unreasonable model for some situations, but I believe it is a reasonable model when discussing very rough (\"back-of-the-envelope\" style) estimates of good accomplished by disparate charities. The key component of this model is that of holding the \"probability that the right cost-effectiveness estimate is actually 'zero' [average]\" constant. Thus, an estimate of 1 has a 67% confidence interval of 0-2; an estimate of 1000 has a 67% confidence interval of 0-2000; the former is a more concentrated probability distribution. </li>\n</ul>\n<p>Now suppose that you make another, <em>independent</em> estimate of the good accomplished by your $1000, for the same charity. Suppose that this estimate is equally rough and comes to the same conclusion: it again has a value of X and a standard deviation of X. So you have two separate, independent \"initial estimates\" of good accomplished, and both are N(X,X). Properly combining these two estimates into one yields an estimate with the same average (X) but less \"estimate error\" (standard deviation = X/sqrt(2)). Now the relationship between X and adjusted expected value changes:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea2.png\" alt=\"\"></p>\n<p>Now you have a higher maximum (for the final estimated good accomplished) <em>and</em> a later inflection point - higher estimates can be taken more seriously. But it's still the case that \"too high\" initial estimates lead to lower final estimates.</p>\n<p>The following charts show what happens if you manage to collect even more independent cost-effectiveness estimates, each one as rough as the others, each one with the same midpoint as the others (i.e., each is N(X,X)).</p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea3.png\" alt=\"\"></p>\n<p>&nbsp;</p>\n<p><img src=\"http://blog.givewell.org/images/cea4.png\" alt=\"\"></p>\n<p>The pattern here is that <strong>when you have many independent estimates, the key figure is X, or \"how good\" your estimates say the charity is. But when you have very few independent estimates, the key figure is K - how many different independent estimates you have.</strong> More broadly - <strong>when information quality is good, you should focus on quantifying your different options; when it isn't, you should focus on raising information quality.</strong></p>\n<p>A few other notes:</p>\n<ul>\n<li>The full calculations behind the above charts are available <a href=\"http://blog.givewell.org/attachments/cea_source.xls\">here (XLS)</a>. We also provide <a href=\"http://blog.givewell.org/attachments/cea_source_alt.xls\">another Excel file</a> that is identical except that it assumes a variance for each estimate of X/2, rather than X. This places \"0\" just inside your 95% confidence interval for the \"correct\" version of your estimate. While the inflection points are later and higher, the basic picture is the same. </li>\n<li><strong>It is important to have a cost-effectiveness estimate.</strong> If the initial estimate is too low, then regardless of evidence quality, the charity isn't a good one. In addition, very high initial estimates can imply higher potential gains to further investigation. However, \"the higher the initial estimate of cost-effectiveness, the better\" is <em>not&nbsp;</em>strictly true.</li>\n<li><em>Independence</em> of estimates is key to the above analysis. In my view, different formal estimates of cost-effectiveness are likely to be <em>very</em> far from independent because they will tend to use the same background data and assumptions and will tend to make the same simplifications that are inherent to cost-effectiveness estimation (see previous discussion of these simplifications <a href=\"http://blog.givewell.org/2011/11/04/some-considerations-against-more-investment-in-cost-effectiveness-estimates/\">here</a> and <a href=\"http://blog.givewell.org/2010/03/19/cost-effectiveness-estimates-inside-the-sausage-factory/\">here</a>).</li>\n<li>\n<p>Instead, when I think about how to improve the robustness of evidence and thus reduce the variance of \"estimate error,\" I think about <em>examining a charity from different angles</em> - asking critical questions and looking for places where reality may or may not match the basic narrative being presented. As one collects more data points that support a charity's basic narrative (and weren't known to do so prior to investigation), the variance of the estimate falls, which is the same thing that happens when one collects more independent estimates. (Though it doesn't fall <em>as much</em> with each new data point as it would with one of the idealized \"fully independent cost-effectiveness estimates\" discussed above.)</p>\n</li>\n<li>The specific assumption of a normal distribution isn't crucial to the above analysis. I believe (based mostly on a conversation with <a href=\"http://blog.givewell.org/2010/06/03/my-donation-for-2009-guest-post-from-dario-amodei/\">Dario Amodei</a>) that for most commonly occurring distribution types, if you hold the \"probability of 0 or less\" constant, then as the midpoint of the \"estimate/estimate error\" distribution approaches infinity the distribution becomes approximately constant (and non-negligible) over the area where the prior probability is non-negligible, resulting in a negligible effect of the estimate on the prior.\n<p>While other distributions may involve later/higher inflection points than normal distributions, the general point that there is a threshold past which higher initial estimates no longer translate to higher final estimates holds for many distributions.</p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p><strong id=\"The_GiveWell_approach\">The GiveWell approach</strong></p>\n<p>Since the beginning of our project, GiveWell has focused on maximizing the amount of good accomplished per dollar donated. Our <a href=\"http://givewell.org/about/progress#Originalbusinessplanpublished472007\">original business plan</a> (written in 2007 before we had raised any funding or gone full-time) lays out \"ideal metrics\" for charities such as</p>\n<p>&nbsp;</p>\n<blockquote>number of people whose jobs produce the income necessary to give them and their families a relatively comfortable lifestyle (including health, nourishment, relatively clean and comfortable shelter, some leisure time, and some room in the budget for luxuries), but would have been unemployed or working completely non-sustaining jobs without the charity\u2019s activities, per dollar per year. (Systematic differences in family size would complicate this.)</blockquote>\n<p>Early on, we weren't sure of whether we would find good enough information to quantify these sorts of things. After some experience, we came to the view that most cost-effectiveness analysis in the world of charity is extraordinarily rough, and we then began using a <a href=\"http://givewell.org/international/technical/criteria/cost-effectiveness#Howcosteffectiveiscosteffective\">threshold approach</a>, preferring charities whose cost-effectiveness is above a certain level but not distinguishing past that level. This approach is conceptually in line with the above analysis.</p>\n<p>It has been <a href=\"http://blog.givewell.org/2011/09/29/errors-in-dcp2-cost-effectiveness-estimate-for-deworming/#comment-236570\">remarked</a> that \"GiveWell takes a deliberately critical stance when evaluating any intervention type or charity.\" This is true, and in line with how the above analysis implies one should maximize cost-effectiveness. We generally investigate charities whose estimated cost-effectiveness is quite high in the scheme of things, and so for these charities the most important input into their <em>actual</em> cost-effectiveness is the robustness of their case and the number of factors in their favor. We critically examine these charities' claims and look for places in which they may turn out not to match reality; when we investigate these and find confirmation rather than refutation of charities' claims, we are finding new data points that support what they're saying. We're thus doing something conceptually similar to \"increasing K\" according to the model above. We've <a href=\"http://blog.givewell.org/2011/10/18/what-it-takes-to-evaluate-impact/\">recently written</a> about all the different angles we examine when strongly recommending a charity.</p>\n<p>We hope that the content we've published over the years, including recent content on cost-effectiveness (see the first paragraph of this post), has made it clear why we think we are in fact in a low-information environment, and why, therefore, the best approach is the one we've taken, which is more similar to investigative journalism or early-stage research (other domains in which people look for surprising but valid claims in low-information environments) than to formal estimation of numerical quantities.</p>\n<p>As long as the impacts of charities remain relatively poorly understood, we feel that focusing on <em>robustness of evidence</em> holds more promise than focusing on <em>quantification of impact</em>.</p>\n<p>*<em>This implies that the variance of your estimate error depends on the estimate itself. I think this is a reasonable thing to suppose in the scenario under discussion. Estimating cost-effectiveness for different charities is likely to involve using quite disparate frameworks, and the value of your estimate does contain information about the possible size of the estimate error. In our model, what stays constant across back-of-the-envelope estimates is the probability that the \"right estimate\" would be 0; this seems reasonable to me.</em></p>", "sections": [{"title": "Conceptual illustration", "anchor": "Conceptual_illustration", "level": 1}, {"title": "The GiveWell approach", "anchor": "The_GiveWell_approach", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RdpqsQ6xbHzyckW9m"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T22:30:33.798Z", "modifiedAt": null, "url": null, "title": "Help with a (potentially Bayesian) statistics / set theory problem?", "slug": "help-with-a-potentially-bayesian-statistics-set-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "joshkaufman", "createdAt": "2010-09-15T15:01:59.235Z", "isAdmin": false, "displayName": "joshkaufman"}, "userId": "EA54TKbHXmctRGueP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gAQjDWi2TcT2o2bGo/help-with-a-potentially-bayesian-statistics-set-theory", "pageUrlRelative": "/posts/gAQjDWi2TcT2o2bGo/help-with-a-potentially-bayesian-statistics-set-theory", "linkUrl": "https://www.lesswrong.com/posts/gAQjDWi2TcT2o2bGo/help-with-a-potentially-bayesian-statistics-set-theory", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20with%20a%20(potentially%20Bayesian)%20statistics%20%2F%20set%20theory%20problem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20with%20a%20(potentially%20Bayesian)%20statistics%20%2F%20set%20theory%20problem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAQjDWi2TcT2o2bGo%2Fhelp-with-a-potentially-bayesian-statistics-set-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20with%20a%20(potentially%20Bayesian)%20statistics%20%2F%20set%20theory%20problem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAQjDWi2TcT2o2bGo%2Fhelp-with-a-potentially-bayesian-statistics-set-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgAQjDWi2TcT2o2bGo%2Fhelp-with-a-potentially-bayesian-statistics-set-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 417, "htmlBody": "<p><strong>Update</strong>: as it turns out, this is a voting system problem, which is a difficult but well-studied topic. Potential solutions include <a href=\"http://en.wikipedia.org/wiki/Ranked_pairs\">Ranked Pairs</a> (complicated) and <a href=\"http://bestthing.info/algorithms.html\">BestThing</a> (simpler). Thanks to everyone for helping me think this through out loud, and for reminding me to kill flies with flyswatters instead of bazookas.</p>\n<p>\n<hr />\n</p>\n<p>I'm working on a problem that I believe involves Bayes, I'm new to Bayes and a bit rusty on statistics, and I'm having a hard time figuring out where to start. (EDIT: it looks like set theory may also be involved.) Your help would be greatly appreciated.</p>\n<p>Here's the problem: assume a set of 7 different objects. Two of these objects are presented at random to a participant, who selects whichever one of the two objects they prefer. (There is no \"indifferent\" option.) The order of these combinations is not important, and repeated combinations are not allowed.</p>\n<p>Basic combination theory says there are 21 different possible combinations: (7!) / ( (2!) * (7-2)! ) = 21.</p>\n<p>Now, assume the researcher wants to know which single option has the highest probability of being the \"most preferred\" to a new participant based on the responses of all previous participants. To complicate matters, each participant can leave at any time, without completing the entire set of 21 responses. Their responses should still factor into the final result, even if they only respond to a single combination.</p>\n<p>At the beginning of the study, there are no priors. (CORRECTION via dlthomas:&nbsp;<span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\"T</span><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">here are necessarily priors...&nbsp;</span><span style=\"background-color: #f7f7f8; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">we start with no information about rankings... and so assume a 1:1 chance of either object being preferred.</span>) If a participant selects B from&nbsp;{A,B}, the probability of B being the \"most preferred\" object should go up, and A should go down, if I'm understanding correctly.</p>\n<p>NOTE:&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Direct ranking of objects 1-7 (instead of pairwise comparison) isn't ideal because it takes longer, which may encourage the participant to rationalize. The \"pick-one-of-two\" approach is designed to be fast, which is better for gut reactions when comparing simple objects like words, photos, etc.</span></p>\n<p>The ideal output looks like this: \"Based on ___ total responses, participants prefer Object A. Object A is preferred __% more than Object B (the second most preferred), and ___% more than Object C (the third most preferred).\"</p>\n<p><strong>Questions:</strong></p>\n<p>1. Is Bayes actually the most straightforward way of calculating the \"most preferred\"? (If not, what is? I don't want to be Maslow's&nbsp;\"man with a hammer\" here.)</p>\n<p>2. If so, can you please walk me through the beginning of how this calculation is done, assuming 10 participants?</p>\n<p>Thanks in advance!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gAQjDWi2TcT2o2bGo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 7.975573984884633e-07, "legacy": true, "legacyId": "10872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-10T22:41:46.098Z", "modifiedAt": null, "url": null, "title": "Meetup : First(New?) Waterloo Meetup", "slug": "meetup-first-new-waterloo-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.162Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paamayim", "createdAt": "2009-04-17T21:20:46.668Z", "isAdmin": false, "displayName": "Paamayim"}, "userId": "4W4hmXdXqHnfMQHHv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bWG2HkkmbuHqMjS8P/meetup-first-new-waterloo-meetup", "pageUrlRelative": "/posts/bWG2HkkmbuHqMjS8P/meetup-first-new-waterloo-meetup", "linkUrl": "https://www.lesswrong.com/posts/bWG2HkkmbuHqMjS8P/meetup-first-new-waterloo-meetup", "postedAtFormatted": "Thursday, November 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First(New%3F)%20Waterloo%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First(New%3F)%20Waterloo%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbWG2HkkmbuHqMjS8P%2Fmeetup-first-new-waterloo-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First(New%3F)%20Waterloo%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbWG2HkkmbuHqMjS8P%2Fmeetup-first-new-waterloo-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbWG2HkkmbuHqMjS8P%2Fmeetup-first-new-waterloo-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4p'>First(New?) Waterloo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2011 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Meetpoint, 150 University Ave W, Waterloo, Ontario</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LWers from Waterloo - of whom I know there are a few - assemble!</p>\n\n<p>It looks like there have been a few meetups in the past, but nobody has been responding to my PM's, so I thought I'd take matters into my own hands. Rationality is too important to wait around for.</p>\n\n<p>As a topic, I was thinking \"what does LW mean to you?\", but in my experience the conversation usually takes its own wings.</p>\n\n<p>Hope to see everyone there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4p'>First(New?) Waterloo Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bWG2HkkmbuHqMjS8P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 7.975613431902013e-07, "legacy": true, "legacyId": "10873", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_New___Waterloo_Meetup\">Discussion article for the meetup : <a href=\"/meetups/4p\">First(New?) Waterloo Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2011 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Meetpoint, 150 University Ave W, Waterloo, Ontario</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>LWers from Waterloo - of whom I know there are a few - assemble!</p>\n\n<p>It looks like there have been a few meetups in the past, but nobody has been responding to my PM's, so I thought I'd take matters into my own hands. Rationality is too important to wait around for.</p>\n\n<p>As a topic, I was thinking \"what does LW mean to you?\", but in my experience the conversation usually takes its own wings.</p>\n\n<p>Hope to see everyone there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_New___Waterloo_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/4p\">First(New?) Waterloo Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First(New?) Waterloo Meetup", "anchor": "Discussion_article_for_the_meetup___First_New___Waterloo_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First(New?) Waterloo Meetup", "anchor": "Discussion_article_for_the_meetup___First_New___Waterloo_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T01:04:37.087Z", "modifiedAt": null, "url": null, "title": "Which fields of learning have clarified your thinking? How and why?", "slug": "which-fields-of-learning-have-clarified-your-thinking-how", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:27.549Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Arkanj3l", "createdAt": "2011-04-23T03:48:47.569Z", "isAdmin": false, "displayName": "Arkanj3l"}, "userId": "nQmA4dnBdX99WyCt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jjf98bkdefG8yuvxc/which-fields-of-learning-have-clarified-your-thinking-how", "pageUrlRelative": "/posts/Jjf98bkdefG8yuvxc/which-fields-of-learning-have-clarified-your-thinking-how", "linkUrl": "https://www.lesswrong.com/posts/Jjf98bkdefG8yuvxc/which-fields-of-learning-have-clarified-your-thinking-how", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20fields%20of%20learning%20have%20clarified%20your%20thinking%3F%20How%20and%20why%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20fields%20of%20learning%20have%20clarified%20your%20thinking%3F%20How%20and%20why%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjf98bkdefG8yuvxc%2Fwhich-fields-of-learning-have-clarified-your-thinking-how%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20fields%20of%20learning%20have%20clarified%20your%20thinking%3F%20How%20and%20why%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjf98bkdefG8yuvxc%2Fwhich-fields-of-learning-have-clarified-your-thinking-how", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJjf98bkdefG8yuvxc%2Fwhich-fields-of-learning-have-clarified-your-thinking-how", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 98, "htmlBody": "<p>Did computer programming make you a clearer, more precise thinker? How about mathematics? If so, what kind? Set theory? Probability theory?</p>\n<p>Microeconomics? Poker? English? Civil Engineering? Underwater Basket Weaving? (For adding... <em>depth.</em>)</p>\n<p>Anything I missed?</p>\n<p>Context: I have a palette of courses to dab onto my university schedule, and I don't know which ones to chose. This much is for certain: I want to come out of university as a problem solving <em>beast</em>. If there are fields of inquiry whose methods easily transfer to other fields, it is those fields that I want to learn in, at least&nbsp;initially.</p>\n<p>Rip apart, Less Wrong!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jjf98bkdefG8yuvxc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 20, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "10875", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T01:26:52.017Z", "modifiedAt": null, "url": null, "title": "Do the people behind the veil of ignorance vote for \"specks\"?", "slug": "do-the-people-behind-the-veil-of-ignorance-vote-for-specks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:26.369Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D227", "createdAt": "2011-07-20T16:22:50.660Z", "isAdmin": false, "displayName": "D227"}, "userId": "MZNDKaBencf7N9kGt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zRX2o4B8JoxH2hzno/do-the-people-behind-the-veil-of-ignorance-vote-for-specks", "pageUrlRelative": "/posts/zRX2o4B8JoxH2hzno/do-the-people-behind-the-veil-of-ignorance-vote-for-specks", "linkUrl": "https://www.lesswrong.com/posts/zRX2o4B8JoxH2hzno/do-the-people-behind-the-veil-of-ignorance-vote-for-specks", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20the%20people%20behind%20the%20veil%20of%20ignorance%20vote%20for%20%22specks%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20the%20people%20behind%20the%20veil%20of%20ignorance%20vote%20for%20%22specks%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRX2o4B8JoxH2hzno%2Fdo-the-people-behind-the-veil-of-ignorance-vote-for-specks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20the%20people%20behind%20the%20veil%20of%20ignorance%20vote%20for%20%22specks%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRX2o4B8JoxH2hzno%2Fdo-the-people-behind-the-veil-of-ignorance-vote-for-specks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzRX2o4B8JoxH2hzno%2Fdo-the-people-behind-the-veil-of-ignorance-vote-for-specks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 872, "htmlBody": "<blockquote>\n<p>The veil of ignorance as Rawls put it<span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">&nbsp;</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">...\"no one knows his place in society, his class position or social status; nor does he know his fortune in the distribution of natural assets and abilities, his intelligence and strength, and the like.\"</span><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19px;\">&nbsp;</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">The device allows for certain issues like slavery and income&nbsp;distribution&nbsp;to be determined beforehand. &nbsp;Would one vote for a society in which there is a chance of severe misfortune, but greater total utility? &nbsp;e.g, a world where 1% earn $1 a day and 99% earn $1,000,000 vs. a world where everyone earns $900,000 a day. &nbsp;Assume that dollars are utilons and they are linear (2 dollars indeed gives twice as much utility). &nbsp;What is the obvious answer? &nbsp;Bob chooses $900,000 a day for everyone. &nbsp;</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">But Bob is clever and he does not trust himself that his choice is the rational choice, so he goes into self-dialogue to investigate:</span></span></p>\n<blockquote>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">Q: \"What is my preference, value or goal<strong>(PVG)</strong>, such that, instrumental rationality may&nbsp;achieve&nbsp;it?\"</span></span></p>\n<p><span style=\"font-family: sans-serif; line-height: 19px;\">A \"I my preference/value/goal is for there to be a world in which total utility is less, but severe misfortune eliminated for everyone\"</span></p>\n<p><span style=\"font-family: sans-serif; line-height: 19px;\">Q \" As an agent are you maximizing your own utility by your actions of choosing a $900,000 a day world?</span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">A \" Yes, my actions are consistent&nbsp;with my preferences; I will maximize my utility by achieving my preference of limiting&nbsp;everyone's&nbsp;utility. &nbsp;This preference takes precedence.</span></span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">Q: \"I will now attack your position with the transitivity&nbsp;argument.&nbsp;&nbsp;At which point does your consistency change? &nbsp;What if the choices where 1% earns $999,999 and 99% earn 1,000,000?\"</span></span></p>\n<p><span style=\"font-family: sans-serif;\"><span style=\"line-height: 19px;\">A: \"My preference,values and goals have already determined a threshold, in fact my threshold is my PVG. &nbsp;Regardless the fact that my threshold may be different from everyone else's threshold, my threshold is my PVG. &nbsp;And achieving my PVG is rational.\"</span></span></p>\n<p>Q: \"I will now attack your position one last time, with the \"piling argument\". &nbsp;If&nbsp;every time&nbsp;you save one person from&nbsp;destitution,&nbsp;you must pile on the punishment on the others such that everyone will be suffering.\"</p>\n<p>A: \"If piling is allowed then it is to me a completely different question. &nbsp;Altering what my PVG is. &nbsp;I have one set of values for a non piling and piling scenario. &nbsp;I am consistent because piling and not piling are two different problems.\"</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>In the insurance industry, purchasing insurance comes with a price. &nbsp;Perhaps 1.5% premium of the cost of reimbursing you for your house that may burn down. &nbsp;The&nbsp;actuaries&nbsp;have run the probabilities and determine that you have a 1% chance that your house will burn down. &nbsp;Assume that all dollar amounts are utilons across all assets. &nbsp;Bob once again is a rational man. &nbsp;Every year Bob is chooses to pay 1.5% in premium even though his average risk is technically a 1% loss, because Bob is risk adverse. So risk adverse that he&nbsp;prefers&nbsp;a world in which he has less wealth, the .5% went to the insurance companies making a profit. Once again Bob questions his rationality on purchasing insurance:</p>\n<blockquote>\n<p>Q: \"What is my preference?\"</p>\n<p>A: \"I would&nbsp;prefer&nbsp;to sacrifice more than my share of losses( .5% more), for the safety-net of zero chance&nbsp;catastrophic&nbsp;loss.\"</p>\n<p>Q \"Are your actions achieving your values?\"</p>\n<p>A \"Yes, I purchased insurance, maximizing my preference for safety.\"</p>\n<p>Q \"Shall I attack you with the transitivity argument?\"</p>\n<p>A \"It wont work. &nbsp;I have already set my PVG, it is a premium price at which I judge to make the costs prohibitive. &nbsp;I will not pay 99% premium to&nbsp;protect&nbsp;my house , but I will pay 5%.\"</p>\n<p>Q \"Piling?\"</p>\n<p>A \"This is a different problem now.\"</p>\n</blockquote>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">Eliezer's&nbsp;post on Torture vs. Dust Specks [<a title=\"Here\" href=\"/lw/kn/torture_vs_dust_specks/\" target=\"_blank\">Here</a>]&nbsp;</span>has generated lots of discussion as well as what Eliezer&nbsp;describes&nbsp;as interesting [<a title=\"Ways\" href=\"/lw/km/motivated_stopping_and_motivated_continuation/\">ways</a>]&nbsp;of [<a title=\"avoiding\" href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\" target=\"_blank\">avoiding</a>]&nbsp;the question. &nbsp;We will do no sort of thing in this post, we will answer the question as intended;&nbsp;I will interpret that eye specks is&nbsp;cumulatively&nbsp;greater suffering than the suffering of 50 years.&nbsp;</p>\n<p>&nbsp;My PVG tells me that I would rather have a speck in my eye, as well as the eye's of 3^^^3 people, than to risk to have one (perhaps me) suffer torture for 50 years, even though my risk is only 1/(3^^^3) which is a lot less than 50 years (Veil of ignorance). &nbsp;My PVG is what I will maximize, and doing so is the definition of&nbsp;instrumental&nbsp;rationality. &nbsp;</p>\n<p>In short, the rational answer is not TORTURE or SPECKS, but <strong>depends on what your preference, values and goals are</strong>. &nbsp;You may be one of those whose preference is to let that one person feel torture for 50 years, as long as your a<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">ctions that steer the future toward outcomes</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">&nbsp;</span><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">ranked higher in your preferences, you are right too.</span></p>\n<p>Correct me if I am wrong but I thought rationality did not imply that there were absolute rational preferences, but rather rational ways to achieve your preferences...</p>\n<p>&nbsp;</p>\n<p><strong>I want to emphasize that in no way did I intend for this post to declare anything. &nbsp;And want to thank everyone in advance for picking apart every single word I have written. &nbsp;Being wrong is like winning the lottery. &nbsp;I do not claim to know anything, the&nbsp;assertive&nbsp;manner in which I wrote this post was merely a way to convey my ideas, of which, I am not sure off. </strong>&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zRX2o4B8JoxH2hzno", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -1, "extendedScore": null, "score": 7.97619470042316e-07, "legacy": true, "legacyId": "10876", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "L32LHWzy9FzSDazEg", "dHQkDNMhj692ayx78"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T01:48:02.411Z", "modifiedAt": null, "url": null, "title": "Transhumanism and Gender Relations", "slug": "transhumanism-and-gender-relations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/porPWLbSbqvySCkea/transhumanism-and-gender-relations", "pageUrlRelative": "/posts/porPWLbSbqvySCkea/transhumanism-and-gender-relations", "linkUrl": "https://www.lesswrong.com/posts/porPWLbSbqvySCkea/transhumanism-and-gender-relations", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transhumanism%20and%20Gender%20Relations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranshumanism%20and%20Gender%20Relations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FporPWLbSbqvySCkea%2Ftranshumanism-and-gender-relations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transhumanism%20and%20Gender%20Relations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FporPWLbSbqvySCkea%2Ftranshumanism-and-gender-relations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FporPWLbSbqvySCkea%2Ftranshumanism-and-gender-relations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 583, "htmlBody": "<p>Upon reading Eliezer's possible gender dystopias ([catgirls](<a href=\"/lw/xt/interpersonal_entanglement/\">http://lesswrong.com/lw/xt/interpersonal_entanglement/</a>), and [verthandi](<a href=\"/lw/xu/failed_utopia_42/\">http://lesswrong.com/lw/xu/failed_utopia_42/</a>)&nbsp;and the other LW comments and posts on the subject of future gender relations, I came to a rather different conclusion than the ones I've seen espoused here. After searching around the internet a bit, I discovered that my ideas tend to fall under the general category of \"postgenderism\", and I am wondering what my fellow LessWrongians think of it.&nbsp;</p>\n<p>This can generally be broken down to the following claims:</p>\n<ol>\n<li>A higher level of egalitarianism between the sexes increases utility.&nbsp;For example, if only men are generally allowed to do Job A, then you are halving the talent pool of people who can do Job A, AND women who would otherwise be happy in Job A lose that utility. It is equally of dis-utility if only women are socially allowed to be emotionally expressive, etc, etc. In other words, Equality = Good.</li>\n<li>The differences between men and women are a mix of environmental factors, such as social conditioning, and biological factors, such as varying levels of hormones.</li>\n<li>Some of these differences are optimal in the current environment, and others are suboptimal. For example-Women have better social skills (good!), but are more prone to depression (bad). Men are better self-promoters (good!), but are more prone to suicide (bad).</li>\n<li>Should transhumanism occur, it will eliminate the suboptimal differences. We can help people become less suicidal and not depressed.</li>\n<li>This will lead to a spiraling effect- Fewer *actual* differences will lead to a lessening of socialized differences, which will lead to less actual differences, etc</li>\n</ol>\n<div>I am positive at least *some* of this will occur. In fact, I would posit that it is already occuring in that we have drugs to treat depression, and hormone therapy to assist people with hormonal problems, etc. Some postgenderists claim there will be no need for different genders at all, as we can have male pregnancies, and female paternity, etc. I rather doubt it would go that far.</div>\n<div>My questions to you all is-&nbsp;When do you see these changes occuring, if at all?&nbsp;How far do you think we will go? Will we end up as essentially a one-gender society? Will we move towards the middle but still retain some gender differences? What would they be? Will they be purely physical, or some psychological differences as well?&nbsp;</div>\n<div><br /></div>\n<div>A possible contention is that the forces of natural selection would not allow this to go too far. In other words, women are attracted to high-testosterone men, and men are attracted to feminine women. However, we already have studies that show that birth control can make women desire lower testosterone men, and I don't see a reason why we wouldn't end up just being attracted to the new/different standard.</div>\n<div>Another possible contention is that people *like* having both men and women about. This is an acceptable argument, but is based solely on personal taste. I would also surmise that most the people who desire the current two-gender system to persist are men (and therefore used to being the dominant gender, and therefore not personally feeling a reason to change it).</div>\n<div><br /></div>\n<div>One Good Reference-</div>\n<div>[Postgenderism: Beyond the Gender Binary](<a href=\"http://ieet.org/archive/IEET-03-PostGender.pdf\">http://ieet.org/archive/IEET-03-PostGender.pdf</a>)&nbsp;,&nbsp;George Dvorsky and James Hughes, PhD, IEET-03, March 2008</div>\n<p>&nbsp;</p>\n<p>EDIT- Due to some really insightful comments;</p>\n<p>I replaced men being prone to aggression as a negative, with men being prone to suicide.</p>\n<p><br />I made the verbiage a little more explicit that no one would be *forced* to change, but would seek out the changes that transhumanism would have available.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jiuackr7B5JAetbF6": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "porPWLbSbqvySCkea", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 11, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "10874", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Py3uGnncqXuEfPtQp", "ctpkTaqTKbmm6uRgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T01:48:58.825Z", "modifiedAt": null, "url": null, "title": "Meetup : The Planning Fallacy", "slug": "meetup-the-planning-fallacy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/n8djCbYsZ4jnd69ES/meetup-the-planning-fallacy", "pageUrlRelative": "/posts/n8djCbYsZ4jnd69ES/meetup-the-planning-fallacy", "linkUrl": "https://www.lesswrong.com/posts/n8djCbYsZ4jnd69ES/meetup-the-planning-fallacy", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20The%20Planning%20Fallacy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20The%20Planning%20Fallacy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8djCbYsZ4jnd69ES%2Fmeetup-the-planning-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20The%20Planning%20Fallacy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8djCbYsZ4jnd69ES%2Fmeetup-the-planning-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fn8djCbYsZ4jnd69ES%2Fmeetup-the-planning-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4q'>The Planning Fallacy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 November 2011 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4133 University Way NE, Seattle, WA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup this Sunday at 4pm since we haven't had one in a while. We'll be talking about The Planning Fallacy, when we've been burned by it, when and why it happens and how to ameliorate it.</p>\n\n<p>See you folks there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4q'>The Planning Fallacy</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "n8djCbYsZ4jnd69ES", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.976272560567999e-07, "legacy": true, "legacyId": "10881", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___The_Planning_Fallacy\">Discussion article for the meetup : <a href=\"/meetups/4q\">The Planning Fallacy</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 November 2011 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4133 University Way NE, Seattle, WA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're going to have a meetup this Sunday at 4pm since we haven't had one in a while. We'll be talking about The Planning Fallacy, when we've been burned by it, when and why it happens and how to ameliorate it.</p>\n\n<p>See you folks there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___The_Planning_Fallacy1\">Discussion article for the meetup : <a href=\"/meetups/4q\">The Planning Fallacy</a></h2>", "sections": [{"title": "Discussion article for the meetup : The Planning Fallacy", "anchor": "Discussion_article_for_the_meetup___The_Planning_Fallacy", "level": 1}, {"title": "Discussion article for the meetup : The Planning Fallacy", "anchor": "Discussion_article_for_the_meetup___The_Planning_Fallacy1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T03:33:25.489Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Halo Effect", "slug": "seq-rerun-the-halo-effect", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:23.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zcLyqW29WiWDmZKEN/seq-rerun-the-halo-effect", "pageUrlRelative": "/posts/zcLyqW29WiWDmZKEN/seq-rerun-the-halo-effect", "linkUrl": "https://www.lesswrong.com/posts/zcLyqW29WiWDmZKEN/seq-rerun-the-halo-effect", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Halo%20Effect&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Halo%20Effect%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzcLyqW29WiWDmZKEN%2Fseq-rerun-the-halo-effect%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Halo%20Effect%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzcLyqW29WiWDmZKEN%2Fseq-rerun-the-halo-effect", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzcLyqW29WiWDmZKEN%2Fseq-rerun-the-halo-effect", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>Today's post, <a href=\"/lw/lj/the_halo_effect/\">The Halo Effect</a> was originally published on 30 November 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Positive qualities <em>seem </em>to correlate with each other, whether or not they <em>actually </em>do.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8ds/seq_rerun_unbounded_scales_huge_jury_awards/\">Unbounded Scales, Huge Jury Awards, &amp; Futurism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb182": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zcLyqW29WiWDmZKEN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.976640326584565e-07, "legacy": true, "legacyId": "10883", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ACGeaAk6KButv2xwQ", "D2adfyjbgPCYz8ua6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T06:04:50.875Z", "modifiedAt": null, "url": null, "title": "Random Flu Thoughts, including vaccination and base rates", "slug": "random-flu-thoughts-including-vaccination-and-base-rates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:09.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/npri4hAytrtBB4XEb/random-flu-thoughts-including-vaccination-and-base-rates", "pageUrlRelative": "/posts/npri4hAytrtBB4XEb/random-flu-thoughts-including-vaccination-and-base-rates", "linkUrl": "https://www.lesswrong.com/posts/npri4hAytrtBB4XEb/random-flu-thoughts-including-vaccination-and-base-rates", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Random%20Flu%20Thoughts%2C%20including%20vaccination%20and%20base%20rates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARandom%20Flu%20Thoughts%2C%20including%20vaccination%20and%20base%20rates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnpri4hAytrtBB4XEb%2Frandom-flu-thoughts-including-vaccination-and-base-rates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Random%20Flu%20Thoughts%2C%20including%20vaccination%20and%20base%20rates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnpri4hAytrtBB4XEb%2Frandom-flu-thoughts-including-vaccination-and-base-rates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnpri4hAytrtBB4XEb%2Frandom-flu-thoughts-including-vaccination-and-base-rates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>I thought some of you might find this interesting, <a href=\"http://www.sciencebasedmedicine.org/index.php/random-flu-thoughts\">Random Flu Thoughts</a>. The last part has a pretty good, if elementary, illustration of the importance of base rates.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "npri4hAytrtBB4XEb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.977173565939072e-07, "legacy": true, "legacyId": "10884", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T09:15:32.070Z", "modifiedAt": null, "url": null, "title": "A Brief Rant On The Future Of Interaction Design [link]", "slug": "a-brief-rant-on-the-future-of-interaction-design-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:24.234Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/asF54qDYgPL3e4tRa/a-brief-rant-on-the-future-of-interaction-design-link", "pageUrlRelative": "/posts/asF54qDYgPL3e4tRa/a-brief-rant-on-the-future-of-interaction-design-link", "linkUrl": "https://www.lesswrong.com/posts/asF54qDYgPL3e4tRa/a-brief-rant-on-the-future-of-interaction-design-link", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Brief%20Rant%20On%20The%20Future%20Of%20Interaction%20Design%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Brief%20Rant%20On%20The%20Future%20Of%20Interaction%20Design%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasF54qDYgPL3e4tRa%2Fa-brief-rant-on-the-future-of-interaction-design-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Brief%20Rant%20On%20The%20Future%20Of%20Interaction%20Design%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasF54qDYgPL3e4tRa%2Fa-brief-rant-on-the-future-of-interaction-design-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FasF54qDYgPL3e4tRa%2Fa-brief-rant-on-the-future-of-interaction-design-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://worrydream.com/ABriefRantOnTheFutureOfInteractionDesign/\">http://worrydream.com/ABriefRantOnTheFutureOfInteractionDesign/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "asF54qDYgPL3e4tRa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 2, "extendedScore": null, "score": 7.977845161849875e-07, "legacy": true, "legacyId": "10886", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T13:52:52.936Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Melbourne, Canberra, Zurich, Mountain View", "slug": "weekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:02.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hGho32EroGZdmwQDG/weekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "pageUrlRelative": "/posts/hGho32EroGZdmwQDG/weekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "linkUrl": "https://www.lesswrong.com/posts/hGho32EroGZdmwQDG/weekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Canberra%2C%20Zurich%2C%20Mountain%20View&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Melbourne%2C%20Canberra%2C%20Zurich%2C%20Mountain%20View%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGho32EroGZdmwQDG%2Fweekly-lw-meetups-melbourne-canberra-zurich-mountain-view%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Melbourne%2C%20Canberra%2C%20Zurich%2C%20Mountain%20View%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGho32EroGZdmwQDG%2Fweekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhGho32EroGZdmwQDG%2Fweekly-lw-meetups-melbourne-canberra-zurich-mountain-view", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/4e\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">04 November 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/4a\">Canberra Meetup Saturday November 5th:&nbsp;<span class=\"date\">05 November 2011 11:00AM</span></a></li>\n<li><a href=\"/meetups/4h\">First Zurich LW Meetup:&nbsp;<span class=\"date\">12 November 2011 03:00PM</span></a></li>\n</ul>\n<p><strong>Notice for the Mountain View/Tortuga meetup: </strong>The Mountain View meetup is moving to *Mondays* (still at 7pm, still at Tortuga); and over the next few weeks will present pieces of the \"Detaching from Sunk Costs\" rationality kata currently under development.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>Despite the handy sidebar of upcoming meetups, we've decided to continue posting an overview of upcoming meetups on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hGho32EroGZdmwQDG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 7.978822156345713e-07, "legacy": true, "legacyId": "10780", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T21:30:21.179Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison Monday Meetup: Tegmark universes", "slug": "meetup-madison-monday-meetup-tegmark-universes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.182Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G8T4ZXfEYWQw3Cz7j/meetup-madison-monday-meetup-tegmark-universes", "pageUrlRelative": "/posts/G8T4ZXfEYWQw3Cz7j/meetup-madison-monday-meetup-tegmark-universes", "linkUrl": "https://www.lesswrong.com/posts/G8T4ZXfEYWQw3Cz7j/meetup-madison-monday-meetup-tegmark-universes", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%20Monday%20Meetup%3A%20Tegmark%20universes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%20Monday%20Meetup%3A%20Tegmark%20universes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG8T4ZXfEYWQw3Cz7j%2Fmeetup-madison-monday-meetup-tegmark-universes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20Monday%20Meetup%3A%20Tegmark%20universes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG8T4ZXfEYWQw3Cz7j%2Fmeetup-madison-monday-meetup-tegmark-universes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG8T4ZXfEYWQw3Cz7j%2Fmeetup-madison-monday-meetup-tegmark-universes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 91, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4r'>Madison Monday Meetup: Tegmark universes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2011 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1831 Monroe St. Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Again, we'll meet at the <a href=\"http://www.barriquesmarket.com/?page_id=1177\" rel=\"nofollow\">Barrique&#39;s on Monroe St.</a> Patrick will describe and discuss Max Tegmark's multiverse theory (<a href=\"http://arxiv.org/PS_cache/arxiv/pdf/0905/0905.1283v1.pdf\" rel=\"nofollow\">pdf</a>. <a href=\"http://arxiv.org/PS_cache/astro-ph/pdf/0302/0302131v1.pdf\" rel=\"nofollow\">pdf</a>). I'll make a number of incredulous comments. Coffee, conversation, maybe a couple rounds of Zendo or The Resistance. And again, if you're interested in meeting LW folk in Madison, you should sign up on the Madison meetup <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4r'>Madison Monday Meetup: Tegmark universes</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G8T4ZXfEYWQw3Cz7j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 7.980434120740152e-07, "legacy": true, "legacyId": "10888", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup__Tegmark_universes\">Discussion article for the meetup : <a href=\"/meetups/4r\">Madison Monday Meetup: Tegmark universes</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2011 06:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1831 Monroe St. Madison, WI</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Again, we'll meet at the <a href=\"http://www.barriquesmarket.com/?page_id=1177\" rel=\"nofollow\">Barrique's on Monroe St.</a> Patrick will describe and discuss Max Tegmark's multiverse theory (<a href=\"http://arxiv.org/PS_cache/arxiv/pdf/0905/0905.1283v1.pdf\" rel=\"nofollow\">pdf</a>. <a href=\"http://arxiv.org/PS_cache/astro-ph/pdf/0302/0302131v1.pdf\" rel=\"nofollow\">pdf</a>). I'll make a number of incredulous comments. Coffee, conversation, maybe a couple rounds of Zendo or The Resistance. And again, if you're interested in meeting LW folk in Madison, you should sign up on the Madison meetup <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Madison_Monday_Meetup__Tegmark_universes1\">Discussion article for the meetup : <a href=\"/meetups/4r\">Madison Monday Meetup: Tegmark universes</a></h2>", "sections": [{"title": "Discussion article for the meetup : Madison Monday Meetup: Tegmark universes", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup__Tegmark_universes", "level": 1}, {"title": "Discussion article for the meetup : Madison Monday Meetup: Tegmark universes", "anchor": "Discussion_article_for_the_meetup___Madison_Monday_Meetup__Tegmark_universes1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-11T21:51:07.735Z", "modifiedAt": null, "url": null, "title": "Sometimes, talking the issue through *works*", "slug": "sometimes-talking-the-issue-through-works", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.637Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bGFvsvohynEp9oaZC/sometimes-talking-the-issue-through-works", "pageUrlRelative": "/posts/bGFvsvohynEp9oaZC/sometimes-talking-the-issue-through-works", "linkUrl": "https://www.lesswrong.com/posts/bGFvsvohynEp9oaZC/sometimes-talking-the-issue-through-works", "postedAtFormatted": "Friday, November 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Sometimes%2C%20talking%20the%20issue%20through%20*works*&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASometimes%2C%20talking%20the%20issue%20through%20*works*%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGFvsvohynEp9oaZC%2Fsometimes-talking-the-issue-through-works%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Sometimes%2C%20talking%20the%20issue%20through%20*works*%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGFvsvohynEp9oaZC%2Fsometimes-talking-the-issue-through-works", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbGFvsvohynEp9oaZC%2Fsometimes-talking-the-issue-through-works", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 422, "htmlBody": "<p>Michael Nielsen's new book <em><a href=\"http://www.amazon.com/Reinventing-Discovery-New-Networked-Science/dp/0691148902/\">Reinventing Discovery</a></em>&nbsp;is invigorating. Here's one passage on how a small group talked an issue through and had a large impact on scientific progress:</p>\n<blockquote>\n<p>\n<p>Why is it that biologists share genetic data in GenBank in the first place? When you think about it, it&rsquo;s a peculiar choice: if you&rsquo;re a professional biologist it&rsquo;s to your advantage to keep data secret as long as possible. Why share your data online before you&nbsp;get a chance to publish a paper or take out a patent on your work? In the scientific world it&rsquo;s papers and, in some fields, patents that are rewarded by jobs and promotions. Publicly releasing data typically does nothing for your career, and might even damage it, by helping your scientific competitors.</p>\n<p>In part for these reasons, GenBank took off slowly after it was launched in 1982. While many biologists were happy to access others&rsquo; data in GenBank, they had little interest in contributing their own data. But that has changed over time. Part of the reason for the change was a historic conference held in Bermuda in 1996, and attended by many of the world&rsquo;s leading biologists, including several of the leaders of the government-sponsored Human Genome Project. Also present was Craig Venter, who would later lead a private effort to sequence the human genome. Although many attendees weren&rsquo;t willing to unilaterally make the first move to share all their genetic data in advance of publication, everyone could see that science as a whole would benefit enormously if open sharing of data became common practice. So they sat and talked the issue over for days, eventually coming to a joint agreement&mdash;now known as the Bermuda Agreement&mdash;that all human genetic data should be immediately shared online. The agreement wasn&rsquo;t just empty rhetoric. The biologists in the room had enough clout that they convinced several major scientific grant agencies to make immediate data sharing a mandatory requirement of working on the human genome. Scientists who refused to share data would get no grant money to do research. This changed the game, and immediate sharing of human genetic data became the norm. The Bermuda agreement eventually made its way to the highest levels of government: on March 14, 2000, US President Bill Clinton and UK Prime Minister Tony Blair issued a joint statement praising the principles described in the Bermuda Agreement, and urging scientists in every country to adopt similar principles. It&rsquo;s because of the Bermuda&nbsp;Agreement and similar subsequent agreements that the human genome and the HapMap are publicly available.</p>\n</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bGFvsvohynEp9oaZC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 26, "extendedScore": null, "score": 7.980507341260047e-07, "legacy": true, "legacyId": "10889", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-12T00:26:02.510Z", "modifiedAt": null, "url": null, "title": "Wiki Spam", "slug": "wiki-spam", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dbaupp", "createdAt": "2011-06-26T05:11:09.036Z", "isAdmin": false, "displayName": "dbaupp"}, "userId": "k7vtKDR5ZwYxrGEaM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gBQ5qTbeTEe8xh4kz/wiki-spam", "pageUrlRelative": "/posts/gBQ5qTbeTEe8xh4kz/wiki-spam", "linkUrl": "https://www.lesswrong.com/posts/gBQ5qTbeTEe8xh4kz/wiki-spam", "postedAtFormatted": "Saturday, November 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Wiki%20Spam&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWiki%20Spam%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBQ5qTbeTEe8xh4kz%2Fwiki-spam%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Wiki%20Spam%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBQ5qTbeTEe8xh4kz%2Fwiki-spam", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgBQ5qTbeTEe8xh4kz%2Fwiki-spam", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>Recently, I have noticed that the \"Recent Wiki Edits\" box (equivalently, <a href=\"http://wiki.lesswrong.com/wiki/Special:RecentChanges\">this page</a>) in the sidebar seems to be almost exclusively filled with the edits from spam bots and either <a href=\"/user/gwern/\">Gwern</a> or <a href=\"/user/Vladimir_Nesov/\">Vladimir Nesov</a> cleaning up after them (thanks!). This seems like it should be fixed, if only to save those two the time they spend on maintenance.</p>\n<p>The <a href=\"http://wiki.lesswrong.com\">wiki</a> <a href=\"/wiki.lesswrong.com/mediawiki/index.php?title=Special:UserLogin&amp;type=signup\">registration form</a> does have a <a href=\"/en.wikipedia.org/wiki/ReCAPTCHA\">reCAPTCHA</a> in an attempt to block spam-bots, but this is apparently not effective enough (maybe because reCAPTCHA has been cracked, or the spammers are using humans in some way).</p>\n<p>I have some possible solutions, but I shall wait a bit before <a href=\"/lw/ka/hold_off_on_proposing_solutions\">suggesting them</a>.</p>\n<p>(I vaguely remember there being a discussion like this previously, but I can't find it again, if it exists.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gBQ5qTbeTEe8xh4kz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 7.98105333882134e-07, "legacy": true, "legacyId": "10890", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-12T06:30:27.822Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Superhero Bias", "slug": "seq-rerun-superhero-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.198Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DBkd8PfnbNQtyLRCT/seq-rerun-superhero-bias", "pageUrlRelative": "/posts/DBkd8PfnbNQtyLRCT/seq-rerun-superhero-bias", "linkUrl": "https://www.lesswrong.com/posts/DBkd8PfnbNQtyLRCT/seq-rerun-superhero-bias", "postedAtFormatted": "Saturday, November 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Superhero%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Superhero%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDBkd8PfnbNQtyLRCT%2Fseq-rerun-superhero-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Superhero%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDBkd8PfnbNQtyLRCT%2Fseq-rerun-superhero-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDBkd8PfnbNQtyLRCT%2Fseq-rerun-superhero-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Today's post, <a href=\"/lw/lk/superhero_bias/\">Superhero Bias</a> was originally published on 01 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is better to risk your life to save 200 people than to save 3. But someone who risks their life to save 3 people is revealing a more altruistic nature than someone risking their life to save 200. And yet comic books are written about heroes who save 200 innocent schoolchildren, and not police officers saving three prostitutes.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8eb/seq_rerun_the_halo_effect/\">The Halo Effect</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DBkd8PfnbNQtyLRCT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 7.982338020079885e-07, "legacy": true, "legacyId": "10896", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["krMzmSXgvEdf7iBT6", "zcLyqW29WiWDmZKEN", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-12T18:22:57.453Z", "modifiedAt": null, "url": null, "title": "The promise of connected science", "slug": "the-promise-of-connected-science", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.291Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xyNP4B5CHfAM3K4Sz/the-promise-of-connected-science", "pageUrlRelative": "/posts/xyNP4B5CHfAM3K4Sz/the-promise-of-connected-science", "linkUrl": "https://www.lesswrong.com/posts/xyNP4B5CHfAM3K4Sz/the-promise-of-connected-science", "postedAtFormatted": "Saturday, November 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20promise%20of%20connected%20science&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20promise%20of%20connected%20science%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyNP4B5CHfAM3K4Sz%2Fthe-promise-of-connected-science%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20promise%20of%20connected%20science%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyNP4B5CHfAM3K4Sz%2Fthe-promise-of-connected-science", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxyNP4B5CHfAM3K4Sz%2Fthe-promise-of-connected-science", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>Sometimes, scientific discovery is just a matter of sitting down and <em>using</em>&nbsp;the tools of \"connected science\" already available to us. Stories like this one underscore the need for generalists:</p>\n<blockquote>\n<p>\n<p>Don Swanson seems an unlikely person to make medical discoveries. A retired but still active information scientist at the University of Chicago, Swanson has no medical training, does no medical experiments, and has never had a laboratory. Despite this, he&rsquo;s made several significant medical discoveries. One of the earliest was in 1988, when he investigated migraine headaches, and discovered evidence suggesting that migraines are caused by magnesium deficiency. At the time the idea was a surprise to other scientists studying migraines, but Swanson&rsquo;s idea was subsequently tested and confirmed in multiple therapeutic trials by traditional medical groups.</p>\n<p>How is it that someone without any medical training could make such a discovery? Although Swanson had none of the conventional credentials of medical research, what he did have was a clever idea. Swanson believed that scientific knowledge had grown so vast that important connections between subjects were going unnoticed, not because they were especially subtle or&nbsp;hard to grasp, but because no one had a broad enough understanding of science to notice those connections: in a big enough haystack, even a 50-foot needle may be hard to find. Swanson hoped to uncover such hidden connections using a medical search engine called Medline, which makes it possible to search millions of scientific papers in medicine&mdash;you can think of Medline as a high-level map of human medical knowledge. He began his work by using Medline to search the scientific literature for connections between migraines and other conditions. Here are two examples of connections he found: (1) migraines are associated with epilepsy; and (2) migraines are associated with blood clots forming more easily than usual. Of course, migraines have been the subject of much research, and so those are just two of a much longer list of connections that he found. But Swanson didn&rsquo;t stop with that list. Instead, he took each of the associated conditions and then used Medline to find further connections to that condition. He learned that, for example, (1) magnesium deficiency increases susceptibility to epilepsy; and (2) magnesium deficiency makes blood clot more easily. Now, when he began his work Swanson had no idea he&rsquo;d end up connecting migraines to magnesium deficiency. But once he&rsquo;d found a few papers suggesting such two-stage connections between magnesium deficiency and migraines, he narrowed his search to concentrate on magnesium deficiency, eventually finding eleven such two-stage connections to migraines. Although this wasn&rsquo;t the traditional sort of evidence favored by medical scientists, it nonetheless made a compelling case that migraines are connected to magnesium deficiency. Before Swanson&rsquo;s work a few papers had tentatively (and mostly in passing) suggested that magnesium deficiency might be connected to migraines. But the earlier work wasn&rsquo;t compelling, and was ignored by most scientists. By contrast, Swanson&rsquo;s evidence was highly suggestive, and it was soon followed by therapeutic trials that confirmed the migraine-magnesium connection.</p>\n</p>\n</blockquote>\n<p>From <em><a href=\"http://www.amazon.com/Reinventing-Discovery-New-Networked-Science/dp/0691148902/\">Reinventing Discovery</a></em>&nbsp;by Michael Nielsen (a past Singularity Summit <a href=\"http://www.singularitysummit.com/bios/nielsen\">speaker</a>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xyNP4B5CHfAM3K4Sz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 33, "extendedScore": null, "score": 7.984850803705547e-07, "legacy": true, "legacyId": "10897", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-12T18:26:12.580Z", "modifiedAt": null, "url": null, "title": "Meetup : San Diego meetup", "slug": "meetup-san-diego-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.633Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KFAHuWkeFcFxBM3nz/meetup-san-diego-meetup", "pageUrlRelative": "/posts/KFAHuWkeFcFxBM3nz/meetup-san-diego-meetup", "linkUrl": "https://www.lesswrong.com/posts/KFAHuWkeFcFxBM3nz/meetup-san-diego-meetup", "postedAtFormatted": "Saturday, November 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20San%20Diego%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20San%20Diego%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFAHuWkeFcFxBM3nz%2Fmeetup-san-diego-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20San%20Diego%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFAHuWkeFcFxBM3nz%2Fmeetup-san-diego-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKFAHuWkeFcFxBM3nz%2Fmeetup-san-diego-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 407, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4s'>San Diego meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 November 2011 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having another San Diego meetup on Sunday, November 20th at 1pm at <a href=\"http://knbwinecellars.com/\" rel=\"nofollow\">our usual location</a>.  As mentioned in <a href=\"http://lesswrong.com/lw/6p5/meetup_first_san_diego_ca_usa_meetup/\">the first San Diego meetup announcement</a>, it's in a kind of strip next to a grocery store.  The room we have is in the far back, so once you come in just go as straight as possible against the left wall.  Minors are allowed; they just can't order alcohol.  (So if you want to order any, be sure to bring your ID.)</p>\n\n<p>Since there was interest expressed in this last time, I'll prepare a presentation on the Enneagram and bring a projector.  (At last check they had removed their projector but still have a projection screen mounted on a wall.)  For those who weren't there for that part of the discussion, the very short version is that the Enneagram is a personality typing system that is ridiculously insightful and <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">predicative</a>.  It intertwines motivation, affect, thought, and behavior into a coherent pattern and details what nine of those patterns are, with the implicit claim that there are <em>only</em> nine such patterns (although there are relationships between them that add variety to types' expression).  It turns out you can make insanely accurate predictions of others' behavior using this tool because you learn to understand where they're coming from on a surprisingly subtle level, and you can also learn things about yourself that <a href=\"http://en.wikipedia.org/wiki/Introspection_illusion\" rel=\"nofollow\">introspection</a> normally fails to pick up on.  If you're curious to learn more before the meetup, <a href=\"http://www.enneagraminstitute.com/intro.asp\" rel=\"nofollow\">read this</a>.  Just be aware that the Enneagram started out as a \"sacred symbol\" used for spiritual purposes, so you'll sometimes encounter some religious elements tossed in randomly.  Just ignore those parts.  Over the last ten years, I've found that the Enneagram works perfectly well from a rational reductionist materialist perspective.</p>\n\n<p>With all that said, I'll offer this only if people who come are interested.  If there's a strong preference for just hanging out and chatting, then so be it!  I just want to hang out with fellow aspiring rationalists.</p>\n\n<p>If there's something you want to present that requires a speaker for a laptop (or equivalent), let me know.  I can almost certainly pick one up at the same time I'm getting the projector.</p>\n\n<p>If you want to carpool, ask!  I'm coming from around UTC.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4s'>San Diego meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KFAHuWkeFcFxBM3nz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 7.984862275592852e-07, "legacy": true, "legacyId": "10898", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___San_Diego_meetup\">Discussion article for the meetup : <a href=\"/meetups/4s\">San Diego meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 November 2011 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having another San Diego meetup on Sunday, November 20th at 1pm at <a href=\"http://knbwinecellars.com/\" rel=\"nofollow\">our usual location</a>.  As mentioned in <a href=\"http://lesswrong.com/lw/6p5/meetup_first_san_diego_ca_usa_meetup/\">the first San Diego meetup announcement</a>, it's in a kind of strip next to a grocery store.  The room we have is in the far back, so once you come in just go as straight as possible against the left wall.  Minors are allowed; they just can't order alcohol.  (So if you want to order any, be sure to bring your ID.)</p>\n\n<p>Since there was interest expressed in this last time, I'll prepare a presentation on the Enneagram and bring a projector.  (At last check they had removed their projector but still have a projection screen mounted on a wall.)  For those who weren't there for that part of the discussion, the very short version is that the Enneagram is a personality typing system that is ridiculously insightful and <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">predicative</a>.  It intertwines motivation, affect, thought, and behavior into a coherent pattern and details what nine of those patterns are, with the implicit claim that there are <em>only</em> nine such patterns (although there are relationships between them that add variety to types' expression).  It turns out you can make insanely accurate predictions of others' behavior using this tool because you learn to understand where they're coming from on a surprisingly subtle level, and you can also learn things about yourself that <a href=\"http://en.wikipedia.org/wiki/Introspection_illusion\" rel=\"nofollow\">introspection</a> normally fails to pick up on.  If you're curious to learn more before the meetup, <a href=\"http://www.enneagraminstitute.com/intro.asp\" rel=\"nofollow\">read this</a>.  Just be aware that the Enneagram started out as a \"sacred symbol\" used for spiritual purposes, so you'll sometimes encounter some religious elements tossed in randomly.  Just ignore those parts.  Over the last ten years, I've found that the Enneagram works perfectly well from a rational reductionist materialist perspective.</p>\n\n<p>With all that said, I'll offer this only if people who come are interested.  If there's a strong preference for just hanging out and chatting, then so be it!  I just want to hang out with fellow aspiring rationalists.</p>\n\n<p>If there's something you want to present that requires a speaker for a laptop (or equivalent), let me know.  I can almost certainly pick one up at the same time I'm getting the projector.</p>\n\n<p>If you want to carpool, ask!  I'm coming from around UTC.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___San_Diego_meetup1\">Discussion article for the meetup : <a href=\"/meetups/4s\">San Diego meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : San Diego meetup", "anchor": "Discussion_article_for_the_meetup___San_Diego_meetup", "level": 1}, {"title": "Discussion article for the meetup : San Diego meetup", "anchor": "Discussion_article_for_the_meetup___San_Diego_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Gr96pCqtCWKJcnzDu", "a7n8GdKiAZRX86T5A"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T00:57:37.271Z", "modifiedAt": null, "url": null, "title": "List of potential cognitive enhancement methods", "slug": "list-of-potential-cognitive-enhancement-methods", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:07.892Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cvQ6rPZPDbriw5W9n/list-of-potential-cognitive-enhancement-methods", "pageUrlRelative": "/posts/cvQ6rPZPDbriw5W9n/list-of-potential-cognitive-enhancement-methods", "linkUrl": "https://www.lesswrong.com/posts/cvQ6rPZPDbriw5W9n/list-of-potential-cognitive-enhancement-methods", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20potential%20cognitive%20enhancement%20methods&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20potential%20cognitive%20enhancement%20methods%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcvQ6rPZPDbriw5W9n%2Flist-of-potential-cognitive-enhancement-methods%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20potential%20cognitive%20enhancement%20methods%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcvQ6rPZPDbriw5W9n%2Flist-of-potential-cognitive-enhancement-methods", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcvQ6rPZPDbriw5W9n%2Flist-of-potential-cognitive-enhancement-methods", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 254, "htmlBody": "<p>\n<p>Participants in the Singularity Summit 2011 workshops held on October 17-18 brainstormed a list of cognitive enhancement methods they would like to see tested &mdash; some of them for the first time, many of them more thoroughly than has been done so far. Here is that list:</p>\n<p>\n<ul>\n<li>rationality instruction</li>\n<li>potassium and nutrients/micronutrients in general</li>\n<li>modafinil and its class</li>\n<li>racetam class, adderall, riatlin</li>\n<li>paleo and other popular diets</li>\n<li>multigenerational embryo selection</li>\n<li>particular TMS interventions</li>\n<li>doing math or logic problems every day</li>\n<li>amount of sleep&nbsp;</li>\n<li>neurofeedback; EEG, etc.</li>\n<li>physical health, specific types of exercise, yoga</li>\n<li>more red blood cells</li>\n<li>certain types of electrical stimulation</li>\n<li>spaced repetition</li>\n<li>practicing visualization</li>\n<li>practicing chess and Go</li>\n<li>dual n-back training</li>\n<li>prolonged sensory deprivation</li>\n<li>experience manipulating physical objects (craftsmanship and engineering)</li>\n<li>listening to music actively (and engaging with other art forms actively)</li>\n<li>using specific visual languages for specific tasks</li>\n<li>happiness in general (gratitude, etc.)</li>\n<li>changing the oxygen content of what you're breathing</li>\n<li>gene expression levels</li>\n<li>operant conditioning</li>\n<li>irradiation or administration of other toxins</li>\n<li>lucid dreaming</li>\n<li>GHB</li>\n<li>intermittent fasting</li>\n<li>creatine</li>\n<li>nicotine, caffeine</li>\n<li>arterial glucose drip to get more sugar to the brain</li>\n<li>Ekman training</li>\n<li>steroids</li>\n</ul>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cvQ6rPZPDbriw5W9n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 22, "extendedScore": null, "score": 7.986241577736507e-07, "legacy": true, "legacyId": "10899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T01:41:43.149Z", "modifiedAt": null, "url": null, "title": "Umbilical cord stem cell banking for future medical use", "slug": "umbilical-cord-stem-cell-banking-for-future-medical-use", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.028Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Synaptic", "createdAt": "2011-09-26T14:13:36.154Z", "isAdmin": false, "displayName": "Synaptic"}, "userId": "cXSPuYAf5pC9XTzcm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8QqYJLJHktGf6NCXa/umbilical-cord-stem-cell-banking-for-future-medical-use", "pageUrlRelative": "/posts/8QqYJLJHktGf6NCXa/umbilical-cord-stem-cell-banking-for-future-medical-use", "linkUrl": "https://www.lesswrong.com/posts/8QqYJLJHktGf6NCXa/umbilical-cord-stem-cell-banking-for-future-medical-use", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Umbilical%20cord%20stem%20cell%20banking%20for%20future%20medical%20use&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUmbilical%20cord%20stem%20cell%20banking%20for%20future%20medical%20use%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QqYJLJHktGf6NCXa%2Fumbilical-cord-stem-cell-banking-for-future-medical-use%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Umbilical%20cord%20stem%20cell%20banking%20for%20future%20medical%20use%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QqYJLJHktGf6NCXa%2Fumbilical-cord-stem-cell-banking-for-future-medical-use", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8QqYJLJHktGf6NCXa%2Fumbilical-cord-stem-cell-banking-for-future-medical-use", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 728, "htmlBody": "<p>I had not heard of this until recently, and I doubt I am alone. I will quote from <a href=\"http://kidshealth.org/parent/_cancer_center/treatment/cord_blood.html\">this</a> source, which looks OK (there is also <a href=\"http://parentsguidecordblood.org/\">this</a>).</p>\n<p>What is the idea?</p>\n<blockquote>\n<p>[D]uring the 1970s, researchers discovered that umbilical cord blood could supply the same kinds of blood-forming (hematopoietic) stem cells as a bone marrow donor. And so, umbilical cord blood began to be collected and stored.</p>\n</blockquote>\n<p>How the cells are collected:&nbsp;</p>\n<blockquote>\n<p>After a vaginal delivery, the umbilical cord is clamped on both sides and cut. In most cases, an experienced obstetrician or nurse collects the cord blood before the placenta is delivered. One side of the umbilical cord is unclamped, and a small tube is passed into the umbilical vein to collect the blood. After blood has been collected from the cord, needles are placed on the side of the surface of the placenta that was connected to the fetus to collect more blood and cells from the large blood vessels that fed the fetus.</p>\n</blockquote>\n<p>How the cells are stored:</p>\n<blockquote>\n<p>After cord-blood collection has taken place, the blood is placed into bags or syringes and is usually taken by courier to the cord-blood bank. Once there, the sample is given an identifying number. Then the stem cells are separated from the rest of the blood and are stored cryogenically (frozen in liquid nitrogen) in a collection facility, also known as a cord-blood bank.</p>\n</blockquote>\n<p>How the cells can be used:&nbsp;</p>\n<blockquote>\n<p>Then, if needed, blood-forming stem cells can be thawed and used in either autologous procedures (when someone receives his or her own umbilical cord blood in a transplant) or allogeneic procedures (when a person receives umbilical cord blood donated from someone else &mdash; a sibling, close relative, or anonymous donor).</p>\n</blockquote>\n<p>The major upside:&nbsp;</p>\n<blockquote>\n<p>The primary reason that parents consider banking their newborn's cord blood is because they have a child or close relative with or a family medical history of diseases that can be treated with bone marrow transplants.&nbsp;</p>\n</blockquote>\n<p>The monetary cost:&nbsp;</p>\n<blockquote>\n<p>The expense of collecting and storing the cord blood can be a deciding factor for many families. At a commercial cord-blood bank, you'll pay approximately $1,000-$2,000 to store a sample of cord blood, in addition to an approximately $100 yearly maintenance fee. You might also pay an additional fee of several hundred dollars for the cord-blood collection kit, courier service to the cord-blood bank, and initial processing.</p>\n</blockquote>\n<p>Is it useful as \"biological insurance\"? Some say no:</p>\n<blockquote>\n<p>Some doctors and organizations, such as the American Academy of Pediatrics (AAP), have expressed concern that cord-blood banks may capitalize on the fears of vulnerable new parents by providing misleading information about the statistics of bone marrow transplants.... The AAP doesn't recommend cord-blood banking for families who don't have a history of disease. That's because research has not yet determined the likelihood that a child would ever need his or her own stem cells, nor has it confirmed that transplantation using self-donated cells rather than cells from a relative or stranger is safer or more effective. According to the AAP, \"private storage of cord blood as 'biological insurance' is unwise. However, banking should be considered if there is a family member with a current or potential need to undergo a stem cell transplantation.\"</p>\n</blockquote>\n<p>Some say yes:&nbsp;</p>\n<blockquote>\n<p>Other doctors and researchers support saving umbilical cord blood as a source of blood-forming stem cells in every delivery &mdash; mainly because of the promise that stem-cell research holds for the future. Most people would have little use for stem cells now, but research into the use of stem cells for treatment of disease is ongoing &mdash; and the future looks promising.</p>\n</blockquote>\n<p>Three questions for LW:</p>\n<p>1) Based on the above and/or outside information, would you recommend this procedure for two parents in their mid-20s living in the US with a middle class income whose babies will have no known risks for&nbsp;disease?</p>\n<p><span>2) This seems like a good analogy for brain preservation using&nbsp;cryogenic nitrogen. Why do you think that&nbsp;</span>umbilical cord stem cell banking<span>&nbsp;is FDA regulated whereas&nbsp;</span>brain preservation<span>&nbsp;procedures are not?</span></p>\n<p>3) Instead of privately banking them, it is possible to donate umbilical cord stem cells to public banks, for the benefit of others. Similarly, it is possible to donate one's postmortem brain (and organs) to the public, for the benefit of others. In both cases, there is the third option to do neither. In making these decisions, how do you weigh your own interests against those of others?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8QqYJLJHktGf6NCXa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 2.2e-05, "legacy": true, "legacyId": "10900", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T06:54:51.904Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Mere Messiahs", "slug": "seq-rerun-mere-messiahs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.244Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W96PXWTrbCWsZe5Lh/seq-rerun-mere-messiahs", "pageUrlRelative": "/posts/W96PXWTrbCWsZe5Lh/seq-rerun-mere-messiahs", "linkUrl": "https://www.lesswrong.com/posts/W96PXWTrbCWsZe5Lh/seq-rerun-mere-messiahs", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Mere%20Messiahs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Mere%20Messiahs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW96PXWTrbCWsZe5Lh%2Fseq-rerun-mere-messiahs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Mere%20Messiahs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW96PXWTrbCWsZe5Lh%2Fseq-rerun-mere-messiahs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW96PXWTrbCWsZe5Lh%2Fseq-rerun-mere-messiahs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<p>Today's post, <a href=\"/lw/ll/mere_messiahs/\">Mere Messiahs</a> was originally published on 02 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>John Perry, an extropian and a transhumanist, died when the north tower of the World Trade Center fell. He knew he was risking his existence to save other people, and he had hope that he might be able to avoid death, but he still helped them. This takes far more courage than someone who dies, expecting to be rewarded in an afterlife for their virtue.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8eo/seq_rerun_superhero_bias/\">Superhero Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W96PXWTrbCWsZe5Lh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.987504106507808e-07, "legacy": true, "legacyId": "10902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Tv7WWhgbKMWzEnMmx", "DBkd8PfnbNQtyLRCT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T11:54:46.483Z", "modifiedAt": null, "url": null, "title": "Modularity, signaling, and belief in belief", "slug": "modularity-signaling-and-belief-in-belief", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.089Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o6CuZk2oPtDXqeY5A/modularity-signaling-and-belief-in-belief", "pageUrlRelative": "/posts/o6CuZk2oPtDXqeY5A/modularity-signaling-and-belief-in-belief", "linkUrl": "https://www.lesswrong.com/posts/o6CuZk2oPtDXqeY5A/modularity-signaling-and-belief-in-belief", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Modularity%2C%20signaling%2C%20and%20belief%20in%20belief&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AModularity%2C%20signaling%2C%20and%20belief%20in%20belief%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6CuZk2oPtDXqeY5A%2Fmodularity-signaling-and-belief-in-belief%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Modularity%2C%20signaling%2C%20and%20belief%20in%20belief%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6CuZk2oPtDXqeY5A%2Fmodularity-signaling-and-belief-in-belief", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo6CuZk2oPtDXqeY5A%2Fmodularity-signaling-and-belief-in-belief", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1421, "htmlBody": "<p><em>This is the fourth part in a <a href=\"/tag/whyeveryonehypocrite\">mini-sequence</a> presenting material from Robert Kurzban's excellent book </em><em><a href=\"http://www.amazon.com/Why-Everyone-Else-Hypocrite-Evolution/dp/0691146748\">Why Everyone (Else) Is a Hypocrite: Evolution and the Modular Mind</a></em><em>.</em></p>\n<p>In the previous post, <a href=\"/lw/72d/strategic_ignorance_and_plausible_deniability/\">Strategic ignorance and plausible deniability</a>, we discussed some ways by which people might have modules designed to keep them away from certain kinds of information. These arguments were relatively straightforward.<br /><br />The next step up is the hypothesis that our \"<a href=\"/lw/72d/strategic_ignorance_and_plausible_deniability/\">press secretary module</a>\" might be designed to contain information that is useful for certain purposes, even if other modules have information that not only conflicts with this information, but is also more likely to be accurate. That is, some modules are designed to acquire systematically biased - i.e. false - information, including information that other modules \"know\" is wrong.<br /><a id=\"more\"></a></p>\n<p>Kurzban builds up this argument as follows:<br /><br />Humans are incredibly social. Our intelligence might have evolved exactly because of competition in social domains, living alone and a lack of social interaction are correlated with poor health and unhappiness, and the pain levels reported by people reliving socially painful events, especially ostracism, are \"comparable to pain levels reported ... for chronic back pain and even childbirth\". Evolution has very strongly wired us for being social and avoiding ostracism and disapproval by the group.<br /><br />There's a lot of competition in the social world. Our survival and reproduction are determined in large part by how well we navigate the social world. Our minds are likely to have been designed to compete fiercely for the benefits of the social world: the best mates, the best friends, membership in the best groups, and so on.<br /><br />Social competition is often framed in terms of competing for the best mate or romantic partner, but Kurzban believes that the importance of friendship jealousy is underappreciated. If you are someone's best friend, they are likely to help you in times of need, and to also take your side in nearly all social conflicts. Having many people consider you to be among their best friends is one of the greatest advantages a person can have, and it would be surprising if we didn't have many modules that caused us to try to be valuable for others. People report satisfaction from helping their friends, and of course from making new ones.<br /><br />We also like to be part of groups. Some groups are very exclusive, picking their members on the basis of formal criteria. Others are less formal, but no less important. In general, groups - like individual people - tend to value people who provide something useful for the group. Persuading others that you are valuable is an important, even crucial adaptive problem for humans. Our efforts to acquire knowledge, skills, and resources might well be driven at least in part by adaptations designed to make one valuable in the social world.<br /><br />One's value in the social world is determined by many factors, such as our wealth, skills and abilities, existing social connections, intelligence, and probably many others. Of these, health is particularly important. Part of friendship is trading favors between each other: I do something for you, and then later on you return the favor and do something for me. But if you happen to die before having a chance to return my favor, my investment is wasted. This suggests that we should prefer to associate with people with good prospects, and to make others believe that our prospects are good even if they aren't.<br /><br />Finally, estimating somebody's value is difficult. It's hard to judge whether somebody is likely to be loyal, caring or giving, or who is intelligent. Making these judgements accurately and choosing the right allies is of paramount importance, as is looking like a good person to ally yourself with. Taking all the above paragraphs together, this suggests that the modules that cause the speech and behavior that lead to others' impressions should be designed to generate as positive a view as possible of our traits and abilities. Likewise, our \"press secretary modules\" should be designed to cause people to behave in a way that sends out the most positive defensible message about the person's worth, history, and future.<br /><br />The \"defensible\" part is important. Suppose that we know that our tribe places immense value on lion tamers, and also that anybody claiming to be a great lion tamer will soon be thrown in the same cage with a lion. If they are not as good as they say, the lion will eat them. In this situation, it would not be beneficial for us to claim great lion taming expertise if we did not actually have it. Likewise, if a person thinks that they are six feet tall, others won't be any less likely to notice that the person is actually only five feet tall.<br /><br />But other kinds of beliefs do affect others' beliefs about us. In particular, our own behavior and actions do tell something about us. And our actions are influenced by the <a href=\"/lw/4e/cached_selves/\">beliefs of ourselves</a> that we happen to have. The influence can be from false beliefs, but since we must let our true beliefs guide our action at least some of the time, every now and then our true beliefs will leak through as well.<br /><br />To name one example (in addition to the countless ones <a href=\"http://www.overcomingbias.com/tag/signaling\">Robin Hanson has provided us</a>), it's useful for other people to think that you're not going to die soon. If they believe you are going to be around for a long time, they are more likely to invest in a friendship with you. And our mental modules seem to reflect this, for we tend to avoid learning about our own medical conditions if the condition in question is both serious and untreatable. Why learn about facts that, if leaked, can only hurt you?<br /><br />The usefulness of many beliefs can be context-dependent. Maybe being seen as a great lion tamer will get you lots of benefits in some contexts, as many people want to ally themselves with you. In other contexts, such as in the ones where you actually had an opportunity to go tame a lion, it would be beneficial to not believe in your lion taming skills if you didn't have any. Now you could be well off if you had a representation in one module that you were a good lion tamer in every single context in which there were no lions to be tamed. When an opportunity actually showed up, you'd want the \"true\" representation, living in some other module, to \"take over\".<br /><br />Long-time readers will recognize the connection to <a href=\"/lw/i4/belief_in_belief/\">belief in belief</a>: someone might believe that there is an invisible dragon living in their garage, but they still know what exactly to say or do to avoid having their belief falsified. One might also think of the way the public seems so incredibly eager to leap on the smallest contradiction between a politician's words and their actions: the public is looking for a sign of their leader's true beliefs leaking out. Kurzban also mentions that Christopher Columbus is believed to have had two estimates of how far his ship had traveled during the first voyage to the New World. One was a deliberate underestimate to reduce the crew's worries, while the other was his best guess, to be used for practical purposes.<br /><br />The connection to supernatural beliefs is also one that Kurzban discusses. Historically, having different religious beliefs from the rest of your social group has been very dangerous. Giordano Bruno is claimed to have been burned at a stake for disagreeing with Rome on the issue of transubstation, for other things. Even in today's world, some surveys indicate that 60% of Americans would refuse to vote for an atheist. Belief in the supernatural, then, is one more way in which it has been crucially important to be wrong in order to survive.</p>\n<p><em>My apologies for taking so long with this series. One of the things  that was holding me up was that I felt I should cover two and a half  chapters in one big post, which would have been exhausting to write and  possibly exhausting to read. So to get over my block, I'm cutting it up  into smaller pieces, even if some - including this one - risk only  saying things that regular readers here already know.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bt2e3HEcZmuHo3xf7": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o6CuZk2oPtDXqeY5A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 7.988561145712105e-07, "legacy": true, "legacyId": "10903", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "uPjHAiXAKrMzvTFyt", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "strategic-ignorance-and-plausible-deniability", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fxgkYCbG5Hgy58TyC", "BHYBdijDcAKQ6e45Z", "CqyJzDZWvGhhFJ7dY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T16:47:31.970Z", "modifiedAt": null, "url": null, "title": "Friendly AI FAQ drafts", "slug": "friendly-ai-faq-drafts", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/23odz5WbpEvXKw5HZ/friendly-ai-faq-drafts", "pageUrlRelative": "/posts/23odz5WbpEvXKw5HZ/friendly-ai-faq-drafts", "linkUrl": "https://www.lesswrong.com/posts/23odz5WbpEvXKw5HZ/friendly-ai-faq-drafts", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendly%20AI%20FAQ%20drafts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendly%20AI%20FAQ%20drafts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23odz5WbpEvXKw5HZ%2Ffriendly-ai-faq-drafts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendly%20AI%20FAQ%20drafts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23odz5WbpEvXKw5HZ%2Ffriendly-ai-faq-drafts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23odz5WbpEvXKw5HZ%2Ffriendly-ai-faq-drafts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 107, "htmlBody": "<p>I and a few others have gradually been writing up a large FAQ on Friendly AI. I'll be posting early drafts of the content to Less Wrong discussion for feedback.</p>\n<p>We aim to keep the answers in this FAQ brief, though they can be significantly longer than those in the <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>.</p>\n<p>Here, I'll keep a table of contents for the posted FAQ sections.</p>\n<ul>\n<li><a href=\"/r/discussion/lw/8ex/fai_faq_draft_what_is_the_history_of_the_friendly/\">What is the history of the Friendly AI concept?</a></li>\n<li><a href=\"/r/discussion/lw/8ez/fai_faq_draft_what_is_friendly_ai/\">What is Friendly AI?</a></li>\n<li><a href=\"/r/discussion/lw/8gh/fai_faq_draft_what_is_the_singularity/\">What is the Singularity?</a></li>\n<li><a href=\"/r/discussion/lw/8ja/fai_faq_draft_what_is_nanotechnology/\">What is nanotechnology?</a></li>\n<li><a href=\"/r/discussion/lw/8jt/fai_faq_draft_general_intelligence_and/\">What is general intelligence</a> and <a href=\"/r/discussion/lw/8jt/fai_faq_draft_general_intelligence_and/\">what is greater-than-human intelligence?</a></li>\n<li>...</li>\n</ul>\n<p>For references, see <a href=\"/lw/8gh/fai_faq_draft_what_is_the_singularity/5awr\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "23odz5WbpEvXKw5HZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 7.989596590263595e-07, "legacy": true, "legacyId": "10904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q55aCakPpSWNFivZm", "aCPmoihtqKbtpCsdz", "vu8LDecutbPYSiJfp", "2un4e7nxedhMsCT5h", "hqxSXSNisLqvSE49e"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T16:47:49.719Z", "modifiedAt": null, "url": null, "title": "FAI FAQ draft: What is the history of the Friendly AI concept?", "slug": "fai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.296Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q55aCakPpSWNFivZm/fai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "pageUrlRelative": "/posts/Q55aCakPpSWNFivZm/fai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "linkUrl": "https://www.lesswrong.com/posts/Q55aCakPpSWNFivZm/fai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20FAQ%20draft%3A%20What%20is%20the%20history%20of%20the%20Friendly%20AI%20concept%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20FAQ%20draft%3A%20What%20is%20the%20history%20of%20the%20Friendly%20AI%20concept%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ55aCakPpSWNFivZm%2Ffai-faq-draft-what-is-the-history-of-the-friendly-ai-concept%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20FAQ%20draft%3A%20What%20is%20the%20history%20of%20the%20Friendly%20AI%20concept%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ55aCakPpSWNFivZm%2Ffai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ55aCakPpSWNFivZm%2Ffai-faq-draft-what-is-the-history-of-the-friendly-ai-concept", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1109, "htmlBody": "<p>I invite your feedback on this snippet from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. This one is an answer to the question \"What is the history of the Friendly AI concept?\"</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>Late in the Industrial Revolution, Samuel Butler (1863) worried about what might happen when machines become more capable than the humans who designed them:</p>\n<blockquote>\n<p>...we are ourselves creating our own successors; we are daily adding to the beauty and delicacy of their physical organisation; we are daily giving them greater power and supplying by all sorts of ingenious contrivances that self-regulating, self-acting power which will be to them what intellect has been to the human race. In the course of ages we shall find ourselves the inferior race.</p>\n<p>...the time will come when the machines will hold the real supremacy over the world and its inhabitants...</p>\n</blockquote>\n<p>This basic idea was picked up by science fiction authors, for example in John W. Campbell&rsquo;s (1932) short story <em><a href=\"http://www.gutenberg.org/files/27462/27462-h/27462-h.htm\">The Last Evolution</a></em>. In the story, humans live lives of leisure because machines are smart enough to do all the work. One day, aliens invade:</p>\n<blockquote>\n<p>Then came the Outsiders. Whence they came, neither machine nor man ever learned, save only that they came from beyond the outermost planet, from some other sun. Sirius&mdash;Alpha Centauri&mdash;perhaps! First a thin scoutline of a hundred great ships, mighty torpedoes of the void [3.5 miles] in length, they came.</p>\n</blockquote>\n<p>Earth&rsquo;s machines, protecting humans, defeat the aliens. The aliens&rsquo; machines survive long enough to render humans extinct, but are eventually defeated by Earth&rsquo;s machines. These machines inherit the solar system, eventually moving to run on substrates of pure &ldquo;Force.&rdquo;</p>\n<p>The concerns of machine ethics are most popularly identified with Isaac Asimov&rsquo;s <a href=\"http://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a>, introduced in his short story <a href=\"http://en.wikipedia.org/wiki/Runaround\">Runaround</a>. Asimov used his stories, including those collected in the popular <em><a href=\"http://en.wikipedia.org/wiki/I,_Robot\">I, Robot</a></em> book, to illustrate all the ways in which such simple rules for governing robot behavior could go wrong.</p>\n<p>In the year of <em>I, Robot</em>&rsquo;s release, mathematician Alan Turing (1950) noted that machines will one day be capable of genuine thought:</p>\n<blockquote>\n<p>I believe that at the end of the century... one will be able to speak of machines thinking without expecting to be contradicted.</p>\n</blockquote>\n<p>Turing (1951/2004) concluded:</p>\n<blockquote>\n<p>...it seems probable that once the machine thinking method has started, it would not take long to outstrip our feeble powers... At some stage therefore we should have to expect the machines to take control...</p>\n</blockquote>\n<p>Bayesian statistician I.J. Good (1965), who had worked with Turing to crack Nazi codes in World War II, made the crucial leap to the &lsquo;intelligence explosion&rsquo; concept:</p>\n<blockquote>\n<p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an &ldquo;intelligence explosion&rdquo;, and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make</p>\n</blockquote>\n<p>Futurist Arthur C. Clarke (1968) agreed:</p>\n<blockquote>\n<p>Though we have to live and work with (and against) today's mechanical morons, their deficiencies should not blind us to the future. In particular, it should be realized that as soon as the borders of electronic intelligence are passed, there will be a kind of chain reaction, because the machines will rapidly improve themselves... there will be a mental explosion; the merely intelligent machine will swiftly give way to the ultraintelligent machine....</p>\n<p>Perhaps our role on this planet is not to worship God but to create Him.</p>\n</blockquote>\n<p>Julius Lukasiewicz (1974) noted that human intelligence may be unable to predict what a superintelligent machine would do:</p>\n<blockquote>\n<p>The survival of man may depend on the early construction of an ultraintelligent machine-or the ultraintelligent machine may take over and render the human race redundant or develop another form of life. The prospect that a merely intelligent man could ever attempt to predict the impact of an ultraintelligent device is of course unlikely but the temptation to speculate seems irresistible.</p>\n</blockquote>\n<p>Even critics of AI like Jack Schwartz (1987) saw the implications:</p>\n<blockquote>\n<p>If artificial intelligences can be created at all, there is little reason to believe that initial successes could not lead swiftly to the construction of artificial superintelligences able to explore significant mathematical, scientific, or engineering alternatives at a rate far exceeding human ability, or to generate plans and take action on them with equally overwhelming speed. Since man's near-monopoly of all higher forms of intelligence has been one of the most basic facts of human existence throughout the past history of this planet, such developments would clearly create a new economics, a new sociology, and a new history.</p>\n</blockquote>\n<p>Novelist Vernor Vinge (1981) called this 'event horizon' in our ability to predict the future a 'singularity':</p>\n<blockquote>\n<p>Here I had tried a straightforward extrapolation of technology, and found myself precipitated over an abyss. It's a problem we face every time we consider the creation of intelligences greater than our own. When this happens, human history will have reached a kind of singularity - a place where extrapolation breaks down and new models must be applied - and the world will pass beyond our understanding.</p>\n</blockquote>\n<p>Eliezer Yudkowsky (1996) used the term 'singularity' to refer instead to Good's 'intelligence explosion', and began work on the task of figuring out how to build a self-improving AI that had a positive rather than negative effect on the world (Yudkowsky 2000) &mdash; a project he eventually called 'Friendly AI' (Yudkowsky 2001).</p>\n<p>Meanwhile, philosophers and AI researchers were considering whether or not machines could have moral value, and how to ensure ethical behavior from less powerful machines or 'narrow AIs', a field of inquiry variously known as 'artificial morality' (Danielson 1992; Floridi &amp; Sanders 2004; Allen et al. 2000), 'machine ethics' (Hall 2000; McLaren 2005; Anderson &amp; Anderson 2006), 'computational ethics' (Allen 2002) and 'computational metaethics' (Lokhorst, 2011), and 'robo-ethics' or 'robot ethics' (Capurro et al. 2006; Sawyer 2007). This vein of research &mdash; what we'll call the 'machine ethics' literature &mdash; was recently summarized in two books: Wallach &amp; Allen (2009); Anderson &amp; Anderson (2011).</p>\n<p>Leading philosopher of mind David Chalmers brought the concepts of intelligence explosion and Friendly AI to mainstream academic attention with his 2010 paper, &lsquo;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/01/Chalmers-The-Singularity-a-philosophical-analysis.pdf\">The Singularity: A Philosophical Analysis</a>&rsquo;, published in <em><a href=\"http://en.wikipedia.org/wiki/Journal_of_Consciousness_Studies\">Journal of Consciousness Studies</a></em>. That journal&rsquo;s January 2012 issue will be devoted to responses to Chalmers&rsquo; article, as will an edited volume from Springer (Eden et al. 2012).</p>\n<p>Friendly AI researchers do not regularly cite the machine ethics literature (e.g. see Bostrom &amp; Yudkowsky 2011). These researchers have put forward preliminary proposals for ensuring ethical behavior in superintelligent or self-improving machines, for example  'Coherent Extrapolated Volition' (Yudkowsky 2004).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q55aCakPpSWNFivZm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 7.98959763492162e-07, "legacy": true, "legacyId": "10905", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["23odz5WbpEvXKw5HZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T16:55:05.103Z", "modifiedAt": null, "url": null, "title": "Learned Helplessness", "slug": "learned-helplessness", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:28.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raw_Power", "createdAt": "2010-09-10T23:59:43.621Z", "isAdmin": false, "displayName": "Raw_Power"}, "userId": "kwSqcED9qTanFyNWG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J5QxWGgEzQB8LHhCf/learned-helplessness", "pageUrlRelative": "/posts/J5QxWGgEzQB8LHhCf/learned-helplessness", "linkUrl": "https://www.lesswrong.com/posts/J5QxWGgEzQB8LHhCf/learned-helplessness", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Learned%20Helplessness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALearned%20Helplessness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5QxWGgEzQB8LHhCf%2Flearned-helplessness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Learned%20Helplessness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5QxWGgEzQB8LHhCf%2Flearned-helplessness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ5QxWGgEzQB8LHhCf%2Flearned-helplessness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>I stumbled upon <a href=\"http://www.youtube.com/watch?feature=player_embedded&amp;v=OtB6RTJVqPM\">this interesting little video</a>. It links to many others on the same topic, which I found surprisingly interesting, and would like to discuss. What is it that makes one lose confidence when they see others succeed at the tasks they fail, to the point of being unable to preform tasks that they otherwise should know how to do? Could it be that this phenomenon holds parts of the key to failure rates throughout the education systems worldwide, as well as other self-destructive, irrational reaction-to-failure phenomena throughout one's life?</p>\n<p>&nbsp;</p>\n<p>BTW, maybe we should be able to post Youtube videos on-page?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J5QxWGgEzQB8LHhCf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 7.989623260401494e-07, "legacy": true, "legacyId": "10906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T19:26:06.395Z", "modifiedAt": null, "url": null, "title": "FAI FAQ draft: What is Friendly AI?", "slug": "fai-faq-draft-what-is-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aCPmoihtqKbtpCsdz/fai-faq-draft-what-is-friendly-ai", "pageUrlRelative": "/posts/aCPmoihtqKbtpCsdz/fai-faq-draft-what-is-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/aCPmoihtqKbtpCsdz/fai-faq-draft-what-is-friendly-ai", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20FAQ%20draft%3A%20What%20is%20Friendly%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20FAQ%20draft%3A%20What%20is%20Friendly%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCPmoihtqKbtpCsdz%2Ffai-faq-draft-what-is-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20FAQ%20draft%3A%20What%20is%20Friendly%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCPmoihtqKbtpCsdz%2Ffai-faq-draft-what-is-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaCPmoihtqKbtpCsdz%2Ffai-faq-draft-what-is-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>I invite your feedback on this snippet from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. This one is an answer to the question \"What is Friendly AI?\"</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>A Friendly AI (FAI) is an artificial intelligence that benefits humanity. More specifically, Friendly AI may refer to:</p>\n<ul>\n<li>a very <em>powerful</em> and <em>general</em> AI that acts autonomously in the world to benefit humanity.</li>\n<li>an AI that continues to benefit humanity during and after an <em>intelligence explosion</em>.</li>\n<li>a research program concerned with the production of such an AI.</li>\n<li><a href=\"http://intelligence.org/\">Singularity Institute</a>'s approach (Yudkowsky 2001, 2004) to designing such an AI:  \n<ul>\n<li>Goals should be defined by the <em>Coherent Extrapolated Volition</em> of humanity.</li>\n<li>Goals should be reliably preserved during recursive self-improvement.</li>\n<li>Design should be mathematically rigorous and proof-apt.</li>\n</ul>\n</li>\n</ul>\n<p>Friendly AI is a more difficult project than often supposed. As explored in other sections, commonly suggested solutions for Friendly AI are likely to fail because of two features possessed by any superintelligence:</p>\n<ol>\n<li>\n<p><em>Superpower</em>: a superintelligent machine will have unprecedented powers to reshape reality, and therefore will achieve its goals with highly efficient methods that confound human expectations and desires.</p>\n</li>\n<li>\n<p><em>Literalness</em>: a superintelligent machine will make decisions using the mechanisms it is designed with, not the hopes its designers had in mind when they programmed those mechanisms. It will act only on precise specifications of rules and values, and will do so in ways that need not respect the complexity and subtlety (Kringelbach &amp; Berridge 2009; Schroeder 2004; Glimcher 2010) of what humans value. A demand like \"maximize human happiness\" sounds simple to us because it contains few words, but philosophers and scientists have failed for centuries to explain exactly what this means, and certainly have not translated it into a form sufficiently rigorous for AI programmers to use.</p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aCPmoihtqKbtpCsdz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 7.990156617830943e-07, "legacy": true, "legacyId": "10907", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["23odz5WbpEvXKw5HZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T22:23:06.443Z", "modifiedAt": null, "url": null, "title": "Toward an overview analysis of intelligence explosion", "slug": "toward-an-overview-analysis-of-intelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ebRZPDBg5qff9oTs5/toward-an-overview-analysis-of-intelligence-explosion", "pageUrlRelative": "/posts/ebRZPDBg5qff9oTs5/toward-an-overview-analysis-of-intelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/ebRZPDBg5qff9oTs5/toward-an-overview-analysis-of-intelligence-explosion", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Toward%20an%20overview%20analysis%20of%20intelligence%20explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AToward%20an%20overview%20analysis%20of%20intelligence%20explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebRZPDBg5qff9oTs5%2Ftoward-an-overview-analysis-of-intelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Toward%20an%20overview%20analysis%20of%20intelligence%20explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebRZPDBg5qff9oTs5%2Ftoward-an-overview-analysis-of-intelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FebRZPDBg5qff9oTs5%2Ftoward-an-overview-analysis-of-intelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 418, "htmlBody": "<p>A few months ago, Anna Salamon and I began to write an academic overview of intelligence explosion scenarios &mdash; something we could hand to people to explain all our major points in one brief article.</p>\n<p>We encountered two major problems.</p>\n<p>First: The <a href=\"http://www.singularitysummit.com/\">Summit</a> happened, taking all of our time. Then I <a href=\"/r/discussion/lw/8c3/qa_with_new_executive_director_of_singularity/\">was made</a> Executive Director, taking all of <em>my</em> time in a more persistent way.</p>\n<p>Second: Being thorough&nbsp;and rigorous&nbsp;in an overview of intelligence explosion requires deep knowledge of a huge spectrum of science and philosophy: history of AI progress, history of planning for the future mattering, AI architectures, hardware progress, algorithms progress, massive datasets, neuroscience, factors in the speed of scientific progress, embryo selection, whole brain emulation, properties of digital minds, AI convergent instrumental values, self-improvement dynamics, takeoff scenarios, heuristics and biases, unipolar and multipolar intelligence explosion scenarios, human values and value extrapolation, decision theory, arms races, human dynamics of technological development, technological forecasting, the economics of machine intelligence, anthropics, evolution, AI-boxing, and <em>much</em>&nbsp;more. Because we were trying to write a <em>short</em>&nbsp;article, we kept having to consume and compress an entire field of knowledge into a single paragraph (or even a single sentence!) with the perfect 2-8 citations, which occasionally meant several days of work for a single paragraph. (This is an extreme example, but it's the <em>kind</em>&nbsp;of problem we often encountered, in different degrees.)</p>\n<p>So, we've decided to take a different approach and involve the broader community.</p>\n<p>We'll be posting short snippets, short pieces of the puzzle, for feedback from the community. Sometimes we'll pose questions, or ask for references about a given topic, or ask for suggested additions to the dialectic we present.</p>\n<p>In the end, we hope to collect and remix the best and most essential snippets, incorporate the feedback and additions provided by the community, and write up the final article.</p>\n<p>Think of it as a <a href=\"http://en.wikipedia.org/wiki/Polymath_project#Polymath_Project\">Polymath Project</a> for intelligence explosion analysis. It's <em>collaborative science and philosophy</em>.&nbsp;Members of Less Wrong tend to be smart, and each one has deep knowledge of one or a few fields that we may not have. We hope you'll join us, and contribute your expertise to this project.</p>\n<p>I'll keep a <strong>table of contents</strong> of all the snippets here, as they are published.</p>\n<p>Draft #1:</p>\n<ol>\n<li><a href=\"/r/discussion/lw/8f9/intelligence_explosion_analysis_draft_introduction/\">Introduction</a></li>\n<li><a href=\"/r/discussion/lw/8ff/intelligence_explosion_analysis_draft_types_of/\">Types of digital intelligence</a></li>\n<li><a href=\"/r/discussion/lw/8jb/intelligence_explosion_analysis_draft_why/\">Why designing digital intelligence gets easier over time</a></li>\n<li><a href=\"/r/discussion/lw/8k8/intelligence_explosion_analysis_draft_how_long/\">How long before digital intelligence?</a></li>\n<li><a href=\"/r/discussion/lw/8l1/intelligence_explosion_analysis_draft_from/\">From digital intelligence to intelligence explosion</a></li>\n<li>[not finished]</li>\n</ol>\n<div><br /></div>\n<div>Draft #2:</div>\n<div><ol>\n<li><a href=\"/r/discussion/lw/8ox/intelligence_explosion_analysis_draft_2_snippet_1/\">Snippet 1</a></li>\n<li>...</li>\n</ol></div>\n<div><br /></div>\n<p>Also see:</p>\n<ul>\n<li><a href=\"/r/discussion/lw/8fa/is_an_intelligence_explosion_a_disjunctive_or/\">Is intelligence explosion a disjunctive or conjunctive event?</a></li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ebRZPDBg5qff9oTs5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 7, "extendedScore": null, "score": 7.990781805605573e-07, "legacy": true, "legacyId": "10901", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G65tLdGma8Xgh3p7L", "dEkSus8QzcCyEKNa6", "yysHcCrQrXYhb4m2s", "7YEkqxoWFCwAQBsMJ", "KMzrqvoC7QGQnriLi", "6kRj95eAFJkGvMoEe", "rWwmRvp8FEKDZ49d6", "j52uErqofDiJZCo76"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-13T22:39:12.519Z", "modifiedAt": null, "url": null, "title": "Drawing Less Wrong: An Introduction", "slug": "drawing-less-wrong-an-introduction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.002Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NwDRdJpZ7n76yQS8u/drawing-less-wrong-an-introduction", "pageUrlRelative": "/posts/NwDRdJpZ7n76yQS8u/drawing-less-wrong-an-introduction", "linkUrl": "https://www.lesswrong.com/posts/NwDRdJpZ7n76yQS8u/drawing-less-wrong-an-introduction", "postedAtFormatted": "Sunday, November 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Less%20Wrong%3A%20An%20Introduction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Less%20Wrong%3A%20An%20Introduction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwDRdJpZ7n76yQS8u%2Fdrawing-less-wrong-an-introduction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Less%20Wrong%3A%20An%20Introduction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwDRdJpZ7n76yQS8u%2Fdrawing-less-wrong-an-introduction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNwDRdJpZ7n76yQS8u%2Fdrawing-less-wrong-an-introduction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 870, "htmlBody": "<blockquote><em>You have not found a way to connect your hobby to rationalism yet. It itches. You are not whole. It is forbidden to post an article entitled Rationalist Hobby on Less Wrong. You lie awake at three in the morning, trying to create puns.</em></blockquote><blockquote><em> -- Alicorn, <a href=\"http://luminousalicorn.tumblr.com/post/115832211805/rationalism-gothic\">Rationality Gothic [2]</a></em></blockquote><p>(Note from 2017: This sequence was written before Alicorn&#x27;s classic jab at people  shoehorning rationality into random hobbies. I&#x27;m fairly embarrassed by it at this point, but don&#x27;t have time to rewrite this into something I&#x27;m not embarrassed by, and several found it a very helpful introduction to drawing.)</p><p>((Also, this sequence ends abruptly when my 2011 life got in the way. If you get to the end and go &quot;aarrg! what do I do next!?&quot; the answer is &quot;purchase &#x27;Drawing on the Right Side of the Brain&#x27; and finding a figure drawing class))</p><hr class=\"dividerBlock\"/><p>This post begins a mini-sequence that discusses how to draw, reports on an experiment about teaching people how to draw, and examines how rationality and good drawing practices are related. (As it turns out, a fair amount)</p><p>I&#x27;m a professional artist. I have a fairly extensive background in traditional drawing, but most of my training is in computer animation. I chose my career because I liked the control offered by the computer - the ability to undo, to manipulate art in procedural ways, and most of all for the flexibility to duplicate things, repurpose them for different projects and combine my love of visual art with my love of game design, animation, and various other mixed media.</p><p>But now I work 10+ hours a day at an advertising agency. I spend all day getting paid to stare at a computer screen. Most of my other hobbies also involve staring at a computer screen. And many types of digital work are taxing on the same set of creative muscles, so at the end of the day I didn&#x27;t have energy to work on the personal projects I wanted.</p><p>One option would be to get a different job that didn&#x27;t tax those creative muscles or involve staring at a screen. I&#x27;ve actually considered getting a &quot;physical&quot; job - after years and thousands of dollars of college to get a nice posh job without physical labor, I actually think it might be better to get PAID to exercise. And instead, use my free time to channel my skills from colleges into personal creative projects that I&#x27;m passionate about.</p><p>I may do that some day, but I DO like my job, I like the people there, and I continue to learn important skills. So instead of modifying my job, I modified my hobbies. A few months ago I began drawing people - on subways, in coffee shops, in parks, etc. This gave me a new creative outlet, as well as a new social outlet. (Starting a conversation with &quot;Hey, can I draw you?&quot; is a pretty useful technique - not only does it provide an excuse to begin talking, but if you follow up with a good drawing, you&#x27;ve established right off the bat that you&#x27;re an interesting person with a valuable skill. You&#x27;ve also flattered the other person a bit, and if the conversation enters a lull, it&#x27;s okay - just draw for a while until you can think of something to say.)</p><p>So I&#x27;ve been getting better at traditional drawing, and better at social interaction, and more confident in general. And at a local Less Wrong meet up, it recently it became clear that...</p><ol><li>Other people wanted to learn to draw</li><li>I wanted to learn to teach</li><li>A few people wanted to model. So the &quot;Drawing Less Wrong&quot; meetup was born. I prepared some lesson plans and began holding 4-hour workshops. </li></ol><p>What interested me was how much the study of drawing was relevant to rationality. Not only do you have to learn to observe reality (this is surprisingly hard), but you have to pretty much scrap your entire model of how you think drawing works. (Almost everything you will naturally gravitate towards is wrong). Most artists don&#x27;t notice that they should be applying these lessons to the rest of their life, but I think the skills can generalize if attention is brought to that notion.</p><p>In the past, I&#x27;ve been to figure drawing workshops where I saw people go from not being able to draw much at all (one person showed up to class with a *horribly* copied manga drawing that they said had taken them 12 hours), to being able to execute a reasonable gesture drawing1 in about 60 seconds. It took them about 8 hours of dedicated practice. I wanted to try and replicate that.</p><p>Soon to follow are a collection of posts discussing the nature of talent, how to draw effectively, and lessons I learned from trying to teach people extremely counterintuitive models of reality.</p><hr class=\"dividerBlock\"/><p>[1] &quot;Reasonable Gesture Drawing&quot; is a specific phrase that means something to trained artists, which non-artists may misinterpret. It doesn&#x27;t mean &quot;looks amazing.&quot; It does mean that this person improved in important ways in a short time.</p><p>[2] I refuse to call it &quot;Rationalism Gothic&quot; because Rationalism means an oddly specific philosophical school that is actually kind of the opposite of what Less Wrong is about.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KDpqtN3MxHSmD4vcB": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NwDRdJpZ7n76yQS8u", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 39, "baseScore": 46, "extendedScore": null, "score": 0.000121, "legacy": true, "legacyId": "10910", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "WPgA9x5ZvKu9oYvgB", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "drawing-less-wrong-should-you-learn-to-draw", "canonicalPrevPostSlug": "", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T04:54:36.078Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-6", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/keWF5YuK35B6fNZqN/meetup-fort-collins-colorado-meetup-wedneday-7pm-6", "pageUrlRelative": "/posts/keWF5YuK35B6fNZqN/meetup-fort-collins-colorado-meetup-wedneday-7pm-6", "linkUrl": "https://www.lesswrong.com/posts/keWF5YuK35B6fNZqN/meetup-fort-collins-colorado-meetup-wedneday-7pm-6", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeWF5YuK35B6fNZqN%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeWF5YuK35B6fNZqN%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkeWF5YuK35B6fNZqN%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4t'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 November 2011 07:00:54PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Do you want to improve your skills at living a rational life. Come meet people who share the need to improve. Plus, fun.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4t'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "keWF5YuK35B6fNZqN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.992164915888656e-07, "legacy": true, "legacyId": "10913", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/4t\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 November 2011 07:00:54PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Do you want to improve your skills at living a rational life. Come meet people who share the need to improve. Plus, fun.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/4t\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T05:34:01.657Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Affective Death Spirals", "slug": "seq-rerun-affective-death-spirals", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MCZK2Zzr5BoygvTAw/seq-rerun-affective-death-spirals", "pageUrlRelative": "/posts/MCZK2Zzr5BoygvTAw/seq-rerun-affective-death-spirals", "linkUrl": "https://www.lesswrong.com/posts/MCZK2Zzr5BoygvTAw/seq-rerun-affective-death-spirals", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Affective%20Death%20Spirals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Affective%20Death%20Spirals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCZK2Zzr5BoygvTAw%2Fseq-rerun-affective-death-spirals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Affective%20Death%20Spirals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCZK2Zzr5BoygvTAw%2Fseq-rerun-affective-death-spirals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMCZK2Zzr5BoygvTAw%2Fseq-rerun-affective-death-spirals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>Today's post, <a href=\"/lw/lm/affective_death_spirals/\">Affective Death Spirals</a> was originally published on 02 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Human beings can fall into a feedback loop around something that they hold dear. Every situation they consider, they use their great idea to explain. Because their great idea explained this situation, it now gains weight. Therefore, they should use it to explain more situations. This loop can continue, until they believe Belgium controls the US banking system, or that they can use an invisible blue spirit force to locate parking spots.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8eu/seq_rerun_mere_messiahs/\">Mere Messiahs</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb0cb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MCZK2Zzr5BoygvTAw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 7.99230422866541e-07, "legacy": true, "legacyId": "10914", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XrzQW69HpidzvBxGr", "W96PXWTrbCWsZe5Lh", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T07:31:46.724Z", "modifiedAt": null, "url": null, "title": "Drawing Less Wrong: Should You Learn to Draw?", "slug": "drawing-less-wrong-should-you-learn-to-draw", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bhTCSdxF4gCuzzTRa/drawing-less-wrong-should-you-learn-to-draw", "pageUrlRelative": "/posts/bhTCSdxF4gCuzzTRa/drawing-less-wrong-should-you-learn-to-draw", "linkUrl": "https://www.lesswrong.com/posts/bhTCSdxF4gCuzzTRa/drawing-less-wrong-should-you-learn-to-draw", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Less%20Wrong%3A%20Should%20You%20Learn%20to%20Draw%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Less%20Wrong%3A%20Should%20You%20Learn%20to%20Draw%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhTCSdxF4gCuzzTRa%2Fdrawing-less-wrong-should-you-learn-to-draw%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Less%20Wrong%3A%20Should%20You%20Learn%20to%20Draw%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhTCSdxF4gCuzzTRa%2Fdrawing-less-wrong-should-you-learn-to-draw", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbhTCSdxF4gCuzzTRa%2Fdrawing-less-wrong-should-you-learn-to-draw", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2687, "htmlBody": "<p><em>This is the second post of the Drawing Less Wrong mini sequence, in which I discuss how to draw, how learning to draw *</em>effectively*<em> relates to rationality, and what the initial results were when I started running a drawing workshop, teaching people with essentially no experience.</em></p><p><em>Information here is a combination of lessons I&#x27;ve learned from numerous art teachers who all agree with each other, and some of my own observations that I&#x27;m pretty confident about. When I talk about &quot;how the brain does things&quot; I&#x27;m using a mix of folk psychology and guesses based on my limited knowledge of neuroscience, which may not be technically accurate but should be sufficient to make useful predictions.</em></p><hr class=\"dividerBlock\"/><h1>The Nature of Talent:</h1><p><em>&quot;Am I talented enough to draw?&quot;</em></p><p>This is a question people think about a lot. It&#x27;s a wrong question.</p><p>Here are a few related, relevant &quot;right&quot; questions:</p><ul><li>Do you have pre-existing skills that can be repurposed for drawing?</li><li>How quickly are you able to acquire skills relevant to drawing?</li><li>Do you naturally enjoy drawing? </li><li>If not, can you easily BECOME the sort of person who naturally enjoys drawing? </li><li>WHY and WHAT do you want to be able to draw?</li><li>How well do you want to be able to draw? How much do you value being able to draw that well? </li><li>How many hours of dedicated practice are you willing to put in to achieve this?</li></ul><p>&quot;Talent&quot; is a real thing, but it doesn&#x27;t mean what most people think it does. It&#x27;s not a magical attribute you either have or don&#x27;t. It&#x27;s not an absolute cap on how good you&#x27;re allowed to get. Talent is two different things:</p><ul><li>How much you naturally enjoy doing something (This is most important, and fortunately I think this is fairly easy to re-wire, though it does take effort and requires some environmental factors)</li><li>How good you are at <em>improving</em> at something. Think of this as a &quot;talent coefficient.&quot; </li></ul><p>The Intrinsic Enjoyment/Improvement-Coefficient model of Talent is simplified, but like <a href=\"https://www.lesserwrong.com/lw/71x/a_crash_course_in_the_neuroscience_of_human/#FolkPsychology\">folk psychology</a>, it&#x27;s a useful way to make some predictions.[1] If you have a talent coefficient of 1 (average), and you put in 4 hours of practice, you&#x27;ll get 4 hours of &quot;effective practice.&quot; If your talent coefficient is 1.5, you&#x27;d get 6 effective hours. If your talent coefficient is .5, you&#x27;ll get 2 effective hours. </p><p>And if you enjoy doing something, you will do a lot of it.</p><p>I have always been bad at sports. A lot of this can be attributed to me not really liking sports and hence not putting in much practice. But in high school, I noticed an interesting trend: We would spend a few weeks on a particular activity (basketball, badminton, tennis, volleyball). We would do the same activities each year. And at the beginning of any particular activity, I would suck at it.</p><p>I more or less liked each activity equally, and put the same effort into each. But after a few weeks, some activities I would noticeably increase in skill. Others I would not. I was terrible at basketball no matter how hard I tried. But I got better at tennis and volleyball, and I got much better at badminton.</p><p>It&#x27;s possible that badminton was just an easier game (the birdie does move slower and you have a wide racket to catch it with). But I got better at badminton *relative* to other people in the class, and other non-athletic people in the class were always relatively better at me at basketball. I didn&#x27;t do a formal study, but my nonscientific guesstimate is that I have a high-ish talent coefficient at badminton (maybe 1.1) and a very low talent coefficient at basketball. (.5? .2? .1?)</p><p>I&#x27;m sure that the things we naturally improve at ALSO tend to be things that we naturally enjoy doing, which confuses the issue. If you naturally improve quickly, you get to feel good about yourself sooner which inspires more effort. It also is probably an activity that feels comfortable and hence enjoyable to you. But there are also things I&#x27;m good at that I didn&#x27;t become motivated to do until recently (for example, programming). So it&#x27;s worth drawing the distinction.</p><p>It&#x27;s also worth noting that skills like &quot;basketball&quot; and &quot;drawing&quot; are really made up of numerous sub skills. For example, various ball games can be broken down into things like:</p><ul><li>Being able to run quickly</li><li>Being able to change direction while running accurately</li><li>Being able to move your hand to intercept a moving object</li><li>Being able to catch said object without dropping it</li><li>Being able to throw an object accurately towards a target</li></ul><p>(I think the key difference between basketball and badminton is my ability to *throw*. In Badminton, Tennis and even Volleyball, the way your hand interacts with the ball is very different from basketball.)</p><p>In drawing, some sub-skills might include:</p><ul><li>Being able to accurately observe shape and value</li><li>Having the coordination to draw marks where you want to</li><li>Being able to fluidly alter the pressure on your pencil to apply different line thickness/darkness in useful ways.</li><li>Weirder skills like &quot;being able to instill emotion into your drawing,&quot; which may be frustrating for logical-brained people to understand. I&#x27;ll try to break them down later.</li></ul><p>When I say &quot;your Drawing Talent Coefficient,&quot; I&#x27;m referring to an approximate average of various relevant skills. If you think you can&#x27;t draw, I&#x27;m about 80% sure that your drawing talent is, at worst, around .75. You probably stopped putting as much practice in at a fairly young age, and/or never received proper instruction.</p><h1>Why do you want to draw?</h1><p>Here&#x27;s a few reasons you might want to draw:</p><ol><li>You naturally enjoy drawing. You want to get better at it, but you&#x27;re not trying to reach a particular level of competence. (Your terminal goal is to draw, and to improve at it enough that you notice yourself improving)</li><li>You enjoy being able to record interesting things on paper (these can be real things like people or imaginary things like dragons). You have a terminal goal, not of drawing, but of drawing particular things in interesting ways.</li><li>You enjoy the creative process - being able to design NEW interesting things. Drawing is an instrumental goal necessary to try out various ideas and see how they look, both to yourself and other people.</li><li>You like to be able to impress people (with good drawings - possibly drawings of cool things, possibly drawings of the particular people you&#x27;re trying to impress.) Drawing is an instrumental goal towards impressing people.</li><li>You want money, you enjoy drawing, you think you can become good at it with less effort relative to other things, so you&#x27;re considering learning to draw as an instrumental goal towards making money.</li></ol><p>All of these are reasonable goals (possible exception of 5 - I don&#x27;t know that <em>drawing</em> is a reliable way to make money, but I do think that drawing is a skill that helps build towards OTHER skills that reliably make money). But whether they&#x27;re a good idea hinges on some additional information.</p><p>Like anything worth doing, learning to draw REALLY well takes somewhere on the order of 10,000 hours.[2] And even when you&#x27;ve put 10,000 hours in, you&#x27;ll start looking at people who&#x27;ve put in 20,000 or 30,000 hours and you&#x27;ll finally comprehend how much skill went into their work and realize how much farther you <em>still</em> have to go and you will never, ever be satisfied.</p><p>But drawing skill doesn&#x27;t follow a linear curve. You&#x27;ll improve more in the first hundred hours or so - a lot of sub-skills are low hanging fruit that can be quickly acquired if you dedicate yourself. If you want to make money, you&#x27;ll need to put in the full 10,000. But if you want to do something reasonably cool, fun, impressive and occasionally useful, you can get achieve that in a relatively short time period.</p><p>Most kids who like drawing have probably put in close to 10,000 hours in when they reach high school. By the end of elementary school, the kids with slight advantages have made enough effort that the kids with slight disadvantages look at them and think &quot;man, I suck at drawing.&quot; They lose whatever intrinsic motivation they had, falling further behind. They come to identify as people who &quot;can&#x27;t draw.&quot; People who can &quot;only draw stick figures.&quot; </p><p>Most of those people are wrong. They can become good. And they don&#x27;t even have to put in the 10,000 hours that the &quot;good artist&quot; kids put in, because most of the &quot;artists&quot; were spending their time doing horribly, horribly inefficient things (which is why they need to put in another 10,000 hours when they get to college and realize they were doing it wrong)</p><h1>The &quot;Right&quot; Side of the Brain</h1><p>Here&#x27;s the problem: most aspiring artists are motivated by goals (1), (2) or (3). Drawing is comfortable and fun. They like drawing cool things. They like being creative.</p><p>But the comfortable, fun way to draw is not the same way to IMPROVE at drawing quickly, if your goal is to be able to draw things that other people recognize. The cool things you want to draw are not the optimal things to practice. The creativity you want to express cannot help you improve at drawing much at all until you&#x27;ve learned to be creative in different ways.</p><p>Learning to draw in an efficient way is initially uncomfortable. It is counterintuitive. It will feel weird and wrong. There will be a period of several hours where you will not understand why you are doing things this way, and your drawings don&#x27;t seem to improve. This is because you&#x27;re building up new skills essentially from scratch - skills you always had the capability to gain, but the relevant parts of your brain are extremely underdeveloped. </p><p>Art teachers in high school and college face the difficult task of convincing students (even &quot;good&quot; students) that they are approaching reality in a fundamentally wrong way using a horribly inefficient method, harnessing the power of the wrong parts of their brain. Most students never make the adjustment. They stick with the comfortable things that motivated them in the first place.</p><p>Learning to draw &quot;the right way&quot; is a <a href=\"https://www.lesserwrong.com/lw/58g/levels_of_action/\">high level action</a>. Eventually you&#x27;ll be able to return to the initial fun, comfortable and creative motivations that first inspired you. And you will be much better at it when you do.[4] But you must be sufficiently motivated to make it through 6-10 hours of difficult work.</p><p>If you AREN&#x27;T intrinsically motivated, you will give up, not try hard enough, and never understand why your teacher was making you do it this way.</p><p>But if you have the motivation and proper instruction, you can rewire yourself.</p><p>In 3-5 hours, you can develop an understanding of WHY you need to rewire yourself. For the next 2 hours, you&#x27;ll have developed to the point that you&#x27;ll understand what&#x27;s SUPPOSED to be happening, but it won&#x27;t be happening yet. This will be extremely frustrating. Somewhere around hour 6-10, you&#x27;ll have developed new skills to the point that you can start showing improvement. (It may still not be clear to other people that you&#x27;ve improved. Your drawings will look messy in a particular way that others might not get. But you and other trained artists will be able to look at your drawings and see that your newfound skill is reflected in your work. And you&#x27;ll probably have produced at least one drawing that untrained bystanders will recognize as much better than what you started with). </p><p>I haven&#x27;t studied the issue as much past the 8 hour mark. Right now I&#x27;ve run two 4-hour workshops. Among students who had practically no drawing experience, my predictions proved accurate. Participants are enthusiastic for more meetups and my new, less certain predictions are:</p><p>In 12-20 hours of concentrated effort, you&#x27;ll have reached a point that the average person will watch you draw and say &quot;hey, you can draw, that&#x27;s cool.&quot; If your goal was to use drawing to develop creative ideas, you&#x27;ll understand how to study things so that you can synthesize new, better creative ideas. If your goal was to enjoy the process of drawing, you&#x27;ll have rewired yourself so that you enjoy a new, faster process of drawing.</p><p>Low hanging fruit will start to drop off after the 20 hour mark. By the hundred hour mark, the average person will look at your work and say &quot;Wow, that&#x27;s a good drawing! You&#x27;re talented!&quot;. (You&#x27;ll also have been able to do that drawing in 30-60 seconds, which makes it even more impressive, if you care about that sort of thing)[5]</p><p>(I expect interest among NYC rationalists to drop off around after 4-5 sessions. I&#x27;ll report on that, in addition to the report about the first two sessions that I&#x27;ll be doing this week. I do not expect to get good data on the hundred-hour prediction, unless I can find good, pre-existing data about similar programs).</p><h1>So... should you learn to draw?</h1><p>I&#x27;ve given you a sense of the time involved. You can figure out how motivated you are. A big remaining question is: Can you find a good teacher?</p><p>Having a good class environment is important for many people&#x27;s learning and motivation. There are a bajillion subtle things you will get wrong (or get right) and not notice - having a teacher who can identify those things is important. A major difficulty I found teaching was finding ways to articulate ideas that I&#x27;ve long stopped thinking about consciously. I will attempt to outline as many techniques as possible, and I may even post some youtube videos (or link to good ones I find). But the participants in my workshop all agreed that it was very useful to actually see me drawing, to understand how they were actually supposed to move their pencil.</p><p>After I&#x27;ve finished this sequence, if you live in the NYC area and think you want to give it a try, shoot me a PM and I&#x27;ll let you know when the next workshop is. (For the time being I am not charging for this, since I&#x27;m still learning a lot myself about how to teach. That may eventually change).</p><p>If you don&#x27;t live near Manhattan, look for a local figure drawing meetup or class that stresses <em>*30 second gesture drawing*</em>. This exercise is the crux of the material I&#x27;m presenting, and a teacher that emphasizes it will probably also emphasize a lot of the other things I talk about.</p><p>If you have previously decided you &quot;can&#x27;t draw,&quot; but are motivated to try again, I recommend going to a class that&#x27;s similar to what I advocate, put in at least 8 hours, and then evaluate from there.</p><hr class=\"dividerBlock\"/><p>[1] If Lukeprog or anyone else has information on the science (neuro or otherwise) of skill acquisition, I&#x27;d love to learn more about it. </p><p>[2] Whenever I say &quot;expect X from Y hours of practice,&quot; I&#x27;m referring to the average person with a coefficient of 1. But I&#x27;m pretty sure the 10,000 number is a highly approximately made up number to begin with, so it&#x27;s not that important. (If I give a range, like 3-5 hours, I&#x27;m accounting for ranges in talent)</p><p>[3] I know &quot;right and left brained&quot; isn&#x27;t exactly a real thing. But the set of abilities generally associated with the Left Brain (i.e. the stuff Less Wrongians are particularly likely to favor, as well as what most novice drawers gravitate towards), are mostly the wrong abilities to be harnessing for the purpose of drawing.</p><p>[4] It actually does take extra effort to translate the kinds of skills I&#x27;m about to talk about over to &quot;cartoonish&quot; drawing. Cartoonish drawing is its own skill that requires it&#x27;s own kind of practice. BUT you will still end up much better at drawing cartoons if you have an understanding of reality. More about that later.</p><p>[5] You may not care about drawing quickly, but fast drawing is actually an instrumental goal towards drawing well. Drawing quickly FORCES you to develop mental processes that make your drawings more energetic and interesting, which drawing slowly never will.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KDpqtN3MxHSmD4vcB": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bhTCSdxF4gCuzzTRa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 32, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "10915", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "WPgA9x5ZvKu9oYvgB", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "drawing-less-wrong-overview-of-skills-and-relevance-to", "canonicalPrevPostSlug": "drawing-less-wrong-an-introduction", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is the second post of the Drawing Less Wrong mini sequence, in which I discuss how to draw, how learning to draw *</em>effectively*<em> relates to rationality, and what the initial results were when I started running a drawing workshop, teaching people with essentially no experience.</em></p><p><em>Information here is a combination of lessons I've learned from numerous art teachers who all agree with each other, and some of my own observations that I'm pretty confident about. When I talk about \"how the brain does things\" I'm using a mix of folk psychology and guesses based on my limited knowledge of neuroscience, which may not be technically accurate but should be sufficient to make useful predictions.</em></p><hr class=\"dividerBlock\"><h1 id=\"The_Nature_of_Talent_\">The Nature of Talent:</h1><p><em>\"Am I talented enough to draw?\"</em></p><p>This is a question people think about a lot. It's a wrong question.</p><p>Here are a few related, relevant \"right\" questions:</p><ul><li>Do you have pre-existing skills that can be repurposed for drawing?</li><li>How quickly are you able to acquire skills relevant to drawing?</li><li>Do you naturally enjoy drawing? </li><li>If not, can you easily BECOME the sort of person who naturally enjoys drawing? </li><li>WHY and WHAT do you want to be able to draw?</li><li>How well do you want to be able to draw? How much do you value being able to draw that well? </li><li>How many hours of dedicated practice are you willing to put in to achieve this?</li></ul><p>\"Talent\" is a real thing, but it doesn't mean what most people think it does. It's not a magical attribute you either have or don't. It's not an absolute cap on how good you're allowed to get. Talent is two different things:</p><ul><li>How much you naturally enjoy doing something (This is most important, and fortunately I think this is fairly easy to re-wire, though it does take effort and requires some environmental factors)</li><li>How good you are at <em>improving</em> at something. Think of this as a \"talent coefficient.\" </li></ul><p>The Intrinsic Enjoyment/Improvement-Coefficient model of Talent is simplified, but like <a href=\"https://www.lesserwrong.com/lw/71x/a_crash_course_in_the_neuroscience_of_human/#FolkPsychology\">folk psychology</a>, it's a useful way to make some predictions.[1] If you have a talent coefficient of 1 (average), and you put in 4 hours of practice, you'll get 4 hours of \"effective practice.\" If your talent coefficient is 1.5, you'd get 6 effective hours. If your talent coefficient is .5, you'll get 2 effective hours. </p><p>And if you enjoy doing something, you will do a lot of it.</p><p>I have always been bad at sports. A lot of this can be attributed to me not really liking sports and hence not putting in much practice. But in high school, I noticed an interesting trend: We would spend a few weeks on a particular activity (basketball, badminton, tennis, volleyball). We would do the same activities each year. And at the beginning of any particular activity, I would suck at it.</p><p>I more or less liked each activity equally, and put the same effort into each. But after a few weeks, some activities I would noticeably increase in skill. Others I would not. I was terrible at basketball no matter how hard I tried. But I got better at tennis and volleyball, and I got much better at badminton.</p><p>It's possible that badminton was just an easier game (the birdie does move slower and you have a wide racket to catch it with). But I got better at badminton *relative* to other people in the class, and other non-athletic people in the class were always relatively better at me at basketball. I didn't do a formal study, but my nonscientific guesstimate is that I have a high-ish talent coefficient at badminton (maybe 1.1) and a very low talent coefficient at basketball. (.5? .2? .1?)</p><p>I'm sure that the things we naturally improve at ALSO tend to be things that we naturally enjoy doing, which confuses the issue. If you naturally improve quickly, you get to feel good about yourself sooner which inspires more effort. It also is probably an activity that feels comfortable and hence enjoyable to you. But there are also things I'm good at that I didn't become motivated to do until recently (for example, programming). So it's worth drawing the distinction.</p><p>It's also worth noting that skills like \"basketball\" and \"drawing\" are really made up of numerous sub skills. For example, various ball games can be broken down into things like:</p><ul><li>Being able to run quickly</li><li>Being able to change direction while running accurately</li><li>Being able to move your hand to intercept a moving object</li><li>Being able to catch said object without dropping it</li><li>Being able to throw an object accurately towards a target</li></ul><p>(I think the key difference between basketball and badminton is my ability to *throw*. In Badminton, Tennis and even Volleyball, the way your hand interacts with the ball is very different from basketball.)</p><p>In drawing, some sub-skills might include:</p><ul><li>Being able to accurately observe shape and value</li><li>Having the coordination to draw marks where you want to</li><li>Being able to fluidly alter the pressure on your pencil to apply different line thickness/darkness in useful ways.</li><li>Weirder skills like \"being able to instill emotion into your drawing,\" which may be frustrating for logical-brained people to understand. I'll try to break them down later.</li></ul><p>When I say \"your Drawing Talent Coefficient,\" I'm referring to an approximate average of various relevant skills. If you think you can't draw, I'm about 80% sure that your drawing talent is, at worst, around .75. You probably stopped putting as much practice in at a fairly young age, and/or never received proper instruction.</p><h1 id=\"Why_do_you_want_to_draw_\">Why do you want to draw?</h1><p>Here's a few reasons you might want to draw:</p><ol><li>You naturally enjoy drawing. You want to get better at it, but you're not trying to reach a particular level of competence. (Your terminal goal is to draw, and to improve at it enough that you notice yourself improving)</li><li>You enjoy being able to record interesting things on paper (these can be real things like people or imaginary things like dragons). You have a terminal goal, not of drawing, but of drawing particular things in interesting ways.</li><li>You enjoy the creative process - being able to design NEW interesting things. Drawing is an instrumental goal necessary to try out various ideas and see how they look, both to yourself and other people.</li><li>You like to be able to impress people (with good drawings - possibly drawings of cool things, possibly drawings of the particular people you're trying to impress.) Drawing is an instrumental goal towards impressing people.</li><li>You want money, you enjoy drawing, you think you can become good at it with less effort relative to other things, so you're considering learning to draw as an instrumental goal towards making money.</li></ol><p>All of these are reasonable goals (possible exception of 5 - I don't know that <em>drawing</em> is a reliable way to make money, but I do think that drawing is a skill that helps build towards OTHER skills that reliably make money). But whether they're a good idea hinges on some additional information.</p><p>Like anything worth doing, learning to draw REALLY well takes somewhere on the order of 10,000 hours.[2] And even when you've put 10,000 hours in, you'll start looking at people who've put in 20,000 or 30,000 hours and you'll finally comprehend how much skill went into their work and realize how much farther you <em>still</em> have to go and you will never, ever be satisfied.</p><p>But drawing skill doesn't follow a linear curve. You'll improve more in the first hundred hours or so - a lot of sub-skills are low hanging fruit that can be quickly acquired if you dedicate yourself. If you want to make money, you'll need to put in the full 10,000. But if you want to do something reasonably cool, fun, impressive and occasionally useful, you can get achieve that in a relatively short time period.</p><p>Most kids who like drawing have probably put in close to 10,000 hours in when they reach high school. By the end of elementary school, the kids with slight advantages have made enough effort that the kids with slight disadvantages look at them and think \"man, I suck at drawing.\" They lose whatever intrinsic motivation they had, falling further behind. They come to identify as people who \"can't draw.\" People who can \"only draw stick figures.\" </p><p>Most of those people are wrong. They can become good. And they don't even have to put in the 10,000 hours that the \"good artist\" kids put in, because most of the \"artists\" were spending their time doing horribly, horribly inefficient things (which is why they need to put in another 10,000 hours when they get to college and realize they were doing it wrong)</p><h1 id=\"The__Right__Side_of_the_Brain\">The \"Right\" Side of the Brain</h1><p>Here's the problem: most aspiring artists are motivated by goals (1), (2) or (3). Drawing is comfortable and fun. They like drawing cool things. They like being creative.</p><p>But the comfortable, fun way to draw is not the same way to IMPROVE at drawing quickly, if your goal is to be able to draw things that other people recognize. The cool things you want to draw are not the optimal things to practice. The creativity you want to express cannot help you improve at drawing much at all until you've learned to be creative in different ways.</p><p>Learning to draw in an efficient way is initially uncomfortable. It is counterintuitive. It will feel weird and wrong. There will be a period of several hours where you will not understand why you are doing things this way, and your drawings don't seem to improve. This is because you're building up new skills essentially from scratch - skills you always had the capability to gain, but the relevant parts of your brain are extremely underdeveloped. </p><p>Art teachers in high school and college face the difficult task of convincing students (even \"good\" students) that they are approaching reality in a fundamentally wrong way using a horribly inefficient method, harnessing the power of the wrong parts of their brain. Most students never make the adjustment. They stick with the comfortable things that motivated them in the first place.</p><p>Learning to draw \"the right way\" is a <a href=\"https://www.lesserwrong.com/lw/58g/levels_of_action/\">high level action</a>. Eventually you'll be able to return to the initial fun, comfortable and creative motivations that first inspired you. And you will be much better at it when you do.[4] But you must be sufficiently motivated to make it through 6-10 hours of difficult work.</p><p>If you AREN'T intrinsically motivated, you will give up, not try hard enough, and never understand why your teacher was making you do it this way.</p><p>But if you have the motivation and proper instruction, you can rewire yourself.</p><p>In 3-5 hours, you can develop an understanding of WHY you need to rewire yourself. For the next 2 hours, you'll have developed to the point that you'll understand what's SUPPOSED to be happening, but it won't be happening yet. This will be extremely frustrating. Somewhere around hour 6-10, you'll have developed new skills to the point that you can start showing improvement. (It may still not be clear to other people that you've improved. Your drawings will look messy in a particular way that others might not get. But you and other trained artists will be able to look at your drawings and see that your newfound skill is reflected in your work. And you'll probably have produced at least one drawing that untrained bystanders will recognize as much better than what you started with). </p><p>I haven't studied the issue as much past the 8 hour mark. Right now I've run two 4-hour workshops. Among students who had practically no drawing experience, my predictions proved accurate. Participants are enthusiastic for more meetups and my new, less certain predictions are:</p><p>In 12-20 hours of concentrated effort, you'll have reached a point that the average person will watch you draw and say \"hey, you can draw, that's cool.\" If your goal was to use drawing to develop creative ideas, you'll understand how to study things so that you can synthesize new, better creative ideas. If your goal was to enjoy the process of drawing, you'll have rewired yourself so that you enjoy a new, faster process of drawing.</p><p>Low hanging fruit will start to drop off after the 20 hour mark. By the hundred hour mark, the average person will look at your work and say \"Wow, that's a good drawing! You're talented!\". (You'll also have been able to do that drawing in 30-60 seconds, which makes it even more impressive, if you care about that sort of thing)[5]</p><p>(I expect interest among NYC rationalists to drop off around after 4-5 sessions. I'll report on that, in addition to the report about the first two sessions that I'll be doing this week. I do not expect to get good data on the hundred-hour prediction, unless I can find good, pre-existing data about similar programs).</p><h1 id=\"So____should_you_learn_to_draw_\">So... should you learn to draw?</h1><p>I've given you a sense of the time involved. You can figure out how motivated you are. A big remaining question is: Can you find a good teacher?</p><p>Having a good class environment is important for many people's learning and motivation. There are a bajillion subtle things you will get wrong (or get right) and not notice - having a teacher who can identify those things is important. A major difficulty I found teaching was finding ways to articulate ideas that I've long stopped thinking about consciously. I will attempt to outline as many techniques as possible, and I may even post some youtube videos (or link to good ones I find). But the participants in my workshop all agreed that it was very useful to actually see me drawing, to understand how they were actually supposed to move their pencil.</p><p>After I've finished this sequence, if you live in the NYC area and think you want to give it a try, shoot me a PM and I'll let you know when the next workshop is. (For the time being I am not charging for this, since I'm still learning a lot myself about how to teach. That may eventually change).</p><p>If you don't live near Manhattan, look for a local figure drawing meetup or class that stresses <em>*30 second gesture drawing*</em>. This exercise is the crux of the material I'm presenting, and a teacher that emphasizes it will probably also emphasize a lot of the other things I talk about.</p><p>If you have previously decided you \"can't draw,\" but are motivated to try again, I recommend going to a class that's similar to what I advocate, put in at least 8 hours, and then evaluate from there.</p><hr class=\"dividerBlock\"><p>[1] If Lukeprog or anyone else has information on the science (neuro or otherwise) of skill acquisition, I'd love to learn more about it. </p><p>[2] Whenever I say \"expect X from Y hours of practice,\" I'm referring to the average person with a coefficient of 1. But I'm pretty sure the 10,000 number is a highly approximately made up number to begin with, so it's not that important. (If I give a range, like 3-5 hours, I'm accounting for ranges in talent)</p><p>[3] I know \"right and left brained\" isn't exactly a real thing. But the set of abilities generally associated with the Left Brain (i.e. the stuff Less Wrongians are particularly likely to favor, as well as what most novice drawers gravitate towards), are mostly the wrong abilities to be harnessing for the purpose of drawing.</p><p>[4] It actually does take extra effort to translate the kinds of skills I'm about to talk about over to \"cartoonish\" drawing. Cartoonish drawing is its own skill that requires it's own kind of practice. BUT you will still end up much better at drawing cartoons if you have an understanding of reality. More about that later.</p><p>[5] You may not care about drawing quickly, but fast drawing is actually an instrumental goal towards drawing well. Drawing quickly FORCES you to develop mental processes that make your drawings more energetic and interesting, which drawing slowly never will.</p>", "sections": [{"title": "The Nature of Talent:", "anchor": "The_Nature_of_Talent_", "level": 1}, {"title": "Why do you want to draw?", "anchor": "Why_do_you_want_to_draw_", "level": 1}, {"title": "The \"Right\" Side of the Brain", "anchor": "The__Right__Side_of_the_Brain", "level": 1}, {"title": "So... should you learn to draw?", "anchor": "So____should_you_learn_to_draw_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "55 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T09:50:22.531Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft: introduction", "slug": "intelligence-explosion-analysis-draft-introduction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:26.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dEkSus8QzcCyEKNa6/intelligence-explosion-analysis-draft-introduction", "pageUrlRelative": "/posts/dEkSus8QzcCyEKNa6/intelligence-explosion-analysis-draft-introduction", "linkUrl": "https://www.lesswrong.com/posts/dEkSus8QzcCyEKNa6/intelligence-explosion-analysis-draft-introduction", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%3A%20introduction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%3A%20introduction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEkSus8QzcCyEKNa6%2Fintelligence-explosion-analysis-draft-introduction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%3A%20introduction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEkSus8QzcCyEKNa6%2Fintelligence-explosion-analysis-draft-introduction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdEkSus8QzcCyEKNa6%2Fintelligence-explosion-analysis-draft-introduction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1066, "htmlBody": "<p>I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on.</p>\n<p>This snippet is a <em>possible</em> introduction to the analysis article. Its purpose is to show readers that we aim to take seriously some common concerns about singularity thinking, to bring readers into Near Mode about the topic, and to explain the purpose and scope of the article.</p>\n<p>Note that the target style is serious but still more chatty than a normal journal article.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"padding-left: 30px;\">The best answer to the question, \"Will computers ever be as smart as humans?\" is probably &ldquo;Yes, but only briefly.\"</p>\n<p align=\"right\">Vernor Vinge</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Humans may create human-level artificial intelligence in this century (Bainbridge 2006; Baum, Goertzel, and Goertzel 2011; Bostrom 2003; Legg 2008; Sandberg and Bostrom 2011). Shortly thereafter, we may see an &ldquo;intelligence explosion&rdquo; or &ldquo;technological Singularity&rdquo; &mdash; a chain of events by which human-level AI leads, fairly rapidly, to intelligent systems whose capabilities far surpass those of biological humanity as a whole (Chalmers 2010).</p>\n<p>How likely is this, and what should we do about it? Others have discussed these questions previously (Turing 1950; Good 1965; Von Neumann 1966; Solomonoff 1985; Vinge 1993; Yudkowsky 2001, 2008a; Russell and Norvig 2010, sec. 26.3); we will build on their thinking in our review of the subject.</p>\n<p>&nbsp;</p>\n<h4>Singularity Skepticism</h4>\n<p>Many are skeptical of Singularity arguments because they associate such arguments with detailed storytelling &mdash; the &ldquo;if and then&rdquo; fallacy of &ldquo;speculative ethics&rdquo; by which an improbable conditional becomes a supposed actual (Nordmann 2007). They are right to be skeptical: hundreds of studies show that humans are overconfident of their beliefs (Moore and Healy 2008), regularly overestimate the probability of detailed visualized scenarios (Tversky and Kahneman 2002), and tend to seek out only information that confirms their current views (Nickerson 1998). AI researchers are not immune from these errors, as evidenced by a history of over-optimistic predictions going back to the 1956 Dartmouth conference on AI (Dreyfus 1972).</p>\n<p>Nevertheless, mere mortals have at times managed to reason usefully and somewhat accurately about the future, even with little data. When Leo Szilard conceived of the nuclear chain reaction, he realized its destructive potential and filed his patent in a way that kept it secret from the Nazis (Rhodes 1995, 224&ndash;225). Svante Arrhenius' (1896) models of climate change lacked modern climate theory and data but, by making reasonable extrapolations from what was known of physics, still managed to predict (within 2&deg;C) how much warming would result from a doubling of CO<sub>2</sub> in the atmosphere (Crawford 1997). Norman Rasmussen's (1975) analysis of the safety of nuclear power plants, written before any nuclear accidents had occurred, correctly predicted several details of the Three Mile Island incident that previous experts had not (McGrayne 2011, 180).</p>\n<p>In planning for the future, how can we be more like Rasmussen and less like the Dartmouth conference? For a start, we can apply the recommendations of cognitive science on how to meliorate overconfidence and other biases (Larrick 2004; Lillienfeld, Ammirati, and Landfield 2009). In keeping with these recommendations, we acknowledge unknowns and do not build models that depend on detailed storytelling. For example, we will not assume the continuation of Moore&rsquo;s law, nor that hardware trajectories determine software progress. To avoid nonsense, it should not be necessary to have superhuman reasoning powers; all that should be necessary is to avoid believing we know something when we do not.</p>\n<p>One might think such caution would prevent us from concluding anything of interest, but in fact it seems that intelligence explosion may be a convergent outcome of many or most future scenarios. That is, an intelligence explosion may have fair probability, not because it occurs in one particular detailed scenario, but because, like the evolution of eyes or the emergence of markets, it can come about through many different paths and can gather momentum once it gets started. Humans tend to underestimate the likelihood of such &ldquo;disjunctive&rdquo; events, because they can result from many different paths (Tversky and Kahneman 1974). We suspect the considerations in this paper may convince you, as they did us, that this particular disjunctive event (intelligence explosion) is worthy of consideration.</p>\n<p>First, we provide evidence which suggests that, barring global catastrophe and other disruptions to scientific progress, there is a significant probability we will see the creation of digital intelligence within a century. Second, we suggest that the arrival of digital intelligence is likely to lead rather quickly to intelligence explosion. Finally, we discuss the possible consequences of an intelligence explosion and which actions we can take now to influence those results.</p>\n<p>These questions are complicated, the future is uncertain, and our chapter is brief. Our aim, then, can only be to provide a quick survey of the issues involved. We believe these matters are important, and our discussion of them must be permitted to begin at a low level because there is no other place to lay the first stones.</p>\n<p>&nbsp;</p>\n<h4>References for this snippet</h4>\n<ul>\n<li>Bainbridge 2006 managing nano-bio-info-cogno innovations</li>\n<li>Baum Goertzel Goertzel 2011 how long until human-level ai</li>\n<li>Bostrom 2003 ethical issues in advanced artificial intelligence</li>\n<li>Chalmers 2010 singularity philosophical analysis</li>\n<li>Legg 2008 machine super intelligence</li>\n<li>Sandberg &amp; Bostrom 2011 machine intelligence survey</li>\n<li>Turing 1950 machine intelligence</li>\n<li>Good 1965 speculations concerning...</li>\n<li>Von neumann 1966 theory of self-reproducing autonomata</li>\n<li>Solomonoff 1985 the time scale of artificial intelligence</li>\n<li>Vinge 1993 coming technological singularity</li>\n<li>Yudkowsky 2001 creating friendly ai</li>\n<li>Yudkowsky 2008a negative and positive factor in global risk</li>\n<li>Russel Norvig 2010 artificial intelligence a modern approach 3e</li>\n<li>Nordman 2007 If and then: a critique of speculative nanoethics</li>\n<li>Moore and Healy the trouble with overconfidence</li>\n<li>Tversky Kahneman 2002 extensional versus intuitive reasoning, the conjunction fallacy</li>\n<li>Nickerson 1998 Confirmation Bias; A Ubiquitous Phenomenon in Many Guises</li>\n<li>Dreyfus 1972 what computers can't do</li>\n<li>Rhodes 1995 making of the atomic bomb</li>\n<li>Arrhenius 1896 On the Influence of Carbonic Acid in the Air Upon the Temperature</li>\n<li>Crawford 1997 &nbsp;Arrhenius' 1896 model of the greenhouse effect in context</li>\n<li>Rasmussen 1975 WASH-1400 report</li>\n<li>McGrayne 2011 theory that would not die</li>\n<li>Larrick 2004 debiasing</li>\n<li>Lillienfeld, Ammirati, and Landfield 2009 giving debiasing away</li>\n<li>Tversky and Kahneman 1974 Judgment under uncertainty: Heuristics and biases</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dEkSus8QzcCyEKNa6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 1, "extendedScore": null, "score": 7.993210138203676e-07, "legacy": true, "legacyId": "10917", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on.</p>\n<p>This snippet is a <em>possible</em> introduction to the analysis article. Its purpose is to show readers that we aim to take seriously some common concerns about singularity thinking, to bring readers into Near Mode about the topic, and to explain the purpose and scope of the article.</p>\n<p>Note that the target style is serious but still more chatty than a normal journal article.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p style=\"padding-left: 30px;\">The best answer to the question, \"Will computers ever be as smart as humans?\" is probably \u201cYes, but only briefly.\"</p>\n<p align=\"right\">Vernor Vinge</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Humans may create human-level artificial intelligence in this century (Bainbridge 2006; Baum, Goertzel, and Goertzel 2011; Bostrom 2003; Legg 2008; Sandberg and Bostrom 2011). Shortly thereafter, we may see an \u201cintelligence explosion\u201d or \u201ctechnological Singularity\u201d \u2014 a chain of events by which human-level AI leads, fairly rapidly, to intelligent systems whose capabilities far surpass those of biological humanity as a whole (Chalmers 2010).</p>\n<p>How likely is this, and what should we do about it? Others have discussed these questions previously (Turing 1950; Good 1965; Von Neumann 1966; Solomonoff 1985; Vinge 1993; Yudkowsky 2001, 2008a; Russell and Norvig 2010, sec. 26.3); we will build on their thinking in our review of the subject.</p>\n<p>&nbsp;</p>\n<h4 id=\"Singularity_Skepticism\">Singularity Skepticism</h4>\n<p>Many are skeptical of Singularity arguments because they associate such arguments with detailed storytelling \u2014 the \u201cif and then\u201d fallacy of \u201cspeculative ethics\u201d by which an improbable conditional becomes a supposed actual (Nordmann 2007). They are right to be skeptical: hundreds of studies show that humans are overconfident of their beliefs (Moore and Healy 2008), regularly overestimate the probability of detailed visualized scenarios (Tversky and Kahneman 2002), and tend to seek out only information that confirms their current views (Nickerson 1998). AI researchers are not immune from these errors, as evidenced by a history of over-optimistic predictions going back to the 1956 Dartmouth conference on AI (Dreyfus 1972).</p>\n<p>Nevertheless, mere mortals have at times managed to reason usefully and somewhat accurately about the future, even with little data. When Leo Szilard conceived of the nuclear chain reaction, he realized its destructive potential and filed his patent in a way that kept it secret from the Nazis (Rhodes 1995, 224\u2013225). Svante Arrhenius' (1896) models of climate change lacked modern climate theory and data but, by making reasonable extrapolations from what was known of physics, still managed to predict (within 2\u00b0C) how much warming would result from a doubling of CO<sub>2</sub> in the atmosphere (Crawford 1997). Norman Rasmussen's (1975) analysis of the safety of nuclear power plants, written before any nuclear accidents had occurred, correctly predicted several details of the Three Mile Island incident that previous experts had not (McGrayne 2011, 180).</p>\n<p>In planning for the future, how can we be more like Rasmussen and less like the Dartmouth conference? For a start, we can apply the recommendations of cognitive science on how to meliorate overconfidence and other biases (Larrick 2004; Lillienfeld, Ammirati, and Landfield 2009). In keeping with these recommendations, we acknowledge unknowns and do not build models that depend on detailed storytelling. For example, we will not assume the continuation of Moore\u2019s law, nor that hardware trajectories determine software progress. To avoid nonsense, it should not be necessary to have superhuman reasoning powers; all that should be necessary is to avoid believing we know something when we do not.</p>\n<p>One might think such caution would prevent us from concluding anything of interest, but in fact it seems that intelligence explosion may be a convergent outcome of many or most future scenarios. That is, an intelligence explosion may have fair probability, not because it occurs in one particular detailed scenario, but because, like the evolution of eyes or the emergence of markets, it can come about through many different paths and can gather momentum once it gets started. Humans tend to underestimate the likelihood of such \u201cdisjunctive\u201d events, because they can result from many different paths (Tversky and Kahneman 1974). We suspect the considerations in this paper may convince you, as they did us, that this particular disjunctive event (intelligence explosion) is worthy of consideration.</p>\n<p>First, we provide evidence which suggests that, barring global catastrophe and other disruptions to scientific progress, there is a significant probability we will see the creation of digital intelligence within a century. Second, we suggest that the arrival of digital intelligence is likely to lead rather quickly to intelligence explosion. Finally, we discuss the possible consequences of an intelligence explosion and which actions we can take now to influence those results.</p>\n<p>These questions are complicated, the future is uncertain, and our chapter is brief. Our aim, then, can only be to provide a quick survey of the issues involved. We believe these matters are important, and our discussion of them must be permitted to begin at a low level because there is no other place to lay the first stones.</p>\n<p>&nbsp;</p>\n<h4 id=\"References_for_this_snippet\">References for this snippet</h4>\n<ul>\n<li>Bainbridge 2006 managing nano-bio-info-cogno innovations</li>\n<li>Baum Goertzel Goertzel 2011 how long until human-level ai</li>\n<li>Bostrom 2003 ethical issues in advanced artificial intelligence</li>\n<li>Chalmers 2010 singularity philosophical analysis</li>\n<li>Legg 2008 machine super intelligence</li>\n<li>Sandberg &amp; Bostrom 2011 machine intelligence survey</li>\n<li>Turing 1950 machine intelligence</li>\n<li>Good 1965 speculations concerning...</li>\n<li>Von neumann 1966 theory of self-reproducing autonomata</li>\n<li>Solomonoff 1985 the time scale of artificial intelligence</li>\n<li>Vinge 1993 coming technological singularity</li>\n<li>Yudkowsky 2001 creating friendly ai</li>\n<li>Yudkowsky 2008a negative and positive factor in global risk</li>\n<li>Russel Norvig 2010 artificial intelligence a modern approach 3e</li>\n<li>Nordman 2007 If and then: a critique of speculative nanoethics</li>\n<li>Moore and Healy the trouble with overconfidence</li>\n<li>Tversky Kahneman 2002 extensional versus intuitive reasoning, the conjunction fallacy</li>\n<li>Nickerson 1998 Confirmation Bias; A Ubiquitous Phenomenon in Many Guises</li>\n<li>Dreyfus 1972 what computers can't do</li>\n<li>Rhodes 1995 making of the atomic bomb</li>\n<li>Arrhenius 1896 On the Influence of Carbonic Acid in the Air Upon the Temperature</li>\n<li>Crawford 1997 &nbsp;Arrhenius' 1896 model of the greenhouse effect in context</li>\n<li>Rasmussen 1975 WASH-1400 report</li>\n<li>McGrayne 2011 theory that would not die</li>\n<li>Larrick 2004 debiasing</li>\n<li>Lillienfeld, Ammirati, and Landfield 2009 giving debiasing away</li>\n<li>Tversky and Kahneman 1974 Judgment under uncertainty: Heuristics and biases</li>\n</ul>", "sections": [{"title": "Singularity Skepticism", "anchor": "Singularity_Skepticism", "level": 1}, {"title": "References for this snippet", "anchor": "References_for_this_snippet", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T11:35:40.518Z", "modifiedAt": null, "url": null, "title": "Is an Intelligence Explosion a Disjunctive or Conjunctive Event?", "slug": "is-an-intelligence-explosion-a-disjunctive-or-conjunctive", "viewCount": null, "lastCommentedAt": "2021-03-04T13:37:51.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j52uErqofDiJZCo76/is-an-intelligence-explosion-a-disjunctive-or-conjunctive", "pageUrlRelative": "/posts/j52uErqofDiJZCo76/is-an-intelligence-explosion-a-disjunctive-or-conjunctive", "linkUrl": "https://www.lesswrong.com/posts/j52uErqofDiJZCo76/is-an-intelligence-explosion-a-disjunctive-or-conjunctive", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20an%20Intelligence%20Explosion%20a%20Disjunctive%20or%20Conjunctive%20Event%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20an%20Intelligence%20Explosion%20a%20Disjunctive%20or%20Conjunctive%20Event%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj52uErqofDiJZCo76%2Fis-an-intelligence-explosion-a-disjunctive-or-conjunctive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20an%20Intelligence%20Explosion%20a%20Disjunctive%20or%20Conjunctive%20Event%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj52uErqofDiJZCo76%2Fis-an-intelligence-explosion-a-disjunctive-or-conjunctive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj52uErqofDiJZCo76%2Fis-an-intelligence-explosion-a-disjunctive-or-conjunctive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3515, "htmlBody": "<p><em>(The following is a summary of some of my previous submissions that I originally created for my personal blog.) </em></p>\n<blockquote>\n<p>...an intelligence explosion may have fair probability, not because it occurs in one particular detailed scenario, but because, like the evolution of eyes or the emergence of markets, it can come about through many different paths and can gather momentum once it gets started. Humans tend to underestimate the likelihood of such &ldquo;disjunctive&rdquo; events, because they can result from many different paths (Tversky and Kahneman 1974). We suspect the considerations in this paper may convince you, as they did us, that this particular disjunctive event (intelligence explosion) is worthy of consideration.</p>\n</blockquote>\n<p>&mdash; lukeprog, <a href=\"/r/discussion/lw/8f9/intelligence_explosion_analysis_draft_introduction/\">Intelligence Explosion analysis draft: introduction</a></p>\n<blockquote>\n<p>It seems to me that all the ways in which we disagree have more to do with philosophy (how to quantify uncertainty; how to deal with conjunctions; how to act in consideration of low probabilities) [...] we are not dealing with well-defined or -quantified probabilities. Any prediction can be rephrased so that it sounds like the product of indefinitely many conjunctions. It seems that I see the &ldquo;SIAI&rsquo;s work is useful scenario&rdquo; as requiring the conjunction of a large number of questionable things [...]</p>\n</blockquote>\n<p>&mdash; Holden Karnofsky, 6/24/11 (<a title=\"GiveWell interview with major SIAI donor Jaan Tallinn\" href=\"/r/discussion/lw/6qo/givewell_interview_with_major_siai_donor_jaan\">GiveWell interview with major SIAI donor Jaan Tallinn</a>, <a href=\"http://kruel.co/JaanTallinn201105.pdf\">PDF</a>)</p>\n<h3>Disjunctive arguments</h3>\n<p>People associated with the&nbsp;<a title=\"SIAI\" href=\"http://intelligence.org/\">Singularity Institute for Artificial Intelligence</a> (SIAI) like to claim that the case for <a title=\"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">risks from AI</a> is supported by years worth of <a href=\"/lw/wm/disjunctions_antipredictions_etc\">disjunctive lines of reasoning</a>. This basically means that there are many reasons to believe that humanity is likely to be wiped out as a result of artificial general intelligence. More precisely it means that not all of the arguments supporting that possibility need to be true, even if all but one are false <em>risks from AI</em> are to be taken seriously.</p>\n<p>The idea of disjunctive arguments is formalized by what is called a <a title=\"Logical disjunction\" href=\"http://en.wikipedia.org/wiki/Logical_disjunction\">logical disjunction</a>. Consider two declarative sentences, A and B. The truth of the conclusion (or output) that follows from the sentences A and B does depend on the truth of A and B. In the case of a logical disjunction the conclusion of A and B is only false if both A <em>and</em> B are false, otherwise it is true. Truth values are usually denoted by 0 for false and 1 for true. A disjunction of declarative sentences is denoted by OR or &or; as an <a title=\"Infix notation\" href=\"http://en.wikipedia.org/wiki/Infix_notation\">infix operator</a>. For example, (A(0)&or;B(1))(1), or in other words, if statement A is false and B is true then what follows is still true because statement B is sufficient to preserve the truth of the overall conclusion.</p>\n<p>Generally there is no problem with <em>disjunctive lines of reasoning</em> as long as the conclusion itself is sound and therefore in principle possible, yet in demand of at least one of several causative factors to become actual. I don&rsquo;t perceive this to be the case for <em>risks from AI</em>. I agree that there are many ways in which artificial general intelligence (AGI) could be dangerous, but only if I accept several presuppositions regarding AGI <a href=\"/r/discussion/lw/8fb/why_an_intelligence_explosion_might_be_a/\">that I actually dispute</a>.</p>\n<p>By <em>presuppositions</em> I mean requirements that need to be true simultaneously (<em>in&nbsp;conjunction)</em>. A <a title=\"Logical conjunction\" href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">logical conjunction</a> is only true if all of its operands are true. In other words, the a conclusion might require all of the arguments leading up to it to be true, otherwise it is false. A conjunction is denoted by AND or &and;.</p>\n<p>Now consider the following prediction: &lt;Mary is going to buy one of thousands of products in the supermarket.&gt;</p>\n<p>The above prediction can be framed as a disjunction:&nbsp;Mary is going to buy one of thousands of products in the supermarket, 1.) if she is hungry 2.) if she is thirsty 3.) if she needs a new coffee machine. Only one of the 3 given possible arguments need to be true in order to leave the overall conclusion to be true, that Mary is going shopping. Or so it seems.</p>\n<p>The same prediction can be framed as a conjunction: Mary is going to buy one of thousands of products in the supermarket 1.) if she has money 2.) if she has some needs 3.) if the supermarket is open. All of the 3 given factors need to be true in order to render the overall conclusion to be true.</p>\n<p>That a prediction is framed to be disjunctive does not speak in favor of the possibility in and of itself. I agree that it is likely that Mary is going to visit the supermarket <em>if</em> I accept the hidden presuppositions. But a prediction is only at most as probable as its basic requirements. In this particular case I don&rsquo;t even know if Mary is a human or a dog, a factor that can influence the probability of the prediction dramatically.</p>\n<p>The same is true for <em>risks from AI</em>. The basic argument in favor of <em>risks from AI</em> is that of an <a title=\"Intelligence Explosion\" href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>, that intelligence can be applied to itself in an iterative process leading to ever greater levels of intelligence. In short, artificial general intelligence will undergo <a title=\"The Hanson-Yudkowsky AI-Foom Debate\" href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\"><em>explosive recursive self-improvement</em></a>.</p>\n<h3>Hidden complexity</h3>\n<p><em>Explosive recursive self-improvement</em> is one of the presuppositions for the possibility of <em>risks from AI</em>. The problem is that this and other presuppositions are largely ignored and left undefined. All of the disjunctive arguments put forth by the SIAI are trying to show that there are many causative factors that will result in the development of <a title=\"unfriendly AI\" href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\"><em>unfriendly</em></a> artificial general intelligence. Only one of those factors needs to be true for us to be wiped out by AGI. But the whole scenario is at most as probable as the assumption hidden in the words &lt;artificial general intelligence&gt; and &lt;explosive recursive self-improvement&gt;.</p>\n<p>&lt;Artificial General Intelligence&gt; and &lt;Explosive Recursive Self-improvement&gt; might appear to be relatively simple and appealing concepts. But most of this superficial simplicity is a result of the vagueness of natural language descriptions. <a title=\"The &ldquo;no self-defeating object&rdquo; argument, and the vagueness paradox\" href=\"http://terrytao.wordpress.com/2010/11/02/the-no-self-defeating-object-argument-and-the-vagueness-paradox/\">Reducing the vagueness</a> of those concepts by being more specific, or by coming up with <a title=\"The Advantages of Being Technical\" href=\"/r/discussion/lw/5rx/the_advantages_of_being_technical\">technical definitions</a> of each of the words they are made up of, reveals the <a title=\"(Algorithmic) Information Theory\" href=\"/lw/2un/references_resources_for_lesswrong/#AIT\">hidden complexity</a> that is comprised in the vagueness of the terms.</p>\n<p>If we were going to define those concepts and each of its terms we would end up with a lot of additional concepts made up of other words or terms. Most of those additional concepts will demand explanations of their own made up of further speculations. If we are precise then any declarative sentence&nbsp;(P#) (all of the terms) used in the final description will have to be true simultaneously (P#&and;P#). And this does reveal the true complexity of all hidden presuppositions and thereby influence the overall probability, P(risks from AI) = P(P1&and;P2&and;P3&and;P4&and;P5&and;P6&and;&hellip;). That is because the conclusion of an argument that is made up of a lot of statements (terms) that can be false is more unlikely to be true since complex arguments can fail in a lot of different ways. You need to support each part of the argument that can be true or false and you can therefore fail to support one or more of its parts, which in turn will render the overall conclusion false.</p>\n<p>To summarize: If we tried to pin down a concept like &lt;Explosive Recursive Self-Improvement&gt; we would end up with requirements that are strongly conjunctive.</p>\n<h3>Making numerical probability estimates</h3>\n<p>But even if the SIAI was going to thoroughly define those concepts, there is still more to the probability of risks from AI than the underlying presuppositions and causative factors. We also have to integrate our uncertainty about the very methods we used to come up with those concepts, definitions and our ability to make correct predictions about the future and integrate all of it into our overall probability estimates.</p>\n<p>Take for example the following contrived quote:</p>\n<blockquote>\n<p>We have to take over the universe to save it by making the seed of an artificial general intelligence, that is undergoing explosive recursive self-improvement, extrapolate the coherent volition of humanity, while acausally trading with other superhuman intelligences across the multiverse.</p>\n</blockquote>\n<p>Although contrived, the above quote does only comprise actual <a href=\"/lw/5u4/what_makes_less_wrong_awesome/48g0\">beliefs hold by people associated with the SIAI</a>. All of those beliefs might seem somewhat plausible inferences and logical implications of speculations and state of the art or <a title=\"Bleeding edge\" href=\"http://www.answers.com/topic/bleeding-edge\">bleeding edge knowledge</a> of various fields. But should we base <a title=\"Real life\" href=\"http://en.wikipedia.org/wiki/Real_life\">real-life</a> decisions on those ideas, should we take those <em>ideas</em> seriously? Should we take into account conclusions whose truth value does depend on the conjunction of those ideas? And is it wise to make further inferences on those speculations?</p>\n<p>Let&rsquo;s take a closer look at the necessary top-level presuppositions to take the above quote seriously:</p>\n<ol>\n<li><a title=\"Many-Worlds Interpretation\" href=\"/lw/r8/and_the_winner_is_manyworlds\">The many-worlds interpretation</a></li>\n<li><a href=\"/lw/pb/belief_in_the_implied_invisible\">Belief in the Implied Invisible</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">Timeless Decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">Intelligence explosion</a></li>\n</ol>\n<p>1: Within the <a title=\"Less Wrong\" href=\"http://lesswrong.com\">lesswrong</a>/SIAI community the many-worlds interpretation of quantum mechanics is proclaimed to be the rational choice of all available interpretations. How to arrive at this conclusion is supposedly also a good exercise in refining the art of rationality.</p>\n<p>2: P(Y|X) &asymp; 1, then P(X&and;Y) &asymp; P(X)</p>\n<p>In other words, logical implications do not have to pay rent in future anticipations.</p>\n<p>3: <em>&ldquo;Decision theory is the study of principles and algorithms for making correct decisions&mdash;that is, decisions that allow an agent to achieve better outcomes with respect to its goals.&rdquo;</em></p>\n<p>4: <em>&ldquo;Intelligence explosion is the idea of a positive feedback loop in which an intelligence is making itself smarter, thus getting better at making itself even smarter. A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a dramatic leap in capability very quickly.&rdquo;</em></p>\n<p>To be able to take the above quote seriously you have to assign a non-negligible probability to the truth of the conjunction of #1,2,3,4, 1&and;2&and;3&and;4. Here the question is not not only if our results are sound but if the very methods we used to come up with those results are sufficiently trustworthy. Because any extraordinary conclusions that are implied by the conjunction of various beliefs might outweigh the benefit of each belief if the overall conclusion is just slightly wrong.</p>\n<h3>Not enough empirical evidence</h3>\n<p>Don&rsquo;t get me wrong, I think that there sure are convincing arguments in favor of risks from AI. But do arguments suffice? Nobody is an expert when it comes to intelligence. My problem is that I fear that some convincing blog posts written in natural language are simply not enough.</p>\n<p>Just imagine that all there was to climate change was someone who never studied the climate but instead wrote some essays about how it might be physical possible for humans to cause a global warming. If the same person then goes on to make further inferences based on the implications of those speculations, am I going to tell everyone to stop emitting CO2 because of <em>that</em>? Hardly!</p>\n<p>Or imagine that all there was to the possibility of asteroid strikes was someone who argued that there <em>might</em> be big chunks of rocks out there which <em>might </em>fall down on our heads and kill us all, inductively based on the fact that the Earth and the moon are also a big rocks. Would I be willing to launch a billion dollar asteroid deflection program solely based on such speculations? I don&rsquo;t think so.</p>\n<p>Luckily, in both cases, we got a lot more than some convincing arguments in support of those risks.</p>\n<p><em>Another example:</em> If there were no studies about the safety of high energy physics experiments then I might assign a 20% chance of a powerful particle accelerator destroying the universe based on some convincing arguments <a href=\"http://lifeboat.com/blog/2011/06/idiotic-blunder-behind-risked-planet\">put forth on a blog by someone who never studied high energy physics</a>. We know that such an estimate would be wrong by many orders of magnitude. Yet the reason for being wrong would largely be a result of my inability to make correct probability estimates, the result of vagueness or a failure of the methods I employed to come up with those estimates. The reason for being wrong by many orders of magnitude would have nothing to do with the arguments in favor of the risks, as they might very well be sound given my epistemic state and the prevalent uncertainty.</p>\n<p><em></em>I believe that mere arguments in favor of one risk do not suffice to neglect other risks that are supported by other kinds of evidence. I believe that logical implications of sound arguments should not reach out indefinitely and thereby outweigh other risks whose implications are fortified by empirical evidence. Sound arguments, predictions, speculations and their logical implications are enough to demand further attention and research, but not much more.</p>\n<h3>Logical implications</h3>\n<p>Artificial general intelligence is already an inference made from what we currently believe to be true, going a step further and drawing further inferences from previous speculations, e.g. explosive recursive self-improvement, is in my opinion a very shaky business.</p>\n<p>What would happen if we were going to let logical implications of vast utilities outweigh other concrete near-term problems that are based on empirical evidence? Insignificant inferences might exhibit hyperbolic growth in utility: 1.) There is no minimum amount of empirical evidence necessary to extrapolate the expected utility of an outcome. 2.) The extrapolation of counterfactual alternatives is unbounded, logical implications can reach out indefinitely without ever requiring new empirical evidence.</p>\n<h3>Hidden disagreement</h3>\n<p>All of the above hints at a general problem that is the reason for why I think that discussions between people associated with the SIAI, its critics and those who try to evaluate the SIAI, won&rsquo;t lead anywhere. Those discussions miss the underlying reason for most of the superficial disagreement about <em>risks from AI</em>, namely that there is no disagreement about <em>risks from AI</em> in and of itself.</p>\n<p>There are a few people who disagree about the possibility of AGI in general, but I don&rsquo;t want to touch on that subject in this post. I am trying to highlight the disagreement between the SIAI and people who accept the notion of artificial general intelligence. With regard to those who are not skeptical of AGI the problem becomes more obvious when you turn your attention to people like John Baez organisations like GiveWell. Most people would <em>sooner question their grasp of &ldquo;rationality&rdquo; than give five dollars to a charity that tries to mitigate risks from AI because their calculations claim it was &ldquo;rational&rdquo;</em> (those who have read the <a title=\"Pascal's Mugging\" href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">article by Eliezer Yudkowsky</a> on <em>&lsquo;<a href=\"/lw/5qi/pascals_mugging_penalizing_the_prior_probability\">Pascal&rsquo;s Mugging</a>&lsquo;</em> know that I used a statement from that post and slightly rephrased it). The disagreement all comes down to a general averseness to options that have a low probability of being factual, even given that the stakes are high.</p>\n<p>Nobody is so far able to beat arguments that bear resemblance to Pascal&rsquo;s Mugging. At least not by showing that it is irrational to give in from the perspective of a utility maximizer. One can only reject it based on a strong gut feeling that something is wrong. And I think that is what many people are unknowingly doing when they argue against the SIAI or <em>risks from AI</em>. They are signaling that they are unable to take such risks into account. What most people mean when they doubt the reputation of people who claim that <em>risks from AI</em> need to be taken seriously, or who say that AGI might be far off, what those people mean is that <em>risks from AI</em> are too vague to be taken into account at this point, that nobody knows enough to make predictions about the topic right now.</p>\n<p>When GiveWell, a charity evaluation service, interviewed the SIAI (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/siai-2011-02-III.pdf\">PDF</a>), they hinted at the possibility that one could consider the SIAI to be a sort of Pascal&rsquo;s Mugging:</p>\n<blockquote>\n<p><strong>GiveWell:</strong> OK. Well that&rsquo;s where I stand &ndash; I accept a lot of the controversial premises of your mission, but I&rsquo;m a pretty long way from sold that you have the right team or the right approach. Now some have argued to me that I don&rsquo;t need to be sold &ndash; that even at an infinitesimal probability of success, your project is worthwhile. I see that as a Pascal&rsquo;s Mugging and don&rsquo;t accept it; I wouldn&rsquo;t endorse your project unless it passed the basic hurdles of credibility and workable approach as well as potentially astronomically beneficial goal.</p>\n</blockquote>\n<p>This shows that lot of people do not doubt the possibility of <em>risks from AI</em> but are simply not sure if they should really concentrate their efforts on such vague possibilities.</p>\n<p>Technically, from the standpoint of maximizing expected utility, given the absence of other existential risks, the answer might very well be yes. But even though we believe to understand this technical viewpoint of rationality very well in principle, it does also lead to problems such as Pascal&rsquo;s Mugging. But <a href=\"http://www.overcomingbias.com/2011/07/ignoring-small-chances.html\">it doesn&rsquo;t take a true Pascal&rsquo;s Mugging scenario</a> to make people feel deeply uncomfortable with what <a href=\"/lw/2un/references_resources_for_lesswrong/#Probability\">Bayes&rsquo; Theorem</a>, the <a title=\"Expected utility hypothesis\" href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">expected utility</a> formula, and <a title=\"Solomonoff Induction\" href=\"http://www.wisegeek.com/what-is-solomonoff-induction.htm\">Solomonoff induction</a> seem to suggest one should do.</p>\n<p>Again, we currently have no rational way to reject arguments that are framed as predictions of worst case scenarios that need to be taken seriously even given a low probability of their occurrence due to the scale of negative consequences associated with them. Many people are nonetheless reluctant to accept this line of reasoning without further evidence supporting the strong claims and request for money made by organisations such as the SIAI.</p>\n<p>Here is what mathematician and climate activist <a href=\"http://johncarlosbaez.wordpress.com/2011/02/28/this-weeks-finds-week-310/#comment-4496\">John Baez</a> has to say:</p>\n<blockquote>\n<p>Of course, anyone associated with Less Wrong would ask if I&rsquo;m really maximizing expected utility. Couldn&rsquo;t a contribution to some place like the Singularity Institute of Artificial Intelligence, despite a lower chance of doing good, actually have a chance to do so much more good that it&rsquo;d pay to send the cash there instead?</p>\n<p>And I&rsquo;d have to say:</p>\n<p>1) Yes, there probably are such places, but it would take me a while to find the one that I trusted, and I haven&rsquo;t put in the work. When you&rsquo;re risk-averse and limited in the time you have to make decisions, you tend to put off weighing options that have a very low chance of success but a very high return if they succeed. This is sensible so I don&rsquo;t feel bad about it.</p>\n<p>2) Just to amplify point 1) a bit: you shouldn&rsquo;t always maximize expected utility if you only live once. Expected values &mdash; in other words, averages &mdash; are very important when you make the same small bet over and over again. When the stakes get higher and you aren&rsquo;t in a position to repeat the bet over and over, it may be wise to be risk averse.</p>\n<p>3) If you let me put the $100,000 into my retirement account instead of a charity, that&rsquo;s what I&rsquo;d do, and I wouldn&rsquo;t even feel guilty about it. I actually think that the increased security would free me up to do more risky but potentially very good things!</p>\n</blockquote>\n<p>All this shows that there seems to be a fundamental problem with the formalized version of rationality. The problem might be human nature itself, that some people are unable to accept what they should do if they want to maximize their expected utility. Or we are missing something else and <a href=\"http://kruel.co/2011/07/22/objections-to-coherent-extrapolated-volition/\">our theories are flawed</a>. Either way, to solve this problem we need to research those issues and thereby increase the confidence in the very methods used to decide what to do about <em>risks from AI</em>, or to <a href=\"/lw/6ct/siais_shortterm_research_program\">increase the confidence in <em>risks from AI</em> directly</a>, enough to make it look like a sensible option, a concrete and discernable problem that needs to be solved.</p>\n<p>Many people perceive the whole world to be at stake, either due to climate change, war or engineered pathogens. Telling them about something like <em>risks from AI</em>, even though nobody seems to have any idea about the nature of intelligence, let alone general intelligence or the possibility of recursive self-improvement, seems like just another problem, one that is too vague to outweigh all the other risks. Most people feel like having a gun pointed to their heads, telling them about superhuman monsters that might turn them into paperclips then needs some really good arguments to outweigh the combined risk of all other problems.</p>\n<p>But there are many other problems with <em>risks from AI</em>. To give a hint at just one example: if there was a risk that might kill us with a probability of .7 and another risk with .1 while our chance to solve the first one was .0001 and the second one .1, which one should we focus on? In other words, our decision to mitigate a certain risk should not only be focused on the probability of its occurence but also on the probability of success in solving it. But as I have written above I believe that the most pressing issue is to increase the confidence into making decisions under extreme uncertainty or to reduce the uncerainty itself.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j52uErqofDiJZCo76", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 9, "extendedScore": null, "score": 3.9e-05, "legacy": true, "legacyId": "10918", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(The following is a summary of some of my previous submissions that I originally created for my personal blog.) </em></p>\n<blockquote>\n<p>...an intelligence explosion may have fair probability, not because it occurs in one particular detailed scenario, but because, like the evolution of eyes or the emergence of markets, it can come about through many different paths and can gather momentum once it gets started. Humans tend to underestimate the likelihood of such \u201cdisjunctive\u201d events, because they can result from many different paths (Tversky and Kahneman 1974). We suspect the considerations in this paper may convince you, as they did us, that this particular disjunctive event (intelligence explosion) is worthy of consideration.</p>\n</blockquote>\n<p>\u2014 lukeprog, <a href=\"/r/discussion/lw/8f9/intelligence_explosion_analysis_draft_introduction/\">Intelligence Explosion analysis draft: introduction</a></p>\n<blockquote>\n<p>It seems to me that all the ways in which we disagree have more to do with philosophy (how to quantify uncertainty; how to deal with conjunctions; how to act in consideration of low probabilities) [...] we are not dealing with well-defined or -quantified probabilities. Any prediction can be rephrased so that it sounds like the product of indefinitely many conjunctions. It seems that I see the \u201cSIAI\u2019s work is useful scenario\u201d as requiring the conjunction of a large number of questionable things [...]</p>\n</blockquote>\n<p>\u2014 Holden Karnofsky, 6/24/11 (<a title=\"GiveWell interview with major SIAI donor Jaan Tallinn\" href=\"/r/discussion/lw/6qo/givewell_interview_with_major_siai_donor_jaan\">GiveWell interview with major SIAI donor Jaan Tallinn</a>, <a href=\"http://kruel.co/JaanTallinn201105.pdf\">PDF</a>)</p>\n<h3 id=\"Disjunctive_arguments\">Disjunctive arguments</h3>\n<p>People associated with the&nbsp;<a title=\"SIAI\" href=\"http://intelligence.org/\">Singularity Institute for Artificial Intelligence</a> (SIAI) like to claim that the case for <a title=\"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">risks from AI</a> is supported by years worth of <a href=\"/lw/wm/disjunctions_antipredictions_etc\">disjunctive lines of reasoning</a>. This basically means that there are many reasons to believe that humanity is likely to be wiped out as a result of artificial general intelligence. More precisely it means that not all of the arguments supporting that possibility need to be true, even if all but one are false <em>risks from AI</em> are to be taken seriously.</p>\n<p>The idea of disjunctive arguments is formalized by what is called a <a title=\"Logical disjunction\" href=\"http://en.wikipedia.org/wiki/Logical_disjunction\">logical disjunction</a>. Consider two declarative sentences, A and B. The truth of the conclusion (or output) that follows from the sentences A and B does depend on the truth of A and B. In the case of a logical disjunction the conclusion of A and B is only false if both A <em>and</em> B are false, otherwise it is true. Truth values are usually denoted by 0 for false and 1 for true. A disjunction of declarative sentences is denoted by OR or \u2228 as an <a title=\"Infix notation\" href=\"http://en.wikipedia.org/wiki/Infix_notation\">infix operator</a>. For example, (A(0)\u2228B(1))(1), or in other words, if statement A is false and B is true then what follows is still true because statement B is sufficient to preserve the truth of the overall conclusion.</p>\n<p>Generally there is no problem with <em>disjunctive lines of reasoning</em> as long as the conclusion itself is sound and therefore in principle possible, yet in demand of at least one of several causative factors to become actual. I don\u2019t perceive this to be the case for <em>risks from AI</em>. I agree that there are many ways in which artificial general intelligence (AGI) could be dangerous, but only if I accept several presuppositions regarding AGI <a href=\"/r/discussion/lw/8fb/why_an_intelligence_explosion_might_be_a/\">that I actually dispute</a>.</p>\n<p>By <em>presuppositions</em> I mean requirements that need to be true simultaneously (<em>in&nbsp;conjunction)</em>. A <a title=\"Logical conjunction\" href=\"http://en.wikipedia.org/wiki/Logical_conjunction\">logical conjunction</a> is only true if all of its operands are true. In other words, the a conclusion might require all of the arguments leading up to it to be true, otherwise it is false. A conjunction is denoted by AND or \u2227.</p>\n<p>Now consider the following prediction: &lt;Mary is going to buy one of thousands of products in the supermarket.&gt;</p>\n<p>The above prediction can be framed as a disjunction:&nbsp;Mary is going to buy one of thousands of products in the supermarket, 1.) if she is hungry 2.) if she is thirsty 3.) if she needs a new coffee machine. Only one of the 3 given possible arguments need to be true in order to leave the overall conclusion to be true, that Mary is going shopping. Or so it seems.</p>\n<p>The same prediction can be framed as a conjunction: Mary is going to buy one of thousands of products in the supermarket 1.) if she has money 2.) if she has some needs 3.) if the supermarket is open. All of the 3 given factors need to be true in order to render the overall conclusion to be true.</p>\n<p>That a prediction is framed to be disjunctive does not speak in favor of the possibility in and of itself. I agree that it is likely that Mary is going to visit the supermarket <em>if</em> I accept the hidden presuppositions. But a prediction is only at most as probable as its basic requirements. In this particular case I don\u2019t even know if Mary is a human or a dog, a factor that can influence the probability of the prediction dramatically.</p>\n<p>The same is true for <em>risks from AI</em>. The basic argument in favor of <em>risks from AI</em> is that of an <a title=\"Intelligence Explosion\" href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>, that intelligence can be applied to itself in an iterative process leading to ever greater levels of intelligence. In short, artificial general intelligence will undergo <a title=\"The Hanson-Yudkowsky AI-Foom Debate\" href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\"><em>explosive recursive self-improvement</em></a>.</p>\n<h3 id=\"Hidden_complexity\">Hidden complexity</h3>\n<p><em>Explosive recursive self-improvement</em> is one of the presuppositions for the possibility of <em>risks from AI</em>. The problem is that this and other presuppositions are largely ignored and left undefined. All of the disjunctive arguments put forth by the SIAI are trying to show that there are many causative factors that will result in the development of <a title=\"unfriendly AI\" href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\"><em>unfriendly</em></a> artificial general intelligence. Only one of those factors needs to be true for us to be wiped out by AGI. But the whole scenario is at most as probable as the assumption hidden in the words &lt;artificial general intelligence&gt; and &lt;explosive recursive self-improvement&gt;.</p>\n<p>&lt;Artificial General Intelligence&gt; and &lt;Explosive Recursive Self-improvement&gt; might appear to be relatively simple and appealing concepts. But most of this superficial simplicity is a result of the vagueness of natural language descriptions. <a title=\"The \u201cno self-defeating object\u201d argument, and the vagueness paradox\" href=\"http://terrytao.wordpress.com/2010/11/02/the-no-self-defeating-object-argument-and-the-vagueness-paradox/\">Reducing the vagueness</a> of those concepts by being more specific, or by coming up with <a title=\"The Advantages of Being Technical\" href=\"/r/discussion/lw/5rx/the_advantages_of_being_technical\">technical definitions</a> of each of the words they are made up of, reveals the <a title=\"(Algorithmic) Information Theory\" href=\"/lw/2un/references_resources_for_lesswrong/#AIT\">hidden complexity</a> that is comprised in the vagueness of the terms.</p>\n<p>If we were going to define those concepts and each of its terms we would end up with a lot of additional concepts made up of other words or terms. Most of those additional concepts will demand explanations of their own made up of further speculations. If we are precise then any declarative sentence&nbsp;(P#) (all of the terms) used in the final description will have to be true simultaneously (P#\u2227P#). And this does reveal the true complexity of all hidden presuppositions and thereby influence the overall probability, P(risks from AI) = P(P1\u2227P2\u2227P3\u2227P4\u2227P5\u2227P6\u2227\u2026). That is because the conclusion of an argument that is made up of a lot of statements (terms) that can be false is more unlikely to be true since complex arguments can fail in a lot of different ways. You need to support each part of the argument that can be true or false and you can therefore fail to support one or more of its parts, which in turn will render the overall conclusion false.</p>\n<p>To summarize: If we tried to pin down a concept like &lt;Explosive Recursive Self-Improvement&gt; we would end up with requirements that are strongly conjunctive.</p>\n<h3 id=\"Making_numerical_probability_estimates\">Making numerical probability estimates</h3>\n<p>But even if the SIAI was going to thoroughly define those concepts, there is still more to the probability of risks from AI than the underlying presuppositions and causative factors. We also have to integrate our uncertainty about the very methods we used to come up with those concepts, definitions and our ability to make correct predictions about the future and integrate all of it into our overall probability estimates.</p>\n<p>Take for example the following contrived quote:</p>\n<blockquote>\n<p>We have to take over the universe to save it by making the seed of an artificial general intelligence, that is undergoing explosive recursive self-improvement, extrapolate the coherent volition of humanity, while acausally trading with other superhuman intelligences across the multiverse.</p>\n</blockquote>\n<p>Although contrived, the above quote does only comprise actual <a href=\"/lw/5u4/what_makes_less_wrong_awesome/48g0\">beliefs hold by people associated with the SIAI</a>. All of those beliefs might seem somewhat plausible inferences and logical implications of speculations and state of the art or <a title=\"Bleeding edge\" href=\"http://www.answers.com/topic/bleeding-edge\">bleeding edge knowledge</a> of various fields. But should we base <a title=\"Real life\" href=\"http://en.wikipedia.org/wiki/Real_life\">real-life</a> decisions on those ideas, should we take those <em>ideas</em> seriously? Should we take into account conclusions whose truth value does depend on the conjunction of those ideas? And is it wise to make further inferences on those speculations?</p>\n<p>Let\u2019s take a closer look at the necessary top-level presuppositions to take the above quote seriously:</p>\n<ol>\n<li><a title=\"Many-Worlds Interpretation\" href=\"/lw/r8/and_the_winner_is_manyworlds\">The many-worlds interpretation</a></li>\n<li><a href=\"/lw/pb/belief_in_the_implied_invisible\">Belief in the Implied Invisible</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">Timeless Decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">Intelligence explosion</a></li>\n</ol>\n<p>1: Within the <a title=\"Less Wrong\" href=\"http://lesswrong.com\">lesswrong</a>/SIAI community the many-worlds interpretation of quantum mechanics is proclaimed to be the rational choice of all available interpretations. How to arrive at this conclusion is supposedly also a good exercise in refining the art of rationality.</p>\n<p>2: P(Y|X) \u2248 1, then P(X\u2227Y) \u2248 P(X)</p>\n<p>In other words, logical implications do not have to pay rent in future anticipations.</p>\n<p>3: <em>\u201cDecision theory is the study of principles and algorithms for making correct decisions\u2014that is, decisions that allow an agent to achieve better outcomes with respect to its goals.\u201d</em></p>\n<p>4: <em>\u201cIntelligence explosion is the idea of a positive feedback loop in which an intelligence is making itself smarter, thus getting better at making itself even smarter. A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a dramatic leap in capability very quickly.\u201d</em></p>\n<p>To be able to take the above quote seriously you have to assign a non-negligible probability to the truth of the conjunction of #1,2,3,4, 1\u22272\u22273\u22274. Here the question is not not only if our results are sound but if the very methods we used to come up with those results are sufficiently trustworthy. Because any extraordinary conclusions that are implied by the conjunction of various beliefs might outweigh the benefit of each belief if the overall conclusion is just slightly wrong.</p>\n<h3 id=\"Not_enough_empirical_evidence\">Not enough empirical evidence</h3>\n<p>Don\u2019t get me wrong, I think that there sure are convincing arguments in favor of risks from AI. But do arguments suffice? Nobody is an expert when it comes to intelligence. My problem is that I fear that some convincing blog posts written in natural language are simply not enough.</p>\n<p>Just imagine that all there was to climate change was someone who never studied the climate but instead wrote some essays about how it might be physical possible for humans to cause a global warming. If the same person then goes on to make further inferences based on the implications of those speculations, am I going to tell everyone to stop emitting CO2 because of <em>that</em>? Hardly!</p>\n<p>Or imagine that all there was to the possibility of asteroid strikes was someone who argued that there <em>might</em> be big chunks of rocks out there which <em>might </em>fall down on our heads and kill us all, inductively based on the fact that the Earth and the moon are also a big rocks. Would I be willing to launch a billion dollar asteroid deflection program solely based on such speculations? I don\u2019t think so.</p>\n<p>Luckily, in both cases, we got a lot more than some convincing arguments in support of those risks.</p>\n<p><em>Another example:</em> If there were no studies about the safety of high energy physics experiments then I might assign a 20% chance of a powerful particle accelerator destroying the universe based on some convincing arguments <a href=\"http://lifeboat.com/blog/2011/06/idiotic-blunder-behind-risked-planet\">put forth on a blog by someone who never studied high energy physics</a>. We know that such an estimate would be wrong by many orders of magnitude. Yet the reason for being wrong would largely be a result of my inability to make correct probability estimates, the result of vagueness or a failure of the methods I employed to come up with those estimates. The reason for being wrong by many orders of magnitude would have nothing to do with the arguments in favor of the risks, as they might very well be sound given my epistemic state and the prevalent uncertainty.</p>\n<p><em></em>I believe that mere arguments in favor of one risk do not suffice to neglect other risks that are supported by other kinds of evidence. I believe that logical implications of sound arguments should not reach out indefinitely and thereby outweigh other risks whose implications are fortified by empirical evidence. Sound arguments, predictions, speculations and their logical implications are enough to demand further attention and research, but not much more.</p>\n<h3 id=\"Logical_implications\">Logical implications</h3>\n<p>Artificial general intelligence is already an inference made from what we currently believe to be true, going a step further and drawing further inferences from previous speculations, e.g. explosive recursive self-improvement, is in my opinion a very shaky business.</p>\n<p>What would happen if we were going to let logical implications of vast utilities outweigh other concrete near-term problems that are based on empirical evidence? Insignificant inferences might exhibit hyperbolic growth in utility: 1.) There is no minimum amount of empirical evidence necessary to extrapolate the expected utility of an outcome. 2.) The extrapolation of counterfactual alternatives is unbounded, logical implications can reach out indefinitely without ever requiring new empirical evidence.</p>\n<h3 id=\"Hidden_disagreement\">Hidden disagreement</h3>\n<p>All of the above hints at a general problem that is the reason for why I think that discussions between people associated with the SIAI, its critics and those who try to evaluate the SIAI, won\u2019t lead anywhere. Those discussions miss the underlying reason for most of the superficial disagreement about <em>risks from AI</em>, namely that there is no disagreement about <em>risks from AI</em> in and of itself.</p>\n<p>There are a few people who disagree about the possibility of AGI in general, but I don\u2019t want to touch on that subject in this post. I am trying to highlight the disagreement between the SIAI and people who accept the notion of artificial general intelligence. With regard to those who are not skeptical of AGI the problem becomes more obvious when you turn your attention to people like John Baez organisations like GiveWell. Most people would <em>sooner question their grasp of \u201crationality\u201d than give five dollars to a charity that tries to mitigate risks from AI because their calculations claim it was \u201crational\u201d</em> (those who have read the <a title=\"Pascal's Mugging\" href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\">article by Eliezer Yudkowsky</a> on <em>\u2018<a href=\"/lw/5qi/pascals_mugging_penalizing_the_prior_probability\">Pascal\u2019s Mugging</a>\u2018</em> know that I used a statement from that post and slightly rephrased it). The disagreement all comes down to a general averseness to options that have a low probability of being factual, even given that the stakes are high.</p>\n<p>Nobody is so far able to beat arguments that bear resemblance to Pascal\u2019s Mugging. At least not by showing that it is irrational to give in from the perspective of a utility maximizer. One can only reject it based on a strong gut feeling that something is wrong. And I think that is what many people are unknowingly doing when they argue against the SIAI or <em>risks from AI</em>. They are signaling that they are unable to take such risks into account. What most people mean when they doubt the reputation of people who claim that <em>risks from AI</em> need to be taken seriously, or who say that AGI might be far off, what those people mean is that <em>risks from AI</em> are too vague to be taken into account at this point, that nobody knows enough to make predictions about the topic right now.</p>\n<p>When GiveWell, a charity evaluation service, interviewed the SIAI (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/05/siai-2011-02-III.pdf\">PDF</a>), they hinted at the possibility that one could consider the SIAI to be a sort of Pascal\u2019s Mugging:</p>\n<blockquote>\n<p><strong>GiveWell:</strong> OK. Well that\u2019s where I stand \u2013 I accept a lot of the controversial premises of your mission, but I\u2019m a pretty long way from sold that you have the right team or the right approach. Now some have argued to me that I don\u2019t need to be sold \u2013 that even at an infinitesimal probability of success, your project is worthwhile. I see that as a Pascal\u2019s Mugging and don\u2019t accept it; I wouldn\u2019t endorse your project unless it passed the basic hurdles of credibility and workable approach as well as potentially astronomically beneficial goal.</p>\n</blockquote>\n<p>This shows that lot of people do not doubt the possibility of <em>risks from AI</em> but are simply not sure if they should really concentrate their efforts on such vague possibilities.</p>\n<p>Technically, from the standpoint of maximizing expected utility, given the absence of other existential risks, the answer might very well be yes. But even though we believe to understand this technical viewpoint of rationality very well in principle, it does also lead to problems such as Pascal\u2019s Mugging. But <a href=\"http://www.overcomingbias.com/2011/07/ignoring-small-chances.html\">it doesn\u2019t take a true Pascal\u2019s Mugging scenario</a> to make people feel deeply uncomfortable with what <a href=\"/lw/2un/references_resources_for_lesswrong/#Probability\">Bayes\u2019 Theorem</a>, the <a title=\"Expected utility hypothesis\" href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">expected utility</a> formula, and <a title=\"Solomonoff Induction\" href=\"http://www.wisegeek.com/what-is-solomonoff-induction.htm\">Solomonoff induction</a> seem to suggest one should do.</p>\n<p>Again, we currently have no rational way to reject arguments that are framed as predictions of worst case scenarios that need to be taken seriously even given a low probability of their occurrence due to the scale of negative consequences associated with them. Many people are nonetheless reluctant to accept this line of reasoning without further evidence supporting the strong claims and request for money made by organisations such as the SIAI.</p>\n<p>Here is what mathematician and climate activist <a href=\"http://johncarlosbaez.wordpress.com/2011/02/28/this-weeks-finds-week-310/#comment-4496\">John Baez</a> has to say:</p>\n<blockquote>\n<p>Of course, anyone associated with Less Wrong would ask if I\u2019m really maximizing expected utility. Couldn\u2019t a contribution to some place like the Singularity Institute of Artificial Intelligence, despite a lower chance of doing good, actually have a chance to do so much more good that it\u2019d pay to send the cash there instead?</p>\n<p>And I\u2019d have to say:</p>\n<p>1) Yes, there probably are such places, but it would take me a while to find the one that I trusted, and I haven\u2019t put in the work. When you\u2019re risk-averse and limited in the time you have to make decisions, you tend to put off weighing options that have a very low chance of success but a very high return if they succeed. This is sensible so I don\u2019t feel bad about it.</p>\n<p>2) Just to amplify point 1) a bit: you shouldn\u2019t always maximize expected utility if you only live once. Expected values \u2014 in other words, averages \u2014 are very important when you make the same small bet over and over again. When the stakes get higher and you aren\u2019t in a position to repeat the bet over and over, it may be wise to be risk averse.</p>\n<p>3) If you let me put the $100,000 into my retirement account instead of a charity, that\u2019s what I\u2019d do, and I wouldn\u2019t even feel guilty about it. I actually think that the increased security would free me up to do more risky but potentially very good things!</p>\n</blockquote>\n<p>All this shows that there seems to be a fundamental problem with the formalized version of rationality. The problem might be human nature itself, that some people are unable to accept what they should do if they want to maximize their expected utility. Or we are missing something else and <a href=\"http://kruel.co/2011/07/22/objections-to-coherent-extrapolated-volition/\">our theories are flawed</a>. Either way, to solve this problem we need to research those issues and thereby increase the confidence in the very methods used to decide what to do about <em>risks from AI</em>, or to <a href=\"/lw/6ct/siais_shortterm_research_program\">increase the confidence in <em>risks from AI</em> directly</a>, enough to make it look like a sensible option, a concrete and discernable problem that needs to be solved.</p>\n<p>Many people perceive the whole world to be at stake, either due to climate change, war or engineered pathogens. Telling them about something like <em>risks from AI</em>, even though nobody seems to have any idea about the nature of intelligence, let alone general intelligence or the possibility of recursive self-improvement, seems like just another problem, one that is too vague to outweigh all the other risks. Most people feel like having a gun pointed to their heads, telling them about superhuman monsters that might turn them into paperclips then needs some really good arguments to outweigh the combined risk of all other problems.</p>\n<p>But there are many other problems with <em>risks from AI</em>. To give a hint at just one example: if there was a risk that might kill us with a probability of .7 and another risk with .1 while our chance to solve the first one was .0001 and the second one .1, which one should we focus on? In other words, our decision to mitigate a certain risk should not only be focused on the probability of its occurence but also on the probability of success in solving it. But as I have written above I believe that the most pressing issue is to increase the confidence into making decisions under extreme uncertainty or to reduce the uncerainty itself.</p>", "sections": [{"title": "Disjunctive arguments", "anchor": "Disjunctive_arguments", "level": 1}, {"title": "Hidden complexity", "anchor": "Hidden_complexity", "level": 1}, {"title": "Making numerical probability estimates", "anchor": "Making_numerical_probability_estimates", "level": 1}, {"title": "Not enough empirical evidence", "anchor": "Not_enough_empirical_evidence", "level": 1}, {"title": "Logical implications", "anchor": "Logical_implications", "level": 1}, {"title": "Hidden disagreement", "anchor": "Hidden_disagreement", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dEkSus8QzcCyEKNa6", "GfTMxwd9L2mkahmq2", "yzzoWR33S9C3m75e8", "ZbgQ3HtPu2cLKGmy8", "Lc7c5yn6Xp2Zf3cy8", "9cgBF6BQ2TRB3Hy4E", "3XMwPNMSbaPm2suGz", "zdk5djtfFXSgfpeWi", "7AQ5SoJkN5hDGnhSw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T11:40:38.917Z", "modifiedAt": null, "url": null, "title": "Why an Intelligence Explosion might be a Low-Priority Global Risk", "slug": "why-an-intelligence-explosion-might-be-a-low-priority-global", "viewCount": null, "lastCommentedAt": "2021-03-04T13:04:26.141Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZbgQ3HtPu2cLKGmy8/why-an-intelligence-explosion-might-be-a-low-priority-global", "pageUrlRelative": "/posts/ZbgQ3HtPu2cLKGmy8/why-an-intelligence-explosion-might-be-a-low-priority-global", "linkUrl": "https://www.lesswrong.com/posts/ZbgQ3HtPu2cLKGmy8/why-an-intelligence-explosion-might-be-a-low-priority-global", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20an%20Intelligence%20Explosion%20might%20be%20a%20Low-Priority%20Global%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20an%20Intelligence%20Explosion%20might%20be%20a%20Low-Priority%20Global%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbgQ3HtPu2cLKGmy8%2Fwhy-an-intelligence-explosion-might-be-a-low-priority-global%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20an%20Intelligence%20Explosion%20might%20be%20a%20Low-Priority%20Global%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbgQ3HtPu2cLKGmy8%2Fwhy-an-intelligence-explosion-might-be-a-low-priority-global", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZbgQ3HtPu2cLKGmy8%2Fwhy-an-intelligence-explosion-might-be-a-low-priority-global", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3101, "htmlBody": "<p><em>(The following is a summary of some of my previous submissions that I originally created for my personal blog.) </em></p>\n<blockquote>\n<p>As we know,<br /> There are known knowns.<br /> There are things<br /> We know we know.<br /> We also know<br /> There are known unknowns.<br /> That is to say<br /> We know there are some things<br /> We do not know.<br /> But there are also unknown unknowns,<br /> The ones we don&rsquo;t know<br /> We don&rsquo;t know.</p>\n</blockquote>\n<p>&mdash; Donald Rumsfeld, Feb. 12, 2002, Department of Defense news briefing</p>\n<h3>Intelligence, a cornucopia?</h3>\n<p>It seems to me that those who believe into the possibility of catastrophic risks from artificial intelligence act on the unquestioned assumption that intelligence is kind of a <a title=\"Black box\" href=\"http://en.wikipedia.org/wiki/Black_box\">black box</a>, a <a title=\"Cornucopia\" href=\"http://en.wikipedia.org/wiki/Cornucopia\">cornucopia</a> that can sprout an abundance of novelty. But this implicitly assumes that if you increase intelligence you also decrease the distance between discoveries.</p>\n<p>Intelligence is no solution in itself, it is merely an effective searchlight for unknown unknowns and who knows that the brightness of the light increases proportionally with the distance between unknown unknowns? To enable an intelligence explosion the light would have to reach out much farther with each increase in intelligence than the increase of the distance between unknown unknowns. I just don&rsquo;t see that to be a reasonable assumption.</p>\n<h3>Intelligence amplification, is it worth it?</h3>\n<p>It seems that if you increase intelligence you also increase the computational cost of its further improvement and the distance to the discovery of some unknown unknown that could enable another quantum leap. It seems that you need to apply a lot more energy to get a bit more complexity.</p>\n<p>If any increase in intelligence is vastly outweighed by its computational cost and the expenditure of time needed to discover it then it might not be instrumental for a perfectly rational agent (such as an artificial general intelligence), as <a title=\"Rational Agent\" href=\"http://xixidu.tumblr.com/post/7648180962/what-game-theorists-somewhat-disturbingly-call\">imagined by game theorists</a>, to increase its intelligence as opposed to using its existing intelligence to pursue its terminal goals directly or to invest its given resources to acquire other means of self-improvement, e.g. more efficient sensors.</p>\n<p>What evidence do we have that the payoff of intelligent, goal-oriented experimentation yields <em>enormous advantages</em> (enough to enable an <a title=\"Intelligence Explosion\" href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>) over evolutionary discovery relative to its cost?</p>\n<p>We simply don&rsquo;t know if intelligence is instrumental or quickly hits diminishing returns.</p>\n<p>Can intelligence be effectively applied to itself at all? How do we know that any given level of intelligence is capable of handling its own complexity efficiently? Many humans are not even capable of handling the complexity of the brain of a worm.</p>\n<h3>Humans and the importance of discovery</h3>\n<p>There is a significant difference between intelligence and evolution if you apply intelligence to the improvement of evolutionary designs:</p>\n<ul>\n<li>Intelligence is goal-oriented.</li>\n<li>Intelligence can think ahead.</li>\n<li>Intelligence can jump fitness gaps.</li>\n<li>Intelligence can engage in direct experimentation.</li>\n<li>Intelligence can observe and incorporate solutions of other optimizing agents.</li>\n</ul>\n<p>But when it comes to unknown unknowns, what difference is there between intelligence and evolution? The critical similarity is that both rely on dumb luck when it comes to genuine novelty. And where else but when it comes to the dramatic improvement of intelligence itself does it take the discovery of novel unknown unknowns?</p>\n<p>We have no idea about the nature of discovery and its importance when it comes to what is necessary to reach a level of intelligence above our own, by ourselves. How much of what we know was actually the result of people thinking quantitatively and attending to scope, probability, and marginal impacts? How much of what we know today is the result of dumb luck versus goal-oriented, intelligent problem solving?</p>\n<p>Our &ldquo;irrationality&rdquo; and the patchwork-architecture of the human brain might constitute an actual feature. The noisiness and patchwork architecture of the human brain might play a significant role in the discovery of unknown unknowns because it allows us to become distracted, to leave the path of evidence based exploration.</p>\n<p>A lot of discoveries were made by people who were not explicitly trying to <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">maximizing expected utility</a>. A lot of progress is due to luck, in the form of the discovery of unknown unknowns.</p>\n<p>A basic argument in support of risks from superhuman intelligence is that we don&rsquo;t know what it could possible come up with. That is also why it is called it a &ldquo;<a href=\"/lw/2un/references_resources_for_lesswrong/#SI\">Singularity</a>&ldquo;. But why does nobody ask how a superhuman intelligence knows what it could possible come up with?</p>\n<p>It is not intelligence in and of itself that allows humans to accomplish great feats. Even people like Einstein, geniuses who were apparently able to come up with great insights on their own, were simply lucky to be born into the right circumstances, <a title=\"Multiple discovery\" href=\"http://en.wikipedia.org/wiki/List_of_multiple_discoveries\">the time was ripe for great discoveries</a>, thanks to previous discoveries of unknown unknowns.</p>\n<h3>Evolution versus Intelligence</h3>\n<p>It is argued that the mind-design space must be large if evolution could stumble upon general intelligence and that there are <a title=\"low-hanging fruit\" href=\"http://www.urbandictionary.com/define.php?term=low-hanging%20fruit\">low-hanging fruits</a> that are much more efficient at general intelligence than humans are, evolution simply went with the first that came along. It is further argued that <a title=\"Evolution myths: Evolution is limitlessly creative \" href=\"http://www.newscientist.com/article/dn13639-evolution-myths-evolution-is-limitlessly-creative.html\">evolution is not limitlessly creative</a>, each step must increase the fitness of its host, and that therefore there are artificial mind designs that can do what no product of natural selection could accomplish.</p>\n<p>I agree with the above, yet given all of the apparent disadvantages of <a href=\"http://wiki.lesswrong.com/wiki/Evolution_as_alien_god\">the blind idiot God</a>, evolution was able to come up with altruism, something that works two levels above the individual and one level above society. So far we haven&rsquo;t been able to show such ingenuity by incorporating successes that are not evident from an individual or even societal position.</p>\n<p>The example of <a href=\"http://en.wikipedia.org/wiki/Altruism#Evolutionary_explanations\">altruism</a> provides evidence that intelligence isn&rsquo;t many levels above evolution. Therefore the crucial question is, <em>how</em> great is the performance advantage? Is it large enough to justify the conclusion that the probability of an intelligence explosion is easily larger than 1%? I don&rsquo;t think so. To answer this definitively we would have to fathom the significance of the <em>discovery</em> (&ldquo;random mutations&rdquo;) of unknown unknowns in the dramatic amplification of intelligence versus the <em>invention</em> (goal-oriented &ldquo;research and development&rdquo;) of an improvement within known conceptual bounds.</p>\n<p>Another example is <a href=\"http://en.wikipedia.org/wiki/Flight\">flight</a>. Artificial flight is not even close to the energy efficiency and maneuverability of birds or insects. We didn&rsquo;t went straight from no artificial flight towards flight that is generally superior to the natural flight that is an effect of biological evolution.</p>\n<p><img src=\"http://kruel.co/wp-content/uploads/2011/07/dragonfly-400x210.jpg\" alt=\"\" width=\"400\" height=\"210\" /></p>\n<p>Take for example a <a href=\"http://en.wikipedia.org/wiki/Dragonfly\">dragonfly</a>.&nbsp;Even if we were handed the design for a perfect artificial dragonfly, minus the design for the flight of a dragonfly, we wouldn&rsquo;t be able to build a dragonfly that can take over the world of dragonflies, <a href=\"http://en.wikipedia.org/wiki/Ceteris_paribus\">all else equal</a>, by means of superior flight characteristics.</p>\n<p>It is true that a Harpy Eagle can <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Harpy_Eagle#Feeding\">lift more than three-quarters of its body weight</a> while the <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Boeing_747_Large_Cargo_Freighter#Specifications\">Boeing 747 Large Cargo Freighter</a> has a maximum take-off weight of almost double its operating empty weight (I suspect that insects can do better). My whole point is that we never reached artificial flight that is strongly above the level of natural flight. An eagle can after all catch its cargo under various circumstances like the slope of a mountain or from beneath the sea, thanks to its superior maneuverability.</p>\n<h3>Humans are biased and irrational</h3>\n<p>It is obviously true that our expert systems are better than we are at their narrow range of expertise. But that expert systems are better at certain tasks does not imply that you can effectively and efficiently combine them into a coherent agency.</p>\n<p>The noisiness of the human brain might be one of the important features that allows it to exhibit general intelligence. Yet the same noise might be the reason that each task a human can accomplish is not put into execution with maximal efficiency. An expert system that features a single stand-alone ability is able to reach the unique equilibrium for that ability. Whereas systems that have not fully relaxed to equilibrium feature the necessary characteristics that are required to exhibit general intelligence. In this sense a decrease in efficiency is a side-effect of general intelligence. If you externalize a certain ability into a coherent framework of agency, you decrease its efficiency dramatically. That is the difference between a tool and the ability of the agent that uses the tool.</p>\n<p>In the above sense, our tendency to be biased and act irrationally might partly be a trade off between plasticity, efficiency and the necessity of goal-stability.</p>\n<h3>Embodied cognition and the environment</h3>\n<p>Another problem is that general intelligence is largely a result of an interaction between an agent and its environment. It might be in principle possible to arrive at various capabilities by means of induction, but it is only a theoretical possibility given unlimited computational resources. To achieve real world efficiency you need to rely on slow environmental feedback and make decision under uncertainty.</p>\n<p><a title=\"Universal Algorithmic Intelligence\" href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a> is often quoted as a&nbsp;<a href=\"http://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> that it is possible for a simple algorithm to improve itself to such an extent that it could in principle reach superhuman intelligence. AIXI proves that there is a general theory of intelligence. But there is a minor problem, AIXI is as far from real world human-level general intelligence as an abstract notion of a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\">Turing machine</a> with an infinite tape is from a supercomputer with the computational capacity of the human brain. An abstract notion of intelligence doesn&rsquo;t get you anywhere in terms of real-world general intelligence. Just as you won&rsquo;t be able to <a href=\"http://www.boingboing.net/2011/07/14/far.html\">upload yourself</a><a title=\"Mind uploading\" href=\"http://en.wikipedia.org/wiki/Mind_uploading\"> to a non-biological substrate</a> because you showed that in some abstract sense <a title=\"Church&ndash;Turing&ndash;Deutsch principle\" href=\"http://en.wikipedia.org/wiki/Church%E2%80%93Turing%E2%80%93Deutsch_principle\">you can simulate every physical process</a>.</p>\n<p><strong></strong>Just imagine you <a title=\"Whole Brain Emulation: The Logical Endpoint of Neuroinformatics? \" href=\"http://www.youtube.com/watch?v=kRB6Qzx9oXs\">emulated a grown up human mind</a> and it wanted to become a <a href=\"http://en.wikipedia.org/wiki/Pickup_artist\">pick up artist</a>, how would it do that with an Internet connection? It would need <a href=\"http://en.wikipedia.org/wiki/Embodied_cognition\">some sort of avatar</a>, at least, and then wait for the environment to provide a lot of feedback.</p>\n<p>Therefore even if we&rsquo;re talking about the emulation of a grown up mind, it will be really hard to acquire some capabilities. Then how is the emulation of a human toddler going to acquire those skills? Even worse, how is some sort of abstract AGI going to do it that misses all of the hard coded capabilities of a human toddler?</p>\n<p>Can we even attempt to imagine what is wrong about a boxed emulation of a human toddler, that makes it unable to become a master of <a href=\"http://en.wikipedia.org/wiki/Social_engineering\">social engineering</a> in a very short time?</p>\n<p>Can we imagine what is missing that would enable one of the existing expert systems to quickly evolve vastly superhuman capabilities in its narrow area of expertise? Why haven&rsquo;t we seen a learning algorithm teaching itself chess intelligence starting with nothing but the rules?</p>\n<p>In a sense an intelligent agent is similar to a stone rolling down a hill, both are moving towards a sort of equilibrium. The difference is that intelligence is following more complex trajectories as its ability to read and respond to environmental cues is vastly greater than that of a stone. Yet intelligent or not, the environment in which an agent is embedded plays a crucial role. There exist a&nbsp;fundamental dependency on unintelligent processes. Our environment is structured in such a way that we use information within it as an extension of our minds. The environment enables us to learn and improve our predictions by providing a testbed and a constant stream of data.</p>\n<h3>Necessary resources for an intelligence explosion</h3>\n<p>If artificial general intelligence is unable to seize the resources necessary to undergo explosive recursive self-improvement then the ability and cognitive flexibility of superhuman intelligence in and of itself, as characteristics alone, would have to be sufficient to self-modify its way up to massive superhuman intelligence within a very short time.</p>\n<p>Without advanced <a href=\"http://en.wikipedia.org/wiki/Molecular_nanotechnology\">real-world nanotechnology</a> it will be considerable more difficult for an AGI to undergo quick self-improvement. It will have to make use of existing infrastructure, e.g. buy stocks of chip manufactures and get them to create more or better CPU&rsquo;s. It will have to rely on puny humans for a lot of tasks. It won&rsquo;t be able to create new computational substrate without the whole economy of the world supporting it. It won&rsquo;t be able to create an army of robot drones overnight without it either.</p>\n<p>Doing so it would have to make use of considerable amounts of social engineering without its creators noticing it. But, more importantly, it will have to make use of its existing intelligence to do all of that. The AGI would have to acquire new resources slowly, as it couldn&rsquo;t just self-improve to come up with faster and more efficient solutions. In other words, self-improvement would demand resources. The AGI could not profit from its ability to self-improve regarding the necessary acquisition of resources to be able to self-improve in the first place.</p>\n<p>Therefore the absence of advanced nanotechnology constitutes an immense blow to the possibility of explosive recursive self-improvement and <em>risks from AI</em> in general.</p>\n<p>One might argue that an AGI will solve nanotechnology on its own and find some way to trick humans into manufacturing a molecular assembler and grant it access to it. But this might be very difficult.</p>\n<p>There is a strong interdependence of resources and manufacturers. The AGI won&rsquo;t be able to simply trick some humans to build a high-end factory to create computational substrate, let alone a molecular assembler. People will ask questions and shortly after get suspicious. Remember, it won&rsquo;t be able to coordinate a world-conspiracy, it hasn&rsquo;t been able to self-improve to that point yet because it is still trying to acquire enough resources, which it has to do the hard way without nanotech.</p>\n<p>Anyhow, you&rsquo;d probably need a brain the size of the moon to effectively run and coordinate a whole world of irrational humans by intercepting their communications and altering them on the fly without anyone freaking out.</p>\n<p>People associated with the SIAI would at this point claim that if the AI can&rsquo;t make use of nanotechnology it might make use of something we haven&rsquo;t even thought about. But what, magic?</p>\n<h3>Artificial general intelligence, a single break-through?</h3>\n<p>Another point to consider when talking about risks from AI is how quickly the invention of artificial general intelligence will take place. What evidence do we have that there is some principle that, once discovered, allows us to grow superhuman intelligence overnight?</p>\n<p>If the development of AGI takes place slowly, a gradual and controllable development, we might be able to learn from small-scale mistakes while having to face other risks in the meantime. This might for example be the case if <em>intelligence</em> can not be captured by a discrete algorithm, or is modular, and therefore never allow us to reach a point where we can suddenly build the smartest thing ever that does just extend itself indefinitely.</p>\n<p>To me it doesn&rsquo;t look like that we will come up with artificial general intelligence quickly, but rather that we will have to painstakingly optimize our expert systems step by step over long periods of times.</p>\n<h3>Paperclip maximizers</h3>\n<p>It is claimed that an artificial general intelligence <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">might wipe us out inadvertently</a> while undergoing explosive recursive self-improvement to more effectively pursue its terminal goals. I think that it is unlikely that most AI designs will not hold.</p>\n<p>I agree with the argument that any AGI that isn&rsquo;t made to care about humans won&rsquo;t care about humans. But I also think that the same argument applies for spatio-temporal scope boundaries and resource limits. Even if the AGI is not told to hold, e.g. compute as many digits of Pi as possible, I consider it an far-fetched assumption that any AGI intrinsically cares to take over the universe as fast as possible to compute as many digits of Pi as possible. Sure, if all of that are presuppositions then it will happen, but I don&rsquo;t see that most of all AGI designs are like that. Most that have the potential for superhuman intelligence, but who are given simple goals, will in my opinion just bob up and down as slowly as possible.</p>\n<p>Complex goals need complex optimization parameters (the design specifications of the subject of the optimization process against which it will measure its success of self-improvement).</p>\n<p>Even the creation of paperclips is a much more complex goal than telling an AI to compute as many digits of Pi as possible.</p>\n<p>For an AGI, that was designed to design paperclips, to pose an existential risk, its creators would have to be capable enough to enable it to take over the universe on its own, yet forget, or fail to, define time, space and energy bounds as part of its optimization parameters. Therefore, given the large amount of restrictions that are inevitably part of any advanced general intelligence, the nonhazardous subset of all possible outcomes might be much larger than that where the AGI works perfectly yet fails to hold before it could wreak havoc.</p>\n<h3>Fermi paradox</h3>\n<p>The <a href=\"http://ieet.org/index.php/IEET/more/treder20100302/\">Fermi paradox</a> does allow for and provide the only conclusions and data we can analyze that amount to <em>empirical criticism</em> of concepts like that of a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip maximizer</a> and general risks from superhuman AI&rsquo;s with non-human values without working directly on AGI to test those hypothesis ourselves.</p>\n<p>If you accept the premise that life is <em>not</em> unique and special then one other technological civilisation in the observable universe should be sufficient to leave potentially observable traces of technological tinkering.</p>\n<p>Due to the absence of any signs of intelligence out there, especially paper-clippers <a title=\"Burning the Cosmic Commons: Evolutionary Strategies for Interstellar Colonization\" href=\"http://hanson.gmu.edu/filluniv.pdf\">burning the cosmic commons</a>, we might conclude that unfriendly AI could <a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big%C2%A0threat/\">not be the most dangerous existential risk</a> that we should worry about.</p>\n<h3>Summary</h3>\n<p>In principle we could build antimatter weapons capable of destroying worlds, but in practise it is much harder to accomplish.</p>\n<p>There are many question marks when it comes to the possibility of superhuman intelligence, and many more about the possibility of recursive self-improvement. Most of the arguments in favor of those possibilities <a href=\"http://kruel.co/2011/07/21/givewell-the-siai-and-risks-from-ai/\">solely derive their appeal from being vague</a>.</p>\n<h3>Further reading</h3>\n<ul>\n<li><a href=\"/r/discussion/lw/8fa/intelligence_explosion_a_disjunctive_or/\">Intelligence Explosion - A Disjunctive or Conjunctive Event?</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li>\n<li><a href=\"http://www.overcomingbias.com/2011/06/the-betterness-explosion.html\">The Betterness Explosion</a></li>\n<li><a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">Is The City-ularity Near?</a></li>\n<li><a href=\"http://meteuphoric.wordpress.com/2009/10/16/how-far-can-ai-jump/\">How far can AI jump?</a></li>\n<li><a href=\"http://blogs.discovermagazine.com/sciencenotfiction/2011/01/20/why-im-not-afraid-of-the-singularity/\">Why I&rsquo;m Not Afraid of the Singularity</a></li>\n<li><a href=\"http://blogs.forbes.com/alexknapp/2011/06/23/whats-the-likelihood-of-the-singularity-part-one-artificial-intelligence/\">What&rsquo;s the Likelihood of the Singularity? Part One: Artificial Intelligence</a></li>\n<li><a href=\"http://www.psychologytoday.com/blog/nature-brain-and-culture/201011/when-exactly-will-computers-go-ape-shi-and-take-over\">When Exactly Will Computers Go Ape-Shi* and Take Over?</a></li>\n<li><a href=\"http://singularityhypothesis.blogspot.com/2011/07/slowdown-hypothesis.html\">The slowdown hypothesis (extended abstract)</a></li>\n<li><a href=\"http://singularityhypothesis.blogspot.com/2011/07/singularity-as-faith.html\">The singularity as faith (extended abstract)</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZbgQ3HtPu2cLKGmy8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 2, "extendedScore": null, "score": 7.99359988894324e-07, "legacy": true, "legacyId": "10919", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(The following is a summary of some of my previous submissions that I originally created for my personal blog.) </em></p>\n<blockquote>\n<p>As we know,<br> There are known knowns.<br> There are things<br> We know we know.<br> We also know<br> There are known unknowns.<br> That is to say<br> We know there are some things<br> We do not know.<br> But there are also unknown unknowns,<br> The ones we don\u2019t know<br> We don\u2019t know.</p>\n</blockquote>\n<p>\u2014 Donald Rumsfeld, Feb. 12, 2002, Department of Defense news briefing</p>\n<h3 id=\"Intelligence__a_cornucopia_\">Intelligence, a cornucopia?</h3>\n<p>It seems to me that those who believe into the possibility of catastrophic risks from artificial intelligence act on the unquestioned assumption that intelligence is kind of a <a title=\"Black box\" href=\"http://en.wikipedia.org/wiki/Black_box\">black box</a>, a <a title=\"Cornucopia\" href=\"http://en.wikipedia.org/wiki/Cornucopia\">cornucopia</a> that can sprout an abundance of novelty. But this implicitly assumes that if you increase intelligence you also decrease the distance between discoveries.</p>\n<p>Intelligence is no solution in itself, it is merely an effective searchlight for unknown unknowns and who knows that the brightness of the light increases proportionally with the distance between unknown unknowns? To enable an intelligence explosion the light would have to reach out much farther with each increase in intelligence than the increase of the distance between unknown unknowns. I just don\u2019t see that to be a reasonable assumption.</p>\n<h3 id=\"Intelligence_amplification__is_it_worth_it_\">Intelligence amplification, is it worth it?</h3>\n<p>It seems that if you increase intelligence you also increase the computational cost of its further improvement and the distance to the discovery of some unknown unknown that could enable another quantum leap. It seems that you need to apply a lot more energy to get a bit more complexity.</p>\n<p>If any increase in intelligence is vastly outweighed by its computational cost and the expenditure of time needed to discover it then it might not be instrumental for a perfectly rational agent (such as an artificial general intelligence), as <a title=\"Rational Agent\" href=\"http://xixidu.tumblr.com/post/7648180962/what-game-theorists-somewhat-disturbingly-call\">imagined by game theorists</a>, to increase its intelligence as opposed to using its existing intelligence to pursue its terminal goals directly or to invest its given resources to acquire other means of self-improvement, e.g. more efficient sensors.</p>\n<p>What evidence do we have that the payoff of intelligent, goal-oriented experimentation yields <em>enormous advantages</em> (enough to enable an <a title=\"Intelligence Explosion\" href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\">intelligence explosion</a>) over evolutionary discovery relative to its cost?</p>\n<p>We simply don\u2019t know if intelligence is instrumental or quickly hits diminishing returns.</p>\n<p>Can intelligence be effectively applied to itself at all? How do we know that any given level of intelligence is capable of handling its own complexity efficiently? Many humans are not even capable of handling the complexity of the brain of a worm.</p>\n<h3 id=\"Humans_and_the_importance_of_discovery\">Humans and the importance of discovery</h3>\n<p>There is a significant difference between intelligence and evolution if you apply intelligence to the improvement of evolutionary designs:</p>\n<ul>\n<li>Intelligence is goal-oriented.</li>\n<li>Intelligence can think ahead.</li>\n<li>Intelligence can jump fitness gaps.</li>\n<li>Intelligence can engage in direct experimentation.</li>\n<li>Intelligence can observe and incorporate solutions of other optimizing agents.</li>\n</ul>\n<p>But when it comes to unknown unknowns, what difference is there between intelligence and evolution? The critical similarity is that both rely on dumb luck when it comes to genuine novelty. And where else but when it comes to the dramatic improvement of intelligence itself does it take the discovery of novel unknown unknowns?</p>\n<p>We have no idea about the nature of discovery and its importance when it comes to what is necessary to reach a level of intelligence above our own, by ourselves. How much of what we know was actually the result of people thinking quantitatively and attending to scope, probability, and marginal impacts? How much of what we know today is the result of dumb luck versus goal-oriented, intelligent problem solving?</p>\n<p>Our \u201cirrationality\u201d and the patchwork-architecture of the human brain might constitute an actual feature. The noisiness and patchwork architecture of the human brain might play a significant role in the discovery of unknown unknowns because it allows us to become distracted, to leave the path of evidence based exploration.</p>\n<p>A lot of discoveries were made by people who were not explicitly trying to <a href=\"http://en.wikipedia.org/wiki/Expected_utility_hypothesis\">maximizing expected utility</a>. A lot of progress is due to luck, in the form of the discovery of unknown unknowns.</p>\n<p>A basic argument in support of risks from superhuman intelligence is that we don\u2019t know what it could possible come up with. That is also why it is called it a \u201c<a href=\"/lw/2un/references_resources_for_lesswrong/#SI\">Singularity</a>\u201c. But why does nobody ask how a superhuman intelligence knows what it could possible come up with?</p>\n<p>It is not intelligence in and of itself that allows humans to accomplish great feats. Even people like Einstein, geniuses who were apparently able to come up with great insights on their own, were simply lucky to be born into the right circumstances, <a title=\"Multiple discovery\" href=\"http://en.wikipedia.org/wiki/List_of_multiple_discoveries\">the time was ripe for great discoveries</a>, thanks to previous discoveries of unknown unknowns.</p>\n<h3 id=\"Evolution_versus_Intelligence\">Evolution versus Intelligence</h3>\n<p>It is argued that the mind-design space must be large if evolution could stumble upon general intelligence and that there are <a title=\"low-hanging fruit\" href=\"http://www.urbandictionary.com/define.php?term=low-hanging%20fruit\">low-hanging fruits</a> that are much more efficient at general intelligence than humans are, evolution simply went with the first that came along. It is further argued that <a title=\"Evolution myths: Evolution is limitlessly creative \" href=\"http://www.newscientist.com/article/dn13639-evolution-myths-evolution-is-limitlessly-creative.html\">evolution is not limitlessly creative</a>, each step must increase the fitness of its host, and that therefore there are artificial mind designs that can do what no product of natural selection could accomplish.</p>\n<p>I agree with the above, yet given all of the apparent disadvantages of <a href=\"http://wiki.lesswrong.com/wiki/Evolution_as_alien_god\">the blind idiot God</a>, evolution was able to come up with altruism, something that works two levels above the individual and one level above society. So far we haven\u2019t been able to show such ingenuity by incorporating successes that are not evident from an individual or even societal position.</p>\n<p>The example of <a href=\"http://en.wikipedia.org/wiki/Altruism#Evolutionary_explanations\">altruism</a> provides evidence that intelligence isn\u2019t many levels above evolution. Therefore the crucial question is, <em>how</em> great is the performance advantage? Is it large enough to justify the conclusion that the probability of an intelligence explosion is easily larger than 1%? I don\u2019t think so. To answer this definitively we would have to fathom the significance of the <em>discovery</em> (\u201crandom mutations\u201d) of unknown unknowns in the dramatic amplification of intelligence versus the <em>invention</em> (goal-oriented \u201cresearch and development\u201d) of an improvement within known conceptual bounds.</p>\n<p>Another example is <a href=\"http://en.wikipedia.org/wiki/Flight\">flight</a>. Artificial flight is not even close to the energy efficiency and maneuverability of birds or insects. We didn\u2019t went straight from no artificial flight towards flight that is generally superior to the natural flight that is an effect of biological evolution.</p>\n<p><img src=\"http://kruel.co/wp-content/uploads/2011/07/dragonfly-400x210.jpg\" alt=\"\" width=\"400\" height=\"210\"></p>\n<p>Take for example a <a href=\"http://en.wikipedia.org/wiki/Dragonfly\">dragonfly</a>.&nbsp;Even if we were handed the design for a perfect artificial dragonfly, minus the design for the flight of a dragonfly, we wouldn\u2019t be able to build a dragonfly that can take over the world of dragonflies, <a href=\"http://en.wikipedia.org/wiki/Ceteris_paribus\">all else equal</a>, by means of superior flight characteristics.</p>\n<p>It is true that a Harpy Eagle can <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Harpy_Eagle#Feeding\">lift more than three-quarters of its body weight</a> while the <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Boeing_747_Large_Cargo_Freighter#Specifications\">Boeing 747 Large Cargo Freighter</a> has a maximum take-off weight of almost double its operating empty weight (I suspect that insects can do better). My whole point is that we never reached artificial flight that is strongly above the level of natural flight. An eagle can after all catch its cargo under various circumstances like the slope of a mountain or from beneath the sea, thanks to its superior maneuverability.</p>\n<h3 id=\"Humans_are_biased_and_irrational\">Humans are biased and irrational</h3>\n<p>It is obviously true that our expert systems are better than we are at their narrow range of expertise. But that expert systems are better at certain tasks does not imply that you can effectively and efficiently combine them into a coherent agency.</p>\n<p>The noisiness of the human brain might be one of the important features that allows it to exhibit general intelligence. Yet the same noise might be the reason that each task a human can accomplish is not put into execution with maximal efficiency. An expert system that features a single stand-alone ability is able to reach the unique equilibrium for that ability. Whereas systems that have not fully relaxed to equilibrium feature the necessary characteristics that are required to exhibit general intelligence. In this sense a decrease in efficiency is a side-effect of general intelligence. If you externalize a certain ability into a coherent framework of agency, you decrease its efficiency dramatically. That is the difference between a tool and the ability of the agent that uses the tool.</p>\n<p>In the above sense, our tendency to be biased and act irrationally might partly be a trade off between plasticity, efficiency and the necessity of goal-stability.</p>\n<h3 id=\"Embodied_cognition_and_the_environment\">Embodied cognition and the environment</h3>\n<p>Another problem is that general intelligence is largely a result of an interaction between an agent and its environment. It might be in principle possible to arrive at various capabilities by means of induction, but it is only a theoretical possibility given unlimited computational resources. To achieve real world efficiency you need to rely on slow environmental feedback and make decision under uncertainty.</p>\n<p><a title=\"Universal Algorithmic Intelligence\" href=\"http://www.hutter1.net/ai/aixigentle.htm\">AIXI</a> is often quoted as a&nbsp;<a href=\"http://en.wikipedia.org/wiki/Proof_of_concept\">proof of concept</a> that it is possible for a simple algorithm to improve itself to such an extent that it could in principle reach superhuman intelligence. AIXI proves that there is a general theory of intelligence. But there is a minor problem, AIXI is as far from real world human-level general intelligence as an abstract notion of a <a href=\"http://en.wikipedia.org/wiki/Turing_machine\">Turing machine</a> with an infinite tape is from a supercomputer with the computational capacity of the human brain. An abstract notion of intelligence doesn\u2019t get you anywhere in terms of real-world general intelligence. Just as you won\u2019t be able to <a href=\"http://www.boingboing.net/2011/07/14/far.html\">upload yourself</a><a title=\"Mind uploading\" href=\"http://en.wikipedia.org/wiki/Mind_uploading\"> to a non-biological substrate</a> because you showed that in some abstract sense <a title=\"Church\u2013Turing\u2013Deutsch principle\" href=\"http://en.wikipedia.org/wiki/Church%E2%80%93Turing%E2%80%93Deutsch_principle\">you can simulate every physical process</a>.</p>\n<p><strong></strong>Just imagine you <a title=\"Whole Brain Emulation: The Logical Endpoint of Neuroinformatics? \" href=\"http://www.youtube.com/watch?v=kRB6Qzx9oXs\">emulated a grown up human mind</a> and it wanted to become a <a href=\"http://en.wikipedia.org/wiki/Pickup_artist\">pick up artist</a>, how would it do that with an Internet connection? It would need <a href=\"http://en.wikipedia.org/wiki/Embodied_cognition\">some sort of avatar</a>, at least, and then wait for the environment to provide a lot of feedback.</p>\n<p>Therefore even if we\u2019re talking about the emulation of a grown up mind, it will be really hard to acquire some capabilities. Then how is the emulation of a human toddler going to acquire those skills? Even worse, how is some sort of abstract AGI going to do it that misses all of the hard coded capabilities of a human toddler?</p>\n<p>Can we even attempt to imagine what is wrong about a boxed emulation of a human toddler, that makes it unable to become a master of <a href=\"http://en.wikipedia.org/wiki/Social_engineering\">social engineering</a> in a very short time?</p>\n<p>Can we imagine what is missing that would enable one of the existing expert systems to quickly evolve vastly superhuman capabilities in its narrow area of expertise? Why haven\u2019t we seen a learning algorithm teaching itself chess intelligence starting with nothing but the rules?</p>\n<p>In a sense an intelligent agent is similar to a stone rolling down a hill, both are moving towards a sort of equilibrium. The difference is that intelligence is following more complex trajectories as its ability to read and respond to environmental cues is vastly greater than that of a stone. Yet intelligent or not, the environment in which an agent is embedded plays a crucial role. There exist a&nbsp;fundamental dependency on unintelligent processes. Our environment is structured in such a way that we use information within it as an extension of our minds. The environment enables us to learn and improve our predictions by providing a testbed and a constant stream of data.</p>\n<h3 id=\"Necessary_resources_for_an_intelligence_explosion\">Necessary resources for an intelligence explosion</h3>\n<p>If artificial general intelligence is unable to seize the resources necessary to undergo explosive recursive self-improvement then the ability and cognitive flexibility of superhuman intelligence in and of itself, as characteristics alone, would have to be sufficient to self-modify its way up to massive superhuman intelligence within a very short time.</p>\n<p>Without advanced <a href=\"http://en.wikipedia.org/wiki/Molecular_nanotechnology\">real-world nanotechnology</a> it will be considerable more difficult for an AGI to undergo quick self-improvement. It will have to make use of existing infrastructure, e.g. buy stocks of chip manufactures and get them to create more or better CPU\u2019s. It will have to rely on puny humans for a lot of tasks. It won\u2019t be able to create new computational substrate without the whole economy of the world supporting it. It won\u2019t be able to create an army of robot drones overnight without it either.</p>\n<p>Doing so it would have to make use of considerable amounts of social engineering without its creators noticing it. But, more importantly, it will have to make use of its existing intelligence to do all of that. The AGI would have to acquire new resources slowly, as it couldn\u2019t just self-improve to come up with faster and more efficient solutions. In other words, self-improvement would demand resources. The AGI could not profit from its ability to self-improve regarding the necessary acquisition of resources to be able to self-improve in the first place.</p>\n<p>Therefore the absence of advanced nanotechnology constitutes an immense blow to the possibility of explosive recursive self-improvement and <em>risks from AI</em> in general.</p>\n<p>One might argue that an AGI will solve nanotechnology on its own and find some way to trick humans into manufacturing a molecular assembler and grant it access to it. But this might be very difficult.</p>\n<p>There is a strong interdependence of resources and manufacturers. The AGI won\u2019t be able to simply trick some humans to build a high-end factory to create computational substrate, let alone a molecular assembler. People will ask questions and shortly after get suspicious. Remember, it won\u2019t be able to coordinate a world-conspiracy, it hasn\u2019t been able to self-improve to that point yet because it is still trying to acquire enough resources, which it has to do the hard way without nanotech.</p>\n<p>Anyhow, you\u2019d probably need a brain the size of the moon to effectively run and coordinate a whole world of irrational humans by intercepting their communications and altering them on the fly without anyone freaking out.</p>\n<p>People associated with the SIAI would at this point claim that if the AI can\u2019t make use of nanotechnology it might make use of something we haven\u2019t even thought about. But what, magic?</p>\n<h3 id=\"Artificial_general_intelligence__a_single_break_through_\">Artificial general intelligence, a single break-through?</h3>\n<p>Another point to consider when talking about risks from AI is how quickly the invention of artificial general intelligence will take place. What evidence do we have that there is some principle that, once discovered, allows us to grow superhuman intelligence overnight?</p>\n<p>If the development of AGI takes place slowly, a gradual and controllable development, we might be able to learn from small-scale mistakes while having to face other risks in the meantime. This might for example be the case if <em>intelligence</em> can not be captured by a discrete algorithm, or is modular, and therefore never allow us to reach a point where we can suddenly build the smartest thing ever that does just extend itself indefinitely.</p>\n<p>To me it doesn\u2019t look like that we will come up with artificial general intelligence quickly, but rather that we will have to painstakingly optimize our expert systems step by step over long periods of times.</p>\n<h3 id=\"Paperclip_maximizers\">Paperclip maximizers</h3>\n<p>It is claimed that an artificial general intelligence <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">might wipe us out inadvertently</a> while undergoing explosive recursive self-improvement to more effectively pursue its terminal goals. I think that it is unlikely that most AI designs will not hold.</p>\n<p>I agree with the argument that any AGI that isn\u2019t made to care about humans won\u2019t care about humans. But I also think that the same argument applies for spatio-temporal scope boundaries and resource limits. Even if the AGI is not told to hold, e.g. compute as many digits of Pi as possible, I consider it an far-fetched assumption that any AGI intrinsically cares to take over the universe as fast as possible to compute as many digits of Pi as possible. Sure, if all of that are presuppositions then it will happen, but I don\u2019t see that most of all AGI designs are like that. Most that have the potential for superhuman intelligence, but who are given simple goals, will in my opinion just bob up and down as slowly as possible.</p>\n<p>Complex goals need complex optimization parameters (the design specifications of the subject of the optimization process against which it will measure its success of self-improvement).</p>\n<p>Even the creation of paperclips is a much more complex goal than telling an AI to compute as many digits of Pi as possible.</p>\n<p>For an AGI, that was designed to design paperclips, to pose an existential risk, its creators would have to be capable enough to enable it to take over the universe on its own, yet forget, or fail to, define time, space and energy bounds as part of its optimization parameters. Therefore, given the large amount of restrictions that are inevitably part of any advanced general intelligence, the nonhazardous subset of all possible outcomes might be much larger than that where the AGI works perfectly yet fails to hold before it could wreak havoc.</p>\n<h3 id=\"Fermi_paradox\">Fermi paradox</h3>\n<p>The <a href=\"http://ieet.org/index.php/IEET/more/treder20100302/\">Fermi paradox</a> does allow for and provide the only conclusions and data we can analyze that amount to <em>empirical criticism</em> of concepts like that of a <a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\">Paperclip maximizer</a> and general risks from superhuman AI\u2019s with non-human values without working directly on AGI to test those hypothesis ourselves.</p>\n<p>If you accept the premise that life is <em>not</em> unique and special then one other technological civilisation in the observable universe should be sufficient to leave potentially observable traces of technological tinkering.</p>\n<p>Due to the absence of any signs of intelligence out there, especially paper-clippers <a title=\"Burning the Cosmic Commons: Evolutionary Strategies for Interstellar Colonization\" href=\"http://hanson.gmu.edu/filluniv.pdf\">burning the cosmic commons</a>, we might conclude that unfriendly AI could <a href=\"http://meteuphoric.wordpress.com/2010/11/11/sia-says-ai-is-no-big%C2%A0threat/\">not be the most dangerous existential risk</a> that we should worry about.</p>\n<h3 id=\"Summary\">Summary</h3>\n<p>In principle we could build antimatter weapons capable of destroying worlds, but in practise it is much harder to accomplish.</p>\n<p>There are many question marks when it comes to the possibility of superhuman intelligence, and many more about the possibility of recursive self-improvement. Most of the arguments in favor of those possibilities <a href=\"http://kruel.co/2011/07/21/givewell-the-siai-and-risks-from-ai/\">solely derive their appeal from being vague</a>.</p>\n<h3 id=\"Further_reading\">Further reading</h3>\n<ul>\n<li><a href=\"/r/discussion/lw/8fa/intelligence_explosion_a_disjunctive_or/\">Intelligence Explosion - A Disjunctive or Conjunctive Event?</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI-Foom Debate</a></li>\n<li><a href=\"http://www.overcomingbias.com/2011/06/the-betterness-explosion.html\">The Betterness Explosion</a></li>\n<li><a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">Is The City-ularity Near?</a></li>\n<li><a href=\"http://meteuphoric.wordpress.com/2009/10/16/how-far-can-ai-jump/\">How far can AI jump?</a></li>\n<li><a href=\"http://blogs.discovermagazine.com/sciencenotfiction/2011/01/20/why-im-not-afraid-of-the-singularity/\">Why I\u2019m Not Afraid of the Singularity</a></li>\n<li><a href=\"http://blogs.forbes.com/alexknapp/2011/06/23/whats-the-likelihood-of-the-singularity-part-one-artificial-intelligence/\">What\u2019s the Likelihood of the Singularity? Part One: Artificial Intelligence</a></li>\n<li><a href=\"http://www.psychologytoday.com/blog/nature-brain-and-culture/201011/when-exactly-will-computers-go-ape-shi-and-take-over\">When Exactly Will Computers Go Ape-Shi* and Take Over?</a></li>\n<li><a href=\"http://singularityhypothesis.blogspot.com/2011/07/slowdown-hypothesis.html\">The slowdown hypothesis (extended abstract)</a></li>\n<li><a href=\"http://singularityhypothesis.blogspot.com/2011/07/singularity-as-faith.html\">The singularity as faith (extended abstract)</a></li>\n</ul>", "sections": [{"title": "Intelligence, a cornucopia?", "anchor": "Intelligence__a_cornucopia_", "level": 1}, {"title": "Intelligence amplification, is it worth it?", "anchor": "Intelligence_amplification__is_it_worth_it_", "level": 1}, {"title": "Humans and the importance of discovery", "anchor": "Humans_and_the_importance_of_discovery", "level": 1}, {"title": "Evolution versus Intelligence", "anchor": "Evolution_versus_Intelligence", "level": 1}, {"title": "Humans are biased and irrational", "anchor": "Humans_are_biased_and_irrational", "level": 1}, {"title": "Embodied cognition and the environment", "anchor": "Embodied_cognition_and_the_environment", "level": 1}, {"title": "Necessary resources for an intelligence explosion", "anchor": "Necessary_resources_for_an_intelligence_explosion", "level": 1}, {"title": "Artificial general intelligence, a single break-through?", "anchor": "Artificial_general_intelligence__a_single_break_through_", "level": 1}, {"title": "Paperclip maximizers", "anchor": "Paperclip_maximizers", "level": 1}, {"title": "Fermi paradox", "anchor": "Fermi_paradox", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Further reading", "anchor": "Further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "97 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["j52uErqofDiJZCo76"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T12:59:13.438Z", "modifiedAt": null, "url": null, "title": "The nocebo effect [link]", "slug": "the-nocebo-effect-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.134Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dfMbXh77vk2rM8HFr/the-nocebo-effect-link", "pageUrlRelative": "/posts/dfMbXh77vk2rM8HFr/the-nocebo-effect-link", "linkUrl": "https://www.lesswrong.com/posts/dfMbXh77vk2rM8HFr/the-nocebo-effect-link", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20nocebo%20effect%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20nocebo%20effect%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfMbXh77vk2rM8HFr%2Fthe-nocebo-effect-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20nocebo%20effect%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfMbXh77vk2rM8HFr%2Fthe-nocebo-effect-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdfMbXh77vk2rM8HFr%2Fthe-nocebo-effect-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.guardian.co.uk/science/2011/nov/13/nocebo-pain-wellcome-trust-prize\">http://www.guardian.co.uk/science/2011/nov/13/nocebo-pain-wellcome-trust-prize</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dfMbXh77vk2rM8HFr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 7.993877627349838e-07, "legacy": true, "legacyId": "10920", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T17:02:03.821Z", "modifiedAt": null, "url": null, "title": "Transcription of Eliezer's January 2010 video Q&A", "slug": "transcription-of-eliezer-s-january-2010-video-q-and-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YduZEfz8usGbJXN4x/transcription-of-eliezer-s-january-2010-video-q-and-a", "pageUrlRelative": "/posts/YduZEfz8usGbJXN4x/transcription-of-eliezer-s-january-2010-video-q-and-a", "linkUrl": "https://www.lesswrong.com/posts/YduZEfz8usGbJXN4x/transcription-of-eliezer-s-january-2010-video-q-and-a", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transcription%20of%20Eliezer's%20January%202010%20video%20Q%26A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranscription%20of%20Eliezer's%20January%202010%20video%20Q%26A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYduZEfz8usGbJXN4x%2Ftranscription-of-eliezer-s-january-2010-video-q-and-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transcription%20of%20Eliezer's%20January%202010%20video%20Q%26A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYduZEfz8usGbJXN4x%2Ftranscription-of-eliezer-s-january-2010-video-q-and-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYduZEfz8usGbJXN4x%2Ftranscription-of-eliezer-s-january-2010-video-q-and-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16673, "htmlBody": "<div style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; padding: 0.5em; margin: 8px;\">\n<p>Spurred by discussion of whether Luke's Q&amp;A session should be on video or text-only, I volunteered to transcribe&nbsp;<a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/\">Eliezer's Q&amp;A videos</a>&nbsp;from January 2010. &nbsp;I finished last night, much earlier than my estimate, mostly due to feeling motivated to finish it and spending&nbsp;more on it than my very conservative estimated 30 minutes a day (estimate of number of words was pretty close; about 16000). &nbsp;I have posted a link to this post as a comment in the original thread <a href=\"/lw/1lq/less_wrong_qa_with_eliezer_yudkowsky_video_answers/58yp\">here</a>, if you would like to upvote that.</p>\n<p>Some advice for transcribing videos: I downloaded the .wmv videos, which allowed me to use VLC's global hotkeys to create a pause and \"short skip backwards and forwards\" buttons (ctrl-space and ctrl-shift left/right arrow), which were so much more convenient than any other method I tried.</p>\n<p>&nbsp;</p>\n<div style=\"background-color: transparent; \">\n<p id=\"internal-source-marker_0.5158945594448596\" style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Edited out: repetition of the question, &ldquo;um/uh&rdquo;, &ldquo;you know,&rdquo; false starts.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Punctuation, capitalization, and structure, etc may not be entirely consistent.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Keep in mind the opinions expressed here are those of Eliezer circa January 2010.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \"><br /></span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">1. What is your information diet like? Do you control it deliberately (do you have a method; is it, er, intelligently designed), or do you just let it happen naturally.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">By that I mean things like: Do you have a reading schedule (x number of hours daily, etc)? Do you follow the news, or try to avoid information with a short shelf-life? Do you frequently stop yourself from doing things that you enjoy (f.ex reading certain magazines, books, watching films, etc) to focus on what is more important? etc.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">It&rsquo;s not very planned, most of the time, in other words Hacker News, Reddit, Marginal Revolution, other random stuff found on the internet. &nbsp;In order to learn something, I usually have to set aside blocks of time and blocks of effort and just focus on specifically reading something. It&rsquo;s only sort of popular level books which I can put on a restroom shelf and get them read that way. &nbsp;In order to learn actually useful information I generally find that I have to set aside blocks of time or run across a pot of gold, and you&rsquo;re about as likely to get a pot of gold from Hacker News as anywhere else really. &nbsp;So not very controlled.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">2. Your \"Bookshelf\" page is 10 years old (and contains a warning sign saying it is obsolete): </span><a href=\"http://yudkowsky.net/obsolete/bookshelf.html\"><span style=\"font-size: 11pt; font-family: Arial; color: #000000; background-color: transparent; font-weight: bold; vertical-align: baseline; white-space: pre-wrap; \">http://yudkowsky.net/obsolete/bookshelf.html</span></a></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Could you tell us about some of the books and papers that you've been reading lately? I'm particularly interested in books that you've read since 1999 that you would consider to be of the highest quality and/or importance (fiction or not).</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I guess I&rsquo;m a bit ashamed of how little I&rsquo;ve been reading whole books and how much I&rsquo;ve been reading small bite-sized pieces on the internet recently. &nbsp;Right now I&rsquo;m reading Predictably Irrational which is a popular book by Dan Ariely about biases, it&rsquo;s pretty good, sort of like a sequence of Less Wrong posts. &nbsp;I&rsquo;ve recently finished reading Good and Real by Gary Drescher, which is something I kept on picking up and putting down, which is very Lesswrongian, it&rsquo;s master level Reductionism and the degree of overlap was incredible enough that I would read something and say &lsquo;OK I should write this up on my own before I read how Drescher wrote it so that you can get sort of independent views of it and see how they compare.&rsquo;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Let&rsquo;s see, other things I&rsquo;ve read recently. &nbsp;I&rsquo;ve fallen into the black hole of Fanfiction.net, well actually fallen into a black hole is probably too extreme. It&rsquo;s got a lot of reading and the reading&rsquo;s broken up into nice block size chapters and I&rsquo;ve yet to exhaust the recommendations of the good stuff, but probably not all that much reading there, relatively speaking. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I guess it really has been quite a while since I picked up a good old-fashioned book and said &lsquo;Wow, what an amazing book&rsquo;. My memory is just returning the best hits of the last 10 years instead of the best hits of the last six months or anything like that. &nbsp;If we expand it out to the best hits of the last 10 years then Artificial Intelligence: A Modern Approach by Russell and Norvig is a really wonderful artificial intelligence textbook. It was on reading through that that I sort of got the epiphany of artificial intelligence really has made a lot more progress than people credit for, it&rsquo;s just not really well organized, so you need someone with good taste to go through and tell you what&rsquo;s been done before you recognize what has been done.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">There was a book on statistical inference, I&rsquo;m trying to remember the exact title, it&rsquo;s by Hastie and Tibshirani, Elements of Statistical Learning, that was it. &nbsp;Elements of Statistical Learning was when I realized that the top people, they really do understand their subject, the people who wrote the Elements of Statistical Learning, they really understand statistics. &nbsp;At the same time you read through and say &lsquo;Gosh, by comparison with these people, the average statistician, to say nothing of the average scientist who&rsquo;s just using statistics, doesn&rsquo;t really understand statistics at all.&rsquo;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Let&rsquo;s see, other really great... Yeah my memory just doesn&rsquo;t really associate all that well I&rsquo;m afraid, &nbsp;it doesn&rsquo;t sort of snap back and cough up a list of the best things I&rsquo;ve read recently. &nbsp;This would probably be something better for me to answer in text than in video I&rsquo;m afraid.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">3. What is a typical EY workday like? How many hours/day on average are devoted to FAI research, and how many to other things, and what are the other major activities that you devote your time to?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I&rsquo;m not really sure I have anything I could call a &lsquo;typical&rsquo; workday. Akrasia, weakness of will, that has always been what I consider to be my Great Bugaboo, and I still do feel guilty about the amount of rest time and downtime that I require to get work done, and even so I sometimes suspect that I&rsquo;m taking to little downtime relative to work time just because on those occasions when something or other prevents me form getting work done, for a couple of days, I come back and I&rsquo;m suddenly much more productive. In general, I feel like I&rsquo;m stupid with respect to organizing my work day, that sort of problem, it used to feel to me like it was chaotic and unpredictable, but I now recognize that when something looks chaotic and unpredictable, that means that you are stupid with respect to that domain. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So it&rsquo;ll probably look like, when I manage to get a work session in the work session will be a couple of hours, I&rsquo;ll sometimes when I run into a difficult problem I&rsquo;ll sometimes stop and go off and read things on the internet for a few minutes or a lot of minutes, until I can come back and I can come back and solve the problem or my brain is rested enough to go to the more tiring high levels of abstraction where I can actually understand what it is that&rsquo;s been blocking me and move on. &nbsp;That&rsquo;s for writing, which I&rsquo;ve been doing a lot of lately.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">A typical workday when I&rsquo;m actually working on Friendly AI with Marcello, that&rsquo;ll look like we get together and sit down and open up a notebook and stare at our notebooks and throw ideas back and forth and sometimes sit in silence and think about things, write things down, I&rsquo;ll propose things, Marcello will point out flaws in them or vice versa, sort of reach the end of a line of thought, go blank, stop and stare at each other and try to think of another line of thought, keep that up for two to three hours, break for lunch, keep it up for another two to three hours, and then break for a day, could spend the off day just recovering or reading math if possible or otherwise just recovering. &nbsp;Marcello doesn&rsquo;t need as much recovery time, but I also suspect that Marcello, because he&rsquo;s still sort of relatively inexperienced isn&rsquo;t quite confronting the most difficult parts of the problem as directly. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So taking a one-day-on one-day-off, with respect to Friendly AI I actually don&rsquo;t feel guilty about it at all, because it really is apparent that I just cannot work two days in a row on this problem and be productive. It&rsquo;s just really obvious, and so instead of the usual cycle of &lsquo;Am I working enough? Could I be working harder?&rsquo; and feeling guilty about it it&rsquo;s just obvious that in that case after I get a solid day&rsquo;s work I have to take a solid day off.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Let&rsquo;s see, any other sorts of working cycles? Back when I was doing the Overcoming Bias/Less Wrong arc at one post per day, I would sometimes get more than one post per day in and that&rsquo;s how I&rsquo;d occasionally get a day off, other times a post would take more than one day. I find that I am usually relatively less productive in the morning; a lot of advice says &lsquo;as soon as you get up in the morning, sit down, start working, get things done&rsquo;; that&rsquo;s never quite worked out for me, and of course that could just be because I&rsquo;m doing it wrong, but even so I find that I tend to be more productive later in the day.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Let&rsquo;s see, other info... Oh yes, at one point I tried to set up my computer to have a separate login without any of the usual distractions, and that caused my productivity to drop down because it meant that when I needed to take some time off, instead of browsing around the internet and then going right back to working, I&rsquo;d actually separated work and so it was harder to switch back and forth between them both, so that was something that seemed like it was a really good idea that ought to work in theory, setting aside this sort of separate space with no distractions to work, and that failed. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">And right now I&rsquo;m working sort of on the preliminaries for the book, The Art of Rationality being the working title, and I haven&rsquo;t started writing the book yet, I&rsquo;m still sort of trying to understand what it is that I&rsquo;ve previously written on Less Wrong, Overcoming Bias, organize it using mind mapping software from FreeMind which is open source mind mapping software; it&rsquo;s really something I wish I&rsquo;d known existed and started using back when the whole Overcoming Bias/Less Wrong thing started, I think it might have been a help. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So right now I&rsquo;m just still sort of trying to understand what did I actually say, what&rsquo;s the point, how do the points relate to each other, and thereby organizing the skeleton of the book, rather than writing it just yet, and the reason I&rsquo;m doing it that way is that when it comes to writing things like books where I don&rsquo;t push out a post every day I tend to be very slow, unacceptably slow even, and so one method of solving that was rite a post every day and this time I&rsquo;m seeing if I can, by planning everything out sufficiently thoroughly in advance and structuring it sufficiently thoroughly in advance, get it done at a reasonable clip.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">4. Could you please tell us a little about your brain? For example, what is your IQ, at what age did you learn calculus, do you use cognitive enhancing drugs or brain fitness programs, are you Neurotypical and why didn't you attend school?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So the question is &lsquo;please tell us a little about your brain.&rsquo; &nbsp;What&rsquo;s your IQ? Tested as 143, that would have been back when I was... 12? 13? Not really sure exactly. &nbsp;I tend to interpret that as &lsquo;this is about as high as the IQ test measures&rsquo; rather than &lsquo;you are three standard deviations above the mean&rsquo;. I&rsquo;ve scored higher than that on(?) other standardized tests; the largest I&rsquo;ve actually seen written down was 99.9998th percentile, but that was not really all that well standardized because I was taking the test and being scored as though for the grade above mine and so it was being scored for grade rather than by age, so I don&rsquo;t know whether or not that means that people who didn&rsquo;t advance through grades tend to get the highest scores and so I was competing well against people who were older than me, or whether if the really smart people all advanced farther through the grades and so the proper competition doesn&rsquo;t really get sorted out, but in any case that&rsquo;s the highest percentile I&rsquo;ve seen written down.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">&lsquo;At what age did I learn calculus&rsquo;, well it would have been before 15, probably 13 would be my guess. I&rsquo;ll also state at just how stunned I am at how poorly calculus is taught. </span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Do I use cognitive enhancing drugs or brain fitness programs? No. &nbsp;I&rsquo;ve always been very reluctant to try tampering with the neurochemistry of my brain because I just don&rsquo;t seem to react to things typically; as a kid I was given Ritalin and Prozac and neither of those seemed to help at all and the Prozac in particular seemed to blur everything out and you just instinctively(?) just... eugh. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">One of the questions over here is &lsquo;are you neurotypical&rsquo;. And my sort of instinctive reaction to that is &lsquo;Hah!&rsquo; &nbsp;And for that reason I&rsquo;m reluctant to tamper with things. Similarly with the brain fitness programs, don&rsquo;t really know which one of those work and which don&rsquo;t, I&rsquo;m sort of waiting for other people in the Less Wrong community to experiment with that sort of thing and come back and tell the rest of us what works and if there&rsquo;s any consensus between them, I might join the crowd.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">&lsquo;Why didn&rsquo;t you attend school?&rsquo; Well I attended grade school, but when I got out of grade school it was pretty clear that I just couldn&rsquo;t handle the system; I don&rsquo;t really know how else to put it. Part of that might have been that at the same time that I hit puberty my brain just sort of... I don&rsquo;t really know how to describe it. &nbsp;Depression would be one word for it, sort of &lsquo;spontaneous massive will failure&rsquo; might be another way to put; it&rsquo;s not that I was getting more pessimistic or anything, just that my will sort of failed and I couldn&rsquo;t get stuff done. &nbsp;Sort of a long process to drag myself out that and you could probably make a pretty good case that I&rsquo;m still there, I just handle it a lot better? Not even really sure quite what I did right, as I said in an answer to a previous question, this is something I&rsquo;ve been struggling with for a while and part of having a poor grasp on something is that even when you do something right you don&rsquo;t understand afterwards quite what it is that you did right. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So... &lsquo;tell us about your brain&rsquo;. &nbsp;I get the impression that it&rsquo;s got a different balance of abilities; like, some neurons got allocated to different areas, other areas got shortchanged, some areas got some extra neurons, other areas got shortchanged, the hypothesis has occurred to me lately that my writing is attracting other people with similar problems because of the extent to which one has noticed a sort of similar tendency to fall on the lines of very reflective, very analytic and has mysterious trouble executing and getting things done and working at sustained regular output for long periods of time, among the people who like my stuff.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">On the whole though, I never actually got around to getting an MRI scan; it&rsquo;s probably a good thing to do one of these days, but this isn&rsquo;t Japan where that sort of thing only costs 100 dollars, and getting it analyzed, you know they&rsquo;re not just looking for some particular thing but just sort of looking at it and saying &lsquo;Hmm, well what is this about your brain?&rsquo;, well I&rsquo;d have to find someone to do that too. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So, I&rsquo;m not neurotypical... asking sort of &lsquo;what else can you tell me about your brain&rsquo; &nbsp;is sort of &lsquo;what else can you tell me about who you are apart from your thoughts&rsquo;, and that&rsquo;s a bit of a large question. I don&rsquo;t try and whack on my brain because it doesn&rsquo;t seem to react typically and I&rsquo;m afraid of being in a sort of narrow local optimum where anything I do is going to knock it off the tip of the local peak, just because it works better than average and so that&rsquo;s sort of what you would expect to find there.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">5. During a panel discussion at the most recent Singularity Summit, Eliezer speculated that he might have ended up as a science fiction author, but then quickly added:</span></p>\n<p style=\"margin-left: 4pt; margin-right: 11pt; text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I have to remind myself that it's not what's the most fun to do, it's not even what you have talent to do, it's what you need to do that you ought to be doing.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Shortly thereafter, Peter Thiel expressed a wish that all the people currently working on string theory would shift their attention to AI or aging; no disagreement was heard from anyone present.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I would therefore like to ask Eliezer whether he in fact believes that the only two legitimate occupations for an intelligent person in our current world are (1) working directly on Singularity-related issues, and (2) making as much money as possible on Wall Street in order to donate all but minimal living expenses to SIAI/Methuselah/whatever.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">How much of existing art and science would he have been willing to sacrifice so that those who created it could instead have been working on Friendly AI? If it be replied that the work of, say, Newton or Darwin was essential in getting us to our current perspective wherein we have a hope of intelligently tackling this problem, might the same not hold true in yet unknown ways for string theorists? And what of Michelangelo, Beethoven, and indeed science fiction? Aren't we allowed to have similar fun today? For a living, even?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So, first, why restrict it to intelligent people in today&rsquo;s world? &nbsp;Why not everyone? &nbsp;And second... the reply to the essential intent of the question is yes, with a number of little details added. So for example, if you&rsquo;re making money on Wall Street, I&rsquo;m not sure you should be donating all but minimal living expenses because that may or may not be sustainable for you. &nbsp;And in particular if you&rsquo;re, say, making 500,000 dollars a year and you&rsquo;re keeping 50,000 dollars of that per year, which is totally not going to work in New York, probably, then it&rsquo;s probably more effective to double your living expenses to 100,000 dollars per year and have the amount donated to the Singularity Institute go from 450,000 to 400,000 when you consider how much more likely that makes it that more people follow in your footsteps. That number is totally not realistic and not even close to the percentage of income donated versus spent on living expenses for present people working on Wall Street who are donors to the Singularity Institute. So considering at present that no one seems willing to do that, I wouldn&rsquo;t even be asking that, but I would be asking for more people to make as much money as possible if they&rsquo;re the sorts of people who can make a lot of money and can donate a substantial amount fraction, never mind all the minimal living expenses, to the Singularity Institute. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Comparative advantage is what money symbolizes; each of us able to specialize in doing what we do best, get a lot of experience doing it, and trade off with other people specialized at what they&rsquo;re doing best with attendant economies of scale and large fixed capital installations as well, that&rsquo;s what money symbolizes, sort of in idealistic reality, as it were; that&rsquo;s what money would mean to someone who could look at human civilization and see what it was really doing. &nbsp;On the other hand, what money symbolizes emotionally in practice, is that it imposes market norms, instead of social norms. If you sort of look at how cooperative people are, they can actually get a lot less cooperative once you offer to pay them a dollar, because that means that instead of cooperating because it&rsquo;s a social norm, they&rsquo;re now accepting a dollar, and a dollar puts it in the realm of market norms, and they become much less altruistic. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So it&rsquo;s sort of a sad fact about how things are set up that people look at the Singularity Institute and think &lsquo;Isn&rsquo;t there some way for me to donate something other than money?&rsquo; partially for the obvious reason and partially because their altruism isn&rsquo;t really emotionally set up to integrate properly with their market norms. &nbsp;For me, money is reified time, reified labor. To me it seems that if you work for an hour on something and then donate the money, that&rsquo;s more or less equivalent to donating the money (time?), or should be, logically. &nbsp;We have very large bodies of experimental literature showing that the difference between even a dollar bill versus a token that&rsquo;s going to be exchanged for a dollar bill at the end of the experiment can be very large, just because that token isn&rsquo;t money. &nbsp;So there&rsquo;s nothing dirty about money, and there&rsquo;s nothing dirty about trying to make money so that you can donate it to a charitable cause; the question is &lsquo;can you get your emotions to line up with reality in this case?&rsquo; </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Part of the question was sort of like &lsquo;What of Michaelangelo, Beethoven, and indeed science fiction? Aren&rsquo;t we allowed to have similar fun today? For a living even?&rsquo; </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">This is crunch time. &nbsp;This is crunch time for the entire human species. This is the hour before the final exam, we are trying to get as much studying done as possible, and it may be that you can&rsquo;t make yourself feel that, for a decade, or 30 years on end or however long this crunch time lasts. But again, the reality is one thing, and the emotions are another. &nbsp;So it may be that you can&rsquo;t make yourself feel that this is crunch time, for more than an hour at a time, or something along those lines. But relative to the broad sweep of human history, this is crunch time; and it&rsquo;s crunch time not just for us, it&rsquo;s crunch time for the intergalactic civilization whose existence depends on us. I think that if you&rsquo;re actually just going to sort of confront it, rationally, full-on, then you can&rsquo;t really justify trading off any part of that intergalactic civilization for any intrinsic thing that you could get nowadays, and at the same time it&rsquo;s also true that there are very few people who can live like that, and I&rsquo;m not one of them myself, so because trying to live with that would even rule out things like ordinary altruism; I hold open doors for little old ladies, because I find that I can&rsquo;t live only as an altruist in theory; I need to commit sort of actual up-front deeds of altruism, or I stop working properly. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So having seen that intergalactic civilization depends on us, in one sense, all you can really do is try not to think about that, and in another sense though, if you spend your whole life creating art to inspire people to fight global warming, you&rsquo;re taking that &lsquo;forgetting about intergalactic civilization&rsquo; thing much too far. If you look over our present civilization, part of that sort of economic thinking that you&rsquo;ve got to master as a rationalist is learning to think on the margins. On the margins, does our civilization need more art and less work on the singularity? I don&rsquo;t think so. I think that the amount of effort that our civilization invests in defending itself against existential risks, and to be blunt, Friendly AI in particular is ludicrously low. &nbsp;Now if it became the sort of pop-fad cause and people were investing billions of dollars into it, all that money would go off a cliff and probably produce anti-science instead of science, because very few people are capable of working on a problem where they don&rsquo;t find immediately whether or not they were wrong, and it would just instantaneously go wrong and generate a lot of noise from people of high prestige who would just drown out the voices of sanity. So wouldn&rsquo;t it be a nice thing if our civilization started devoting billions of dollars to Friendly AI research because our civilization is not set up to do that sanely. &nbsp;But at the same time, the Singularity Institute exists, the Singularity Institute, now that Michael Vassar is running it, should be able to scale usefully; that includes actually being able to do interesting things with more money, now that Michael Vassar&rsquo;s the president.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">To say &lsquo;No, on the margin, what human civilization, at this present time, needs to do is not put more money in the Singularity Institute, but rather do this thing that I happen to find fun&rsquo; not that I&rsquo;m doing this and I&rsquo;m going to professionally specialize in it and become good in it and sort of trade hours of doing this thing that I&rsquo;m very good at for hours that go into the Singularity Institute via the medium of money, but rather &lsquo;no, this thing that I happen to find fun and interesting is actually what our civilization needs most right now, not Friendly AI&rsquo;, that&rsquo;s not defensible; and, you know, these are all sort of dangerous things to think about possibly, but I think if you sort of look at that face-on, up-front, take it and stare at it, there&rsquo;s no possible way the numbers could work out that way.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">It might be helpful to visualize a Friendly Singularity so that the kid who was one year old at the time is now 15 years old and still has something like a 15 year old human psychology and they&rsquo;re asking you &lsquo;So here&rsquo;s this grand, dramatic moment in history, not human history, but history, on which the whole future of the intergalactic civilization that we now know we will build; it hinged on this one moment, and you knew that was going to happen. What were you doing?&rsquo; and you say, &lsquo;Well, I was creating art to inspire people to fight global warming.&rsquo; The kid says &lsquo;What&rsquo;s global warming?&rsquo; </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">That&rsquo;s what you get for not even taking into account at all the whole &lsquo;crunch time, fate of the world depends on it, squeaking through by a hair if we do it at all, already played into a very poor position in terms of how much work has been done and how much work we need to do relative to the amount of work that needs to be done to destroy the world as opposed to saving it; how long we could have been working on this previously and how much trouble it&rsquo;s been to still get started.&rsquo; &nbsp;When this is all over, it&rsquo;s going to be difficult to explain to that kid, what in the hell the human species was thinking. &nbsp;It&rsquo;s not going to be a baroque tale. It&rsquo;s going to be a tale of sheer insanity. &nbsp;And you don&rsquo;t want you to be explaining yourself to that kid afterward as part of the insanity rather than the sort of small core of &lsquo;realizing what&rsquo;s going on and actually doing something about it that got it done.&rsquo;</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">6. I know at one point you believed in staying celibate, and currently your main page mentions you are in a relationship. What is your current take on relationships, romance, and sex, how did your views develop, and how important are those things to you? (I'd love to know as much personal detail as you are comfortable sharing.)</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">This is not a topic on which I consider myself an expert, and so it shouldn&rsquo;t be shocking to hear that I don&rsquo;t have incredibly complicated and original theories about these issues. &nbsp;Let&rsquo;s see, is there anything else to say about that... It&rsquo;s asking &lsquo;at one point I believed in staying celibate and currently your main page mentions your are in a relationship.&rsquo; So, it&rsquo;s not that I believed in staying celibate as a matter of principle, but that I didn&rsquo;t know where I could find a girl who would put up with me and the life that I intended to lead, and said as much, and then one woman, Erin, read about the page I&rsquo;d put up to explain why I didn&rsquo;t think any girl would put up with me and my life and said essentially &lsquo;Pick me! Pick me!&rsquo; and it was getting pretty difficult to keep up with the celibate lifestyle by then so I said &lsquo;Ok!&rsquo; And that&rsquo;s how we got together, and if that sounds a bit odd to you, or like, &lsquo;What!? What do you mean...?&rsquo; then... that&rsquo;s why you&rsquo;re not my girlfriend. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I really do think that in the end I&rsquo;m not an expert; that might be as much as there is to say.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">7. What's your advice for Less Wrong readers who want to help save the human race?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \"> Find whatever you&rsquo;re best at; if that thing that you&rsquo;re best at is inventing new math[s] of artificial intelligence, then come work for the Singularity Institute. &nbsp;If the thing that you&rsquo;re best at is investment banking, then work for Wall Street and transfer as much money as your mind and will permit to the Singularity institute where [it] will be used by other people. &nbsp;And for a number of sort of intermediate cases, if you&rsquo;re familiar with all the issues of AI and all the issues of rationality and you can write papers at a reasonable clip, and you&rsquo;re willing to work for a not overwhelmingly high salary, then the Singularity Institute is, as I understand it, hoping to make a sort of push toward getting some things published in academia. I&rsquo;m not going to be in charge of that, Michael Vassar and Anna Salamon would be in charge of that side of things. &nbsp;There&rsquo;s an internship program whereby we provide you with room and board and you drop by for a month or whatever and see whether or not this is work you can do and how good you are at doing it. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Aside from that, though, I think that saving the human species eventually comes down to, metaphorically speaking, nine people and a brain in a box in a basement, and everything else feeds into that. &nbsp;Publishing papers in academia feeds into either attracting attention that gets funding, or attracting people who read about the topic, not necessarily reading the papers directly even but just sort of raising the profile of the issues where intelligent people wonder what they can do with their lives think artificial intelligence instead of string theory. Hopefully not too many of them are thinking that because that would just generate noise, but the very most intelligent people... string theory is a marginal waste of the most intelligent people. Artificial intelligence and Friendly Artificial Intelligence, sort of developing precise, precision grade theories of artificial intelligence that you could actually use to actually build a Friendly AI instead of blowing up the world; the need for one more genius there is much greater than the need for one more genius in string theory. Most of us can&rsquo;t work on that problem directly. &nbsp;I, in a sense, have been lucky enough not to have to confront a lot of the hard issues here, because of being lucky enough to be able to work on the problem directly, which simplifies my choice of careers. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">For everyone else, I&rsquo;ll just sort of repeat what I said in an earlier video about comparative advantage, professional specialization, doing what we do best at and practicing a lot; everyone doing that and trading with each other is the essence of economics, and the symbol of this is money, and it&rsquo;s completely respectable to work hours doing what you&rsquo;re best at, and then transfer the sort of expected utilons that a society assigns to that to the Singularity Institute, where it can pay someone else to work at it such that it&rsquo;s an efficient trade, because the total amount of labor and effectiveness that they put into it that you can purchase is more than you could do by working an equivalent number of hours on the problem yourself. And as long as that&rsquo;s the case, the economically rational thing to do is going to be to do what you&rsquo;re best at and trade those hours to someone else, and let them do it. And there should probably be fewer people, one expects, who working on the problem directly, full time; stuff just does not get done if you&rsquo;re not working on it full time, that&rsquo;s what I&rsquo;ve discovered, anyway; I can&rsquo;t even do more than one thing at a time. And that&rsquo;s the way grown ups do it, essentially, that&rsquo;s the way a grown up economy does it. </span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">8. Autodidacticism</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Eliezer, first congratulations for having the intelligence and courage to voluntarily drop out of school at age 12! Was it hard to convince your parents to let you do it? AFAIK you are mostly self-taught. How did you accomplish this? Who guided you, did you have any tutor/mentor? Or did you just read/learn what was interesting and kept going for more, one field of knowledge opening pathways to the next one, etc...?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">EDIT: Of course I would be interested in the details, like what books did you read when, and what further interests did they spark, etc... Tell us a little story. ;)</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Well, amazingly enough, I&rsquo;ve discovered the true, secret, amazing formula for teaching yourself and... I lie, I just winged it. &nbsp;Yeah, just read whatever interested me until age 15-16 thereabouts which is when I started to discover the Singularity as opposed to background low-grade Transhumanism that I&rsquo;d been engaged with up until that point; started thinking that cognitive technologies, creating smarter than human level intelligence was the place to be and initially thought that neural engineering was going to be the sort of leading, critical path of that. Studied a bit of neuroscience and didn&rsquo;t get into that too far before I started thinking that artificial intelligence was going to be the route; studied computer programming, studied a bit of business type stuff because at one point I thought I&rsquo;d do a start up at something I&rsquo;m very glad I didn&rsquo;t end up doing, in order to get the money to do the AI thing, and I&rsquo;m very glad that I didn&rsquo;t go that route, and I won&rsquo;t even say that the knowledge has served me all that good instead, it&rsquo;s just not my comparative advantage. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">At some point sort of woke up and smelled the Bayesian coffee and started studying probability theory and decision theory and statistics and that sort of thing, but really I haven&rsquo;t had and opportunity to study anywhere near as much as I need to know. &nbsp;And part of that, I won&rsquo;t apologize for because a lot of sort of fact memorization is more showing off than because you&rsquo;re going to use that fact every single day; part of that I will apologize for because I feel that I don&rsquo;t know enough to get the job done and that when I&rsquo;m done writing the book I&rsquo;m just going to have to take some more time off and just study some of the sort of math and mathematical technique that I expect to need in order to get this done. I come across as very intelligent, but a surprisingly small amount of that relies on me knowing lots of facts, or at least that&rsquo;s the way it feels to me. So I come across as very intelligent, but that&rsquo;s because I&rsquo;m good at winging it, might be one way to put it. The road of the autodidact, I feel... I used to think that anyone could just go ahead and do it and that the only reason to go to college was for the reputational &lsquo;now people can hire you&rsquo; aspect which sadly is very important in today&rsquo;s world. Since then I&rsquo;ve come to realize both that college is less valuable and less important than I used to think and also that autodidacticism might be a lot harder for the average person than I thought because the average person is less similar to myself than my sort of intuitions would have it. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">&lsquo;How do you become an autodidact&rsquo;; the question you would ask before that would be &lsquo;what am I going to do, and is it something that&rsquo;s going to rely on me having memorized lots of standard knowledge and worked out lots of standard homework problems, or is it going to be something else, because if you&rsquo;re heading for a job where you going to want to memorize lots of the same standardized facts as people around you, then autodidacticism might not be the best way to go. If you&rsquo;re going to be a computer programmer, on the other hand, then [going] into a field where every day is a new adventure, and most jobs in computer programming will not require you to know the Nth detail of computer science, and even if they did, the fact that this is math means you might even have a better chance of learning it out of a book, and above all it&rsquo;s a field where people have some notion that you&rsquo;re allowed to teach yourself; if you&rsquo;re good, other people can see it by looking at your code, and so there&rsquo;s sort of a tradition of being willing to hire people who don&rsquo;t have a Masters. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So I guess I can&rsquo;t really give all that much advice about how to be successful autodidact in terms of... studying hard, doing the same sort of thing you&rsquo;d be doing in college only managing to do it on your own because you&rsquo;re that self-disciplined, because that is completely not the route I took. I would rather advise you to think very hard about what it is you&rsquo;re going to be doing, whether or not anyone will let you do it if you don&rsquo;t have the official credential, and to what degree the road you&rsquo;re going is going to depend on the sort of learning that you have found that you can get done on your own.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">9. Is your pursuit of a theory of FAI similar to, say, Hutter's AIXI, which is intractable in practice but offers an interesting intuition pump for the implementers of AGI systems? Or do you intend on arriving at the actual blueprints for constructing such systems? I'm still not 100% certain of your goals at SIAI.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Definitely actual blueprint, but, on the way to an actual blueprint, you probably have to, as an intermediate step, construct intractable theories that tell you what you&rsquo;re trying to do, and enable you to understand what&rsquo;s going on when you&rsquo;re trying to do something. If you want a precise, practical AI, you don&rsquo;t get there by starting with an imprecise, impractical AI and going to a precise, practical AI. You start with a precise, impractical AI and go to a precise, practical AI. I probably should write that down somewhere else because it&rsquo;s extremely important, and as(?) various people who will try to dispute it, and at the same time hopefully ought to be fairly obvious if you&rsquo;re not motivated to arrive at a particular answer there. You don&rsquo;t just run out and construct something imprecise because, yeah, sure, you&rsquo;ll get some experimental observations out of that, but what are your experimental observations telling you? &nbsp;And one might say along the lines of &lsquo;well, I won&rsquo;t know that until I see it,&rsquo; and suppose that has been known to happen a certain number of times in history; just inventing the math has also happened a certain number of times in history. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">We already have a very large body of experimental observations of various forms of imprecise AIs, both the domain specific types we have now, and the sort of imprecise AI constituted by human beings, and we already have a large body of experimental data, and eyeballing it... well, I&rsquo;m not going to say it doesn&rsquo;t help, but on the other hand, we already have this data and now there is this sort of math step in which we understand what exactly is going on; and then the further step of translating the math back into reality. It is the goal of the Singularity Institute to build a Friendly AI. That&rsquo;s how the world gets saved, someone has to do it. A lot of people tend to think that this is going to require, like, a country&rsquo;s worth of computing power or something like that, but that&rsquo;s because the problem seems very difficult because they don&rsquo;t understand it, so they imagine throwing something at it that seems very large and powerful and gives this big impression of force, which might be a country-size computing grid, or it might be a Manhattan Project where some computer scientists... but size matters not, as Yoda says. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">What matters is understanding, and if the understanding is widespread enough, then someone is going to grab the understanding and use it to throw together the much simpler AI that does destroy the world, the one that&rsquo;s build to much lower standards, so the model of &lsquo;yes, you need the understanding, the understanding has to be concentrated within a group of people small enough that there is not one defector in the group who goes off and destroys the world, and then those people have to build an AI.&rsquo; &nbsp;If you condition on that the world got saved, and look back and within history, I expect that that is what happened in the majority of cases where a world anything like this one gets saved, and working back from there, they will have needed a precise theory, because otherwise they&rsquo;re doomed. You can make mistakes and pull yourself up, even if you think you have a precise theory, but if you don&rsquo;t have a precise theory then you&rsquo;re completely doomed, or if you don&rsquo;t think you have a precise theory then you&rsquo;re completely doomed. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">And working back from there, you probably find that there were people spending a lot of time doing math based on the experimental results that other people had sort of blundered out into the dark and gathered because it&rsquo;s a lot easier to blunder out into the dark; more people can do it, lots more people have done it; it&rsquo;s the math part that&rsquo;s really difficult. &nbsp;So I expect that if you look further back in time, you see a small group of people who had honed their ability to understand things to a very high pitch, and then were working primarily on doing math and relying on either experimental data that other people had gathered by accident, or doing experiments where they have a very clear idea why they&rsquo;re doing the experiment and what different results will tell them. </span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">10. What was the story purpose and/or creative history behind the legalization and apparent general acceptance of non-consensual sex in the human society from Three Worlds Collide?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">The notion that non-consensual sex is not illegal and appears to be socially accepted might seem a bit out of place in the story, as if it had been grafted on. &nbsp;This is correct. &nbsp;It was grafted on from a different story in which, for example, theft is while not so much legal, because they don&rsquo;t have what would you call a strong, centralized government, but rather, say, theft is, in general, something you pull off by being clever rather than a horrible crime; but of course, you would never steal a book. I have yet to publish a really good story set in this world; most of them I haven&rsquo;t finished, the one I have finished &nbsp;has other story problems. &nbsp;But if you were to see the story set in this world, then you would see that it develops out of a much more organic thing than say... dueling, theft, non-consensual sex; all of these things are governed by tradition rather than by law, and they certainly aren&rsquo;t prohibited outright. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So why did I pick up that one aspect form that story and put it into Three Worlds Collide? &nbsp;Well, partially it was because I wanted that backpoint to introduce a culture clash between their future and our past, and that&rsquo;s what came to mind, more or less, it was more something to test out to see what sort of reaction it got, to see if I could get away with putting it into this other story. Because one can&rsquo;t use theft; Three Worlds Collide&rsquo;s society actually does run on private propety. One can&rsquo;t use dueling; their medical technology isn&rsquo;t advanced enough to make that trivial. But you can use non-consensual sex and try to explain sort of what happens in a society in which people are less afraid, and not afraid of the same things. They&rsquo;re stronger than we are in some senses, they don&rsquo;t need as much protection, the consequences aren&rsquo;t the same consequences that we know, and the people there sort of generally have a higher grade of ethics and are less likely to abuse things. &nbsp;That&rsquo;s what made that sort of particular culture clash feature a convenient thing to pick up from one story and graft onto another, but ultimately it was a graft, and any feelings of &lsquo;why is that there?&rsquo; that you have, might make a bit more sense if you saw the other story, if I can ever repair the flaws in it, or manage to successfully complete and publish a story set in that world that actually puts the world on display.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">11. If you were to disappear (freak meteorite accident), what would the impact on FAI research be?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Do you know other people who could continue your research, or that are showing similar potential and working on the same problems? Or would you estimate that it would be a significant setback for the field (possibly because it is a very small field to begin with)?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Marcello Herreshoff is the main person whom I&rsquo;ve worked with on this, and Marcello doesn&rsquo;t yet seem to be to the point where he could replace me, although he&rsquo;s young so he could easily develop further in coming years and take over as the lead, or even, say, &lsquo;Aha! Now I&rsquo;ve got it! No more need for Eliezer Yudkowsky.&rsquo; That sort of thing would be very nice if it happened, but it&rsquo;s not the sort of thing I would rely on. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So if I got hit by a meteor right now, what would happen is that Michael Vassar would take over responsibility for seeing the planet through to safety, and say &lsquo;Yeah I&rsquo;m personally just going to get this done, not going to rely on anyone else to do it for me, this is my problem, I have to handle it.&rsquo; And Marcello Herreshoff would be the one who would be tasked with recognizing another Eliezer Yudkowsky if one showed up and could take over the project, but at present I don&rsquo;t know of any other person who could do that, or I&rsquo;d be working with them. &nbsp;There&rsquo;s not really much of a motive in a project like this one to have the project split into pieces; whoever can do work on it is likely to work on it together.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">12. Your approach to AI seems to involve solving every issue perfectly (or very close to perfection). Do you see any future for more approximate, rough and ready approaches, or are these dangerous?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">More approximate, rough and ready approaches might produce interesting data that math theorist types can learn something from even though the people who did it didn&rsquo;t have that in mind. The thing is, though, there&rsquo;s already a lot of people running out and doing that and really failing at AI, or even approximate successes at AI, result in much fewer sudden thunderbolts of enlightenment about the structure of intelligence than the people that are busily producing ad hoc AI programs because that&rsquo;s easier to do and you can get a paper out of it and you get respect out of it and prestige and so on. So it&rsquo;s a lot harder for that sort of work to result in sudden thunderbolts of enlightenment about the structure of intelligence than the people doing it would like to think, because that way it gives them an additional justification for doing the work. The basic answer to the question is &lsquo;no&rsquo;, or at least I don&rsquo;t see a future for Singularity Institute funding, going as marginal effort, into sort of rough and ready &lsquo;forages&rsquo; like that. &nbsp;It&rsquo;s been done already. If we had more computer power and our AIs were more sophisticated, then the level of exploration that we&rsquo;re doing right now would not be a good thing, as it is, it&rsquo;s probably not a very dangerous thing because the AIs are weak more or less. It&rsquo;s not something you would ever do with AI that was powerful enough to be dangerous. If you know what it is that you want to learn by running a program, you may go ahead and run it; if you&rsquo;re just foraging out at random, well other people are doing that, and even then they probably won&rsquo;t understand what their answers mean until you on your end, the sort of math structure of intelligence type people, understand what it means. And mostly the result of an awful lot of work in domain specific AIs tell us that we don&rsquo;t understand something, and this can often be surprisingly easy to figure out, simply by querying your brain without being overconfident. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So, I think that at this point, what&rsquo;s needed is math structure of intelligence type understanding, and not just any math, not just &lsquo;Ooh, I&rsquo;m going to make a bunch of Greek symbols and now I can publish a paper and everyone will be impressed by how hard it is to understand,&rsquo; but sort of very specific math, the sort that results in thunderbolts of enlightenment; the usual example I hold up is the Bayesian Network Causality insight as depicted in Judea Pearl&rsquo;s Probabilistic Reasoning in Intelligent Systems and (later book of causality?). So if you sort of look at the total amount of papers that have been written with neat Greek symbols and things that are mathematically hard to understand and compare that to those Judea Pearl books I mentioned, though one should always mention this is the culmination of a lot of work not just by Judea Pearl; that will give you a notion of just how specific the math has to be. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">In terms of solving every issue perfectly or very close to perfection, there&rsquo;s kinds of perfection. As long as I know that any proof is valid, I might not know how long it takes to do a proof; if there&rsquo;s something that does &nbsp;proof, then I may not know how long the algorithm takes to produce a proof but I may know that anything it claims is a proof is definitely a proof, so there&rsquo;s different kinds of perfection and types of precision. &nbsp;But basically, yeah, if you want to build a recursively self-improving AI, have it go through a billion sequential self-modifications, become vastly smarter than you, and not die, you&rsquo;ve got to work to a pretty precise standard.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">13. How young can children start being trained as rationalists? And what would the core syllabus / training regimen look like?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I am not an expert in the education of young children. One has these various ideas that one has written up on Less Wrong, and one could try to distill those ideas, popularize them, illustrate them through simpler and simpler stories and so take these ideas and push them down to a lower level, but in terms of sort of training basic though skills, training children to be self-aware, to be reflective, getting them into the habit of reading and storing up lots of pieces of information, trying to get them more interested in being fair to both sides of an argument, the virtues of honest curiosity over rationalization, not in the way that I do it by sort of telling people and trying to lay out stories and parables that illustrate it and things like that, but if there&rsquo;s some other way to do it with children, I&rsquo;m not sure that my grasp of this concept of teaching rationality extends to before the young adult level. &nbsp;I believe that we had some sort of thread on Less Wrong about this, sort of recommended reading for young rationalists, I can&rsquo;t quite remember. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Oh, but one thing that does strike me as being fairly important is that if this ever starts to happen on a larger scale and individual parents teaching individual children, the number one thing we want to do is test out different approaches and see which one works experimentally.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">14. Could you elaborate a bit on your \"infinite set atheism\"? How do you feel about the set of natural numbers? What about its power set? What about that thing's power set, etc?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">From the other direction, why aren't you an ultrafinitist?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">The question is &lsquo;can you elaborate on your infinite set atheism&rsquo;, that&rsquo;s where I say &lsquo;I don&rsquo;t believe in infinite sets because I&rsquo;ve never seen one.&rsquo; &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So first of all, my infinite set atheism is a bit tongue-in-cheek. I mean, I&rsquo;ve seen a whole lot of natural numbers, and I&rsquo;ve seen that times tend to have successor times, and in my experience, at least, time doesn&rsquo;t return to its starting point; as I understand current cosmology, the universe is due to keep on expanding, and not return to its starting point. So it&rsquo;s entirely possible that I&rsquo;m faced with certain elements that have successors where if the successors of two elements are the same and the two elements are the same, in which there&rsquo;s no cycle. So in that sense I might be forced to recognize the empirical existence of every member of what certainly looks like an infinite set. &nbsp;As for the question of whether this collection of infinitely many finite things constitutes an infinite thing exists is an interesting metaphysical one, or it would be if we didn&rsquo;t have the fact that even though by looking at time we can see that it looks like infinite things ought to exist, nonetheless, we&rsquo;ve never encountered an infinite thing in certain, in person. We&rsquo;ve never encountered a physical process that performs a super task. If you look more at physics, you find that actually matters are even worse than this. &nbsp;We&rsquo;ve got real numbers down there, or at least if you postulate that it&rsquo;s something other than real numbers underlying physics then you have to postulate something that looks continuous but isn&rsquo;t continuous, and in this way, by Occam&rsquo;s Razor, one might very easily suspect that the appearance of continuity arises from actual continuity, so that we have, say, an amplitude distribution, a neighborhood in configuration space, and the amplitude[s that] flows in configuration space are continuous, instead of having a discrete time with a discrete successor, we actually have a flow of time, so when you write the rules of causality, it&rsquo;s not possible to write the rules of causality the way we write them for a Turing machine, you have to write the rules of causality as differential equations. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So these are the two main cases in which the universe is defined by infinite set atheism. &nbsp;The universe is handing me what looks like an infinite collection of things, namely times; the universe is handing me things that exist and are causes and the simplest explanation would have them being described by continuous differential equations, not by discrete ticks. &nbsp;So that&rsquo;s the main sense in which my infinite set atheism is challenged by the universe&rsquo;s actual presentation of things to me of things that look infinite. Aside from this, however, if you start trying to hand me paradoxes that are being produced by just assuming that you have an infinite thing in hand as an accomplished fact, an infinite thing of the sort where you can&rsquo;t just present to me a physical example of it, you&rsquo;re just assuming that that infinity exists, and then you&rsquo;re generating paradoxes from it, well, we do have these nice mathematical rules for reasoning about infinities, but, rather than putting the blame on the person for having violated these elaborate mathematical rules that we develop to reason about infinities, I&rsquo;m even more likely to cluck my tongue and say &lsquo;But what good is it?&rsquo; Now it may be a tongue-in-cheek tongue cluck... I&rsquo;m trying to figure out how to put this into words... &nbsp;Map that corresponds to the territory, if you can&rsquo;t have infinities in your map, because your neurons, they fire discretely, and you only have a finite number of neurons in your head, so if you can&rsquo;t have infinities in the map, what makes you think that you can make them correspond to infinities in the territory, especially if you&rsquo;ve never actually seen that sort of infinity? And so the sort of math of the higher infinities, I tend to view as works of imaginative literature, like Lord of the Rings; they may be pretty, in the same way that Tolkien Middle Earth is pretty, but they don&rsquo;t correspond to anything real until proven otherwise.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">15. Why do you have a strong interest in anime, and how has it affected your thinking?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">&lsquo;Well, as a matter of sheer, cold calculation I decided that...&rsquo; </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">It&rsquo;s </span><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">anime</span><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">! (laughs)</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">How has it affected my thinking? I suppose that you could view it as a continuity of reading dribs and drabs of westernized eastern philosophy from Godel, Escher, Bach or Raymon Smullyan, concepts like &lsquo;Tsuyoku Naritai&rsquo;, &lsquo;I want to become stronger&rsquo;, are things that being exposed to the alternative eastern culture as found in anime might have helped me to develop concepts of. &nbsp;But on the whole... it&rsquo;s anime! There&rsquo;s not some kind of elaborate calculation behind it, and I can&rsquo;t quite say that when I&rsquo;m encountering a daily problem, I think to myself &lsquo;How would Light Yagami solve this?&rsquo; If the point of studying a programing language is to change the way you think, then I&rsquo;m not sure that studying anime has change the way I think all that much.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">16. What are your current techniques for balancing thinking and meta-thinking?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">For example, trying to solve your current problem, versus trying to improve your problem-solving capabilities.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I tend to focus on thinking, and it&rsquo;s only when my thinking gets stuck or I run into a particular problem that I will resort to meta-thinking, unless it&rsquo;s a particular meta skill that I already have, in which case I&rsquo;ll just execute it. For example, the meta skill of trying to focus on the original problem. &nbsp;In one sense, a whole chunk of Less Wrong is more or less my meta-thinking skills. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So I guess on reflection (ironic look), I would say that there&rsquo;s a lot of routine meta-thinking that I already know how to do, and that I do without really thinking of it as meta-thinking. On the other hand, original meta-thinking, which is the time consuming part is something I tend to resort to only when my current meta-thinking skills have broken down. &nbsp;And that&rsquo;s probably a reasonably exceptional circumstance even though it&rsquo;s something of comparative advantage and so I expect it to do a bit more of it than average. &nbsp;Even so, when I&rsquo;m trying to work on an object-level problem at any given point, I&rsquo;m probably not doing original meta-level questioning about how to execute these meta-level skills. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">If I bog down in writing something I may execute my sort of existing meta-level skill of &lsquo;try to step back and look at this from a more abstract level&rsquo;, and if that fails, then I may have to sort of think about what kind of abstract levels can you view this problem on, similar problems as opposed to tasks, and in that sense go into original meta-level thinking mode. But one of those meta-level skills I would say is the notion that your meta-level problem comes from an object-level problem and you&rsquo;re supposed to keep one eye on the object-level problem the whole time you&rsquo;re working on the meta-level.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">17. Could you give an uptodate estimate of how soon non-Friendly general AI might be developed? With confidence intervals, and by type of originator (research, military, industry, unplanned evolution from non-general AI...)</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">We&rsquo;re talking about this very odd sector of program space and programs that self-modify and wander around that space and sort of amble into a pot of gold that enables them to keep going and... I have no idea...</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">There are all sorts of different ways that it could happen, I don&rsquo;t know which one of them are plausible or implausible or how hard or difficult they are relative to modern hardware or computer science. I have no idea what the odds are; I know they aren&rsquo;t getting any better as time goes on or that is, the probabilities of Unfriendly AI are increasing over time. So if you were actually to &nbsp;make some kind of graph, then you&rsquo;d see the probability rising over time as the odds got worse, and then the graph would slope down again as you entered into regions where it was more likely than not that Unfriendly AI had actually occurred before that; the slope would actually fall off faster as you went forward in time because the amount of probability mass has been drained away by Unfriendly AI happening now.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">&lsquo;By type of originator&rsquo; or something, I might have more luck answering. I would put academic research at the top of it, because academic research that actually can try blue sky things. Or... OK, first commercial, that wasn&rsquo;t quite on the list, as in people doing startup-ish things, hedge funds, people trying to improve the internal AI systems that they&rsquo;re using for something, or build weird new AIs to serve commercial needs; those are the people most likely to build AI &lsquo;stews&rsquo;(?) &nbsp;Then after that, academic research, because in academia you have a chance of trying blue sky things. And then military, because they can hire smart people and give the smart people lots of computing power and have a sense of always trying to be on the edge of things. Then industry, if that&rsquo;s supposed to mean car factories and so on because... that actually strikes me as pretty unlikely; they&rsquo;re just going to be trying to automate ordinary processes, that sort of thing, it&rsquo;s generally unwise to sort of push the bounds of theoretical limits while you&rsquo;re trying to do that sort of thing; you can count Google as industry, but that&rsquo;s the sort of thing I had in mind when I was talking about commercial. &nbsp;Unplanned evolution from non-general AI [is] not really all that likely to happen. These things aren&rsquo;t magic. If something can happen by itself spontaneously, it&rsquo;s going to happen before that because humans are pushing on it. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">As for confidence intervals... doing that just feels like pulling numbers out of thin air. &nbsp;I&rsquo;m kind of reluctant to do it because of the extent to which I feel that, even to the extent that my brain has a grasp on this sort of thing; by making up probabilities and making up times, I&rsquo;m not even translating the knowledge that I do have into reality, so much as pulling things out of thin air. And if you were to sort of ask &lsquo;what do sort of attitude do your revealed actions indicate?&rsquo; then I would say that my revealed actions don&rsquo;t indicate that I expect to die tomorrow of Unfriendly AI, and my revealed actions don&rsquo;t indicate that we can safely take until 2050. And that&rsquo;s not even a probability estimate, that&rsquo;s sort of looking at what I&rsquo;m doing and trying to figure out what my brain thinks the probabilities are.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">18. What progress have you made on FAI in the last five years and in the last year?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">The last five years would take us back to the end of 2004, which is fairly close to the beginning of my Bayesian enlightenment, so the whole &lsquo;coming to grasps with the Bayesian structure of it all&rsquo;, a lot of that would fall into the last five years. &nbsp;And if you were to ask me... the development of Timeless Decision Theory would be in the last five years. &nbsp;I&rsquo;m tyring to think if there&rsquo;s anything else I can say about that. &nbsp;Getting a lot of clarification of what the problems were. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">In the last year, I managed to get in a decent season of work with Marcello after I stopped regular posting to OBLW over the summer, before I started writing the book. That, there&rsquo;s not much I can say about; there was something I suspected was going to be a problem and we tried to either solve the problem or at least nail down exactly what the problem was, and i think that we did a fairly good job of the latter, we now have a nice precise, formal explanation of what it is we want to do and why we can&rsquo;t do it in the obvious way; we came up with sort of one hack for getting around it that&rsquo;s a hack and doesn't have all the properties that we want a real solution to have. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So, step one, figure out what the problem is, step two, understand the problem, and step three, solve the problem. &nbsp;Some degree of progress on step two but not finished with it, and we didn&rsquo;t get to step three, but that&rsquo;s not overwhelmingly discouraging. &nbsp;Most of the real progress that has been made when we sit down and actually work on the problem [are] things I&rsquo;d rather not talk about and the main exception to that is Timeless Decision Theory which has been posted to Less Wrong.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">19. How do you characterize the success of your attempt to create rationalists?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">It&rsquo;s a bit of an ambiguous question, and certainly an ongoing project. Recently, for example, I was in a room with a group of people with a problem of what Robin Hanson called a far-type and what I would call the type where it&rsquo;s difficult because you don&rsquo;t get immediate feedback when you say something stupid, and it really was clear who in that room was an &lsquo;X-rationalist&rsquo; or &lsquo;neo-rationalist&rsquo;, or &lsquo;Lesswrongian&rsquo; or &lsquo;Lessiath&rsquo; and who was not. The main distinction was that the sort of non-X-rationalists were charging straight off and were trying to propose complicated policy solutions right off the bat, and the rationalists were actually holding off, trying to understand the problem, break it down into pieces, analyze the pieces modularly, and just that one distinction was huge; it was the difference between &lsquo;these are the people who can make progress on the problem&rsquo; and &lsquo;these are the people who can&rsquo;t make progress on the problem&rsquo;. So in that sense, once you hand this deep, Lesswrongian types a difficult problem, the distinction between them and someone who has merely had a bunch of successful life experiences and so on is really obvious. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">There&rsquo;s a number of other interpretations that can be attached to the question, but I don&rsquo;t really know what it means aside from that, even though it was voted up by 17 people.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">20. What is the probability that this is the ultimate base layer of reality?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I would answer by saying, hold on, this is going to take me a while to calculate... um.... uh... um... 42 percent! (sarcastic)</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">21. Who was the most interesting would-be FAI solver you encountered?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Most people do not spontaneously try to solve the FAI problem. &nbsp;If they&rsquo;re spontaneously doing something, they try to solve the AI problem. If we&rsquo;re talking about sort of &lsquo;who&rsquo;s made interesting progress on FAI problems without being a Singularity Institute Eliezer supervised person,&rsquo; then I would have to say: Wei Dai.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">22. If Omega materialized and told you Robin was correct and you are wrong, what do you do for the next week? The next decade?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">If Robin&rsquo;s correct, then we&rsquo;re on a more or less inevitable path to competing intelligences driving existence down to subsistence level, but this does not result in the loss of everything we regard as valuable, and there seem to be some values disputes here, or things that are cleverly disguised as values disputes while probably not being very much like values disputes at all.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I&rsquo;m going to take the liberty of reinterpreting this question as &lsquo;Omega materializes and tells you &ldquo;You&rsquo;re Wrong&rdquo;&rsquo;, rather than telling me Robin in particular is right; for one thing that&rsquo;s a bit more probable. And, Omega materializes and tells me &lsquo;Friendly AI is important but you can make no contribution to that problem, in fact everything you&rsquo;ve done so far is worse than nothing.&rsquo; So, publish a retraction... Ordinarily I would say that the next most important thing after this is to go into talking about rationality, but then if Omega tells me that I&rsquo;ve actually managed to do worse than nothing on Friendly AI, that of course has to change my opinion of how good I am at rationality or teaching others rationality, unless this is a sort of counterfactual surgery type of thing where it doesn&rsquo;t affect my opinion of how useful I can be by teaching people rationality, and mostly the thing I&rsquo;d be doing if Friendly AI weren&rsquo;t an option would probably be pushing human rationality. And if that were blocked out of existence, I&rsquo;d probably end up as a computer programmer whose hobby was writing science fiction.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I guess I have enough difficulty visualizing what it means for Robin to be correct or how the human species isn&rsquo;t just plain screwed in that situation that I could wish that Omega had materialized and either told me someone else was correct or given me a bit more detail about what I was wrong about exactly; I mean I can&rsquo;t be wrong about everything; I think that two plus two equals four.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">23. In one of the discussions surrounding the AI-box experiments, you said that you would be unwilling to use a hypothetical fully general argument/\"mind hack\" to cause people to support SIAI. You've also repeatedly said that the friendly AI problem is a \"save the world\" level issue. Can you explain the first statement in more depth? It seems to me that if anything really falls into \"win by any means necessary\" mode, saving the world is it.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Ethics are not pure personal disadvantages that you take on for others&rsquo; benefit. Ethics are not just penalties to the current problem you&rsquo;re working on that have sort of side benefits for other things. When I first started working on the Singularity problem, I was making non-reductionist type mistakes about Friendly AI, even though I thought of myself as a rationalist at the time. And so I didn&rsquo;t quite realize that Friendly AI was going to be a problem, and I wanted to sort of go all-out on any sort of AI, as quickly as possible; and actually, later on when I realized that Friendly AI was an issue, the sort of sneers that I now get about not writing code or being a luddite were correctly anticipated by my past self with the result that my past self sort of kept on advocating the kind of &lsquo;rush ahead and write code&rsquo; strategy, rather than face the sneers, instead of going back and replanning everything from scratch once my past self realized that Friendly AI was going to be an issue, on which basis all the plans had been made before then. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So if I&rsquo;d lied to get people to do what I had wanted them to do at that point, to just get AI done, to rush ahead and write code rather than doing theory; being honest as I actually was, I could just come back and say &lsquo;OK, here&rsquo;s what I said, I&rsquo;m honestly mistaken, here&rsquo;s the new information that I encountered that caused me to change my mind, here&rsquo;s the new strategy that we need to use after taking this new information into account&rsquo;. If you lie, there&rsquo;s not necessarily any equally easy way to retract your lies. ... So for example, one sort of lie that I used to hear advocated back in the old days was by other people working on AI projects and it was something along the lines of &lsquo;AI is going to be safe and harmless and will inevitably cure cancer, but not really take over the world or anything&rsquo; and if you tell that lie in order to get people to work on your AI project, then it&rsquo;s going to be a bit more difficult to explain to them why you suddenly have to back off and do math and work on Friendly AI. Now, if I were an expert liar, I&rsquo;d probably be able to figure out some sort of way to reconfigure those lies as well, I mean I don&rsquo;t really know what an expert liar could accomplish by way of lying because I don&rsquo;t have enough practice. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So I guess in that sense it&rsquo;s not all that defensible... a defensive ethics, because I haven&rsquo;t really tried it both ways, but it does seem to me, looking over my history, my ethics have played a pretty large role in protecting me from myself. Another example is [that] the whole reason that I originally pursued the thought of Friendly AI long enough to realize that it was important was not so much out of a personal desire as out of a sense that this was something I owed to the other people who were funding the project, Brian Atkins in particular back then, and that if there&rsquo;s a possibility from their perspective that you can do better by Friendly AI, or that a fully honest account would cause them to go off and fund someone who was more concerned about Friendly AI, then I owed it to them to make sure that they didn&rsquo;t suffer by helping me. And so it was a sense of ethical responsibility for others at that time which cause me to focus in on this sort of small, discordant note, &lsquo;Well, this minor possibility that doesn&rsquo;t look all that important, follow it long enough to get somewhere&rsquo;. So maybe there are people who could defend the Earth by any means necessary and recruit other people to defend the Earth by any means necessary, and nonetheless have that all and well and happily smiling ever after, rather than bursting into flames and getting arrested for murder and robbing banks and being international outlaws, or more likely just arrested and attracting the &lsquo;wrong&rsquo; sort of people who are trying to go along with this and people being corrupted by power and deciding that &lsquo;no, the world really would be a better place with them in charge&rsquo; and etcetera etcetera etcetera. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I think if you sort of survey the Everett branches of the Many Worlds and look at the ones with successful Singularities, or pardon me, look at the conditional probability of successful Singularities, my guess is that the worlds that start out with programming teams who are trying to play it ethical versus the worlds that start off with programming teams that figure &lsquo;well no, this is a planetary-class problem, we should throw away all our ethics and do whatever is necessary to get it done&rsquo; that the former world will have a higher proportion of happy outcomes. &nbsp;I could be mistaken, but if it does take a sort of master ruthless type person to do it optimally, then I am not that person, and that is not my comparative advantage, and I am not really all that willing to work with them either; so I supposed if there was any way you could end up with two Friendly AI projects, then I suppose the possibility of there actually being sort of completely ruthless programmers versus ethical programmers, they might both have good intentions and separate into two groups that refuse to work with one another, but I&rsquo;m sort of skeptical about these alleged completely ruthless altruists. Has there ever, in history, been a completely ruthless altruist with that turning out well. Knut Haukelid, if I&rsquo;m pronouncing his name correctly, the guy who blew up a civilian ferry in order to sink the Deuterium that the Nazis needed for their nuclear weapons program; you know you never see that in a Hollywood movie; so you killed civilians and did it to end the Nazi nuclear weapons program. So that&rsquo;s about the best historical example I can think of a ruthless altruist and it turns out well, and I&rsquo;m not really sure that&rsquo;s quite enough to persuade me, to give up my ethics.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">24. What criteria do you use to decide upon the class of algorithms / computations / chemicals / physical operations that you consider \"conscious\" in the sense of \"having experiences\" that matter morally? I assume it includes many non-human animals (including wild animals)? Might it include insects? Is it weighted by some correlate of brain / hardware size? Might it include digital computers? Lego Turing machines? China brains? Reinforcement-learning algorithms? Simple Python scripts that I could run on my desktop? Molecule movements in the wall behind John Searle's back that can be interpreted as running computations corresponding to conscious suffering? Rocks? How does it distinguish interpretations of numbers as signed vs. unsigned, or ones complement vs. twos complement? What physical details of the computations matter? Does it regard carbon differently from silicon?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">This is something that I don&rsquo;t know, and would like to know. What you&rsquo;re really being asked is &lsquo;what do you consider as people? &nbsp;Who you consider as people is a value. How can you not know what your own values are?&rsquo; &nbsp;Well, for one, it&rsquo;s very easy to not know what your own values are. And for another thing, my judgement of what is a person, I do want to rely, if I can, about the notion of &lsquo;what has... (hesitant) subjective experience&rsquo;. For example, one reason that I&rsquo;m not very concerned about my laptop&rsquo;s feelings is because I&rsquo;m fairly sure that whatever else is going on in there, it&rsquo;s not &lsquo;feeling&rsquo; it. And this is really something I wish I knew more about. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">And the number one reason I wish I knew more about it is because the most accurate possible model of a person is probably a person; not necessarily the same person, but if you had an Unfriendly AI and it was looking at a person and using huge amounts of computing power, or just very efficient computing power, to model that person and predict the next event as accurately and as precisely as it could, then its model of that person might not be the same person, but it would probably be a person in its own right. So, one of the problems that I don&rsquo;t even try talking to other AI researchers about, because it&rsquo;s so much more difficult than what they signed up to handle that I just assume that they don&rsquo;t want to hear about it; I&rsquo;ve confronted them with much less difficult sounding problems like this and they just make stuff up or run away, and don&rsquo;t say &lsquo;Hmm, I better solve this problem before I go on with my plans to... destroy the world,&rsquo; or whatever it is they think they&rsquo;re doing. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">But in terms of danger points; three example danger points. &nbsp;First, if you have an AI with a pleasure-pain reinforcement architecture and any sort of reflectivity, the ability to sort of learn about its own thoughts and so on, then I might consider that a possible danger point, because then, who knows, it might be able to hurt and be aware that it was hurting; in particular because pleasure-pain reinforcement architecture is something that I think of as an evolutionary legacy architecture rather than an incredibly brilliant way to do things; that scenario space is easy to clear out of. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">If you had an AI with terminal values over how it was treated and its role in surrounding social networks; like you had an AI that could... just, like, not as a means to an end but just, like, in its own right, the fact that you are treating it as a non-person; even if you don&rsquo;t know whether or not it was feeling that about that, you might still be treading into territory where, just for the sake of safety, it might be worth steering out of it in terms of what we would consider as a person. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Oh, and the third consideration is that if your AI spontaneously starts talking about the mystery of subjective experience and/or the solved problem of subjective experience, and a sense of its own existence, and whether or not it seems mysterious to the AI; it could be lying, but you are now in probable trouble; you have wandered out of the safe zone. &nbsp;And conversely, as long as we go on about building AIs that don&rsquo;t have pleasure, pain, and internal reflectivity, and anything resembling social emotions or social terminal values, and that exhibit no signs at all of spontaneously talking about a sense of their own existence, we&rsquo;re hopefully still safe. I mean ultimately, if you push these things far enough without knowing what your doing, sooner or later you&rsquo;re going to open the black box that contains the black swan surprise from hell. But at least as long as you sort of steer clear of those three land mines, and things just haven&rsquo;t gone further and further and further, it gives you a way of looking at a pocket calculator and saying that the pocket calculator is probably safe. </span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">25. I admit to being curious about various biographical matters. So for example I might ask: What are your relations like with your parents and the rest of your family? Are you the only one to have given up religion?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">As far as I know I&rsquo;m the only one in my family to give up religion except for one grand-uncle. I still talk to my parents, still phone calls and so on, amicable relations and so on. They&rsquo;re Modern Orthodox Jews, and mom&rsquo;s a psychiatrist and dad&rsquo;s a physicist, so... &lsquo;Escher painting&rsquo; minds; thinking about some things but always avoiding the real weak points of their beliefs and developing more and more complicated rationalizations. I tried confronting them directly about it a couple of times and each time have been increasingly surprised at the sheer depth of tangledness in there. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I might go on trying to confront them about it a bit, and it would be interesting to see what happens to them if i finish my rationality book and they read it. But certainly among the many things to resent religion for is the fact that I feel that it prevents me from having the sort of family relations that I would like; that I can&rsquo;t talk with my parents about a number of things that I would like to talk with them about. The kind of closeness that I have with my fellow friends and rationalists is a kind of closeness that I can never have with them; even though they&rsquo;re smart enough to learn the skills, they&rsquo;re blocked off by this boulder of religion squatting in their minds. &nbsp;That may not be much to lay against religion, it&rsquo;s not like I&rsquo;m being burned at the stake, or even having my clitoris cut off, but it is one more wound to add to the list. And yeah, I resent it.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">I guess even when I do meet with my parents and talk with my parents, the fact of their religion is never very far from my mind. It&rsquo;s always there as the block, as a problem to be solved that dominates my attention, as something that prevented me from saying the things I want to say, and as the thing that&rsquo;s going to kill them when they don&rsquo;t sign up for cryonics. My parents may make it without cryonics, but all four of my grandparents are probably going to die, because of their religion. So even though they didn&rsquo;t cut off all contact with me when I turned Atheist, I still feel like their religion has put a lot of distance between us.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">26. Is there any published work in AI (whether or not directed towards Friendliness) that you consider does not immediately, fundamentally fail due to the various issues and fallacies you've written on over the course of LW? (E.g. meaningfully named Lisp symbols, hiddenly complex wishes, magical categories, anthropomorphism, etc.)</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">ETA: By AI I meant AGI.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">There&rsquo;s lots of work that&rsquo;s regarded as plain old AI that does not immediately fail. There&rsquo;s lots of work in plain old AI that succeeds spectacularly, and Judea Pearl is sort of like my favorite poster child there. But one could also name the whole Bayesian branch of statistical inference can be regarded with some equanimity as part of AI. &nbsp;There&rsquo;s the sort of Bayesian methods that are used in robotics as well, which is sort of a surprisingly... how do I put it, it&rsquo;s not theoretically distinct because it&rsquo;s all Bayesian at heart, but in terms of the algorithms, it looks to me like there&rsquo;s quite a bit of work that&rsquo;s done in robotics that&rsquo;s a separate branch of Bayesianism from the work done in statistical learning type stuff. &nbsp;That&rsquo;s all well and good. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">But if we&rsquo;re asking about works that are sort of billing themselves as &lsquo;I am Artificial General Intelligence&rsquo;, then I would say that most of that does indeed fail immediately and indeed I cannot think of a counterexample which fails to fail immediately, but that&rsquo;s a sort of extreme selection effect, and it&rsquo;s because if you&rsquo;ve got a good partial solution, or solution to a piece of the problem, and you&rsquo;re an academic working in AI, and you&rsquo;re anything like sane, you&rsquo;re just going to bill it as plain old AI, and not take the reputational hit from AGI. &nbsp;The people who are bannering themselves around as AGI tend to be people who think they&rsquo;ve solved the whole problem, and of course they&rsquo;re mistaken. So to me it really seems like to say that all the things I&rsquo;ve read on AGI immediately fundamentally fail is not even so much a critique of AI as rather a comment on what sort of more tends to bill itself as Artificial General Intelligence.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">27. Do you feel lonely often? How bad (or important) is it?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">(Above questions are a corollary of:) Do you feel that &mdash; as you improve your understanding of the world more and more &mdash;, there are fewer and fewer people who understand you and with whom you can genuinely relate in a personal level?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">That&rsquo;s a bit hard to say exactly. &nbsp;I often feel isolated to some degree, but the fact of isolation is a bit different from the emotional reaction of loneliness. &nbsp;I suspect and put some probability to the suspicion that I&rsquo;ve actually just been isolated for so long that I don&rsquo;t have a state of social fulfillment to contrast it to, whereby I could feel lonely, or as it were, lonelier, or that I&rsquo;m too isolated relative to my baseline or something like that. &nbsp;There's also the degree to which I, personality-wise, don&rsquo;t hold with trying to save the world in an Emo fashion...? And as I improve my understanding of the world more and more, I actually would not say that I felt any more isolated as I&rsquo;ve come to understand the world better. &nbsp;</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">There&rsquo;s some degree to which hanging out with cynics like Robin Hanson has caused me to feel that the world is even more insane than I started out thinking it was, but that&rsquo;s more a function of realizing that the rest of world is crazier than I thought rather than myself improving. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Writing Less Wrong has, I think, helped a good deal. I now feel a great deal less like I&rsquo;m walking around with all of this stuff inside my head that causes most of my thoughts to be completely incomprehensible to anyone. &nbsp;Now my thoughts are merely completely incomprehensible to the vast majority of people, but there&rsquo;s a sizable group out there who can understand up to, oh, I don&rsquo;t know, like one third of my thoughts without a years worth of explanation because I actually put in the year&rsquo;s worth of explanation. And even attracted a few people whom I feel like I can relate to on a personal level, and Michael Vassar would be the poster child there.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">28. Previously, you endorsed this position:</span></p>\n<p style=\"margin-left: 4pt; margin-right: 11pt; text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Never try to deceive yourself, or offer a reason to believe other than probable truth; because even if you come up with an amazing clever reason, it's more likely that you've made a mistake than that you have a reasonable expectation of this being a net benefit in the long run.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">One counterexample has been proposed a few times: holding false beliefs about oneself in order to increase the appearance of confidence, given that it's difficult to directly manipulate all the subtle signals that indicate confidence to others.</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">What do you think about this kind of self-deception?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">So... Yeah, &lsquo;cuz y&rsquo;know people are always criticizing me on the grounds that I come across as too hesitant and not self confident enough. (sarcastic)</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">But to just sort of answer the broad thrust of the question; four legs good, two legs bad, self-honest good, self-deception bad. &nbsp;You can&rsquo;t sort of say &lsquo;Ok now I&rsquo;m going to execute a 180 degree turn from the entire life I&rsquo;ve led up until this point and now, for the first time, I&rsquo;m going to throw away all the systematic training I&rsquo;ve put into noticing when I&rsquo;m deceiving myself, finding the truth, noticing thoughts that are hidden away in the corner of my mind, and taking reflectivity on a serious, gut level, so that if I know I have no legitimate reason to believe something I will actually stop believing it because, by golly, when you have no legitimate reason to believe something, it&rsquo;s usually wrong. I&rsquo;m now going to throw that out the window; I&rsquo;m going to deceive myself about something and I&rsquo;m not going to realize it&rsquo;s hopeless and I&rsquo;m going to forget the fact that I tried to deceive myself.&rsquo; &nbsp;I don&rsquo;t see any way that you can turn away from self-honesty and towards self-deception, once you&rsquo;ve gone far enough down toward the path of self-honesty without &lsquo;A&rsquo; relinquishing The Way and losing your powers, and &lsquo;B&rsquo; it doesn&rsquo;t work anyway. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Most of the time, deceiving yourself is much harder than people think. But, because they don&rsquo;t realize this, they can easily deceive themselves into believing that they&rsquo;ve deceived themselves, and since they&rsquo;re expecting a placebo effect, they get most of the benefits of the placebo effect. &nbsp;However, at some point, you become sufficiently skilled in reflection that this sort of thing does not confuse you anymore, and you actually realize that that&rsquo;s what&rsquo;s going on, and at that point, you&rsquo;re just stuck with the truth. How sad. &nbsp;I&rsquo;ll take it.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">29. In the spirit of considering semi abyssal plans, what happens if, say, next week you discover a genuine reduction of consciousness and in turns out that... There's simply no way to construct the type of optimization process you want without it being conscious, even if very different from us?</span></p>\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">ie, what if it turned out that The Law turned out to have the consequence of \"to create a general mind is to create a conscious mind. No way around that\"? Obviously that shifts the ethics a bit, but my question is basically if so, well... \"now what?\" what would have to be done differently, in what ways, etc?</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">Now, this question actually comes in two flavors. The difficult flavor is, you build this Friendly AI, and you realize there&rsquo;s no way for it to model other people at the level of resolution that you need without every imagination that it has of another person being conscious. And so the first obvious question is &lsquo;why aren&rsquo;t my imaginations of other people conscious?&rsquo; and of course the obvious answer would be &lsquo;they are!&rsquo; The models in your mind that you have of your friends are not your friends, they&rsquo;re not identical with your friends, they&rsquo;re not as complicated as the people you&rsquo;re trying to model, so the person that you have in your imagination does not much resemble the person that you&rsquo;re imagining; it doesn&rsquo;t even much resemble the referent... like I think Michael Vassar is a complicated person, but my model of him is simple and then the person who that model is is not as complicated as my model says Michael Vassar is, etcetera, etcetera. But nonetheless, every time that I&rsquo;ve modeled a person, and I write my stories, the characters that I create are real people. They may not hurt as intensely as the people do in my stories, but they nonetheless hurt when I make bad things happen to them, and as you scale up to superintelligence the problem just gets worse and worse and the people get realer and realer.</span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">What do I do if this turns out to be the law? &nbsp;Now, come to think of it, I haven&rsquo;t much considered what I would do in that case; and I can probably justify that to you by pointing out the fact that if I actually knew that this was the case I would know a great number of things I do not currently know. But mostly I guess I would have to start working on sort of different Friendly AI designs so that the AI could model other people less, and still get something good done. </span></p>\n<br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">And as for the question of &lsquo;Well, the AI can go ahead and model other people but it has to be conscious itself, and then it might experience empathically what it imagines conscious beings experiencing the same way that I experience some degree of pain and shock, although a not a correspondingly large amount of pain and shock when I imagine one of my characters watching their home planet be destroyed. &nbsp;So in this case, when one is now faced with the question of creating a AI such that it can, in the future, become a good person; to the extent that you regard it as having human rights, it hasn&rsquo;t been set on to a trajectory that would lock it out of being a good person. And this would entail a number of complicated issues, but it&rsquo;s not like you have to make a true good person right of the bat, you just have to avoid putting it into horrible pain, or making it so that it doesn&rsquo;t want to be what we would think of as a humane person later on. &hellip; You might have to give it goals beyond the sort of thing I talk about in Coherent Extrapolated Volition, and at the same time, perhaps a sort of common sense understanding that it will later be a full citizen in society, but for now it can sort of help the rest of us save the world.</span></p>\n<br /><br />\n<p style=\"text-align: justify; margin-top: 0pt; margin-bottom: 0pt; \" dir=\"ltr\"><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; font-weight: bold; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">30. What single technique do you think is most useful for a smart, motivated person to improve their own rationality in the decisions they encounter in everyday life?</span></p>\n<br /><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">It depends on where that person has deficit; so, the first thought that came to mind for that answer is &lsquo;hold off on proposing solutions until you&rsquo;ve analyzed the problem for a bit&rsquo;, but on the other hand, if dealing with someone who&rsquo;s given to extensive, deliberate rationalization, then the first thing I tell them is &lsquo;stop doing that&rsquo;. If I&rsquo;m dealing with someone who&rsquo;s ended up stuck in a hole because they now have this immense library of flaws to accuse other people of, so that no matter what is presented to them, they can find a flaw in that and yet they don&rsquo;t turn, at full force, that ability upon themselves, then the number one technique that they need is &lsquo;avoid motivated skepticism&rsquo;. If I&rsquo;m dealing with someone who tends to be immensely driven by cognitive dissonance and rationalizing mistakes that they already made, then I might advise them on Cialdini&rsquo;s time machine technique; ask yourself &lsquo;would you do it differently if you could go back in time, in your heart of hearts&rsquo;, or pretend that you have now been teleported into your situation spontaneously; some technique like that, say. </span><br /><br /><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \">But these are all matters of &lsquo;here&rsquo;s a single flaw that the person has that is stopping them&rsquo;. So if you move aside from that a bit and ask &lsquo;what sort of positive counter intuitive technique you might use&rsquo;, I might say &lsquo;hold off on proposing solutions until you understand the problem. &nbsp;Well, the question was about everyday life, so, in everyday life, I guess I would still say that people&rsquo;s intelligence might probably still be improved a bit if they sort of paused and looked at more facets of the situation before jumping to a policy solution; or it might be rationalization, cognitive dissonance, the tendency to just sort of reweave their whole life stories just to make it sound better and to justify their past mistake, that doing something to help tone that down a bit might be the most important thing they could do in their everyday lives. &nbsp;Or if you got someone who&rsquo;s giving away their entire income to their church then they could do with a bit more reductionism in their lives, but my guess is that, in terms of everyday life, then either one of &lsquo;holding off on proposing solutions until thinking about the problem&rsquo; or &lsquo;against rationalization, against cognitive dissonance, against sour grapes, not reweaving your whole life story to make sure that you didn&rsquo;t make any mistakes, to make sure that you&rsquo;re always in the right and everyone else is in the wrong, etcetera, etcetera&rsquo;, that one of those two would be the most important thing.</span></div>\n<div><span style=\"font-size: 11pt; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap; \"><br /></span></div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"9DNZfxFvY5iKoZQbz": 1, "BhfefamXXee6c2CH8": 1, "sYm3HiWcfZvrGu3ui": 1, "ZFrgTgzwEfStg26JL": 1, "NrvXXL3iGjjxu5B7d": 1, "DigEmY3RrF3XL5cwe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YduZEfz8usGbJXN4x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 80, "baseScore": 110, "extendedScore": null, "score": 0.000215, "legacy": true, "legacyId": "10921", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 110, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qyix5Z5YPSGYxf7GG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T17:07:26.242Z", "modifiedAt": null, "url": null, "title": "Cryonics costs: given estimates are low", "slug": "cryonics-costs-given-estimates-are-low", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:34.601Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dQWCDERoZjLLHWpys/cryonics-costs-given-estimates-are-low", "pageUrlRelative": "/posts/dQWCDERoZjLLHWpys/cryonics-costs-given-estimates-are-low", "linkUrl": "https://www.lesswrong.com/posts/dQWCDERoZjLLHWpys/cryonics-costs-given-estimates-are-low", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20costs%3A%20given%20estimates%20are%20low&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20costs%3A%20given%20estimates%20are%20low%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdQWCDERoZjLLHWpys%2Fcryonics-costs-given-estimates-are-low%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20costs%3A%20given%20estimates%20are%20low%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdQWCDERoZjLLHWpys%2Fcryonics-costs-given-estimates-are-low", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdQWCDERoZjLLHWpys%2Fcryonics-costs-given-estimates-are-low", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1232, "htmlBody": "<p>One of the sticking points for cryonics is how expensive it is. Unfortunately, the estimates on LW (eg. in <a href=\"/lw/1mc/normal_cryonics/\">Normal Cryonics</a>) are likely to be low as they are <em>current</em> costs. This is starting to come to a head for Alcor, with Alcor's low growth rate meaning it faces a rising tide of aging members (hence that emphasis on young cryonicists) and fundamental flaws in its prices; the official word has come down in the latest issue of <em><a href=\"http://www.alcor.org/CryonicsMagazine/cryonics2011.html#four\">Cryonics</a>, </em><a href=\"http://www.alcor.org/cryonics/Cryonics2011-4.pdf\">issue 2011 q4</a>:</p>\n<p><a id=\"more\"></a></p>\n<h3>Cryopreservation Funding and Inflation: The need for Action; A Discussion Article by the Management and Board of Directors of Alcor</h3>\n<blockquote>\n<p>The cryonics economies anticipated by Robert Ettinger in 1965 were never realized. By the 1970s, the cost of whole body cryopreservation as offered by TransTime and Soma (the for-profit arm of IABS, which later merged with Alcor) was $60,000 (1). As shown in Fig. 1, the nominal dollar cost of cryonics has risen steadily with Consumer Price Index (CPI) inflation since then. By 2011, the minimum funding for whole body cryopreservation with Alcor was $200,000. Even this large number has not kept pace with inflation, so another increase will be necessary soon.<br /><br />Whenever Alcor has increased cryopreservation minimums, it has traditionally only required new members to meet new minimum funding requirements. Existing members were &ldquo;grandfathered,&rdquo; and allowed to remain members even if their cryopreservation funding fell below new minimums. This was and is believed to be important for members who due to age or disability become uninsurable, and would otherwise have to leave Alcor after many years of supporting the organization.<br /><br />...The sustainability of this has been questioned on numerous occasions. In 1991, Ben Best and others expressed concerns about grandfathering in a series of articles and letters in Cryonics magazine (2,3,4). Ideas for addressing the inflation problem were sought (5), but none were implemented. There was renewed public concern in 2009 when Charles Platt published an article about inflation and cryonics funding in Cryonics magazine (6), followed by a critical article on CryoNet in 2010 that accused Alcor of negligently ignoring the grandfathering problem (7,8). That same year Rob Freitas published a detailed quantitative analysis of Alcor finances based on publicly available information, and concluded that grandfathering was a serious long-term problem (9,10). Ralph Merkle subsequently published an article on cryopreservation funding that outlined 14 possible options for addressing the grandfathering problem (11). In 2011, the Alcor Board of Directors undertook its own quantitative analysis of grandfathering using internal data. The results of that analysis are below.<br /><br />As of August, 2011, 944 members were signed up in expectation of Alcor performing cryopreservations costing $142.6 million as measured by 2011 funding minimums. 533 members were signed up for whole body cryopreservation, and 411 members were signed up for neuropreservation. The total cryopreservation funding of those members was $122.2 million, a funding shortfall of $19.4 million. This net $19.4 million shortfall consists of the total underfunding ($32.6 million due to 641 under-minimum funded members) adjusted for the total over-minimum funding ($13.2 million due to 229 over-minimum funded members). Most of this over-minimum funding was from 173 members signed up for neuropreservation with $9.7 million in funding greater than minimum.<br /><br />...In 2011, as a group, neuropreservation members were not underfunded. Underfunding is a much more serious problem for whole body members. 444 whole body members were underfunded with underfunding totaling $27 million. The problem is worsened by the fact that Alcor has failed to increase whole body minimums sufficiently to keep pace with inflation over the past two decades, so another increase in whole body minimums is necessary soon. <br />&nbsp;&nbsp;&nbsp; <br />Ordinary inflation of 3% per year will increase the $142.6 million 2011 cost of cryopreservation procedures for Alcor's 944 members by $4.3 million per year. This is an unfunded liability that will grow for decades until underfunded members are cryopreserved. (Most Alcor members are middle-aged as seen in Fig. 4.) The effects of this are already being felt. Actuarial analysis indicates that Alcor in 2011 can expect 9 cases per year, of which 7 will be underfunded by a total of $380,000. This would be offset by an expected $70,000 per year from cases with above-minimum funding, still leaving an expected case funding deficit of $310,000 per year. This annual deficit will grow with time.<br /><br />...The effects of this can be insidious because in absence of careful monitoring, chronic underfunding of the Patient Care Trust (PCT) might not become obvious for years. For example, by 2010 Alcor was drawing on the PCT at a rate of 5% per year to pay the costs of maintaining its patients in cryopreservation. The PCT draw grew to this unsustainable percentage because underfunded cases led to the PCT principal not being as large as it should have been. The draw only retreated to 2.5% in 2011 after an unforeseen bequest fortuitously doubled the value of the PCT in late 2010.</p>\n</blockquote>\n<p>What to do?</p>\n<blockquote>\n<p>Option 6: Increase Membership Dues to Cover Grandfathering<br /><br />In his 2010 econometric analysis of Alcor finances (10), Rob Freitas calculated that dues and CMS fees would have to be increased to $1500 - $1850 per year for every Alcor member to sustain the practice of grandfathering. This is likely unaffordable for most present Alcor members. Such a practice might even worsen the underfunding problem by disincentivizing members from providing any more funding than minimum at time of signup. Indeed, most members would need the savings in insurance premiums to pay such high membership dues.<br /><br />...Option 10: Establish an Underfunding Reserve Account Funded by Underfunding Charges<br /><br />After extensive consideration and study, the Alcor board and management believes this is the best idea so far for coping with cryonics cost inflation. An Underfunding Reserve Account would be established. Whenever an underfunded cryopreservation was performed, the Underfunding Reserve would be drawn upon as necessary to pay the PCT, CMS fund, and Operations accounts the amounts they require according to current minimums.<br /><br />The Underfunding Reserve Account would be funded by annual charges to members proportional to the extent of their underfunding. In the first year of implementation, the charge would be 0.33% of the member underfunding amount (e.g. $165 for a member underfunded by $50,000). The charge would escalate to 0.67% in the second year, and finally to 1% of the underfunding amount in the third year and thereafter. If by the third year no members changed their funding or cryopreservation method, charges collected from all underfunded members would generate $320,000 per year. This would be a sufficient contribution to the Underfunding Reserve Account to cover the actuarial expectation of underfunded case expenses for the present time. In the longer term, it is hoped that this charge would be an incentive for members to increase their funding with inflation if they are able to do so, and for new members to plan funding according to life expectancy.</p>\n</blockquote>\n<p>Hope the old grandfathered members like Mike Darwin (who predicted this, in the <a href=\"http://www.alcor.org/cryonics/cryonics8802.txt\">February</a> and <a href=\"http://www.alcor.org/cryonics/cryonics8803.txt\">March</a> 1988 issues of <a href=\"http://www.alcor.org/magazine/\"><em>Cryonic</em></a>s) can afford that.</p>\n<p>On a parting note, I read somewhere that CI's low prices have rarely risen. I wonder what<em> their</em> projections look like...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 1, "8XiMxJaWbjNtWLsEj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dQWCDERoZjLLHWpys", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 43, "extendedScore": null, "score": 7.994755093300259e-07, "legacy": true, "legacyId": "10922", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 30, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hiDkhLyN5S2MEjrSE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-14T23:07:55.586Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft: types of digital intelligence", "slug": "intelligence-explosion-analysis-draft-types-of-digital", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.469Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yysHcCrQrXYhb4m2s/intelligence-explosion-analysis-draft-types-of-digital", "pageUrlRelative": "/posts/yysHcCrQrXYhb4m2s/intelligence-explosion-analysis-draft-types-of-digital", "linkUrl": "https://www.lesswrong.com/posts/yysHcCrQrXYhb4m2s/intelligence-explosion-analysis-draft-types-of-digital", "postedAtFormatted": "Monday, November 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%3A%20types%20of%20digital%20intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%3A%20types%20of%20digital%20intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyysHcCrQrXYhb4m2s%2Fintelligence-explosion-analysis-draft-types-of-digital%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%3A%20types%20of%20digital%20intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyysHcCrQrXYhb4m2s%2Fintelligence-explosion-analysis-draft-types-of-digital", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyysHcCrQrXYhb4m2s%2Fintelligence-explosion-analysis-draft-types-of-digital", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 644, "htmlBody": "<p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<h4>From here to digital intelligence</h4>\n<p>Our first step is to survey the evidence suggesting that, barring global catastrophe and other disruptions to scientific progress,<sup>1</sup> there is a significant probability we will see the creation of digital intelligence<sup>2</sup> within a century.</p>\n<p>Why focus on digital intelligence instead of, say, the cognitive enhancement of biological humans? As we discuss in a later section, digital intelligence has certain advantages (e.g. copyability) that make it likely to lead to intelligence explosion.&nbsp;</p>\n<p>Below, we discuss the different types of digital intelligence, what kinds of progress are likely to push us closer to digital intelligence, and how to estimate the time at which digital intelligence will arrive.</p>\n<p>&nbsp;</p>\n<h4>Types of digital intelligence</h4>\n<p>To count as a \"digital intelligence,\" an artificial agent must have at least a human-level general capacity<sup>3</sup> to achieve its goals in a wide range of environments, including novel ones.<sup>4</sup></p>\n<p>IBM's <em>Jeopardy!</em>-playing Watson computer is not a digital intelligence in this sense because it can only solve a narrow problem. Imagine instead a machine that can invent new technologies, manipulate humans with acquired social skills, and otherwise learn to navigate new environments on the fly. A digital intelligence need not be sentient, though, so long as it has a human-level capacity to achieve goals in a wide variety of environments.</p>\n<p>There are many types of digital intelligence. To name just four:</p>\n<ul>\n<li>The code of a <em>transparent AI</em> is written explicitly by, and largely understood by, its programmers.<sup>5</sup></li>\n<li>An <em>opaque AI</em> is not transparent to its creators. For example it could be, like the human brain, a messy ensemble of cognitive modules. In an AI, these modules might be written by different teams for different purposes using different languages and approaches.</li>\n<li>A <em>whole brain emulation</em> (WBE) is a computer emulation of the brain structures required to functionally reproduce human thought and perhaps consciousness (Sandberg and Bostrom 2008). We need not understand the detailed operation of a brain to reproduce it functionally on a computing substrate.</li>\n<li>A <em>hybrid AI</em> is a mix of any two or three of the above types of digital intelligence (transparent AI, opaque AI, and WBE).</li>\n</ul>\n<p>&nbsp;</p>\n<h4>Notes for this snippet</h4>\n<p><sup>1</sup> By &ldquo;disruptions to scientific progress&rdquo; we have in mind &ldquo;external&rdquo; disruptions like catastrophe or a global totalitarianism that prevents the further progress of science (Caplan, 2008). We do not mean to include, for example, Horgan&rsquo;s (1997) hypothesis that scientific progress may soon stop because there will be nothing left to discover that can be discovered, which we find unlikely.</p>\n<p><sup>2</sup>&nbsp;We introduce the term &ldquo;digital intelligence&rdquo; because we want a new term that refers to both human-level AI and whole brain emulations, and we don&rsquo;t wish to expand the meaning of the common term \"AI.\"</p>\n<p><sup>3</sup>&nbsp;The notion of \"human-level intelligence\" is fuzzy, but nevertheless we can identify clear examples of intelligences below the human level (rhesus monkeys) and above the human level (a human brain running at 1000 times its normal speed). A human-level intelligence is any intelligent system not clearly below or above the human level.</p>\n<p><sup>4</sup>&nbsp;Legg (2008) argues that many definitions of intelligence converge on this idea. We mean to endorse this informal definition, not Legg&rsquo;s attempt to formalize intelligence in a later section of&nbsp;his manuscript.</p>\n<p><sup>5</sup>&nbsp;Examples include many of today&rsquo;s reinforcement learners (Sutton and Barto 1998), and also many abstract models such as AIXI (Hutter 2004), G&ouml;del machines (Schmidhuber 2007), and Dewey&rsquo;s (2011) &ldquo;implemented agents.&rdquo;</p>\n<p>&nbsp;</p>\n<h4>References for this snippet</h4>\n<ul>\n<li>Sandberg &amp; Bostrom 2008 whole brain emulation</li>\n<li>Caplan 2008 the totalitarian threat</li>\n<li>Horgan 1997 end of science</li>\n<li>Legg 2008 machine superintelligence</li>\n<li>Sutton &amp; Barto 1998 reinforcement learning</li>\n<li>Hutter 2004 universal ai</li>\n<li>Schmidhuber 2007 godel machines</li>\n<li>Dewey 2011 learning what to value</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yysHcCrQrXYhb4m2s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 7.996029771913796e-07, "legacy": true, "legacyId": "10923", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<h4 id=\"From_here_to_digital_intelligence\">From here to digital intelligence</h4>\n<p>Our first step is to survey the evidence suggesting that, barring global catastrophe and other disruptions to scientific progress,<sup>1</sup> there is a significant probability we will see the creation of digital intelligence<sup>2</sup> within a century.</p>\n<p>Why focus on digital intelligence instead of, say, the cognitive enhancement of biological humans? As we discuss in a later section, digital intelligence has certain advantages (e.g. copyability) that make it likely to lead to intelligence explosion.&nbsp;</p>\n<p>Below, we discuss the different types of digital intelligence, what kinds of progress are likely to push us closer to digital intelligence, and how to estimate the time at which digital intelligence will arrive.</p>\n<p>&nbsp;</p>\n<h4 id=\"Types_of_digital_intelligence\">Types of digital intelligence</h4>\n<p>To count as a \"digital intelligence,\" an artificial agent must have at least a human-level general capacity<sup>3</sup> to achieve its goals in a wide range of environments, including novel ones.<sup>4</sup></p>\n<p>IBM's <em>Jeopardy!</em>-playing Watson computer is not a digital intelligence in this sense because it can only solve a narrow problem. Imagine instead a machine that can invent new technologies, manipulate humans with acquired social skills, and otherwise learn to navigate new environments on the fly. A digital intelligence need not be sentient, though, so long as it has a human-level capacity to achieve goals in a wide variety of environments.</p>\n<p>There are many types of digital intelligence. To name just four:</p>\n<ul>\n<li>The code of a <em>transparent AI</em> is written explicitly by, and largely understood by, its programmers.<sup>5</sup></li>\n<li>An <em>opaque AI</em> is not transparent to its creators. For example it could be, like the human brain, a messy ensemble of cognitive modules. In an AI, these modules might be written by different teams for different purposes using different languages and approaches.</li>\n<li>A <em>whole brain emulation</em> (WBE) is a computer emulation of the brain structures required to functionally reproduce human thought and perhaps consciousness (Sandberg and Bostrom 2008). We need not understand the detailed operation of a brain to reproduce it functionally on a computing substrate.</li>\n<li>A <em>hybrid AI</em> is a mix of any two or three of the above types of digital intelligence (transparent AI, opaque AI, and WBE).</li>\n</ul>\n<p>&nbsp;</p>\n<h4 id=\"Notes_for_this_snippet\">Notes for this snippet</h4>\n<p><sup>1</sup> By \u201cdisruptions to scientific progress\u201d we have in mind \u201cexternal\u201d disruptions like catastrophe or a global totalitarianism that prevents the further progress of science (Caplan, 2008). We do not mean to include, for example, Horgan\u2019s (1997) hypothesis that scientific progress may soon stop because there will be nothing left to discover that can be discovered, which we find unlikely.</p>\n<p><sup>2</sup>&nbsp;We introduce the term \u201cdigital intelligence\u201d because we want a new term that refers to both human-level AI and whole brain emulations, and we don\u2019t wish to expand the meaning of the common term \"AI.\"</p>\n<p><sup>3</sup>&nbsp;The notion of \"human-level intelligence\" is fuzzy, but nevertheless we can identify clear examples of intelligences below the human level (rhesus monkeys) and above the human level (a human brain running at 1000 times its normal speed). A human-level intelligence is any intelligent system not clearly below or above the human level.</p>\n<p><sup>4</sup>&nbsp;Legg (2008) argues that many definitions of intelligence converge on this idea. We mean to endorse this informal definition, not Legg\u2019s attempt to formalize intelligence in a later section of&nbsp;his manuscript.</p>\n<p><sup>5</sup>&nbsp;Examples include many of today\u2019s reinforcement learners (Sutton and Barto 1998), and also many abstract models such as AIXI (Hutter 2004), G\u00f6del machines (Schmidhuber 2007), and Dewey\u2019s (2011) \u201cimplemented agents.\u201d</p>\n<p>&nbsp;</p>\n<h4 id=\"References_for_this_snippet\">References for this snippet</h4>\n<ul>\n<li>Sandberg &amp; Bostrom 2008 whole brain emulation</li>\n<li>Caplan 2008 the totalitarian threat</li>\n<li>Horgan 1997 end of science</li>\n<li>Legg 2008 machine superintelligence</li>\n<li>Sutton &amp; Barto 1998 reinforcement learning</li>\n<li>Hutter 2004 universal ai</li>\n<li>Schmidhuber 2007 godel machines</li>\n<li>Dewey 2011 learning what to value</li>\n</ul>", "sections": [{"title": "From here to digital intelligence", "anchor": "From_here_to_digital_intelligence", "level": 1}, {"title": "Types of digital intelligence", "anchor": "Types_of_digital_intelligence", "level": 1}, {"title": "Notes for this snippet", "anchor": "Notes_for_this_snippet", "level": 1}, {"title": "References for this snippet", "anchor": "References_for_this_snippet", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "25 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T01:18:52.184Z", "modifiedAt": null, "url": null, "title": "Meetup : First Eugene Meetup", "slug": "meetup-first-eugene-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Rubix", "createdAt": "2011-09-18T03:27:30.977Z", "isAdmin": false, "displayName": "Rubix"}, "userId": "3HSB6NdZm49DvQRFh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JpqGdgdE5NFZhDTrp/meetup-first-eugene-meetup", "pageUrlRelative": "/posts/JpqGdgdE5NFZhDTrp/meetup-first-eugene-meetup", "linkUrl": "https://www.lesswrong.com/posts/JpqGdgdE5NFZhDTrp/meetup-first-eugene-meetup", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Eugene%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Eugene%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJpqGdgdE5NFZhDTrp%2Fmeetup-first-eugene-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Eugene%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJpqGdgdE5NFZhDTrp%2Fmeetup-first-eugene-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJpqGdgdE5NFZhDTrp%2Fmeetup-first-eugene-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4u'>First Eugene Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">20 November 2011 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">100 W 10th Avenue, Eugene, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Any Eugene or Springfield based Less Wrongers are welcome to attend! We will be meeting at 1 PM at the caf\u00e9 in front of the library, and may move from there to the library proper or to a study room if so impelled. This initial meetup will not be so structured; my hope is that we'll mostly do introductions, get to know each other and discuss Less Wrong in general. Anyone with a laptop is emphatically invited to bring one.</p>\n\n<p>My hope is that if enough people are interested, this could become a bi-weekly or even weekly meetup. Let me know if you plan to attend, or if another date or time would would enable you to do so.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4u'>First Eugene Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JpqGdgdE5NFZhDTrp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 7.996492872067328e-07, "legacy": true, "legacyId": "10928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Eugene_Meetup\">Discussion article for the meetup : <a href=\"/meetups/4u\">First Eugene Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">20 November 2011 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">100 W 10th Avenue, Eugene, OR</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Any Eugene or Springfield based Less Wrongers are welcome to attend! We will be meeting at 1 PM at the caf\u00e9 in front of the library, and may move from there to the library proper or to a study room if so impelled. This initial meetup will not be so structured; my hope is that we'll mostly do introductions, get to know each other and discuss Less Wrong in general. Anyone with a laptop is emphatically invited to bring one.</p>\n\n<p>My hope is that if enough people are interested, this could become a bi-weekly or even weekly meetup. Let me know if you plan to attend, or if another date or time would would enable you to do so.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Eugene_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/4u\">First Eugene Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Eugene Meetup", "anchor": "Discussion_article_for_the_meetup___First_Eugene_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First Eugene Meetup", "anchor": "Discussion_article_for_the_meetup___First_Eugene_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T01:59:45.762Z", "modifiedAt": null, "url": null, "title": "Cancer scientist meets amateur (This American Life)", "slug": "cancer-scientist-meets-amateur-this-american-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:28.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "arundelo", "createdAt": "2009-03-01T18:19:40.865Z", "isAdmin": false, "displayName": "arundelo"}, "userId": "nC4NpcrnXPWe4P3td", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zFuwz8r3fvN8qMbGJ/cancer-scientist-meets-amateur-this-american-life", "pageUrlRelative": "/posts/zFuwz8r3fvN8qMbGJ/cancer-scientist-meets-amateur-this-american-life", "linkUrl": "https://www.lesswrong.com/posts/zFuwz8r3fvN8qMbGJ/cancer-scientist-meets-amateur-this-american-life", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cancer%20scientist%20meets%20amateur%20(This%20American%20Life)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACancer%20scientist%20meets%20amateur%20(This%20American%20Life)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFuwz8r3fvN8qMbGJ%2Fcancer-scientist-meets-amateur-this-american-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cancer%20scientist%20meets%20amateur%20(This%20American%20Life)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFuwz8r3fvN8qMbGJ%2Fcancer-scientist-meets-amateur-this-american-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFuwz8r3fvN8qMbGJ%2Fcancer-scientist-meets-amateur-this-american-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p><em>This American Life</em> episode 450: <a href=\"http://www.thisamericanlife.org/radio-archives/episode/450/so-crazy-it-just-might-work\">\"So Crazy It Just Might Work\"</a>. The whole episode is good, but act one (6:48-42:27) is relevant to <acronym title=\"Less Wrong\">LW</acronym>, about a trained scientist teaming up with an amateur on a cancer cure.</p>\n<p>It's downloadable until 19 Nov 2011 or so, and streamable thereafter.</p>\n<p>(Technical nit: It sounds to me like the reporter doesn't know the difference between sound and electromagnetism.)</p>\n<p><strong>Edit:</strong> Here's a quick rot13ed summary: <a href=\"http://rot13.com/index.php?text=Vg+qbrfa%27g+tb+jryy.++Nagubal+Ubyynaq+frrf+rkcrevzragny+pbagebyf+naq+ercebqhpvovyvgl+nf+guvatf+gung+trg+va+uvf+jnl.++Ur+frrzf+gb+unir+gnxra+[gur+Penpxcbg+Bssre]%28uggc%3A%2F%2Fyrffjebat.pbz%2Fyj%2Fw8%2Fgur_penpxcbg_bssre%2F%29%2E\">Vg qbrfa'g tb jryy. Nagubal Ubyynaq frrf rkcrevzragny pbagebyf naq ercebqhpvovyvgl nf guvatf gung trg va uvf jnl. Ur frrzf gb unir gnxra [gur Penpxcbg Bssre](uggc://yrffjebat.pbz/yj/w8/gur_penpxcbg_bssre/).</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zFuwz8r3fvN8qMbGJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 7.996637505803094e-07, "legacy": true, "legacyId": "10930", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T02:16:52.963Z", "modifiedAt": null, "url": null, "title": "Skepticon IV meetup: planning", "slug": "skepticon-iv-meetup-planning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.026Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZJovXWGuKJnBkDjAZ/skepticon-iv-meetup-planning", "pageUrlRelative": "/posts/ZJovXWGuKJnBkDjAZ/skepticon-iv-meetup-planning", "linkUrl": "https://www.lesswrong.com/posts/ZJovXWGuKJnBkDjAZ/skepticon-iv-meetup-planning", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Skepticon%20IV%20meetup%3A%20planning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASkepticon%20IV%20meetup%3A%20planning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJovXWGuKJnBkDjAZ%2Fskepticon-iv-meetup-planning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Skepticon%20IV%20meetup%3A%20planning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJovXWGuKJnBkDjAZ%2Fskepticon-iv-meetup-planning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZJovXWGuKJnBkDjAZ%2Fskepticon-iv-meetup-planning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>If any members of the Less Wrong community are planning to attend <a href=\"http://skepticon.org/\">Skepticon IV</a> this weekend (Nov. 18-20) in Springfield, Missouri (USA), it might be nice to see if we can arrange a meetup.&nbsp;</p>\n<p>Feel free to comment and say \"I'll be there!\". (At least one prominent Less Wronger is even among the <a href=\"http://skepticon.org/speakers.php\">speakers</a>.) Suggestions regarding&nbsp;locations, etc.&nbsp;from folks familiar with the local area are particularly encouraged.</p>\n<p>Attractions this year include (besides what was mentioned above): a panel on \"How Should Rationalists Approach Death?\"; and a talk by <a href=\"http://en.wikipedia.org/wiki/Richard_Carrier\">Richard Carrier</a> entitled&nbsp;\"Bayes' Theorem: Key to the Universe\" (and subtitled:&nbsp;<em>that's right, I'm teaching you math, bitches!</em>).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZJovXWGuKJnBkDjAZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 7.996698058586629e-07, "legacy": true, "legacyId": "10931", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T02:57:46.294Z", "modifiedAt": null, "url": null, "title": "Behavioral psychology and buying a warranty at Menards", "slug": "behavioral-psychology-and-buying-a-warranty-at-menards", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.111Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwhendy", "createdAt": "2011-01-04T19:53:21.160Z", "isAdmin": false, "displayName": "jwhendy"}, "userId": "ZaJctSZkCvg7qvSEC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8ZWcbJW8Fw6TA2JhK/behavioral-psychology-and-buying-a-warranty-at-menards", "pageUrlRelative": "/posts/8ZWcbJW8Fw6TA2JhK/behavioral-psychology-and-buying-a-warranty-at-menards", "linkUrl": "https://www.lesswrong.com/posts/8ZWcbJW8Fw6TA2JhK/behavioral-psychology-and-buying-a-warranty-at-menards", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Behavioral%20psychology%20and%20buying%20a%20warranty%20at%20Menards&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABehavioral%20psychology%20and%20buying%20a%20warranty%20at%20Menards%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZWcbJW8Fw6TA2JhK%2Fbehavioral-psychology-and-buying-a-warranty-at-menards%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Behavioral%20psychology%20and%20buying%20a%20warranty%20at%20Menards%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZWcbJW8Fw6TA2JhK%2Fbehavioral-psychology-and-buying-a-warranty-at-menards", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8ZWcbJW8Fw6TA2JhK%2Fbehavioral-psychology-and-buying-a-warranty-at-menards", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 688, "htmlBody": "<p>I just returned from buying a multimeter at Menards and wanted<span style=\"white-space: pre;\"> </span>to post my thoughts while they were still fresh. I hardly ever have the need to use a multimeter. In diagnosing my non-heating microwave, I fried my 2-3 year old meter (don't ask) and went out for another to finish the job. I had many choices. I essentially went with the best of the lowest tier: $14. The next options were $35 and then $55.</p>\n<p>I got to the checkout register and was waiting at the end of the conveyor belt ready to swipe my card when the cashier came over to me, stood very close, and in an almost confiding sort of hushed tone, said something like so: \"With anything fragile like this, electronics and other things, you want to be careful. Check it out. Make sure it looks good and works. If it doesn't you just bring it back within a year and we'll replace it, no questions asked. Just two ninety seven.\"</p>\n<p>Now, I believe as he said that last part, he was kind of walking back toward the register and I almost reflexively said, \"Okay.\"</p>\n<p>Once that word was uttered and I saw him then start doing something with the register, the words I <em>heard</em>&nbsp;all of the sudden <em>registered.</em>&nbsp;I recall thinking, \"<em>Oh</em>! He was selling me a warranty of some sort.\" I grimaced internally but didn't speak up about it.</p>\n<p>On my way out of the store, I was angry with myself and feeling very stupid. I wanted a cheap multi-meter. My $14 was now $18 after tax. Using it once or twice a year and then having it sit pristinely in my tool box isn't even worth the $3 insurance policy, especially since it was so cheap to begin with. I tried to catch myself and stop being angry; I thought, \"No, let's <em>learn</em> from this situation rather than just feeling stupid. What in the world happened back there?\"</p>\n<p>Here's what I noticed about the interaction:</p>\n<p>\n<ul>\n<li>There was a sense of trust built just in him approaching me so closely</li>\n<li>The affirmative and hushed tone conveyed both that he was something of an expert and that he was looking out for me, almost as if doing so against the wishes of \"The Store.\"</li>\n<li>The lack of the use of a currency value (\"two ninety seven\"), saying it as he walked away, and not using the word \"warranty\" kept me from registering that all of that walkthrough was <em>really</em>&nbsp;about a warranty. I was also just a little off guard in general, as it just never occurred that he would have any reason to approach me.&nbsp;</li>\n<li>Though confused in following his instructions, it felt like standard social protocol to reply in the affirmative (\"Okay\")</li>\n<li>Once I realized I'd definitely <em>not</em> understood, I felt too foolish to renege, and the low cost of staying with the default didn't help that impulse</li>\n</ul>\n<div>What did I learn?</div>\n<div>\n<ul>\n<li>I had a low probability estimate that this gentleman was working for his best interests (to sell me extra stuff), and, conversely, too high of an estimate that he was trying to help me as a fellow human by his seemingly secretive, buddy-buddy approach. Fix that.</li>\n<li>I chose to look good (seem like I understood) and feel bad (be regretful) rather than look good (be an affirmative, confident customer) and feel good (reject a poor investment of $3 and know it). Trying to look good for a salesman is not a worthy trade for feeling swindled and regretful.</li>\n</ul>\n<div>I mainly thought it was quite interesting to try and recall how all that happened. It felt like it took place within microseconds, and just blew my mind as to how unsuspecting I'd been. I thought it better to try and recall the details I could and post it here rather than just regret not doing things better.</div>\n</div>\n<div>Feel free to offer feedback or other anecdotes like this.</div>\n<div>I'm still unsure as to whether that \"just happened,\" or whether the salesman knew that his approach was more likely to be successful in selling me a warranty.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XYHzLjwYiqpeqaf4c": 1, "rnvHPB3X2TiD5NMwY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8ZWcbJW8Fw6TA2JhK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 38, "extendedScore": null, "score": 7.996842684268507e-07, "legacy": true, "legacyId": "10932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T04:26:26.258Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Resist the Happy Death Spiral", "slug": "seq-rerun-resist-the-happy-death-spiral", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.975Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nw4AmsYu7P88Jcbwi/seq-rerun-resist-the-happy-death-spiral", "pageUrlRelative": "/posts/nw4AmsYu7P88Jcbwi/seq-rerun-resist-the-happy-death-spiral", "linkUrl": "https://www.lesswrong.com/posts/nw4AmsYu7P88Jcbwi/seq-rerun-resist-the-happy-death-spiral", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Resist%20the%20Happy%20Death%20Spiral&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Resist%20the%20Happy%20Death%20Spiral%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnw4AmsYu7P88Jcbwi%2Fseq-rerun-resist-the-happy-death-spiral%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Resist%20the%20Happy%20Death%20Spiral%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnw4AmsYu7P88Jcbwi%2Fseq-rerun-resist-the-happy-death-spiral", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnw4AmsYu7P88Jcbwi%2Fseq-rerun-resist-the-happy-death-spiral", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>Today's post, <a href=\"/lw/ln/resist_the_happy_death_spiral/\">Resist the Happy Death Spiral</a> was originally published on 04 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>You can avoid a Happy Death Spiral by (1) splitting the Great Idea into parts (2) treating every additional detail as burdensome (3) thinking about the specifics of the causal chain instead of the good or bad feelings (4) not rehearsing evidence (5) not adding happiness from claims that \"you can't prove are wrong\"; but not by (6) refusing to admire anything too much (7) conducting a biased search for negative points until you feel unhappy again (8) forcibly shoving an idea into a safe box.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8f6/seq_rerun_affective_death_spirals/\">Affective Death Spirals</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nw4AmsYu7P88Jcbwi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 7.99715631625557e-07, "legacy": true, "legacyId": "10935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hwi8JQjspnMWyWs4g", "MCZK2Zzr5BoygvTAw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T08:45:57.191Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 11-16-2011", "slug": "meetup-west-la-meetup-11-16-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:26.066Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fEzTxugNwCc7GKazp/meetup-west-la-meetup-11-16-2011", "pageUrlRelative": "/posts/fEzTxugNwCc7GKazp/meetup-west-la-meetup-11-16-2011", "linkUrl": "https://www.lesswrong.com/posts/fEzTxugNwCc7GKazp/meetup-west-la-meetup-11-16-2011", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2011-16-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2011-16-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEzTxugNwCc7GKazp%2Fmeetup-west-la-meetup-11-16-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2011-16-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEzTxugNwCc7GKazp%2Fmeetup-west-la-meetup-11-16-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfEzTxugNwCc7GKazp%2Fmeetup-west-la-meetup-11-16-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4v'>West LA Meetup 11-16-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 November 2011 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:30pm - 9:30pm Wednesday, November 2nd.</p>\n\n<p><em>Notice that this is 30 minutes later than usual!</em></p>\n\n<p><strong>Where:</strong> <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">The Westside Tavern</a> <em>in the upstairs Wine Bar</em>, located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Topic:</strong> This week we will explore ethics and morality. Our meeting will almost immediately follow the talk <a href=\"https://www.facebook.com/event.php?eid=317732211576121\" rel=\"nofollow\">The &#39;Objective&#39; Basis for Morality</a> by Dr. Alan Fiske at UCLA, hosted by the Bruin Atheists.</p>\n\n<p><strong>Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/5eh/what_is_metaethics/\">What is Metaethics?</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">Shut up and multiply</a> - check out the articles linked from here.</li>\n<li>If you want extra reading on <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">FAI</a>, read the paper introducing <a href=\"http://intelligence.org/upload/CEV.html\" rel=\"nofollow\">CEV</a>.</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4v'>West LA Meetup 11-16-2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fEzTxugNwCc7GKazp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 7.99807440679815e-07, "legacy": true, "legacyId": "10938", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_11_16_2011\">Discussion article for the meetup : <a href=\"/meetups/4v\">West LA Meetup 11-16-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 November 2011 07:30:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:30pm - 9:30pm Wednesday, November 2nd.</p>\n\n<p><em>Notice that this is 30 minutes later than usual!</em></p>\n\n<p><strong>Where:</strong> <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">The Westside Tavern</a> <em>in the upstairs Wine Bar</em>, located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Topic:</strong> This week we will explore ethics and morality. Our meeting will almost immediately follow the talk <a href=\"https://www.facebook.com/event.php?eid=317732211576121\" rel=\"nofollow\">The 'Objective' Basis for Morality</a> by Dr. Alan Fiske at UCLA, hosted by the Bruin Atheists.</p>\n\n<p><strong id=\"Recommended_Reading_\">Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/5eh/what_is_metaethics/\">What is Metaethics?</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Shut_up_and_multiply\">Shut up and multiply</a> - check out the articles linked from here.</li>\n<li>If you want extra reading on <a href=\"http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence\">FAI</a>, read the paper introducing <a href=\"http://intelligence.org/upload/CEV.html\" rel=\"nofollow\">CEV</a>.</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_11_16_20111\">Discussion article for the meetup : <a href=\"/meetups/4v\">West LA Meetup 11-16-2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 11-16-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_11_16_2011", "level": 1}, {"title": "Recommended Reading:", "anchor": "Recommended_Reading_", "level": 2}, {"title": "Discussion article for the meetup : West LA Meetup 11-16-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_11_16_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["s4Mcg9aLMeRwdW7fh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T11:32:40.630Z", "modifiedAt": null, "url": null, "title": "New Q&A by Nick Bostrom", "slug": "new-q-and-a-by-nick-bostrom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.034Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/A9BZ6wEQqFbacR2E2/new-q-and-a-by-nick-bostrom", "pageUrlRelative": "/posts/A9BZ6wEQqFbacR2E2/new-q-and-a-by-nick-bostrom", "linkUrl": "https://www.lesswrong.com/posts/A9BZ6wEQqFbacR2E2/new-q-and-a-by-nick-bostrom", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Q%26A%20by%20Nick%20Bostrom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Q%26A%20by%20Nick%20Bostrom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9BZ6wEQqFbacR2E2%2Fnew-q-and-a-by-nick-bostrom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Q%26A%20by%20Nick%20Bostrom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9BZ6wEQqFbacR2E2%2Fnew-q-and-a-by-nick-bostrom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FA9BZ6wEQqFbacR2E2%2Fnew-q-and-a-by-nick-bostrom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 22, "htmlBody": "<p><span style=\"font-family: arial, sans-serif; font-size: 13px; line-height: 18px;\">Underground Q&amp;A session with Nick Bostrom (</span><a class=\"yt-uix-redirect-link\" style=\"font-size: 13px; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: #4272db; text-decoration: none; font-family: arial, sans-serif; line-height: 18px; text-align: left; padding: 0px; margin: 0px; border: 0px initial initial;\" title=\"http://www.nickbostrom.com\" dir=\"ltr\" rel=\"nofollow\" href=\"http://www.nickbostrom.com/\" target=\"_blank\">http://www.nickbostrom.com</a><span style=\"font-family: arial, sans-serif; font-size: 13px; line-height: 18px; text-align: left;\">) on existential risks and artificial intelligence with the Oxford Transhumanists (recorded 10 October 2011).</span></p>\n<p><a href=\"http://www.youtube.com/watch?v=KQeijCRJSog\">http://www.youtube.com/watch?v=KQeijCRJSog</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "A9BZ6wEQqFbacR2E2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 16, "extendedScore": null, "score": 7.998664326007662e-07, "legacy": true, "legacyId": "10940", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T14:23:18.220Z", "modifiedAt": null, "url": null, "title": "Existential Risk", "slug": "existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:07.185Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FGTgeweYNxmMBx4fz/existential-risk", "pageUrlRelative": "/posts/FGTgeweYNxmMBx4fz/existential-risk", "linkUrl": "https://www.lesswrong.com/posts/FGTgeweYNxmMBx4fz/existential-risk", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Existential%20Risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExistential%20Risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGTgeweYNxmMBx4fz%2Fexistential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Existential%20Risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGTgeweYNxmMBx4fz%2Fexistential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFGTgeweYNxmMBx4fz%2Fexistential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1283, "htmlBody": "<p><small>This is a \"basics\" article, intended for <em>introducing</em> people to the concept of existential risk.</small></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/shadowy-planet.jpg\" alt=\"\" /></p>\n<p>On September 26, 1983, Soviet officer Stanislav Petrov saved the world.</p>\n<p>Three weeks earlier, Soviet interceptors had shot down a commercial jet, thinking it was on a spy mission. All 269 passengers were killed, including active U.S. senator Lawrence McDonald. President Reagan called the Soviet Union an &ldquo;evil empire\" in response. It was one of the most intense periods of the Cold War.</p>\n<p>Just after midnight on September 26, Petrov sat in a secret bunker, monitoring early warning systems. He did this only twice a month, and it wasn&rsquo;t his usual shift; he was filling in for the shift crew leader.</p>\n<p>One after another, five missiles from the USA appeared on the screen. A siren wailed, and the words \"\u0440\u0430\u043a\u0435\u0442\u043d\u043e\u043c \u043d\u0430\u043f\u0430\u0434\u0435\u043d\u0438\u0438\" (\"Missile Attack\") appeared in red letters. Petrov checked with his crew, who reported that all systems were operating properly. The missiles would reach their targets in Russia in mere minutes.</p>\n<p>Protocol dictated that he press the flashing red button before him to inform his superiors of the attack so they could decide whether to launch a nuclear counterattack. More than 100 crew members stood in silence behind him, awaiting his decision.</p>\n<p>\"I thought for about a minute,\" Petrov recalled. \"I thought I&rsquo;d go crazy... It was as if I was sitting on a bed of hot coals.\"</p>\n<p><a id=\"more\"></a></p>\n<p>Petrov broke protocol and went with his gut. He refused to believe what the early warning system was telling him.</p>\n<p>His gut was right. Russian satellites had misinterpreted shiny reflections on the Earth&rsquo;s surface as missile launches. Russia was not under attack.</p>\n<p>If Petrov had pressed the red button, and his superiors had launched a counterattack, the USA would have detected the incoming Russian missiles and launched their own missiles before they could be destroyed in the ground. Soviet and American missiles would have passed in the night sky over the still, silent Arctic before detonating over hundreds of targets &mdash; each detonation more destructive than all the bombs dropped in World War II combined, including the atomic bombs that vaporized Hiroshima and Nagasaki. Most of the Northern Hemisphere would have been destroyed.</p>\n<p>Petrov was reprimanded and offered early retirement. To pay his bills, he took jobs as a taxi driver and a security guard. The biggest award he ever received for saving the world was a \"World Citizen Award\" and $1000 from a small organization based in San Francisco. He spent half the award on a new vacuum cleaner.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/dinkus.png\" alt=\"\" /></p>\n<p>During <a href=\"http://www.youtube.com/watch?v=84G6An1Ff2E\">his talk</a> at Singularity Summit 2011 in New York City, hacker <a href=\"http://en.wikipedia.org/wiki/Jaan_Tallinn\">Jaan Tallinn</a> drew an important lesson from the story of Stanislav Petrov:</p>\n<blockquote>\n<p>Contrary to our intuition that society is more powerful than any individual or group, it was not society that wrote history on that day... It was Petrov.</p>\n<p>...Our future is increasingly determined by individuals and small groups wielding powerful technologies. And society is quite incompetent when it comes to predicting and handling the consequences.</p>\n</blockquote>\n<p>Tallinn knows a thing or two about powerful technologies making global impact. Kazaa, the file-sharing program he co-developed, was once responsible for half of all Internet traffic. He went on to develop the internet calling program Skype, which in 2010 accounted for 13% of all international calls.</p>\n<p>Where could he go from there? After reading <a href=\"/\">dozens</a> of <a href=\"http://www.overcomingbias.com/\">articles</a> about the <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">cognitive science of rationality</a>, Tallinn realized:</p>\n<blockquote>\n<p>In order to maximize your impact in the world, you should behave as a prudent investor. You should look for underappreciated [concerns] with huge potential.</p>\n</blockquote>\n<p>Tallinn found the biggest pool of underappreciated concerns in the domain of &ldquo;existential risks\": things that might go horribly wrong and wipe out our entire species, like nuclear war.</p>\n<p>The documentary <em><a href=\"http://en.wikipedia.org/wiki/Countdown_to_Zero\">Countdown to Zero</a></em> shows how serious the nuclear threat is. At least 8 nations have their own nuclear weapons, and the USA has given nuclear weapons to 5 others. There are enough nuclear weapons around to destroy the world several times over, and the risk of a mistake remains even after the cold war. In 1995, Russian president Boris Yeltsin had the &ldquo;nuclear suitcase\" &mdash; capable of launching a barrage of nuclear missiles &mdash; open in front of him. Russian radar had mistaken a weather rocket for a US submarine-launched ballistic missile. Like Petrov before him, Yeltsin disbelieved his equipment and refused to press the red button. Next time we might not be so lucky.</p>\n<p>But it's not just nuclear risks we have to worry about. As Sun Microsystems&rsquo; co-founder Bill Joy warned in his much-discussed article <a href=\"http://www.wired.com/wired/archive/8.04/joy_pr.html\">Why the Future Doesn&rsquo;t Need Us</a>, emerging technologies like synthetic biology, nanotechnology, and artificial intelligence may quickly become even more powerful than nuclear bombs, and even greater threats to the human species. Perhaps the <a href=\"http://en.wikipedia.org/wiki/IUCN_Red_List\">International Union for Conservation of Nature</a> will need to reclassify <em>Homo sapiens</em> as an endangered species.</p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/dinkus.png\" alt=\"\" /></p>\n<p>Academics are beginning to accept that humanity lives on a knife&rsquo;s edge. The famous physicists Martin Rees and John Leslie have written books about existential risk, titled <em><a href=\"http://www.amazon.com/Our-Final-Hour-Scientists-Warning/dp/0465068634/\">Our Final Hour: A Scientist&rsquo;s Warning</a></em> and <em><a href=\"http://www.amazon.com/End-World-Science-Ethics-Extinction/dp/0415184479/\">The End of the World: The Science and Ethics of Human Extinction</a></em>. In 2008, Oxford University Press published <a href=\"http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0199606501/\"><em>Global Catastrophic Risks</em></a>, inviting experts to summarize what we know about a variety of existential risks. New research institutes have been formed to investigate the subject, including the <a href=\"http://intelligence.org/\">Singularity Institute</a> in San Francisco and the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford University.</p>\n<p>Governments, too, are taking notice. In the USA, NASA was given a congressional mandate to catalogue all near-earth objects that are one kilometer or more in diameter, because an impact with such a large object would be catastrophic. President Bush established the <a href=\"http://nano.gov/\">National Nanotechnology Initiative</a> to ensure the safe development of molecule-sized materials and machines. (Precisely self-replicating molecular machines could multiply themselves out of control, consuming resources required for human survival.) Many nations are working to reduce nuclear armaments, which pose the risk of human extinction by global nuclear war.</p>\n<p>The public, however, remains mostly unaware of the risks. Existential risk is an unpleasant and scary topic, and may sound too distant or complicated to discuss in the mainstream media. For now, discussion of existential risk remains largely constrained to academia and a few government agencies.</p>\n<p>The concern for existential risks may appeal to one other group: analytically-minded \"social entrepreneurs\" who want to have a positive impact on the world, and are accustomed to making decisions based on calculation. Tallinn fits this description, as does Paypal co-founder <a href=\"http://en.wikipedia.org/wiki/Peter_Thiel\">Peter Thiel</a>. These two are among the largest donors to Singularity Institute, an organization focused on the reduction of existential risks from artificial intelligence.</p>\n<p>What is it about the topic of existential risk that appeals to people who act by calculation? The analytic case for doing good by reducing existential risk was laid out decades ago by moral philosopher Derek Parfit:</p>\n<blockquote>\n<p>The Earth will remain inhabitable for at least another billion years. Civilization began only a few thousand years ago. If we do not destroy mankind, these few thousand years may be only a tiny fraction of the whole of civilized human history.</p>\n<p>...Classical Utilitarians... would claim... that the destruction of mankind would be by far the greatest of all conceivable crimes. The badness of this crime would lie in the vast reduction of the possible sum of happiness...</p>\n<p>For [others] what matters are... the Sciences, the Arts, and moral progress... The destruction of mankind would prevent further achievements of these three kinds.</p>\n</blockquote>\n<p>Our technology gives us great power. If we can avoid using this power to destroy ourselves, then we can use it to spread throughout the galaxy and create structures and experiences of value on an unprecedented scale.</p>\n<p>Reducing existential risk &mdash; that is, carefully and thoughtfully preparing to <em>not</em>&nbsp;kill ourselves &mdash; may be the greatest moral imperative we have.</p>\n<p align=\"center\"><a href=\"http://www.richardfraser.co.uk/?page_id=18\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/ringworld.jpg\" alt=\"\" /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2i3w84KCkqZzpnQ4d": 1, "Rz5jb3cYHTSRmqNnN": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FGTgeweYNxmMBx4fz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": 34, "extendedScore": null, "score": 7.999268133379324e-07, "legacy": true, "legacyId": "10908", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 108, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xLm9mgJRPvmPGpo7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T16:45:58.003Z", "modifiedAt": null, "url": null, "title": "If life is unlikely, SIA and SSA expectations are similar", "slug": "if-life-is-unlikely-sia-and-ssa-expectations-are-similar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.678Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qGSYSrzKJAX99pnpm/if-life-is-unlikely-sia-and-ssa-expectations-are-similar", "pageUrlRelative": "/posts/qGSYSrzKJAX99pnpm/if-life-is-unlikely-sia-and-ssa-expectations-are-similar", "linkUrl": "https://www.lesswrong.com/posts/qGSYSrzKJAX99pnpm/if-life-is-unlikely-sia-and-ssa-expectations-are-similar", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20life%20is%20unlikely%2C%20SIA%20and%20SSA%20expectations%20are%20similar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20life%20is%20unlikely%2C%20SIA%20and%20SSA%20expectations%20are%20similar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGSYSrzKJAX99pnpm%2Fif-life-is-unlikely-sia-and-ssa-expectations-are-similar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20life%20is%20unlikely%2C%20SIA%20and%20SSA%20expectations%20are%20similar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGSYSrzKJAX99pnpm%2Fif-life-is-unlikely-sia-and-ssa-expectations-are-similar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqGSYSrzKJAX99pnpm%2Fif-life-is-unlikely-sia-and-ssa-expectations-are-similar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 143, "htmlBody": "<p>Consider a scenario in which there are three rooms. In each room there is an independent 1/1000 chance of an agent being created.&nbsp;There is thus a 1/10<sup>9</sup>&nbsp;probability of there being an agent in every room, a (3*999)/10<sup>9</sup>&nbsp;probability of there being two agents, and&nbsp;a&nbsp;(3*999<sup>2</sup>)/10<sup>9</sup>&nbsp;probability of there being one.</p>\n<p>Given that you are one of these agents, the <a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">SIA</a> and <a href=\"http://en.wikipedia.org/wiki/Self-Sampling_Assumption\">SSA</a> probabilities of there being n agents are:</p>\n<p>\n<table style=\"color: #000000; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; font-size: small;\" border=\"2\" align=\"center\">\n<tbody>\n<tr>\n<th>Number of agents</th> <th align=\"center\">SIA</th> <th align=\"center\">SSA</th>\n</tr>\n<tr>\n<th align=\"center\">0</th>\n<td style=\"font-size: 15px;\" align=\"center\">0</td>\n<td style=\"font-size: 15px;\" align=\"center\">0</td>\n</tr>\n<tr>\n<th align=\"center\">1</th>\n<td style=\"font-size: 15px;\" align=\"center\">(<strong>1</strong>*3*999<sup>2</sup>)/(<strong>3</strong>*1+<strong>2</strong>*3*999+<strong>1</strong>*3*999<sup>2</sup>)</td>\n<td style=\"font-size: 15px;\" align=\"center\">(3*999<sup>2</sup>)/(1+3*999+3*999<sup>2</sup>)</td>\n</tr>\n<tr>\n<th align=\"center\">2</th>\n<td style=\"font-size: 15px;\" align=\"center\">(<strong>2</strong>*3*999)/(<strong>3</strong>*1+<strong>2</strong>*3*999+<strong>1</strong>*3*999<sup>2</sup>)</td>\n<td style=\"font-size: 15px;\" align=\"center\">(3*999)/(1+3*999+3*999<sup>2</sup>)</td>\n</tr>\n<tr>\n<th align=\"center\">3</th>\n<td style=\"font-size: 15px;\" align=\"center\">(<strong>3</strong>*1)/(<strong>3</strong>*1+<strong>2</strong>*3*999+<strong>1</strong>*3*999<sup>2</sup>)</td>\n<td style=\"font-size: 15px;\" align=\"center\">(1)/(1+3*999+3*999<sup>2</sup>)</td>\n</tr>\n</tbody>\n</table>\n</p>\n<p>The expected numbers of agents is (1(3*999<sup>2</sup>) + 2(2*3*999) + 3(3*1))/(3*1+2*3*999+1*3*999<sup>2</sup>) = <strong>1.002</strong> for SIA, and&nbsp;(1(3*999<sup>2</sup>) + 2(3*999) + 3(1))/(1+3*999+3*999<sup>2</sup>) &asymp; <strong>1.001</strong> for SSA. The high&nbsp;unlikelihood&nbsp;of life means that, given that we are alive, both SIA and SSA probabilities get dominated by worlds with very few agents.</p>\n<p>This of course only applies to agents who existence is independent (for instance, separate galactic civilizations). If you're alive, chance are that your parents were also alive at some point too.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qGSYSrzKJAX99pnpm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 5, "extendedScore": null, "score": 7.999773046451785e-07, "legacy": true, "legacyId": "10941", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T20:15:25.446Z", "modifiedAt": null, "url": null, "title": "Babyeater's dilemma", "slug": "babyeater-s-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.373Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SBzFXzntLBMdxwByu/babyeater-s-dilemma", "pageUrlRelative": "/posts/SBzFXzntLBMdxwByu/babyeater-s-dilemma", "linkUrl": "https://www.lesswrong.com/posts/SBzFXzntLBMdxwByu/babyeater-s-dilemma", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Babyeater's%20dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABabyeater's%20dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBzFXzntLBMdxwByu%2Fbabyeater-s-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Babyeater's%20dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBzFXzntLBMdxwByu%2Fbabyeater-s-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSBzFXzntLBMdxwByu%2Fbabyeater-s-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 424, "htmlBody": "<p>Imagine it's the future, and everything has gone according to plan. Humanity has worked out its own utility function, <em>f<sub>0</sub></em>, and has worked out a strategy <em>S<sub>0</sub></em> to optimize it.<br /><br />Humanity has also run a large number of simulations of how alien worlds evolve. It has determined that of those civilizations which reach the same level of advancement - that know their own utility function and have a strategy for optimizing it - there is an equal probability that they will end up with each of 10 possible utility functions. Call these <em>f<sub>0</sub>...f<sub>9</sub></em>.<br /><br />(Of course, these simulations are coarse-grained enough to satisfy the <a href=\"/lw/x4/nonperson_predicates/\">nonperson predicate</a>).<br /><br />Humanity has also worked out the optimal strategy <em>S<sub>0</sub>...S<sub>9</sub></em> for each utility function. But they just happen to score poorly on all of the others:<br /><br /><em>f<sub>i</sub>(S<sub>i</sub>) = 10</em><br /><em>f<sub>i</sub>(S<sub>j</sub>) = 1</em> for i != j<br /><br />In addition, there is a compromise strategy C:<br /><br /><em>f<sub>i</sub>(C) = 3</em> for all i.<br /><br />The utility functions, <em>f<sub>0</sub></em> through <em>f<sub>9</sub></em>, satisfy certain properties:<strong></strong></p>\n<p><strong>They are altruistic</strong>, in the sense that they care just as much about far-away aliens that they can't even see as they do about members of their own species.</p>\n<p><strong>They are additive</strong>: if one planet implements <em>S<sub>j</sub></em> and another implements <em>S<sub>k</sub></em>, then:<br /><em>f<sub>i</sub>(S<sub>j</sub></em> on one planet and <em>S<sub>k</sub></em> on the other<em>) = f<sub>i</sub>(S<sub>j</sub>) + f<sub>i</sub>(S<sub>k</sub>)</em>.</p>\n<p>(This is just to make things easier - the problem I'm describing will still apply in cases where this rule doesn't hold).</p>\n<p><strong>They are non-negotiable</strong>. They won't \"change\" if that civilization encounters aliens with a different utility function. So if two of these civilisations were to meet, we would expect it to be like the <a href=\"/lw/y4/three_worlds_collide_08/\">humans and the babyeaters</a>: the stronger would attempt to conquer the weaker and impose their own values.</p>\n<p>In addition, humanity has worked out that it's very likely that a lot of alien worlds exist, i.e. aliens are really really real. They are just too far away to see or exist in other <a href=\"http://wiki.lesswrong.com/wiki/Everett_branch\">Everett branches</a>.<br /><br />So given these not entirely ridiculous assumptions, it seems that we have a multiplayer <a href=\"http://wiki.lesswrong.com/wiki/Prisoner%27s_dilemma\">prisoner's dilemma</a> even though none of the players has any causal influence on any other. If the universe contains 10 worlds, and each chooses its own best strategy, then each expects to score 19. If they all choose the compromise strategy then each expects to score 30.<br /><br />Anyone else worried by this result, or have I made a mistake?<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SBzFXzntLBMdxwByu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 19, "extendedScore": null, "score": 4.6e-05, "legacy": true, "legacyId": "10943", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wqDRRx9RqwKLzWt7R", "HawFh7RvDM4RyoJ2d"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T21:28:58.282Z", "modifiedAt": null, "url": null, "title": "Rationality Dojo Examples?", "slug": "rationality-dojo-examples", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.089Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SwGhpL2uk3JzrkD2c/rationality-dojo-examples", "pageUrlRelative": "/posts/SwGhpL2uk3JzrkD2c/rationality-dojo-examples", "linkUrl": "https://www.lesswrong.com/posts/SwGhpL2uk3JzrkD2c/rationality-dojo-examples", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Dojo%20Examples%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Dojo%20Examples%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwGhpL2uk3JzrkD2c%2Frationality-dojo-examples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Dojo%20Examples%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwGhpL2uk3JzrkD2c%2Frationality-dojo-examples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSwGhpL2uk3JzrkD2c%2Frationality-dojo-examples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 115, "htmlBody": "<p>Early on in my exposure to Less Wrong I encountered the metaphor of <a href=\"/lw/gn/the_martial_art_of_rationality/\">Rationality as Martial Art</a>. I assumed at some point I would be a member of an active Rationality Dojo, regularly training and becoming progressively more formidable as I learned the Art.</p>\n<p>Several years later, though I meet regularly with an <a href=\"https://www.facebook.com/pages/Less-Wrong-Ottawa/221226327934037\">awesome local group</a> whose company I greatly enjoy, I still feel as though my training has not yet begun.</p>\n<p>Can anyone point to an example of an active Rationality Dojo? What do you do there (Games? Exercises? <a href=\"http://en.wikipedia.org/wiki/Kata\">Kata</a>?)? Who are the <a href=\"/lw/9c/mandatory_secret_identities/\">instructors</a>? The closest examples that I've seen are the <a href=\"/lw/4wm/rationality_boot_camp/\">Mega</a>- and <a href=\"/lw/5ec/minicamp_on_rationality_awesomeness_and/\">Mini</a>-camps; can anyone shed some additional light on what went on there?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SwGhpL2uk3JzrkD2c", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 16, "extendedScore": null, "score": 8.000773112611778e-07, "legacy": true, "legacyId": "10944", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["teaxCFgtmCQ3E9fy8", "gBewgmzcEiks2XdoQ", "s887k4Hcqj28cchYo", "9vBasHrBtCmC6zAzD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T22:21:20.561Z", "modifiedAt": null, "url": null, "title": "Poker with Lennier", "slug": "poker-with-lennier", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hynHR5YBcNx62TsSb/poker-with-lennier", "pageUrlRelative": "/posts/hynHR5YBcNx62TsSb/poker-with-lennier", "linkUrl": "https://www.lesswrong.com/posts/hynHR5YBcNx62TsSb/poker-with-lennier", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poker%20with%20Lennier&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoker%20with%20Lennier%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhynHR5YBcNx62TsSb%2Fpoker-with-lennier%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poker%20with%20Lennier%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhynHR5YBcNx62TsSb%2Fpoker-with-lennier", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhynHR5YBcNx62TsSb%2Fpoker-with-lennier", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1851, "htmlBody": "<p class=\"p1\">In J. Michael Straczynski's science fiction TV show <em>Babylon 5</em>, there's a character named Lennier. He's pretty Spock-like: he's a long-lived alien who avoids displaying emotion and feels superior to humans in intellect and wisdom. He's sworn to always speak the truth. In one episode, he and another character, the corrupt and rakish Ambassador Mollari, are chatting. Mollari is bored. But then Lennier mentions that he's spent decades studying probability. Mollari perks up, and offers to introduce him to this game the humans call <em>poker.<a id=\"more\"></a></em></p>\n<p class=\"p1\"><img src=\"http://img163.imageshack.us/img163/4438/londoinviteslennier.jpg\" alt=\"Mollari invites Lennier to play\" width=\"405\" /></p>\n<p class=\"p1\">Later, we see Mollari, Lennier, and some others playing poker. Lennier squints at his hand and remarks, \"Interesting. The odds of this combination are 5000:1, against.\" Everybody considers this revelation for a moment, then folds, conceding the hand. Mollari is exasperated, and tells him to stop doing that. Because Lennier is essentially announcing that he has a good hand, Lennier's winning far fewer chips than he should; your biggest wins in poker are when people underestimate you.</p>\n<p class=\"p1\">The other poker players, and the audience, are picturing Lennier as having a hand something like this:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/3c.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3h.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3s.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\" /></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is a four of a kind, the second-best hand in most poker games. The odds against being dealt a four of a kind in a hand of five cards are 4164:1--one might, in a moment of excitement, round that up to an even five thousand.</p>\n<p class=\"p1\">We the audience are meant to have a hearty chuckle over how theory doesn't translate into practice. But! We never get to see Lennier's cards, which means we get to picture whatever we want. I choose to believe, and I urge you to do so as well, that Lennier had this hand:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/ac.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/2c.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/5d.gif\" alt=\"\" /> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\" /></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is one of the worst hands possible in poker: ace-high. It loses to almost everything. By causing everyone else to fold, Lennier won a hand he probably would otherwise have lost. He knew exactly what he was doing.</p>\n<p class=\"p1\">\"Wait,\" I hear you say. \"Like most members of the proud Minbari race, Lennier is sworn to always tell the truth. How could he ever make a verbal bluff in a poker game?\" Well. Let's consider a few different ways we can interpret the phrase \"the odds of this combination.\"</p>\n<p class=\"p1\">First of all, the specific two hands I've given above are <em>equally likely </em>to be dealt. Any specific set of five cards is just as likely to show up as any other. There are 2,598,960 distinct hands of poker, all created equal, so the odds against any particular one showing up are 2598959:1. That's all the hands of poker but one, lined up against that one. Throw a ball, then notice which blade of grass it crushes. The odds against it crushing that blade of grass would have seemed nigh-impossible if you tried to predict it ahead of time. Lennier would have been fully justified in gaping in astonishment at his hand, and announcing that the odds were millions-to-one against. But if he does that, he's obligated to spend every waking moment in a perpetual state of amazement at everything that happens.</p>\n<p class=\"p1\">That's not how we normally talk about poker hands. Instead, we describe them as falling into relevant categories. There are 624 different four-of-a-kind hands, so the odds of getting a four of a kind, any four of a kind plus any other card, are 624 times better than the odds of getting one specific hand. There are 502,860 ace-high hands, which makes the odds against getting an ace-high hand, any ace high hand, 2096099:502860, which reduces to a mere 4.2:1. Throw a ball into the air: you can almost guarantee it's going to crush <em>some </em>blade of grass or other. Since all similar blades of grass on the lawn fall into the same category, we're not surprised when one in particular gets crushed.</p>\n<p class=\"p1\">But Lennier's an alien, and more importantly, a novice to poker, and <em>more </em>importantly, a dirty rotten sneak. He's under no obligation to lump poker hands into the same categories as a human poker player does. He's also fully capable of noticing that his cards have the Fibbonaci relation: when placed in ascending order with the ace counting as 1, they form a sequence such that each card N+2 is the sum of card N and card N+1. Furthermore, the first two cards and the second two cards each share a suit! The odds against such a combination are 5076:1. When he gives the odds as 5000:1, he's committed no sin other than a little rounding (down!). We're surprised if the ball lands right on the particular patch of grass we remember once burying a goldfish under, while a stranger who didn't know about the goldfish wouldn't find this spot remarkable.</p>\n<p class=\"p1\">So what are the odds of a combination? It depends on what you're trying to accomplish! There is no gap between proper theory and proper practice, because theory is only coherent when it is instrumental. That's how I know that, whatever J. Michael Straczynski might think, Lennier won with a bad hand. &nbsp;We can further illustrate the way \"the odds of a combination\" depends on your goals by looking at two other things it might mean, and how the math changes depending on what you're trying to do.</p>\n<h3>Suspiciousness: Use a <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">likelihood ratio</a> of your rival categorizations</h3>\n<p class=\"p1\">\"But!\" you cry. \"I still feel like I'll be more surprised to be dealt a four of a kind than a two-striped-Fibbonaci-hand. Is that irrational?\" Not exactly. The poker hand categories are relevant to probabilistic analysis in one important way: the hand you got may not actually be random. Maybe the dealer is crooked. Maybe this is a dream. If you get dealt a four of a kind, the odds of either being true rise significantly. But we can't say this possibility makes the four of a kind less probable. Indeed it makes it <em>more probable</em>. A nonrandom process is equally likely to give you a four of a kind as an ace high. So if we allow for the possibility of a crooked dealer, all of our previous probability estimates were wrong. Rather than talking about this suddenly murky idea of the probability of a hand, we should talk about the <em>suspiciousness </em>of a hand. We can calculate it as follows.</p>\n<p class=\"p1\">First divide the set of poker hands into the relevant categories. We've got straight flush, four of a kind, full house, flush, straight, three of a kind, two pairs, one pair, high card, and then separate categories for when they're topped by an ace (the straight flush becomes a royal flush, high card becomes ace high, etc.) That's eighteen categories. This is a culturally-dependent count. In some poker circles, it might make more sense to subdivide pairs into \"jacks-or-better\" and \"tens-or-worse,\" and in Hypothetical Minbari Poker, Fibbonaci hands are important. Whatever natural categories are in your head are the appropriate ones. If we knew the dealer thought as you did and was crooked and in complete control of what was dealt, but we had no other information on motives or behavior, the probability of being dealt any one of those categories would be 1/18. On the other hand, suppose, as we assumed earlier, we knew the deal was perfectly random. Then the probability of getting a non-ace four of a kind is about 1/4512, while the probability of an ace high is about 1/5. We'll calculate the <em>suspiciousness </em>of each hand by dividing the first figure (1/18 in both cases) by the second. So the suspiciousness of the four of a kind is about (1/18)/(1/4512) = 4512/18=250 2/3, while the suspiciousness of the ace high is (1/18)/(1/5)=5/18. In general, this kind of calculation is called a likelihood ratio.</p>\n<p class=\"p1\">Here's what you can do with this number. Suppose you currently believe that the odds against the dealer being crooked are 100:1. Once you see your hand, you can multiply that 1 by the suspiciousness of the hand. So if you're dealt a four of a kind, your new odds are now 100:250 2/3 against, which reduces to roughly 5:2 in <em>favor </em>of the dealer being crooked. On the other hand, if you're dealt ace high, your new odds are 100:(5/18), which reduce to 360:1. After seeing such an ordinary hand, you're now more confident the hands are random. Every time you see another hand, you can do the same calculation again on your current beliefs. Each time, you'll be performing a Bayesian Update, which is a sacred sacrament.</p>\n<h3>Remarkability: <a href=\"/lw/jp/occams_razor/\">Is the description of the interestingness of a hand shorter than the list of cards?</a></h3>\n<p class=\"p1\">\"Hold on,\" you say, and at this point I'm starting to suspect you just enjoy interrupting. \"What if I <em>want </em>to notice patterns in my poker hands? Can I quantify the <em>remarkability </em>of a hand?\" Absolutely. We could say that a pattern is worth remarking on if its length is low relative to the number of cards it's talking about. The four of a kind hand can be described as \"Four threes and the eight of hearts,\" which is much quicker than \"The four of clubs, the four of diamonds, the four of spades, the four of hearts, and the eight of hearts.\" Here we see that Lennier's hand does okay in one respect: \"One,two,Fibbonaci\" is a little shorter than \"One, two, three, five, eight\". The suit pattern doesn't save any space, so it doesn't actually count as remarkable. And this is still subjective: alien cultures wouldn't know what \"Fibbonaci\" meant, and they might not even have a name for that particular relation. For the hand to be <em>objectively </em>remarkable, you'd have to be able to describe it succinctly even when including a definition of the term. That's not possible in this case. The odds of getting an <em>objectively remarkable </em>hand by pure chance are always low, no matter how good at spotting patterns you are, because there's not enough room in language to describe the majority of n-card hands more succinctly than you could just the list the hand. This type of analysis is a useful scientific principle, referred to as <em>minimum message length, </em>a generalized and formalized Occam's Razor. &nbsp;We can quantify it by noting that with a perfect compression algorithm, the odds of being able to compress a message by 1 bit are 1:1, by two bits are 3:1 against, and so on. &nbsp;It takes 21 bits to list out a poker hand, so a pattern that can be uniquely described in 20 bits or less is remarkable. &nbsp;If you're getting 20-bit hands more than half the time, or 19-bit hands more than a quarter of the time, it might be wise to modify the suspiciousness calculation, and do a Bayesian Update on the possibility that the dealer, like Lennier, is both crooked and a mathematician.</p>\n<p class=\"p1\"><em>(Thanks to Alicorn, Phlebas, and Unnamed for comments on an earlier draft of this post. &nbsp;If you liked this, please consider reading </em><a href=\"http://www.makefoil.com\">The Tragedy of Prince Hamlet and the Philosopher's Stone, or, A Will Most Incorrect to Heaven By William Shakespeare</a>. <em>If it's inconvenient to pay the $3 and you have 50+ karma on this site, PM me your email address.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "GBpwq8cWvaeRoE9X5": 1, "bh7uxTTqmsQ8jZJdB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hynHR5YBcNx62TsSb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 26, "extendedScore": null, "score": 8.000960231432959e-07, "legacy": true, "legacyId": "10945", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p class=\"p1\">In J. Michael Straczynski's science fiction TV show <em>Babylon 5</em>, there's a character named Lennier. He's pretty Spock-like: he's a long-lived alien who avoids displaying emotion and feels superior to humans in intellect and wisdom. He's sworn to always speak the truth. In one episode, he and another character, the corrupt and rakish Ambassador Mollari, are chatting. Mollari is bored. But then Lennier mentions that he's spent decades studying probability. Mollari perks up, and offers to introduce him to this game the humans call <em>poker.<a id=\"more\"></a></em></p>\n<p class=\"p1\"><img src=\"http://img163.imageshack.us/img163/4438/londoinviteslennier.jpg\" alt=\"Mollari invites Lennier to play\" width=\"405\"></p>\n<p class=\"p1\">Later, we see Mollari, Lennier, and some others playing poker. Lennier squints at his hand and remarks, \"Interesting. The odds of this combination are 5000:1, against.\" Everybody considers this revelation for a moment, then folds, conceding the hand. Mollari is exasperated, and tells him to stop doing that. Because Lennier is essentially announcing that he has a good hand, Lennier's winning far fewer chips than he should; your biggest wins in poker are when people underestimate you.</p>\n<p class=\"p1\">The other poker players, and the audience, are picturing Lennier as having a hand something like this:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/3c.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/3h.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/3s.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\"></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is a four of a kind, the second-best hand in most poker games. The odds against being dealt a four of a kind in a hand of five cards are 4164:1--one might, in a moment of excitement, round that up to an even five thousand.</p>\n<p class=\"p1\">We the audience are meant to have a hearty chuckle over how theory doesn't translate into practice. But! We never get to see Lennier's cards, which means we get to picture whatever we want. I choose to believe, and I urge you to do so as well, that Lennier had this hand:</p>\n<p class=\"p2\">&nbsp;</p>\n<p><img src=\"http://www.stevedawson.com/pokercards/ac.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/2c.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/3d.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/5d.gif\" alt=\"\"> <img src=\"http://www.stevedawson.com/pokercards/8h.gif\" alt=\"\"></p>\n<p class=\"p2\">&nbsp;</p>\n<p class=\"p1\">This is one of the worst hands possible in poker: ace-high. It loses to almost everything. By causing everyone else to fold, Lennier won a hand he probably would otherwise have lost. He knew exactly what he was doing.</p>\n<p class=\"p1\">\"Wait,\" I hear you say. \"Like most members of the proud Minbari race, Lennier is sworn to always tell the truth. How could he ever make a verbal bluff in a poker game?\" Well. Let's consider a few different ways we can interpret the phrase \"the odds of this combination.\"</p>\n<p class=\"p1\">First of all, the specific two hands I've given above are <em>equally likely </em>to be dealt. Any specific set of five cards is just as likely to show up as any other. There are 2,598,960 distinct hands of poker, all created equal, so the odds against any particular one showing up are 2598959:1. That's all the hands of poker but one, lined up against that one. Throw a ball, then notice which blade of grass it crushes. The odds against it crushing that blade of grass would have seemed nigh-impossible if you tried to predict it ahead of time. Lennier would have been fully justified in gaping in astonishment at his hand, and announcing that the odds were millions-to-one against. But if he does that, he's obligated to spend every waking moment in a perpetual state of amazement at everything that happens.</p>\n<p class=\"p1\">That's not how we normally talk about poker hands. Instead, we describe them as falling into relevant categories. There are 624 different four-of-a-kind hands, so the odds of getting a four of a kind, any four of a kind plus any other card, are 624 times better than the odds of getting one specific hand. There are 502,860 ace-high hands, which makes the odds against getting an ace-high hand, any ace high hand, 2096099:502860, which reduces to a mere 4.2:1. Throw a ball into the air: you can almost guarantee it's going to crush <em>some </em>blade of grass or other. Since all similar blades of grass on the lawn fall into the same category, we're not surprised when one in particular gets crushed.</p>\n<p class=\"p1\">But Lennier's an alien, and more importantly, a novice to poker, and <em>more </em>importantly, a dirty rotten sneak. He's under no obligation to lump poker hands into the same categories as a human poker player does. He's also fully capable of noticing that his cards have the Fibbonaci relation: when placed in ascending order with the ace counting as 1, they form a sequence such that each card N+2 is the sum of card N and card N+1. Furthermore, the first two cards and the second two cards each share a suit! The odds against such a combination are 5076:1. When he gives the odds as 5000:1, he's committed no sin other than a little rounding (down!). We're surprised if the ball lands right on the particular patch of grass we remember once burying a goldfish under, while a stranger who didn't know about the goldfish wouldn't find this spot remarkable.</p>\n<p class=\"p1\">So what are the odds of a combination? It depends on what you're trying to accomplish! There is no gap between proper theory and proper practice, because theory is only coherent when it is instrumental. That's how I know that, whatever J. Michael Straczynski might think, Lennier won with a bad hand. &nbsp;We can further illustrate the way \"the odds of a combination\" depends on your goals by looking at two other things it might mean, and how the math changes depending on what you're trying to do.</p>\n<h3 id=\"Suspiciousness__Use_a_likelihood_ratio_of_your_rival_categorizations\">Suspiciousness: Use a <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">likelihood ratio</a> of your rival categorizations</h3>\n<p class=\"p1\">\"But!\" you cry. \"I still feel like I'll be more surprised to be dealt a four of a kind than a two-striped-Fibbonaci-hand. Is that irrational?\" Not exactly. The poker hand categories are relevant to probabilistic analysis in one important way: the hand you got may not actually be random. Maybe the dealer is crooked. Maybe this is a dream. If you get dealt a four of a kind, the odds of either being true rise significantly. But we can't say this possibility makes the four of a kind less probable. Indeed it makes it <em>more probable</em>. A nonrandom process is equally likely to give you a four of a kind as an ace high. So if we allow for the possibility of a crooked dealer, all of our previous probability estimates were wrong. Rather than talking about this suddenly murky idea of the probability of a hand, we should talk about the <em>suspiciousness </em>of a hand. We can calculate it as follows.</p>\n<p class=\"p1\">First divide the set of poker hands into the relevant categories. We've got straight flush, four of a kind, full house, flush, straight, three of a kind, two pairs, one pair, high card, and then separate categories for when they're topped by an ace (the straight flush becomes a royal flush, high card becomes ace high, etc.) That's eighteen categories. This is a culturally-dependent count. In some poker circles, it might make more sense to subdivide pairs into \"jacks-or-better\" and \"tens-or-worse,\" and in Hypothetical Minbari Poker, Fibbonaci hands are important. Whatever natural categories are in your head are the appropriate ones. If we knew the dealer thought as you did and was crooked and in complete control of what was dealt, but we had no other information on motives or behavior, the probability of being dealt any one of those categories would be 1/18. On the other hand, suppose, as we assumed earlier, we knew the deal was perfectly random. Then the probability of getting a non-ace four of a kind is about 1/4512, while the probability of an ace high is about 1/5. We'll calculate the <em>suspiciousness </em>of each hand by dividing the first figure (1/18 in both cases) by the second. So the suspiciousness of the four of a kind is about (1/18)/(1/4512) = 4512/18=250 2/3, while the suspiciousness of the ace high is (1/18)/(1/5)=5/18. In general, this kind of calculation is called a likelihood ratio.</p>\n<p class=\"p1\">Here's what you can do with this number. Suppose you currently believe that the odds against the dealer being crooked are 100:1. Once you see your hand, you can multiply that 1 by the suspiciousness of the hand. So if you're dealt a four of a kind, your new odds are now 100:250 2/3 against, which reduces to roughly 5:2 in <em>favor </em>of the dealer being crooked. On the other hand, if you're dealt ace high, your new odds are 100:(5/18), which reduce to 360:1. After seeing such an ordinary hand, you're now more confident the hands are random. Every time you see another hand, you can do the same calculation again on your current beliefs. Each time, you'll be performing a Bayesian Update, which is a sacred sacrament.</p>\n<h3 id=\"Remarkability__Is_the_description_of_the_interestingness_of_a_hand_shorter_than_the_list_of_cards_\">Remarkability: <a href=\"/lw/jp/occams_razor/\">Is the description of the interestingness of a hand shorter than the list of cards?</a></h3>\n<p class=\"p1\">\"Hold on,\" you say, and at this point I'm starting to suspect you just enjoy interrupting. \"What if I <em>want </em>to notice patterns in my poker hands? Can I quantify the <em>remarkability </em>of a hand?\" Absolutely. We could say that a pattern is worth remarking on if its length is low relative to the number of cards it's talking about. The four of a kind hand can be described as \"Four threes and the eight of hearts,\" which is much quicker than \"The four of clubs, the four of diamonds, the four of spades, the four of hearts, and the eight of hearts.\" Here we see that Lennier's hand does okay in one respect: \"One,two,Fibbonaci\" is a little shorter than \"One, two, three, five, eight\". The suit pattern doesn't save any space, so it doesn't actually count as remarkable. And this is still subjective: alien cultures wouldn't know what \"Fibbonaci\" meant, and they might not even have a name for that particular relation. For the hand to be <em>objectively </em>remarkable, you'd have to be able to describe it succinctly even when including a definition of the term. That's not possible in this case. The odds of getting an <em>objectively remarkable </em>hand by pure chance are always low, no matter how good at spotting patterns you are, because there's not enough room in language to describe the majority of n-card hands more succinctly than you could just the list the hand. This type of analysis is a useful scientific principle, referred to as <em>minimum message length, </em>a generalized and formalized Occam's Razor. &nbsp;We can quantify it by noting that with a perfect compression algorithm, the odds of being able to compress a message by 1 bit are 1:1, by two bits are 3:1 against, and so on. &nbsp;It takes 21 bits to list out a poker hand, so a pattern that can be uniquely described in 20 bits or less is remarkable. &nbsp;If you're getting 20-bit hands more than half the time, or 19-bit hands more than a quarter of the time, it might be wise to modify the suspiciousness calculation, and do a Bayesian Update on the possibility that the dealer, like Lennier, is both crooked and a mathematician.</p>\n<p class=\"p1\"><em>(Thanks to Alicorn, Phlebas, and Unnamed for comments on an earlier draft of this post. &nbsp;If you liked this, please consider reading </em><a href=\"http://www.makefoil.com\">The Tragedy of Prince Hamlet and the Philosopher's Stone, or, A Will Most Incorrect to Heaven By William Shakespeare</a>. <em>If it's inconvenient to pay the $3 and you have 50+ karma on this site, PM me your email address.)</em></p>", "sections": [{"title": "Suspiciousness: Use a likelihood ratio of your rival categorizations", "anchor": "Suspiciousness__Use_a_likelihood_ratio_of_your_rival_categorizations", "level": 1}, {"title": "Remarkability: Is the description of the interestingness of a hand shorter than the list of cards?", "anchor": "Remarkability__Is_the_description_of_the_interestingness_of_a_hand_shorter_than_the_list_of_cards_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["f4txACqDWithRi7hs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-15T22:38:11.016Z", "modifiedAt": null, "url": null, "title": "Less Wrong/Rationality Symbol or Seal?", "slug": "less-wrong-rationality-symbol-or-seal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wsBkYaMGBQ743Wm5u/less-wrong-rationality-symbol-or-seal", "pageUrlRelative": "/posts/wsBkYaMGBQ743Wm5u/less-wrong-rationality-symbol-or-seal", "linkUrl": "https://www.lesswrong.com/posts/wsBkYaMGBQ743Wm5u/less-wrong-rationality-symbol-or-seal", "postedAtFormatted": "Tuesday, November 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%2FRationality%20Symbol%20or%20Seal%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%2FRationality%20Symbol%20or%20Seal%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsBkYaMGBQ743Wm5u%2Fless-wrong-rationality-symbol-or-seal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%2FRationality%20Symbol%20or%20Seal%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsBkYaMGBQ743Wm5u%2Fless-wrong-rationality-symbol-or-seal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwsBkYaMGBQ743Wm5u%2Fless-wrong-rationality-symbol-or-seal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p>Hey Everyone,<br /><br />I was wondering if the LW community has a particular symbol or sign that would serve to act as a graphical representation of the community?<br />Something we could wear or include in things like business cards, that would act as an acknowledgement to others of our committment to rationality.<br />Any such thing exist, and if not, any good ideas?</p>\r\n<p>I think the letters LW work pretty well if you could make them look more appealling.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wsBkYaMGBQ743Wm5u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 4, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "10946", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T04:07:53.088Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Uncritical Supercriticality", "slug": "seq-rerun-uncritical-supercriticality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gRpfoLLFCcT4R5p77/seq-rerun-uncritical-supercriticality", "pageUrlRelative": "/posts/gRpfoLLFCcT4R5p77/seq-rerun-uncritical-supercriticality", "linkUrl": "https://www.lesswrong.com/posts/gRpfoLLFCcT4R5p77/seq-rerun-uncritical-supercriticality", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Uncritical%20Supercriticality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Uncritical%20Supercriticality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRpfoLLFCcT4R5p77%2Fseq-rerun-uncritical-supercriticality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Uncritical%20Supercriticality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRpfoLLFCcT4R5p77%2Fseq-rerun-uncritical-supercriticality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgRpfoLLFCcT4R5p77%2Fseq-rerun-uncritical-supercriticality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 210, "htmlBody": "<p>Today's post, <a href=\"/lw/lo/uncritical_supercriticality/\">Uncritical Supercriticality</a> was originally published on 04 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>One of the most dangerous mistakes that a human being with human psychology can make, is to begin thinking that any argument against their favorite idea must be wrong, because it is against their favorite idea. Alternatively, they could think that any argument that supports their favorite idea must be right. This failure of reasoning has led to massive amounts of suffering and death in world history.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8fr/seq_rerun_resist_the_happy_death_spiral/\">Resist the Happy Death Spiral</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gRpfoLLFCcT4R5p77", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 8.002187269718605e-07, "legacy": true, "legacyId": "10954", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NCefvet6X3Sd4wrPc", "nw4AmsYu7P88Jcbwi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T04:25:59.965Z", "modifiedAt": null, "url": null, "title": "Open Research ", "slug": "open-research", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:29.305Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N3a7GQuYAEirFHyni/open-research", "pageUrlRelative": "/posts/N3a7GQuYAEirFHyni/open-research", "linkUrl": "https://www.lesswrong.com/posts/N3a7GQuYAEirFHyni/open-research", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Research%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Research%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3a7GQuYAEirFHyni%2Fopen-research%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Research%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3a7GQuYAEirFHyni%2Fopen-research", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN3a7GQuYAEirFHyni%2Fopen-research", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 228, "htmlBody": "<p>This is a <a href=\"http://www.ted.com/talks/michael_nielsen_open_science_now.html\">TED talk</a> about open science. It starts with a description of a new math problem which is offered on a blog, and which eventually attracts enough mathematicians working on it to solve, not just the original problem, but a more difficult version of it. It was enough easier than the usual way of doing math that it was described as being like driving a car instead of pushing it.</p>\n<p>Then the speaker talks about more ambitious projects-- like a wiki about quantum computing-- which get started, but no one is actually willing to do the work, so that the wiki lies all but vacant.</p>\n<p>He suggests that public science isn't what scientists get paid for nor what builds their careers, and has some ideas for pushing the standards of science to change. There's been at least <a href=\"http://en.wikipedia.org/wiki/Bermuda_Principles\">one success</a> involving publishing genomes.</p>\n<p>Perhaps the reason the math project succeeded was because the problem was small enough that success was both well-defined and possible, not to mention that working on it was probably more fun than figuring out how to do tolerable and sensibly-linked wiki articles.</p>\n<p>There <em>may</em> be a way to get publicly funded science to be open source. We're already got proof of concept for solving math problems if they're interesting enough, so I suggest going public if you've got a math problem people might like to work on.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N3a7GQuYAEirFHyni", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 22, "extendedScore": null, "score": 8.002251419146566e-07, "legacy": true, "legacyId": "10955", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T05:07:54.985Z", "modifiedAt": null, "url": null, "title": "Stanovich, 'The Robot's Rebellion' (mini-review)", "slug": "stanovich-the-robot-s-rebellion-mini-review", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PsvzbYxasvPPLBCZC/stanovich-the-robot-s-rebellion-mini-review", "pageUrlRelative": "/posts/PsvzbYxasvPPLBCZC/stanovich-the-robot-s-rebellion-mini-review", "linkUrl": "https://www.lesswrong.com/posts/PsvzbYxasvPPLBCZC/stanovich-the-robot-s-rebellion-mini-review", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stanovich%2C%20'The%20Robot's%20Rebellion'%20(mini-review)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStanovich%2C%20'The%20Robot's%20Rebellion'%20(mini-review)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsvzbYxasvPPLBCZC%2Fstanovich-the-robot-s-rebellion-mini-review%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stanovich%2C%20'The%20Robot's%20Rebellion'%20(mini-review)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsvzbYxasvPPLBCZC%2Fstanovich-the-robot-s-rebellion-mini-review", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPsvzbYxasvPPLBCZC%2Fstanovich-the-robot-s-rebellion-mini-review", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 325, "htmlBody": "<p>The jacket text for Keith Stanovich's <em><a href=\"http://www.amazon.com/Robots-Rebellion-Finding-Meaning-Darwin/dp/0226771253/\">The Robot's Rebellion</a></em>&nbsp;sums up the book well:</p>\n<blockquote>\n<p>The idea that we might be robots is no longer the stuff of science fiction; decades of research in evolutionary biology and cognitive science have led many esteemed scientists to the conclusion that... humans are merely the hosts for two replicators (genes and memes) that have no interest in us except as conduits for replication...</p>\n<p>Accepting [this] disturbing idea, Keith Stanovich here provides the tools for the \"robot's rebellion,\" a program of cognitive reform necessary to advance human interests over the limited interest of the replicators and define our own autonomous goals as individual human beings. Drawing on the latest research... <em>The Robot's Rebellion</em>&nbsp;describes how short-term and reflexive thinking processes dominate the higher-order thinking necessary for achieving autonomy from our biological programming. These higher-order evaluative activities of the brain... hold the potential to fulfill our need to ascribe significance to human life.</p>\n<p>We may well be robots, but we are the only robots who have discovered that fact. [This] is the first step in constructing a radical new concept of self based on what is truly singular about humans: that they gain control of their lives in a way unique among life forms on Earth &mdash; through rational self-determination.</p>\n</blockquote>\n<p>The book is an excellent introduction to the first stage of Yudkowskian philosophy: We are robots in a mechanistic universe running on a swiss army knife of cognitive modules. But at least we finally&nbsp;<em>noticed</em> we're robots, and we can use the skills of rationality to hop off our habit treadmills and pursue our values instead. These values are complex and often arbitrary, but we can use our reflective capacities to extrapolate our values based on \"higher-order\" desires, a desire for preference consistency, and other considerations. All this is argued for at length in Stanovich's book. The only thing missing is a discussion of what to do about all this when AI arrives.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PsvzbYxasvPPLBCZC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "10956", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T08:25:08.980Z", "modifiedAt": null, "url": null, "title": "Meetup : Houston Meetup: Saturday, 11/19", "slug": "meetup-houston-meetup-saturday-11-19", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Cog", "createdAt": "2011-04-25T04:58:53.803Z", "isAdmin": false, "displayName": "Cog"}, "userId": "xkp87vCZ56dp2tWnN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YcobJQhpvdT9RiJEH/meetup-houston-meetup-saturday-11-19", "pageUrlRelative": "/posts/YcobJQhpvdT9RiJEH/meetup-houston-meetup-saturday-11-19", "linkUrl": "https://www.lesswrong.com/posts/YcobJQhpvdT9RiJEH/meetup-houston-meetup-saturday-11-19", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Houston%20Meetup%3A%20Saturday%2C%2011%2F19&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Houston%20Meetup%3A%20Saturday%2C%2011%2F19%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcobJQhpvdT9RiJEH%2Fmeetup-houston-meetup-saturday-11-19%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Houston%20Meetup%3A%20Saturday%2C%2011%2F19%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcobJQhpvdT9RiJEH%2Fmeetup-houston-meetup-saturday-11-19", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYcobJQhpvdT9RiJEH%2Fmeetup-houston-meetup-saturday-11-19", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4w'>Houston Meetup: Saturday, 11/19</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2011 02:25:07AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Saturday at 3PM, we will be having a meetup on pure logic and mental \ncontent, hosted by a philosopher friend of mine. If there is time \nafterward, I'll have a game of paranoid debating ready to play.</p>\n\n<p>As always, if you want to come but can't make it, send me an email or a message, and we'll work out plans for future meetings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4w'>Houston Meetup: Saturday, 11/19</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YcobJQhpvdT9RiJEH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.003098410406199e-07, "legacy": true, "legacyId": "10957", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup__Saturday__11_19\">Discussion article for the meetup : <a href=\"/meetups/4w\">Houston Meetup: Saturday, 11/19</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2011 02:25:07AM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2010 Commerce St, Houston, Tx. 77002</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Saturday at 3PM, we will be having a meetup on pure logic and mental \ncontent, hosted by a philosopher friend of mine. If there is time \nafterward, I'll have a game of paranoid debating ready to play.</p>\n\n<p>As always, if you want to come but can't make it, send me an email or a message, and we'll work out plans for future meetings.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Houston_Meetup__Saturday__11_191\">Discussion article for the meetup : <a href=\"/meetups/4w\">Houston Meetup: Saturday, 11/19</a></h2>", "sections": [{"title": "Discussion article for the meetup : Houston Meetup: Saturday, 11/19", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup__Saturday__11_19", "level": 1}, {"title": "Discussion article for the meetup : Houston Meetup: Saturday, 11/19", "anchor": "Discussion_article_for_the_meetup___Houston_Meetup__Saturday__11_191", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T18:03:39.359Z", "modifiedAt": null, "url": null, "title": "Poll results: LW probably doesn't cause akrasia", "slug": "poll-results-lw-probably-doesn-t-cause-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:06.722Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3rBapm2TjQ6tx9Td/poll-results-lw-probably-doesn-t-cause-akrasia", "pageUrlRelative": "/posts/q3rBapm2TjQ6tx9Td/poll-results-lw-probably-doesn-t-cause-akrasia", "linkUrl": "https://www.lesswrong.com/posts/q3rBapm2TjQ6tx9Td/poll-results-lw-probably-doesn-t-cause-akrasia", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poll%20results%3A%20LW%20probably%20doesn't%20cause%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoll%20results%3A%20LW%20probably%20doesn't%20cause%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3rBapm2TjQ6tx9Td%2Fpoll-results-lw-probably-doesn-t-cause-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poll%20results%3A%20LW%20probably%20doesn't%20cause%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3rBapm2TjQ6tx9Td%2Fpoll-results-lw-probably-doesn-t-cause-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3rBapm2TjQ6tx9Td%2Fpoll-results-lw-probably-doesn-t-cause-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2396, "htmlBody": "<p>Test of:&nbsp;<a href=\"/r/discussion/lw/7ov/decision_fatigue_rationality_and_akrasia/\">Decision Fatigue, Rationality, and Akrasia</a>.</p>\n<p>Shortly before the Summit, Alexandros posted a <a href=\"/r/discussion/lw/7ov/decision_fatigue_rationality_and_akrasia/\">short discussion post</a> wondering whether rationality training might cause akrasia by prompting folks to make more decisions using deliberate, conscious, \"system II\" reasoning (instead of rapid, automatic, \"system I\" heuristics) and, thereby, causing <a href=\"http://en.wikipedia.org/wiki/Decision_fatigue\">decision fatigue</a>.</p>\n<p class=\"p1\">This conjecture sounded interesting to me, and I'd wondered similar things myself, so I put up a poll to gather data.&nbsp;</p>\n<h2><a id=\"more\"></a>Procedure</h2>\n<p class=\"p1\">I put <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE6MQ#gid=0\">this poll</a> on up LW, asking a number of questions that I hoped bore on: (1) akrasia levels; (2) how the person's akrasia levels had shifted since they came to LW; and (3) how many decisions they made via deliberate \"system II\" processing. &nbsp;70 LW-ers completed the survey in time to get included in my data analysis; perhaps because the survey was on Discussion, these were mostly folks who'd been on LW for a while; median response to \"months since you started reading LW/OB\" was 19.</p>\n<p class=\"p1\">I also wanted a control group so as to distinguish real LW anomalies from <a href=\"http://en.wikipedia.org/wiki/Illusory_superiority\">random</a> <a href=\"http://en.wikipedia.org/wiki/Worse-than-average_effect\">bugs</a> in most humans' self-reporting architecture. &nbsp;I tried to ask Reddit, but got only 7 responses; then I tried Mechanical Turk and received my full 100 desired responses... but it is hard to be sure that Mechanical Turk data is from real humans.</p>\n<h2>Validity of &ldquo;akrasia/procrastination&rdquo; self-reports</h2>\n<p class=\"p2\">It would be nice if self-reported akrasia levels correlated with (lack of) success with common goals, such as income, exercise, and living in a non-filthy house. &nbsp;To assess participants' akrasia, I asked the following questions:</p>\n<ul>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">2. As a kid, how much trouble did you have with procrastination?[1]</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">3. How much trouble do you have now with procrastination?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">4. Have you had any bills go to a collection agency in the last six months?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">5. When was your kitchen floor most recently cleaned?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">6. How many times have you exercised in the last 7 days?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">11. What was your college GPA?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">12. What was your high school GPA?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">18. What's your present income, in dollars per year?</span></li>\n</ul>\n<p class=\"p2\">I replaced all responses with z-score estimates, replaced income with \"income controlled for age\", and started looking at correlations. &nbsp;Of these items, #3 and #4 failed to correlate with the other \"akrasia\" questions[2], so I discarded them and noted that \"akrasia\" might be less of a dimension than I was hoping. &nbsp;The others all correlated in the expected directions, although weakly, as shown:</p>\n<p class=\"p2\"><img src=\"http://images.lesswrong.com/t3_7s4_7.png?v=8ab031706f5095361c2b3871bdd7b044\" alt=\"\" width=\"470\" height=\"144\" /></p>\n<p class=\"p2\">(This table contains all correlations between the listed variables that occurred with p-value &lt; .25, together with the associated p-value; note that the dataset was fairly small, so the absence of a statistically significant correlation does not necessarily imply the absence of a correlation.)</p>\n<p class=\"p2\"><span style=\"font-size: 15px; font-weight: bold;\">LW (thinks it is?) more akratic than average</span></p>\n<p class=\"p1\">On average, LW-er survey participants regarded themselves as having more trouble than usual with procrastination:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_2.png?v=0ce1a519b0c91d9bbc83b821682ee11e\" alt=\"\" width=\"396\" height=\"238\" /></p>\n<p class=\"p1\">Mechanical Turk-ers regarded themselves as more average (and, especially, regarded their childhoods as more average), suggesting that this isn't just a \"everyone thinks <em>they</em> have the most trouble\" effect (though such effects&nbsp;<a href=\"http://psycnet.apa.org/journals/psp/77/2/221/\">do exist</a>)[3]:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_3.png?v=3e46a7b994dcb543b2abd6d947703f89\" alt=\"\" width=\"394\" height=\"236\" /></p>\n<p class=\"p1\">My guess is that LWers' perceptions of having more trouble than average with procrastination represents a real difference in folks' getting-things-done powers, and not just&nbsp;a difference in self-image.&nbsp; One piece of data supporting this is that LW-ers report higher than baseline rates of autism/asperger's: 5% of survey participants, compared <a href=\"http://en.wikipedia.org/wiki/Autism_spectrum#Epidemiology\">about one percent</a> of the general population. &nbsp;20% of LW-ers report having ever had a depression diagnosis, which also seems to be somewhat above baseline rates. &nbsp;(Depression and autism both correlate with difficulty getting things done.)</p>\n<p class=\"p1\"><span style=\"font-size: 15px; font-weight: bold;\">Improvement over time?</span></p>\n<p class=\"p1\">LW-ers report improving over time. Moreover, they report improving more *since finding LW* than in an equal-sized chunk of time before finding LW; the difference in reported improvement over the two intervals is fairly small, but is statistically significant at the p=.001 significance level.</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_4.png?v=ce9e78c73a11fea610354fa6c27cca0e\" alt=\"\" width=\"395\" height=\"237\" /></p>\n<p class=\"p1\">Mechanical Turk-ers do not report improving, and are not rosier about their last two years than about the two years before that &nbsp;-- suggesting that the above isn't just due to a bug in the human self-assessment system:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_5.png\" alt=\"\" width=\"391\" height=\"235\" /></p>\n<p class=\"p1\">However, reported LW-er akrasia levels do not decrease with respondent age, which pulls against the thesis that LWers start out akratic but generically improve as we get older.&nbsp; They also do not decrease with \"months since discovering LW/OB\", which pulls against the thesis that LW helps.</p>\n<h2>Deliberate decision making not harmful</h2>\n<p class=\"p1\">To test the conjecture that excessive conscious decision-making (over-reliance on deliberate, conscious, \"System II\" reasoning instead of on automatic heuristics) causes akrasia, I asked:</p>\n<ul>\n<li><span style=\"font-style: normal;\">7. When you go grocery shopping, how often do you think carefully about which product to buy, vs.&nbsp; </span>just grabbing something and putting it in your cart?</li>\n<li>8. When you sit down to do work, how often do you think carefully about what subtasks to do and/or how to do them, vs. just doing things?<span style=\"white-space: pre;\"> </span></li>\n<li>19. Anne is looking at Bob, and Bob is looking at Carol. &nbsp;Anne is married; Carol is unmarried. &nbsp;Is a married person looking at an unmarried person?<span style=\"white-space: pre;\"> (Yes / No / Can't be determined)</span></li>\n</ul>\n<p><span style=\"font-style: normal;\"><strong></strong></span></p>\n<p>Question 19 is a question from the research literature that is designed to test individuals' tendency to engage in \"fully disjunctive reasoning\", and, thus, to assess at least one aspect of folks' tendency to use system II reasoning in preference to automatic system I heuristics. &nbsp;The <a href=\"http://whilecharliesleeps.blogspot.com/2006/03/take-cognitive-reflection-test.html\">CRT</a> is <a href=\"http://psych.fullerton.edu/mbirnbaum/psych466/articles/Frederick_CRT_2005.pdf\">more standardly </a>used for such measurement, but previous testing had indicated that LW-ers mostly hit the ceiling on the CRT, so I used the more difficult Anne question instead.</p>\n<p class=\"p1\">None of these questions correlated positively (to any discernably above-chance extent) with the indicators of akrasia. In fact, question 8 had a significant negative&nbsp;correlation with current self-reported procrastination levels (correlation -.26, p-value .03), suggesting that a tendency to deliberately choose one's work tasks may help procrastination/akrasia, and does not harm it. &nbsp;</p>\n<p class=\"p1\">On the other hand, questions 7, 8, and 19 also did not correlate strongly with one another; so it is possible that these are just not good indicators of folks' degree of reliance on deliberate, conscious, \"System II\" decision-making. &nbsp;Nevertheless, when you combine these fact that none 7, 8, or 19 indicated procrastination with the reported improvement after LWers find LW,&nbsp;it seems to together constitute reasonable evidence against LW and rationality training being harmful, or at minimum against them being sufficiently harmful to show up with such datasets.</p>\n<p class=\"p1\"><span style=\"font-size: 16px; font-weight: bold;\">Other correlations</span></p>\n<p>Most of the remaining variables correlated with one another in the manner that common sense would suggest (e.g., being in school correlated with being young). &nbsp;Still, for completeness, here are all correlations among the questions that appeared correlated with a p-value &lt;0.03; note that since I compared 25 variables with one another, we should expect about (25*24/2)*.03 = 9 correlations at this significance level, and 0 to 1 correlations at the p=.001 significance level just by chance. [4]</p>\n<p>For ease of scanning,&nbsp;correlations that I personally found interesting are in bold. &nbsp;\"Income\" is instead \"income adjusted for age and student status\"; I adjusted kitchen cleanliness for student status as well.</p>\n<p>The correlations:</p>\n<ul>\n<li>High reported levels of procrastination as a kid correlated with: \n<ul>\n<li>Reported current procrastination levels (corr. coef. = .3, p-value = .02);</li>\n<li>Low high school GPA (c = .3, p=.01);&nbsp;</li>\n<li>ADD diagnosis (c=.3, p=.02);</li>\n<li>Consuming coffee/tea/caffeine most days (now, not as a kid) (c=.3, p=.02)</li>\n<li><strong>Not being in school right now (c=.3, p=.005).</strong></li>\n</ul>\n</li>\n<li>High reported levels of current procrastination correlated with: \n<ul>\n<li>Procrastination as a kid (see above);</li>\n<li><strong>Lack of present exercise (c=.3, p=.02);</strong></li>\n<li><strong>Working on something at random, instead of making deliberate choices as to what to work on (Q8 above) (c=.26; p=.03);</strong></li>\n<li>Reporting a lack of improvement, or a worsening, of procrastination since finding LW (c=.4,&nbsp;p&lt;.001);</li>\n<li>Diagnoses of depression and of \"other\" (c = .3, p= .01);</li>\n<li><strong>Unhappiness&nbsp;(c = .4,&nbsp;p=.0003);</strong></li>\n<li><strong>Anxiety&nbsp;(c =.3 , p=.01).</strong></li>\n</ul>\n</li>\n<li>Having bills go to a collections agency correlated with: \n<ul>\n<li>Diagnoses of ADD, of autism/asperger's, and of depression (c=.5, .4, and .3 respectively; associated p-values were p=.0001, p=.0007, and p=.02 respectively).&nbsp;</li>\n</ul>\n</li>\n<li>Having a kitchen floor that had/hadn't been cleaned in the last month correlated with: nothing.</li>\n<li>Regular recent exercise correlated with: \n<ul>\n<li>Lack of reported current procrastination (see above);</li>\n<li>Choosing items deliberately in the grocery store&nbsp;(c = .3, p=.01);</li>\n<li><strong>Consuming coffee/tea/caffeine most days&nbsp;(c = .3, p=.008).</strong></li>\n</ul>\n</li>\n<li>Choosing items deliberately in the grocery store correlated with: regular recent exercise (see above).</li>\n<li>Choosing work tasks and subtasks deliberately (vs. just doing things) correlated with: \n<ul>\n<li>Reported lack of current procrastination&nbsp;(see above);</li>\n<li>Having never had a depression diagnosis&nbsp;(c =.3, p=.02).</li>\n</ul>\n</li>\n<li>Reporting improved procrastination since finding LW correlated with: \n<ul>\n<li>Reported lack of current procrastination&nbsp;(see above);</li>\n<li>Lack of depression diagnoses (c = .36, p=.003).</li>\n</ul>\n</li>\n<li>Reporting improved procrastination in an equal-sized time period <em>before</em> finding LW correlated with: nothing.</li>\n<li>High college GPA correlated with: \n<ul>\n<li>High high school GPA&nbsp;(c = .4,&nbsp;p=.0006);</li>\n<li>Being in school&nbsp;(c = .3, p=.01).</li>\n</ul>\n</li>\n<li>High school GPA correlated with: \n<ul>\n<li>Lack of procrastination as a kid; high college GPA (see above)</li>\n<li>Lack of \"other\" diagnoses&nbsp;(c = .3, p=.01);</li>\n<li><strong>Getting a lot of sleep (now, not as a kid) (c = .36, p=.002 ).</strong></li>\n</ul>\n</li>\n<li>IQ correlated with: nothing.</li>\n<li>Having \"other\" diagnoses correlated with: \n<ul>\n<li>Reported current procrastination; low high school GPA (see above);</li>\n<li>Diagnoses of depression&nbsp;(c = .5,&nbsp;p&lt;.0001);</li>\n<li>Lack of sleep&nbsp;(c = .4,&nbsp;p=.0004);</li>\n<li>Reported anxiety(c = .4,&nbsp;p=.0002).</li>\n</ul>\n</li>\n<li>ADD diagnoses correlated with: \n<ul>\n<li>Reported childhood procrastination; bills sent to collection agencies (see above);</li>\n<li>Diagnoses of autism/asperger's and depression&nbsp;(c = .55 and .44 respectively, p=.0001 for both).</li>\n</ul>\n</li>\n<li>Autism/Asperger's diagnoses correlated with: \n<ul>\n<li>Bills sent to collection agencies; diagnoses of ADD (see above);</li>\n<li>Diagnoses of depression&nbsp;(c = .36,&nbsp;p=.003).</li>\n</ul>\n</li>\n<li>Depression diagnoses correlated with: \n<ul>\n<li>Reported current procrastination; bills sent to collection agencies; tendency to just do work without thinking about which tasks or subtasks to do; lack of reported procrastination improvement since finding LW; and diagnoses of ADD, autism/asperger's, and \"other\" (see above);</li>\n<li>Reported unhappiness&nbsp;(c = .4,&nbsp;p=.0002);</li>\n<li>Reported anxiety (c = .3,&nbsp;p=.01).</li>\n</ul>\n</li>\n<li>Income correlated with: nothing. [After controlling for age, student status].</li>\n<li><strong>Answering the Anne question correctly (Q19 above) correlated with:&nbsp; </strong> \n<ul>\n<li><strong>Consuming coffee/tea/caffeine most days (c=.34, p=.004).</strong></li>\n</ul>\n</li>\n<li>Time since the respondent started reading LW correlated with: nothing.</li>\n<li>Hours of sleep per night correlated with: \n<ul>\n<li>High college GPA; and not having \"other\" diagnoses (see above).</li>\n</ul>\n</li>\n<li>Age correlated with: not being a student.</li>\n<li>Consuming coffee/tea/caffeine most days correlated with: \n<ul>\n<li>Procrastination in childhood; successfully getting exercise; answering the Anne question correctly (see above).</li>\n</ul>\n</li>\n<li>Reported happiness correlated with: \n<ul>\n<li>Reported lack of current procrastination; and lack of depression diagnoses (see above);</li>\n<li><strong>Being a student (c=.3,&nbsp;p=.02).</strong></li>\n</ul>\n</li>\n<li>Reported anxiety correlated with: \n<ul>\n<li>Reported current procrastination; diagnoses of \"depression\" and of \"other\" (see above).</li>\n</ul>\n</li>\n<li>Being a student correlated with:&nbsp; \n<ul>\n<li>Lack of reported procrastination as a child; high college GPA; youth; happiness (see above).</li>\n</ul>\n</li>\n</ul>\n<p class=\"p1\"><span style=\"font-size: 16px; font-weight: bold;\">Raw data</span></p>\n<p class=\"p1\">In case you want to play with the raw data yourself, here it is:</p>\n<ul>\n<li>The poll questions: <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE6MQ#gid=0\">for the LW poll</a>; <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHVPRDVpbEJHWnVzZldHa0tkWGIzMVE6MA#gid=0\">for the mechanical Turk poll</a>.</li>\n<li>The raw responses: <a href=\"https://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE&amp;output=html\">for the LW poll</a>; <a href=\"http://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdEhCUjhnMnBIV1l2VTJUR2RibjJSYlE&amp;output=html\">for the mechanical Turk poll</a>.</li>\n<li>The raw responses converted into numbers, in case you want to run correlations but don't want to mess with text: <a href=\"http://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdHYtYXQzU3VzdXg3eUhFSHNQdGFqaWc&amp;output=html\">for the LW poll</a> (note that this includes only the first 68 responses, which are all I analyzed above);<a href=\"https://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdGxpLXNqdmhYTEx4dUhiVjd2clFGeWc&amp;output=html\"> for the mechanical Turk poll</a>.</li>\n</ul>\n<p>Given the importance of taking actions that actually relate to one's goals (for happiness, income, world-saving, you name it -- and, hence, for real rationality), further investigation here would be welcome, either via further polls on LW-ers or others, or, perhaps even more easily and usefully, via <a href=\"http://scholar.google.com/\">Google</a> <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">Scholar</a>.</p>\n<p>&nbsp;</p>\n<hr />\n<p>[1] Originally, I asked about \"trouble with akrasia\" (now and as a kid) rather than about \"trouble with procrastination\". &nbsp;I edited the question after realizing I'd want a control group with non-LWers, and that that group would not reliably know the term \"akrasia\". &nbsp;So, the LW-er responses are partly to one wording and partly to the other.</p>\n<p>[2] Both items correlated strongly with student status; when I controlled for student status (subtracted out the constant necessary to remove the correlation), \"have had late bills reported to credit agencies\" correlated strongly with diagnoses of ADD, autism/Asperger's, and depression, but with nothing else; dirty kitchen floors correlated with nothing.</p>\n<p>[3] This is further suggested by the fact that Mechanical Turk-ers may well *have* more akrasia than average at present; they are working on Mechanical Turk, and have fairly high numbers of depression diagnoses.</p>\n<p>[4] There are 34 correlations at the p&lt;.03 significance level, and 12 at the p&lt;.001 significance level, which is more than we should expect by chance; this is not surprising, since of course e.g. being in school is correlated with being young, and so of course we see some non-chance correlations; the question is how many of the non-obvious correlations are just chance.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1, "r7qAjcbfhj2256EHH": 1, "dqx5k65wjFfaiJ9sQ": 1, "9YFoDPFwMoWthzgkY": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3rBapm2TjQ6tx9Td", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 73, "extendedScore": null, "score": 0.000144, "legacy": true, "legacyId": "10084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Test of:&nbsp;<a href=\"/r/discussion/lw/7ov/decision_fatigue_rationality_and_akrasia/\">Decision Fatigue, Rationality, and Akrasia</a>.</p>\n<p>Shortly before the Summit, Alexandros posted a <a href=\"/r/discussion/lw/7ov/decision_fatigue_rationality_and_akrasia/\">short discussion post</a> wondering whether rationality training might cause akrasia by prompting folks to make more decisions using deliberate, conscious, \"system II\" reasoning (instead of rapid, automatic, \"system I\" heuristics) and, thereby, causing <a href=\"http://en.wikipedia.org/wiki/Decision_fatigue\">decision fatigue</a>.</p>\n<p class=\"p1\">This conjecture sounded interesting to me, and I'd wondered similar things myself, so I put up a poll to gather data.&nbsp;</p>\n<h2 id=\"Procedure\"><a id=\"more\"></a>Procedure</h2>\n<p class=\"p1\">I put <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE6MQ#gid=0\">this poll</a> on up LW, asking a number of questions that I hoped bore on: (1) akrasia levels; (2) how the person's akrasia levels had shifted since they came to LW; and (3) how many decisions they made via deliberate \"system II\" processing. &nbsp;70 LW-ers completed the survey in time to get included in my data analysis; perhaps because the survey was on Discussion, these were mostly folks who'd been on LW for a while; median response to \"months since you started reading LW/OB\" was 19.</p>\n<p class=\"p1\">I also wanted a control group so as to distinguish real LW anomalies from <a href=\"http://en.wikipedia.org/wiki/Illusory_superiority\">random</a> <a href=\"http://en.wikipedia.org/wiki/Worse-than-average_effect\">bugs</a> in most humans' self-reporting architecture. &nbsp;I tried to ask Reddit, but got only 7 responses; then I tried Mechanical Turk and received my full 100 desired responses... but it is hard to be sure that Mechanical Turk data is from real humans.</p>\n<h2 id=\"Validity_of__akrasia_procrastination__self_reports\">Validity of \u201cakrasia/procrastination\u201d self-reports</h2>\n<p class=\"p2\">It would be nice if self-reported akrasia levels correlated with (lack of) success with common goals, such as income, exercise, and living in a non-filthy house. &nbsp;To assess participants' akrasia, I asked the following questions:</p>\n<ul>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">2. As a kid, how much trouble did you have with procrastination?[1]</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">3. How much trouble do you have now with procrastination?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">4. Have you had any bills go to a collection agency in the last six months?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">5. When was your kitchen floor most recently cleaned?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">6. How many times have you exercised in the last 7 days?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">11. What was your college GPA?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">12. What was your high school GPA?</span></li>\n<li><span style=\"font-family: Arial, sans-serif; font-size: 13px;\">18. What's your present income, in dollars per year?</span></li>\n</ul>\n<p class=\"p2\">I replaced all responses with z-score estimates, replaced income with \"income controlled for age\", and started looking at correlations. &nbsp;Of these items, #3 and #4 failed to correlate with the other \"akrasia\" questions[2], so I discarded them and noted that \"akrasia\" might be less of a dimension than I was hoping. &nbsp;The others all correlated in the expected directions, although weakly, as shown:</p>\n<p class=\"p2\"><img src=\"http://images.lesswrong.com/t3_7s4_7.png?v=8ab031706f5095361c2b3871bdd7b044\" alt=\"\" width=\"470\" height=\"144\"></p>\n<p class=\"p2\">(This table contains all correlations between the listed variables that occurred with p-value &lt; .25, together with the associated p-value; note that the dataset was fairly small, so the absence of a statistically significant correlation does not necessarily imply the absence of a correlation.)</p>\n<p class=\"p2\"><span style=\"font-size: 15px; font-weight: bold;\">LW (thinks it is?) more akratic than average</span></p>\n<p class=\"p1\">On average, LW-er survey participants regarded themselves as having more trouble than usual with procrastination:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_2.png?v=0ce1a519b0c91d9bbc83b821682ee11e\" alt=\"\" width=\"396\" height=\"238\"></p>\n<p class=\"p1\">Mechanical Turk-ers regarded themselves as more average (and, especially, regarded their childhoods as more average), suggesting that this isn't just a \"everyone thinks <em>they</em> have the most trouble\" effect (though such effects&nbsp;<a href=\"http://psycnet.apa.org/journals/psp/77/2/221/\">do exist</a>)[3]:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_3.png?v=3e46a7b994dcb543b2abd6d947703f89\" alt=\"\" width=\"394\" height=\"236\"></p>\n<p class=\"p1\">My guess is that LWers' perceptions of having more trouble than average with procrastination represents a real difference in folks' getting-things-done powers, and not just&nbsp;a difference in self-image.&nbsp; One piece of data supporting this is that LW-ers report higher than baseline rates of autism/asperger's: 5% of survey participants, compared <a href=\"http://en.wikipedia.org/wiki/Autism_spectrum#Epidemiology\">about one percent</a> of the general population. &nbsp;20% of LW-ers report having ever had a depression diagnosis, which also seems to be somewhat above baseline rates. &nbsp;(Depression and autism both correlate with difficulty getting things done.)</p>\n<p class=\"p1\"><span style=\"font-size: 15px; font-weight: bold;\">Improvement over time?</span></p>\n<p class=\"p1\">LW-ers report improving over time. Moreover, they report improving more *since finding LW* than in an equal-sized chunk of time before finding LW; the difference in reported improvement over the two intervals is fairly small, but is statistically significant at the p=.001 significance level.</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_4.png?v=ce9e78c73a11fea610354fa6c27cca0e\" alt=\"\" width=\"395\" height=\"237\"></p>\n<p class=\"p1\">Mechanical Turk-ers do not report improving, and are not rosier about their last two years than about the two years before that &nbsp;-- suggesting that the above isn't just due to a bug in the human self-assessment system:</p>\n<p class=\"p1\"><img src=\"http://images.lesswrong.com/t3_7s4_5.png\" alt=\"\" width=\"391\" height=\"235\"></p>\n<p class=\"p1\">However, reported LW-er akrasia levels do not decrease with respondent age, which pulls against the thesis that LWers start out akratic but generically improve as we get older.&nbsp; They also do not decrease with \"months since discovering LW/OB\", which pulls against the thesis that LW helps.</p>\n<h2 id=\"Deliberate_decision_making_not_harmful\">Deliberate decision making not harmful</h2>\n<p class=\"p1\">To test the conjecture that excessive conscious decision-making (over-reliance on deliberate, conscious, \"System II\" reasoning instead of on automatic heuristics) causes akrasia, I asked:</p>\n<ul>\n<li><span style=\"font-style: normal;\">7. When you go grocery shopping, how often do you think carefully about which product to buy, vs.&nbsp; </span>just grabbing something and putting it in your cart?</li>\n<li>8. When you sit down to do work, how often do you think carefully about what subtasks to do and/or how to do them, vs. just doing things?<span style=\"white-space: pre;\"> </span></li>\n<li>19. Anne is looking at Bob, and Bob is looking at Carol. &nbsp;Anne is married; Carol is unmarried. &nbsp;Is a married person looking at an unmarried person?<span style=\"white-space: pre;\"> (Yes / No / Can't be determined)</span></li>\n</ul>\n<p><span style=\"font-style: normal;\"><strong></strong></span></p>\n<p>Question 19 is a question from the research literature that is designed to test individuals' tendency to engage in \"fully disjunctive reasoning\", and, thus, to assess at least one aspect of folks' tendency to use system II reasoning in preference to automatic system I heuristics. &nbsp;The <a href=\"http://whilecharliesleeps.blogspot.com/2006/03/take-cognitive-reflection-test.html\">CRT</a> is <a href=\"http://psych.fullerton.edu/mbirnbaum/psych466/articles/Frederick_CRT_2005.pdf\">more standardly </a>used for such measurement, but previous testing had indicated that LW-ers mostly hit the ceiling on the CRT, so I used the more difficult Anne question instead.</p>\n<p class=\"p1\">None of these questions correlated positively (to any discernably above-chance extent) with the indicators of akrasia. In fact, question 8 had a significant negative&nbsp;correlation with current self-reported procrastination levels (correlation -.26, p-value .03), suggesting that a tendency to deliberately choose one's work tasks may help procrastination/akrasia, and does not harm it. &nbsp;</p>\n<p class=\"p1\">On the other hand, questions 7, 8, and 19 also did not correlate strongly with one another; so it is possible that these are just not good indicators of folks' degree of reliance on deliberate, conscious, \"System II\" decision-making. &nbsp;Nevertheless, when you combine these fact that none 7, 8, or 19 indicated procrastination with the reported improvement after LWers find LW,&nbsp;it seems to together constitute reasonable evidence against LW and rationality training being harmful, or at minimum against them being sufficiently harmful to show up with such datasets.</p>\n<p class=\"p1\"><span style=\"font-size: 16px; font-weight: bold;\">Other correlations</span></p>\n<p>Most of the remaining variables correlated with one another in the manner that common sense would suggest (e.g., being in school correlated with being young). &nbsp;Still, for completeness, here are all correlations among the questions that appeared correlated with a p-value &lt;0.03; note that since I compared 25 variables with one another, we should expect about (25*24/2)*.03 = 9 correlations at this significance level, and 0 to 1 correlations at the p=.001 significance level just by chance. [4]</p>\n<p>For ease of scanning,&nbsp;correlations that I personally found interesting are in bold. &nbsp;\"Income\" is instead \"income adjusted for age and student status\"; I adjusted kitchen cleanliness for student status as well.</p>\n<p>The correlations:</p>\n<ul>\n<li>High reported levels of procrastination as a kid correlated with: \n<ul>\n<li>Reported current procrastination levels (corr. coef. = .3, p-value = .02);</li>\n<li>Low high school GPA (c = .3, p=.01);&nbsp;</li>\n<li>ADD diagnosis (c=.3, p=.02);</li>\n<li>Consuming coffee/tea/caffeine most days (now, not as a kid) (c=.3, p=.02)</li>\n<li><strong>Not being in school right now (c=.3, p=.005).</strong></li>\n</ul>\n</li>\n<li>High reported levels of current procrastination correlated with: \n<ul>\n<li>Procrastination as a kid (see above);</li>\n<li><strong>Lack of present exercise (c=.3, p=.02);</strong></li>\n<li><strong>Working on something at random, instead of making deliberate choices as to what to work on (Q8 above) (c=.26; p=.03);</strong></li>\n<li>Reporting a lack of improvement, or a worsening, of procrastination since finding LW (c=.4,&nbsp;p&lt;.001);</li>\n<li>Diagnoses of depression and of \"other\" (c = .3, p= .01);</li>\n<li><strong>Unhappiness&nbsp;(c = .4,&nbsp;p=.0003);</strong></li>\n<li><strong>Anxiety&nbsp;(c =.3 , p=.01).</strong></li>\n</ul>\n</li>\n<li>Having bills go to a collections agency correlated with: \n<ul>\n<li>Diagnoses of ADD, of autism/asperger's, and of depression (c=.5, .4, and .3 respectively; associated p-values were p=.0001, p=.0007, and p=.02 respectively).&nbsp;</li>\n</ul>\n</li>\n<li>Having a kitchen floor that had/hadn't been cleaned in the last month correlated with: nothing.</li>\n<li>Regular recent exercise correlated with: \n<ul>\n<li>Lack of reported current procrastination (see above);</li>\n<li>Choosing items deliberately in the grocery store&nbsp;(c = .3, p=.01);</li>\n<li><strong>Consuming coffee/tea/caffeine most days&nbsp;(c = .3, p=.008).</strong></li>\n</ul>\n</li>\n<li>Choosing items deliberately in the grocery store correlated with: regular recent exercise (see above).</li>\n<li>Choosing work tasks and subtasks deliberately (vs. just doing things) correlated with: \n<ul>\n<li>Reported lack of current procrastination&nbsp;(see above);</li>\n<li>Having never had a depression diagnosis&nbsp;(c =.3, p=.02).</li>\n</ul>\n</li>\n<li>Reporting improved procrastination since finding LW correlated with: \n<ul>\n<li>Reported lack of current procrastination&nbsp;(see above);</li>\n<li>Lack of depression diagnoses (c = .36, p=.003).</li>\n</ul>\n</li>\n<li>Reporting improved procrastination in an equal-sized time period <em>before</em> finding LW correlated with: nothing.</li>\n<li>High college GPA correlated with: \n<ul>\n<li>High high school GPA&nbsp;(c = .4,&nbsp;p=.0006);</li>\n<li>Being in school&nbsp;(c = .3, p=.01).</li>\n</ul>\n</li>\n<li>High school GPA correlated with: \n<ul>\n<li>Lack of procrastination as a kid; high college GPA (see above)</li>\n<li>Lack of \"other\" diagnoses&nbsp;(c = .3, p=.01);</li>\n<li><strong>Getting a lot of sleep (now, not as a kid) (c = .36, p=.002 ).</strong></li>\n</ul>\n</li>\n<li>IQ correlated with: nothing.</li>\n<li>Having \"other\" diagnoses correlated with: \n<ul>\n<li>Reported current procrastination; low high school GPA (see above);</li>\n<li>Diagnoses of depression&nbsp;(c = .5,&nbsp;p&lt;.0001);</li>\n<li>Lack of sleep&nbsp;(c = .4,&nbsp;p=.0004);</li>\n<li>Reported anxiety(c = .4,&nbsp;p=.0002).</li>\n</ul>\n</li>\n<li>ADD diagnoses correlated with: \n<ul>\n<li>Reported childhood procrastination; bills sent to collection agencies (see above);</li>\n<li>Diagnoses of autism/asperger's and depression&nbsp;(c = .55 and .44 respectively, p=.0001 for both).</li>\n</ul>\n</li>\n<li>Autism/Asperger's diagnoses correlated with: \n<ul>\n<li>Bills sent to collection agencies; diagnoses of ADD (see above);</li>\n<li>Diagnoses of depression&nbsp;(c = .36,&nbsp;p=.003).</li>\n</ul>\n</li>\n<li>Depression diagnoses correlated with: \n<ul>\n<li>Reported current procrastination; bills sent to collection agencies; tendency to just do work without thinking about which tasks or subtasks to do; lack of reported procrastination improvement since finding LW; and diagnoses of ADD, autism/asperger's, and \"other\" (see above);</li>\n<li>Reported unhappiness&nbsp;(c = .4,&nbsp;p=.0002);</li>\n<li>Reported anxiety (c = .3,&nbsp;p=.01).</li>\n</ul>\n</li>\n<li>Income correlated with: nothing. [After controlling for age, student status].</li>\n<li><strong>Answering the Anne question correctly (Q19 above) correlated with:&nbsp; </strong> \n<ul>\n<li><strong>Consuming coffee/tea/caffeine most days (c=.34, p=.004).</strong></li>\n</ul>\n</li>\n<li>Time since the respondent started reading LW correlated with: nothing.</li>\n<li>Hours of sleep per night correlated with: \n<ul>\n<li>High college GPA; and not having \"other\" diagnoses (see above).</li>\n</ul>\n</li>\n<li>Age correlated with: not being a student.</li>\n<li>Consuming coffee/tea/caffeine most days correlated with: \n<ul>\n<li>Procrastination in childhood; successfully getting exercise; answering the Anne question correctly (see above).</li>\n</ul>\n</li>\n<li>Reported happiness correlated with: \n<ul>\n<li>Reported lack of current procrastination; and lack of depression diagnoses (see above);</li>\n<li><strong>Being a student (c=.3,&nbsp;p=.02).</strong></li>\n</ul>\n</li>\n<li>Reported anxiety correlated with: \n<ul>\n<li>Reported current procrastination; diagnoses of \"depression\" and of \"other\" (see above).</li>\n</ul>\n</li>\n<li>Being a student correlated with:&nbsp; \n<ul>\n<li>Lack of reported procrastination as a child; high college GPA; youth; happiness (see above).</li>\n</ul>\n</li>\n</ul>\n<p class=\"p1\"><span style=\"font-size: 16px; font-weight: bold;\">Raw data</span></p>\n<p class=\"p1\">In case you want to play with the raw data yourself, here it is:</p>\n<ul>\n<li>The poll questions: <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE6MQ#gid=0\">for the LW poll</a>; <a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHVPRDVpbEJHWnVzZldHa0tkWGIzMVE6MA#gid=0\">for the mechanical Turk poll</a>.</li>\n<li>The raw responses: <a href=\"https://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdHpaX2M5dnhwSjhXaG9EV2NNNTdMLUE&amp;output=html\">for the LW poll</a>; <a href=\"http://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdEhCUjhnMnBIV1l2VTJUR2RibjJSYlE&amp;output=html\">for the mechanical Turk poll</a>.</li>\n<li>The raw responses converted into numbers, in case you want to run correlations but don't want to mess with text: <a href=\"http://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdHYtYXQzU3VzdXg3eUhFSHNQdGFqaWc&amp;output=html\">for the LW poll</a> (note that this includes only the first 68 responses, which are all I analyzed above);<a href=\"https://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0AnoM_ZsIBBwEdGxpLXNqdmhYTEx4dUhiVjd2clFGeWc&amp;output=html\"> for the mechanical Turk poll</a>.</li>\n</ul>\n<p>Given the importance of taking actions that actually relate to one's goals (for happiness, income, world-saving, you name it -- and, hence, for real rationality), further investigation here would be welcome, either via further polls on LW-ers or others, or, perhaps even more easily and usefully, via <a href=\"http://scholar.google.com/\">Google</a> <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">Scholar</a>.</p>\n<p>&nbsp;</p>\n<hr>\n<p>[1] Originally, I asked about \"trouble with akrasia\" (now and as a kid) rather than about \"trouble with procrastination\". &nbsp;I edited the question after realizing I'd want a control group with non-LWers, and that that group would not reliably know the term \"akrasia\". &nbsp;So, the LW-er responses are partly to one wording and partly to the other.</p>\n<p>[2] Both items correlated strongly with student status; when I controlled for student status (subtracted out the constant necessary to remove the correlation), \"have had late bills reported to credit agencies\" correlated strongly with diagnoses of ADD, autism/Asperger's, and depression, but with nothing else; dirty kitchen floors correlated with nothing.</p>\n<p>[3] This is further suggested by the fact that Mechanical Turk-ers may well *have* more akrasia than average at present; they are working on Mechanical Turk, and have fairly high numbers of depression diagnoses.</p>\n<p>[4] There are 34 correlations at the p&lt;.03 significance level, and 12 at the p&lt;.001 significance level, which is more than we should expect by chance; this is not surprising, since of course e.g. being in school is correlated with being young, and so of course we see some non-chance correlations; the question is how many of the non-obvious correlations are just chance.</p>", "sections": [{"title": "Procedure", "anchor": "Procedure", "level": 1}, {"title": "Validity of \u201cakrasia/procrastination\u201d self-reports", "anchor": "Validity_of__akrasia_procrastination__self_reports", "level": 1}, {"title": "Deliberate decision making not harmful", "anchor": "Deliberate_decision_making_not_harmful", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "106 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rR9Rfjpq5LoBCu7Zs", "37sHjeisS9uJufi4u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T18:08:38.397Z", "modifiedAt": null, "url": null, "title": "Evidence against Calorie Restriction", "slug": "evidence-against-calorie-restriction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gNgBzCcD4Yxw4wHss/evidence-against-calorie-restriction", "pageUrlRelative": "/posts/gNgBzCcD4Yxw4wHss/evidence-against-calorie-restriction", "linkUrl": "https://www.lesswrong.com/posts/gNgBzCcD4Yxw4wHss/evidence-against-calorie-restriction", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evidence%20against%20Calorie%20Restriction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvidence%20against%20Calorie%20Restriction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNgBzCcD4Yxw4wHss%2Fevidence-against-calorie-restriction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evidence%20against%20Calorie%20Restriction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNgBzCcD4Yxw4wHss%2Fevidence-against-calorie-restriction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgNgBzCcD4Yxw4wHss%2Fevidence-against-calorie-restriction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 93, "htmlBody": "<p><a href=\"http://www.slate.com/articles/health_and_science/the_mouse_trap/2011/11/lab_mice_are_they_limiting_our_understanding_of_human_disease_.html\">An article</a> about the pitfalls of using mice for animal research leads off with the example of calorie restriction. The controls for calorie restriction mice experiments were obese mice, suggesting that the health benefits of calorie restriction might be conflated with the health benefits of not being obese. I get the impression that people who study calorie restriction still think it worthwhile for life extension, but it's useful to try and integrate all evidence you come across.</p>\n<p>&nbsp;</p>\n<p>[edit] <a href=\"/r/discussion/lw/8ge/evidence_against_calorie_restriction/59jj\">timtyler suggests</a> this is a well-understood effect that's already been taken into account by CR scientists.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gNgBzCcD4Yxw4wHss", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 12, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "10958", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T18:43:43.815Z", "modifiedAt": null, "url": null, "title": "A Rational Approach to Education", "slug": "a-rational-approach-to-education", "viewCount": null, "lastCommentedAt": "2017-06-17T04:35:07.424Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jrichardliston", "createdAt": "2011-11-15T15:48:20.692Z", "isAdmin": false, "displayName": "jrichardliston"}, "userId": "Sgga8yGd48PGJFuxy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrtmhMph5PHGfCeoo/a-rational-approach-to-education", "pageUrlRelative": "/posts/RrtmhMph5PHGfCeoo/a-rational-approach-to-education", "linkUrl": "https://www.lesswrong.com/posts/RrtmhMph5PHGfCeoo/a-rational-approach-to-education", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Rational%20Approach%20to%20Education&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Rational%20Approach%20to%20Education%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrtmhMph5PHGfCeoo%2Fa-rational-approach-to-education%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Rational%20Approach%20to%20Education%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrtmhMph5PHGfCeoo%2Fa-rational-approach-to-education", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrtmhMph5PHGfCeoo%2Fa-rational-approach-to-education", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 794, "htmlBody": "<p>I invite the LW community to help me identify where my thinking about education is wrong here...</p>\n<p>While it is necessary to continue to improve our educational systems for youths, we must do a better job of focusing our energies on educating adults, for it is adults who make the decisions about how our society runs, and how we educate our youths. If adults are not sufficiently educated, then youths will not be sufficiently educated.</p>\n<p>What kind of educational program do adults require? I advocate a \"pull model\" of education: adults must be compelled of their own accord to seek knowledge, because learning can at times be quite difficult. Why is it difficult? It challenges our world views, which can be a very scary thing, particularly for people whose world views have remained essentially unchanged for decades.</p>\n<p>On the whole, however, the model we currently employ for education is a \"push model\": we tend to tell people what they must learn in order to be educated. And in the name of efficiency we pre-construct programs of study. But the push model is impractical. It does not account for the range of human experience and desire. To try to force a person to learn probability and statistics, for example, when they have not yet developed a healthy desire to learn it is too stressful and does not allow their minds to be open to reflecting on and integrating the new concepts. The time for a person to learn a given topic is when they begin to ask questions about it.</p>\n<p>How can we create an environment that fosters a pull model of education? First, we can offer a series of courses in multiple disciplines that people can sample. This would consist of a broad range of courses in the arts, sciences and humanities. Rather than lasting for several months at a time, they would last for only a few weeks---long enough for the student to get a sufficient introduction to the topic. And they would be fun. For example, people could get a sense of what they can gain from learning probability and statistics by giving them play money at the beginning of a class and having them gamble on some well-constructed games. Then they can be taught some simple, but perhaps non-intuitive mathematical \"tricks\" that would allow them to be more successful as they play the games.</p>\n<p>By sampling a broad range of topics while engaging in discussions about issues facing current society they will both identify things they are talented at and things they would like to see changed in the world. Once these have become clear, with sufficient mentorship the thing they want to focus their studies on will crystallize for them. Once their focus has crystallized, it is time to work with them to build an individualized, interdisciplinary program of study in which everything they study is directly related to what they wish to accomplish, and they understand the connection. Then they will spend some period of time acquiring the skills they need to accomplish their goals.</p>\n<p>How should they acquire these skills? By and large we offer only a single model of teaching and learning---lecture, exam, grade (often followed by forgetting; see Father Guido Sarducci's Five Minute University: http://www.youtube.com/watch?v=kO8x8eoU3L4)---although some people are making a lot of money convincing large groups of people that online learning is the way to go, but it appears that few people actually learn well this way. The real point---and people who study education know this, they just don't implement it because the current system is so entrenched---is that people process information in different ways, and we must provide an environment for people to learn in the way they process information.</p>\n<p>As people acquire the skills they need, they must put them to use in order to maintain them. This will be done by having them engage in projects that are directly related to what they wish to accomplish. This will also give them practical experience in the world.</p>\n<p>What should be the cost of such an education? This depends on the economic situation of the individual. We live in a stressful world. The very people who most need education have very little money to devote to it, even if they desire it. Their education must cost only as much as the individual feels he or she can afford, otherwise they will simply not show up.</p>\n<p>Will this create a world in which everyone, or even most people, understand topics like probability and statistics? Maybe, maybe not. But if a large group of people learn even a little more about a wide range of topics, and learn to enjoy and respect learning and knowledge, then we will be far more likely to be living in a more rational world.</p>\n<p>Ok, there you go. Have at it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrtmhMph5PHGfCeoo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -8, "extendedScore": null, "score": 8.005289954602376e-07, "legacy": true, "legacyId": "10959", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T18:52:09.519Z", "modifiedAt": null, "url": null, "title": "New AI risks research institute at Oxford University", "slug": "new-ai-risks-research-institute-at-oxford-university", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.771Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wTgS6J73XMjYCKGC7/new-ai-risks-research-institute-at-oxford-university", "pageUrlRelative": "/posts/wTgS6J73XMjYCKGC7/new-ai-risks-research-institute-at-oxford-university", "linkUrl": "https://www.lesswrong.com/posts/wTgS6J73XMjYCKGC7/new-ai-risks-research-institute-at-oxford-university", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20AI%20risks%20research%20institute%20at%20Oxford%20University&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20AI%20risks%20research%20institute%20at%20Oxford%20University%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTgS6J73XMjYCKGC7%2Fnew-ai-risks-research-institute-at-oxford-university%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20AI%20risks%20research%20institute%20at%20Oxford%20University%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTgS6J73XMjYCKGC7%2Fnew-ai-risks-research-institute-at-oxford-university", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTgS6J73XMjYCKGC7%2Fnew-ai-risks-research-institute-at-oxford-university", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<p>The <a href=\"http://www.futuretech.ox.ac.uk/\">Oxford Martin Programme on the Impacts of Future Technology</a>&nbsp;(aka FutureTech) is a new research department at Oxford University, roughly a spin-off of FHI, but <em>focusing</em> on AI and nanotech risks and <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>. Like FHI, this department is directed by <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>. They'll be hiring more researchers soon. Basically, this means more people and money being devoted to existential risk reduction.</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/party.gif\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p>Okay, now back to work.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jZF2jwLnPKBv6m3Ag": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wTgS6J73XMjYCKGC7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 36, "extendedScore": null, "score": 8.005319822401074e-07, "legacy": true, "legacyId": "10960", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T19:11:14.253Z", "modifiedAt": null, "url": null, "title": "FAI FAQ draft: What is the Singularity?", "slug": "fai-faq-draft-what-is-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vu8LDecutbPYSiJfp/fai-faq-draft-what-is-the-singularity", "pageUrlRelative": "/posts/vu8LDecutbPYSiJfp/fai-faq-draft-what-is-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/vu8LDecutbPYSiJfp/fai-faq-draft-what-is-the-singularity", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20FAQ%20draft%3A%20What%20is%20the%20Singularity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20FAQ%20draft%3A%20What%20is%20the%20Singularity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvu8LDecutbPYSiJfp%2Ffai-faq-draft-what-is-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20FAQ%20draft%3A%20What%20is%20the%20Singularity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvu8LDecutbPYSiJfp%2Ffai-faq-draft-what-is-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvu8LDecutbPYSiJfp%2Ffai-faq-draft-what-is-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 483, "htmlBody": "<p>I invite your feedback on this snippet from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. This one is an answer to the question \"What is the Singularity?\"</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>There are <a href=\"http://en.wikipedia.org/wiki/Singularity\">many types</a> of mathematical and physical singularities, but in this FAQ we use the term 'Singularity' to refer to the <em>technological</em> singularity.</p>\n<p>There are also many things someone might have in mind when they refer to a 'technological Singularity' (Sandberg 2010). Below, we&rsquo;ll explain just three of them (Yudkowsky 2007):</p>\n<ol>\n<li>Intelligence explosion</li>\n<li>Event horizon</li>\n<li>Accelerating change</li>\n</ol>\n<p>&nbsp;</p>\n<h5>Intelligence explosion</h5>\n<p>Every year, computers surpass human abilities in new ways. A program written in 1956 was able to prove mathematical theorems, and found a more elegant proof for one of them than Russell and Whitehead had given in <em>Principia Mathematica</em> (MacKenzie 1995). By the late 1990s, 'expert systems' had surpassed human skill for a wide range of tasks (Nilsson 2009). In 1997, IBM's Deep Blue computer beat the world chess champion (Campbell et al. 2002), and in 2011 IBM's Watson computer beat the best human players at a much more complicated game: <em>Jeopardy!</em> (Markoff 2011). Recently, a robot named Adam was programmed with our scientific knowledge about yeast, then posed its own hypotheses, tested them, and assessed the results (King et al. 2009; King 2011).</p>\n<p>Computers remain far short of human intelligence, but the resources that aid AI design are accumulating (including hardware, large datasets, neuroscience knowledge, and AI theory). We may one day design a machine that surpasses human skill <em>at designing artificial intelligences</em>. After that, this machine could improve its own intelligence faster and better than humans can, which would make it even <em>more</em> skilled at improving its own intelligence. This could continue in a positive feedback loop such that the machine quickly becomes vastly more intelligent than the smartest human being on Earth: an 'intelligence explosion' resulting in a machine superintelligence (Good 1965).</p>\n<p>&nbsp;</p>\n<h5>Event horizon</h5>\n<p>Vernor Vinge (1993) wrote that the arrival of machine superintelligence represents an 'event horizon' beyond which humans cannot model the future, because events beyond the Singularity will be stranger than science fiction: too weird for human minds to predict. So far, all social and technological progress has resulted from human brains, but humans cannot predict what future radically different and more powerful intelligences will create. He made an analogy to the <a href=\"http://en.wikipedia.org/wiki/Event_horizon\">event horizon</a> of a black hole, beyond which the predictive power of physics at the <a href=\"http://en.wikipedia.org/wiki/Gravitational_singularity\">gravitational singularity</a> breaks down.</p>\n<p>&nbsp;</p>\n<h5>Accelerating Change</h5>\n<p>A third concept of technological singularity refers to accelerating change in technological development.</p>\n<p>Ray Kurzweil (2005) has done the most to promote this idea. He suggests that although we expect linear technological change, information technological progress is <em>exponential</em>, and so the future will be more different than most of us expect. Technological progress enables even faster technological progress. Kurzweil suggests that technological progress may become so fast that humans cannot keep up unless they amplify their own intelligence by integrating themselves with machines.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vu8LDecutbPYSiJfp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": -1, "extendedScore": null, "score": 8.005387433426888e-07, "legacy": true, "legacyId": "10961", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["23odz5WbpEvXKw5HZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-16T21:14:30.443Z", "modifiedAt": null, "url": null, "title": "Writing feedback requested: activists should pursue a positive Singularity", "slug": "writing-feedback-requested-activists-should-pursue-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.839Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9TixrDHucfwBBZp3r/writing-feedback-requested-activists-should-pursue-a", "pageUrlRelative": "/posts/9TixrDHucfwBBZp3r/writing-feedback-requested-activists-should-pursue-a", "linkUrl": "https://www.lesswrong.com/posts/9TixrDHucfwBBZp3r/writing-feedback-requested-activists-should-pursue-a", "postedAtFormatted": "Wednesday, November 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Writing%20feedback%20requested%3A%20activists%20should%20pursue%20a%20positive%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWriting%20feedback%20requested%3A%20activists%20should%20pursue%20a%20positive%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TixrDHucfwBBZp3r%2Fwriting-feedback-requested-activists-should-pursue-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Writing%20feedback%20requested%3A%20activists%20should%20pursue%20a%20positive%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TixrDHucfwBBZp3r%2Fwriting-feedback-requested-activists-should-pursue-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9TixrDHucfwBBZp3r%2Fwriting-feedback-requested-activists-should-pursue-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 247, "htmlBody": "<p><span style=\"font-family: arial;\">I managed to turn an essay assignment into an opportunity to write about the Singularity, and I thought I'd turn to LW for feedback on the paper. The paper is about&nbsp;</span><a style=\"font-family: arial;\" href=\"http://en.wikipedia.org/wiki/Thomas_Pogge\" target=\"_blank\">Thomas Pogge</a><span style=\"font-family: arial;\">, a German philosopher who works on <a href=\"http://en.wikipedia.org/wiki/Health_Impact_Fund\">institutional efforts to end poverty</a> and is a pledger for&nbsp;</span><a style=\"font-family: arial;\" href=\"http://en.wikipedia.org/wiki/Giving_What_We_Can\" target=\"_blank\">Giving What We Can</a><span style=\"font-family: arial;\">.&nbsp;</span></p>\n<p><span style=\"font-family: arial;\">I offer a basic argument that he and other poverty activists should work on creating a positive Singularity, sampling liberally from well-known Less Wrong arguments. It's more academic than I would prefer, and it includes some loose talk of 'duties' (which bothers me), but for its goals, these things shouldn't be a huge problem. But maybe they are - I want to know that too.</span></p>\n<p><span style=\"font-family: arial;\">I've already turned the assignment in, but when I make a better version, I'll send the paper to Pogge himself. I'd like to see if I can successfully introduce him to these ideas. My one conversation with him indicates that he would be open to actually changing his mind. He's clearly thought deeply about how to do good, and may simply have not been exposed to the idea of the Singularity yet.</span></p>\n<p><span style=\"font-family: arial;\">I want feedback on all aspects of the paper &nbsp;- style, argumentation, clarity. Be as constructively cruel as I know only you can.</span></p>\n<p><span style=\"font-family: arial;\">If anyone's up for it, fee free to add feedback using Track Changes and email me a copy - mjcurzi[at]wustl.edu. I obviously welcome comments on the thread as well.</span></p>\n<p>You can read the paper <a href=\"http://michaelcurzi.com/media/\">here</a>&nbsp;in various formats.</p>\n<p>Upvotes for all. Thank you!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9TixrDHucfwBBZp3r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 4, "extendedScore": null, "score": 8.005824297460459e-07, "legacy": true, "legacyId": "10962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T03:22:25.901Z", "modifiedAt": null, "url": null, "title": "Human augmented with CBT algorithms games Jeopardy [link]", "slug": "human-augmented-with-cbt-algorithms-games-jeopardy-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FrDgwasL5QMWxkbyy/human-augmented-with-cbt-algorithms-games-jeopardy-link", "pageUrlRelative": "/posts/FrDgwasL5QMWxkbyy/human-augmented-with-cbt-algorithms-games-jeopardy-link", "linkUrl": "https://www.lesswrong.com/posts/FrDgwasL5QMWxkbyy/human-augmented-with-cbt-algorithms-games-jeopardy-link", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Human%20augmented%20with%20CBT%20algorithms%20games%20Jeopardy%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHuman%20augmented%20with%20CBT%20algorithms%20games%20Jeopardy%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrDgwasL5QMWxkbyy%2Fhuman-augmented-with-cbt-algorithms-games-jeopardy-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Human%20augmented%20with%20CBT%20algorithms%20games%20Jeopardy%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrDgwasL5QMWxkbyy%2Fhuman-augmented-with-cbt-algorithms-games-jeopardy-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFrDgwasL5QMWxkbyy%2Fhuman-augmented-with-cbt-algorithms-games-jeopardy-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://thenextweb.com/shareables/2011/11/16/mind-blown-this-guy-broke-jeopardys-all-time-record-with-an-app/\">http://thenextweb.com/shareables/2011/11/16/mind-blown-this-guy-broke-jeopardys-all-time-record-with-an-app/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FrDgwasL5QMWxkbyy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 19, "extendedScore": null, "score": 8.007128458798626e-07, "legacy": true, "legacyId": "10970", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T03:30:25.430Z", "modifiedAt": null, "url": null, "title": "LessWrong virtual meetup this Saturday evening", "slug": "lesswrong-virtual-meetup-this-saturday-evening", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "erratio", "createdAt": "2010-06-29T09:32:42.768Z", "isAdmin": false, "displayName": "erratio"}, "userId": "ty7er2ZYEnPYALnnJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y3AhHL82apjrumEam/lesswrong-virtual-meetup-this-saturday-evening", "pageUrlRelative": "/posts/y3AhHL82apjrumEam/lesswrong-virtual-meetup-this-saturday-evening", "linkUrl": "https://www.lesswrong.com/posts/y3AhHL82apjrumEam/lesswrong-virtual-meetup-this-saturday-evening", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20virtual%20meetup%20this%20Saturday%20evening&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20virtual%20meetup%20this%20Saturday%20evening%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3AhHL82apjrumEam%2Flesswrong-virtual-meetup-this-saturday-evening%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20virtual%20meetup%20this%20Saturday%20evening%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3AhHL82apjrumEam%2Flesswrong-virtual-meetup-this-saturday-evening", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy3AhHL82apjrumEam%2Flesswrong-virtual-meetup-this-saturday-evening", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Several months ago jwhendy called for people who can't make it to regular LW meetups (or just can't get enough of the LW crowd) to <a title=\"meet online\" href=\"/lw/54t/interest_in_videoconference_discussion_about/\">meet online</a> instead. Several months later, a small number of us still do, on a near-weekly basis. We socialise, teach each other about our fields, throw theories around and discuss LW material. We're looking for fresh blood after losing some of our regulars to real life commitments.</p>\n<p>Meetup details:</p>\n<p>Time: Saturday, 19th November, 9pm EST</p>\n<p>Location: Skype (add me as erratio1 on there)</p>\n<p>Format: Variable. The last few meetups have been text-only due to connection problems at my end, but since the overall quality of the meetup was much higher when we had voice chat I'd like to get back to that if possible. Although if a majority of interested people would prefer text-only, we can do that too.</p>\n<p>For those who can't make it to this meetup but are interested, comment with your availabilities here or at <a title=\"our Google Group\" href=\"http://groups.google.com/group/lesswrong-virtual-meetups?hl=en\">our Google Group</a> so we can potentially accommodate you in future weeks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y3AhHL82apjrumEam", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 8.007156792149037e-07, "legacy": true, "legacyId": "10971", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["krvQb6uFeYCKXB5Aa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T05:39:31.671Z", "modifiedAt": null, "url": null, "title": "Justification Through Pragmatism", "slug": "justification-through-pragmatism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Irgy", "createdAt": "2011-10-26T20:04:09.551Z", "isAdmin": false, "displayName": "Irgy"}, "userId": "dF9dmip5RhNq9CCM3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JWuebGF3AZZeXv6H4/justification-through-pragmatism", "pageUrlRelative": "/posts/JWuebGF3AZZeXv6H4/justification-through-pragmatism", "linkUrl": "https://www.lesswrong.com/posts/JWuebGF3AZZeXv6H4/justification-through-pragmatism", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Justification%20Through%20Pragmatism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJustification%20Through%20Pragmatism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJWuebGF3AZZeXv6H4%2Fjustification-through-pragmatism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Justification%20Through%20Pragmatism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJWuebGF3AZZeXv6H4%2Fjustification-through-pragmatism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJWuebGF3AZZeXv6H4%2Fjustification-through-pragmatism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1280, "htmlBody": "<p>Justification Through Pragmatism<br /><br />In this article, I propose a new method of justifying fundamental philosophical assumptions.<br /><br />The fundamental assumptions on which we base our thinking cannot ever be proven to be true, as such a proof must rely on our own thinking and be circular as a result. The proposed alternative is that rather than even worry about whether these assumptions are true at all, instead demonstrate that regardless of their truth, there is no conceivable benefit to negating them. This article demonstrates this point by way of a number of examples of such assumptions.<br /><br /><br />0. Basic Capacity for Reason<br /><br />Assumption: That one has the most basic ability to understand anything correctly.<br />Negation: One has no ability to understand anything nor to make any rational decisions whatsoever. One may have a delusion of understanding, but it bears no correlation with truth.<br /><br />I start at 0 because this is such a fundamental assumption that it's on a level below everything else. Descartes started by doubting even his own existance, but found a proof of his existence from the very doubt itself. However, he could have gone a step further and doubted his basic ability to even understand that proof. Sure it seems a compelling argument that you must exist in order to doubt your own existance, but just seeming compelling doesn't make it true. How do we know for sure that anything at all we think is true?<br /><br />The fact is, we demonstrably don't. One visible property of the insane is that they often do not know they are insane, and no-one is able to tell them. Every thought we have really could be worthless, and their seeming correspondence with reality a delusion. But if so, well, there's nothing you can really do about it is there? So why worry about it?<br /><br />If every thought is meaningless, then it doesn't matter what you think. So you might as well assume that at least some of what you think makes sense. Thus the assumption that we have some basic capacity for reason may be justified pragmatically, without any concern for whether it is even true.<br /><br />This assumption of basic capacity for reason is absolutely not to be confused with assuming every thought you have to be correct, nor even assuming any particular thought or belief to be correct. By all means question your beliefs and your thought processes. Indeed choosing not to do so is a good step towards failing to live up to this assumption in the first place. This assumption is simply that such questioning need not go on for ever in an endless chain. At some point we have to just accept that at least some our methods of basic reasoning actually work.<br /><br /><br />1. Better and Worse<br /><br />Assumption: There exist experiences which are better and worse than other experiences.<br />Negation: Every possible experience is identical in merit. Nothing is better, more desirable, preferable or superior to anything else.<br /><br />I say \"experiences\" to bring it down to the most fundamental interface, and avoid even implicitly assuming the existance of a real world.<br /><br />If nothing is better or worse than anything else, in any way, then it fundamentally does not matter what we do. So there cannot be any harm in acting as if better and worse really do exist.<br /><br />Of course this says nothing about what better and worse actually are, nor even how to go about figuring such a thing out. It's also possible that both the existance and nature of better and worse can be learned through experience, or even that they or aspects of them are fundamentally self-evident. So this assumption may or may not be either necessary or helpful, but it still remains as another good example of a pragmatically justifiable assumption.<br /><br /><br />2. Future, Causality, Free Will and Control<br /><br />Assumption: That there are experiences to be had in the future, and that choices we make have some impact on those experiences.<br />Negation: Either there will not be any future experiences, or there will be but we have no control whatsoever about what they will be.<br /><br />Yes, this is quite clearly about four assumptions rolled into one - as listed in the name. However, it's really quite difficult to talk about any of them on their own. It's hard to even define any of these without the ones earlier in the list, but each is also somewhat worthless without the next.<br /><br />Without some sort of control over our future experiences, all of our choices and actions are meaningless. Could time suddenly come to an unexpected halt? Could we be simply be riding along on some sort of experience-movie, under some sort of illusion that our minds control the ride? Sure, it's possible, but if we really do have no control over the future it doesn't really matter what we do. So again, there can be no harm in assuming that we do have control, just in case it is true.<br /><br />Again, whether this needs to be assumed or can be learned is a separate issue, the point is there's no benefit to removing this assumption so it might just as well be made regardless.<br /><br /><br />3. Sufficient Information<br /><br />Assumption: That we are able to determine (using #0), which of the choices (#2) we make will lead to a better (#1) outcome.<br />Negation: That although we may have motivation and ability to control our future, we have no way to figure out what the right choice actually is.<br /><br />To explain the need for this, consider the hypothetical universe of the left-handed god (this is not my own original idea, but I have no clue who I've stolen it from, so, well, sorry whoever you are). In this universe, people who have their left hand raised (and explicitly not their right hand, nor neither hand) when they die go to heaven, and experience an eternity of peace and fullfillment. Everyone else is doomed to an eternity of torment. However, in this universe there is also talk of a right-handed god, a strikingly similar entity but somewhat reflected in nature. The trouble is, the living inhabitants of this universe have no means of determining which of these gods is the true figure. The world has a built in symmetry about it and no clue was given.<br /><br />Such situations, or at least their smaller scale approximations, can and do occur in life. But in these situations, there's nothing for it but to pick a hand and move on. We might as well assume though that not every situtation is like that, and figure out and concentrate our efforts on the ones which are not.<br /><br />Again, note that this assumption is not that we have sufficient information about everything, only that we have sufficient information about something. Distinguishing left-handed gods from solvable dilemnas is still clearly a worthwhile task.<br /><br /><br />Overall then I have shown four examples of assumptions which may be taken for purely pragmatic reasons, regardless of the actual likelihood of their truth. It is my further view that these assumptions (and indeed possibly even just #0) are sufficient in the sense that no other base assumptions are necessary. But that is a much longer story.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JWuebGF3AZZeXv6H4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -2, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "10973", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T05:50:43.120Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Fake Utility Functions", "slug": "seq-rerun-fake-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PkuEBzJWA9LgKZPhd/seq-rerun-fake-utility-functions", "pageUrlRelative": "/posts/PkuEBzJWA9LgKZPhd/seq-rerun-fake-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/PkuEBzJWA9LgKZPhd/seq-rerun-fake-utility-functions", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Fake%20Utility%20Functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Fake%20Utility%20Functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkuEBzJWA9LgKZPhd%2Fseq-rerun-fake-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Fake%20Utility%20Functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkuEBzJWA9LgKZPhd%2Fseq-rerun-fake-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPkuEBzJWA9LgKZPhd%2Fseq-rerun-fake-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/lq/fake_utility_functions/\">Fake Utility Functions</a> was originally published on 06 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Describes the seeming fascination that many have with trying to compress morality down to a single principle. The sequence leading up to this post tries to explain the cognitive twists whereby people smuggle all of their complicated other preferences into their choice of exactly which acts they try to justify using their single principle; but if they were really following only that single principle, they would choose other acts to justify.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8ga/seq_rerun_uncritical_supercriticality/\">Uncritical Supercriticality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PkuEBzJWA9LgKZPhd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.007654189786857e-07, "legacy": true, "legacyId": "10974", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NnohDYHNnKDtbiMyp", "gRpfoLLFCcT4R5p77", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T10:45:37.460Z", "modifiedAt": null, "url": null, "title": "Bayes Slays Goodman's Grue", "slug": "bayes-slays-goodman-s-grue", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:37.367Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "potato", "createdAt": "2011-06-15T09:18:51.735Z", "isAdmin": false, "displayName": "Ronny"}, "userId": "kY5hs2WkacnSZd937", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xfJo9DFNQL9ZR4R6D/bayes-slays-goodman-s-grue", "pageUrlRelative": "/posts/xfJo9DFNQL9ZR4R6D/bayes-slays-goodman-s-grue", "linkUrl": "https://www.lesswrong.com/posts/xfJo9DFNQL9ZR4R6D/bayes-slays-goodman-s-grue", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bayes%20Slays%20Goodman's%20Grue&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABayes%20Slays%20Goodman's%20Grue%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxfJo9DFNQL9ZR4R6D%2Fbayes-slays-goodman-s-grue%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bayes%20Slays%20Goodman's%20Grue%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxfJo9DFNQL9ZR4R6D%2Fbayes-slays-goodman-s-grue", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxfJo9DFNQL9ZR4R6D%2Fbayes-slays-goodman-s-grue", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2462, "htmlBody": "<p>This is a first stab at solving Goodman's famous grue problem. I haven't seen a post on LW about the grue paradox, and this surprised me since I had figured that if any arguments would be raised against Bayesian LW doctrine, it would be the grue problem. I haven't looked at many proposed solutions to this paradox, besides some of the basic ones in \"The New Problem of Induction\". So, I apologize now if my solution is wildly unoriginal. I am willing to put you through this dear reader because:</p>\n<ol>\n<li>I wanted to see how I would fare against this still largely open, devastating, and classic problem, using only the arsenal provided to me by my minimal Bayesian training, and my regular LW reading.</li>\n<li>I wanted the first LW article about the grue problem to attack it from a distinctly <em>Lesswrongian</em> aproach without the benefit of hindsight knowledge of the solutions of non-LW philosophy.&nbsp;</li>\n<li>And lastly, because, even if this solution has been found before, if it is the right solution, it is to LW's credit that its students can solve the grue problem with only the use of LW skills and cognitive tools. </li>\n</ol>\n<p>I would also like to warn the savvy subjective Bayesian that just because I think that probabilities model frequencies, and that I require frequencies out there in the world, does not mean that I am a frequentest or a realist about probability. I am a formalist with a grain of salt. There are no probabilities anywhere in my view, not even in minds; but the theorems of probability theory when interpreted share a fundamental contour with many important tools of the inquiring mind, including both, the nature of frequency, and the set of rational subjective belief systems. There is nothing more to probability than that system which produces its theorems.&nbsp;</p>\n<p>Lastly, I would like to say, that even if I have not succeeded here (which I think I have), there is likely something valuable that can be made from the leftovers of my solution after the onslaught of penetrating critiques that I expect form this community. Solving this problem is essential to LW's methods, and our arsenal is fit to handle it. If we are going to be taken seriously in the philosophical community as a new movement, we must solve serious problems from academic philosophy, and we must do it in distinctly <em>Lesswrongian</em> ways.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<blockquote>\"The first emerald ever observed was green.<br />The second emerald ever observed was green.<br />The third emerald ever observed was green.<br />&hellip; etc. <br />The nth emerald ever observed was green.<br />(conclusion):<br />There is a very high probability that a never before observed emerald will be green.\"</blockquote>\n<p>That is the inference that the grue problem threatens, courtesy of Nelson Goodman.&nbsp; The grue problem starts by defining \"grue\":</p>\n<blockquote>\n<p>\"An object is grue iff it is first observed before time T, and it is green, or it is first observed after time T, and it is blue.\"</p>\n</blockquote>\n<p>So you see that before time T, from the list of premises:</p>\n<blockquote>\n<p>\"The first emerald ever observed was green.<br />&nbsp;The second emerald ever observed was green.<br />&nbsp;The third emerald ever observed was green.<br />&nbsp;&hellip; etc. <br />&nbsp;The nth emerald ever observed was green.\"<br />&nbsp;(we will call these the green premises)</p>\n</blockquote>\n<p>it follows that:</p>\n<blockquote>\n<p>\"The first emerald ever observed was grue.<br />The second emerald ever observed was grue.<br />The third emerald ever observed was grue.<br />&hellip; etc. <br />The nth emerald ever observed was grue.\"<br />(we will call these the grue premises)</p>\n</blockquote>\n<p>The proposer of the grue problem asks at this point: \"So if the green premises are evidence that the next emerald will be green, why aren't the grue premises evidence for the next emerald being grue?\" If an emerald is grue after time T, it is not green. Let's say that the green premises brings the probability of \"A new unobserved emerald is green.\" to 99%. In the skeptic's hypothesis, by symmetry it should also bring the probability of \"A new unobserved emerald is grue.\" to 99%. But of course after time T, this would mean that the probability of observing a green emerald is 99%, and the probability of not observing a green emerald is at least 99%, since these sentences have no intersection, i.e., they cannot happen together, to find the probability of their disjunction we just add their individual probabilities. This must give us a number at least as big as 198%, which is of course, a contradiction of the Komolgorov axioms. We should not be able to form a statement with a probability greater than one. <br /><br />This threatens the whole of science, because you cannot simply keep this isolated to emeralds and color. We may think of the emeralds as trials, and green as the value of a random variable. Ultimately, every result of a scientific instrument is a random variable, with a very particular and useful distribution over its values. If we can't justify inferring probability distributions over random variables based on their previous results, we cannot justify a single bit of natural science. This, of course, says nothing about how it works in practice. We all know it works in practice. \"A philosopher is someone who say's, 'I know it works in practice, I'm&nbsp; trying to see if it works in principle.'\" - Dan Dennett<br /><br />We may look at an analogous problem. Let's suppose that there is a table and that there are balls being dropped on this table, and that there is an infinitely thin line drawn perpendicular to the edge of the table somewhere which we are unaware of. The problem is to figure out the probability of the next ball being right of the line given the last results. Our first prediction should be that there is a 50% chance of the ball being right of the line, by symmetry. If we get the result that one ball landed right of the line, by Laplace's rule of succession we infer that there is a 2/3ds chance that the next ball will be right of the line. After n trials, if every trial gives a positive result, the probability we should assign to the next trial being positive as well is n+1/n +2.<br /><br />If this line was placed 2/3ds down the table, we should expect that the ratio of rights to lefts should approach 2:1. This gives us a 2/3ds chance of the next ball being a right, and the fraction of Rights out of trials approaches 2/3ds ever more closely as more trials are performed. <br /><br />Now let us suppose a grue skeptic approaching this situation. He might make up two terms \"reft\" and \"light\". Defined as you would expect, but just in case:</p>\n<blockquote>\n<p>\"A ball is reft of the line iff it is right of it before time T when it lands, or if it is left of it after time T when it lands.<br />&nbsp;A ball is light of the line iff it is left of the line before time T when it lands, or if it is right of the line after time T when it first lands.\"</p>\n</blockquote>\n<p>The skeptic would continue:</p>\n<blockquote>\n<p>\"Why should we treat the observation of several occurrences of Right, as evidence for 'The next ball will land on the right.' and not as evidence for 'The next ball will land reft of the line.'?\"</p>\n</blockquote>\n<p>Things for some reason become perfectly clear at this point for the defender of Bayesian inference, because now we have an easy to imaginable model. Of course, if a ball landing right of the line is evidence for Right, then it cannot possibly be evidence for ~Right; to be evidence for Reft, after time T, is to be evidence for&nbsp; ~Right, because after time T, Reft is logically identical to ~Right; hence it is not evidence for Reft, after time T, for the same reasons it is not evidence for ~Right. Of course, before time T, any evidence for Reft is evidence for Right for analogous reasons.<br /><br />But now the grue skeptic can say something brilliant, that stops much of what the Bayesian has proposed dead in its tracks:</p>\n<blockquote>\n<p>\"Why can't I just repeat that paragraph back to you and swap every occurrence of 'right' with 'reft' and 'left' with 'light', and vice versa? They are perfectly symmetrical in terms of their logical realtions to one another.<br />If we take 'reft' and 'light' as primitives, then we have to define 'right' and 'left' in terms of 'reft' and 'light' with the use of time intervals.\"</p>\n</blockquote>\n<p>What can we possibly reply to this? Can he/she not do this with every argument we propose then? Certainly, the skeptic admits that Bayes, and the contradiction in Right &amp; Reft, after time T, prohibits previous Rights from being evidence of both Right and Reft after time T; where he is challenging us is in choosing Right as the result which it is evidence for, even though \"Reft\" and \"Right\" have a completely symmetrical syntactical relationship. There is nothing about the definitions of reft and right which distinguishes them from each other, except their spelling. So is that it? No, this simply means we have to propose an argument that doesn't rely on purely syntactical reasoning. So that if the skeptic performs the swap on our argument, the resulting argument is no longer sound.<br /><br />What would happen in this scenario if it were actually set up? I know that seems like a strangely concrete question for a philosophy text, but its answer is a helpful hint. What would happen is that after time T, the behavior of the ratio: 'Rights:Lefts' as more trials were added, would proceed as expected, and the behavior of the ratio: 'Refts:Lights' would approach the reciprocal of the ratio: 'Rights:Lefts'. The only way for this to not happen, is for us to have been calling the right side of the table \"reft\", or for the line to have moved. We can only figure out where the line is by knowing where the balls landed relative to it; anything we can figure out about where the line is from knowing which balls landed Reft and which ones landed Light, we can only figure out because in knowing this and and time, we can know if the ball landed left or right of the line. <br /><br />To this I know of no reply which the grue skeptic can make. If he/she say's the paragraph back to me with the proper words swapped, it is not true, because&nbsp; In the hypothetical where we have a table, a line, and we are calling one side right and another side left, the only way for Refts:Lefts behave as expected as more trials are added is to move the line (if even that), otherwise the ratio of Refts to Lights will approach the reciprocal of Rights to Lefts. <br /><br />This thin line is analogous to the frequency of emeralds that turn out green out of all the emeralds that get made. This is why we can assume that the line will not move, because that frequency has one precise value, which never changes. Its other important feature is reminding us that even if two terms are syntactically symmetrical, they may have semantic conditions for application which are ignored by the syntactical model, e.g., checking to see which side of the line the ball landed on.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>In conclusion:</p>\n<p>Every random variable has as a part of it, stored in<em> </em>its<em> definition/code</em>, a <em>frequency distribution</em> over its values. By the fact that somethings happen sometimes, and others happen other times, we know that the world contains random variables, even if they are never<em> fundamental in the source code. </em>Note that \"frequency\" is not used as a state of partial knowledge, it is a fact about a set and one of its subsets.</p>\n<p>The reason that:</p>\n<blockquote>\n<p>\"The first emerald ever observed was green.<br />The second emerald ever observed was green.<br />The third emerald ever observed was green.<br />&hellip; etc. <br />The nth emerald ever observed was green.<br />(conclusion):<br />There is a very high probability that a never before observed emerald will be green.\"</p>\n</blockquote>\n<p>is a valid inference, but the grue equivalent isn't, is that grue is not a property that the emerald construction sites of our universe deal with. They are <em>blind</em> to the grueness of their emeralds, they only say anything about whether or not the next emerald will be green. It may be that the rule that the emerald construction sites use to get either a green or non-green emerald change at time T, but the frequency of some particular result out of all trials will never change; the line will not move. As long as we know what symbols we are using for what values, observing many green emeralds is evidence that the next one will be grue, as long as it is before time T, every record of an observation of a green emerald is evidence against a grue one after time T. \"Grue\" changes meanings from green to blue at time T, 'green'''s meaning stays the same since we are using the same physical test to determine green-hood as before; just as we use the same test to tell whether the ball landed right or left. There is no reft in the universe's source code, and there is no grue. Green is not fundamental in the source code, but green can be reduced to some particular range of <em>quanta states; </em>if you had the universes source code, you couldn't write grue without first writing green; writing green without knowing a thing about grue would be just as hard as while knowing grue. Having a physical test, or primary condition for applicability, is what privileges green over grue after time T; to have a physical consistent test is the same as to reduce to a specifiable range of physical parameters; the existence of such a test is what prevents the skeptic from performing his/her swaps on our arguments.</p>\n<p>\n<hr />\nTake this more as a brainstorm than as a final solution. It wasn't originally but it should have been. I'll write something more organized and consize after I think about the comments more, and make some graphics I've designed that make my argument much clearer, even to myself. But keep those comments coming, and tell me if you want specific credit for anything you may have added to my grue toolkit in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xfJo9DFNQL9ZR4R6D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 0, "extendedScore": null, "score": 7e-06, "legacy": true, "legacyId": "10934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T17:04:48.138Z", "modifiedAt": null, "url": null, "title": "Is latent Toxoplasmosis worth doing something about?", "slug": "is-latent-toxoplasmosis-worth-doing-something-about", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:06.448Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TnEjvi2bhTD3ocJie/is-latent-toxoplasmosis-worth-doing-something-about", "pageUrlRelative": "/posts/TnEjvi2bhTD3ocJie/is-latent-toxoplasmosis-worth-doing-something-about", "linkUrl": "https://www.lesswrong.com/posts/TnEjvi2bhTD3ocJie/is-latent-toxoplasmosis-worth-doing-something-about", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20latent%20Toxoplasmosis%20worth%20doing%20something%20about%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20latent%20Toxoplasmosis%20worth%20doing%20something%20about%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnEjvi2bhTD3ocJie%2Fis-latent-toxoplasmosis-worth-doing-something-about%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20latent%20Toxoplasmosis%20worth%20doing%20something%20about%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnEjvi2bhTD3ocJie%2Fis-latent-toxoplasmosis-worth-doing-something-about", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTnEjvi2bhTD3ocJie%2Fis-latent-toxoplasmosis-worth-doing-something-about", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 385, "htmlBody": "<p><em><a href=\"http://en.wikipedia.org/wiki/Toxoplasma_gondii\">Toxoplasmodi gondii</a></em> is a parasitic protozoa who's primary host is cats but also infects other mammals, primarily mice and rats but including humans, as part of its life cycle. Infection by <em>Toxoplasmodi gondii</em> is called <a href=\"http://en.wikipedia.org/wiki/Toxoplasmosis\">Toxoplasmosis</a> and may be acute (flu like symptoms) or latent.&nbsp;</p>\n<p>Toxoplasmosis is extremely common. Worldwide, about <a href=\"http://en.wikipedia.org/wiki/Toxoplasmosis#In_humans\">30% (US 11%; France 88%!)</a>&nbsp;of people about of people have Toxoplasmosis.</p>\n<p>Toxoplasmosis is known to cause <a href=\"http://en.wikipedia.org/wiki/Toxoplasmosis#Biological_modifications_of_the_host\">behavioral changes in rats</a>:</p>\n<blockquote>\n<p>It has been found that the parasite has the ability to change the behaviour of its host: infected rats and mice are less fearful of cats&mdash;in fact, some of the infected rats seek out cat-urine-marked areas. This effect is advantageous to the parasite, which is able proliferate if a cat eats the infected rat and thereby becomes a carrier.</p>\n</blockquote>\n<p>Observational studies suggest that latent Toxoplasmosis may also cause&nbsp;behavioral changes in humans&nbsp;(<a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2526142/pdf/sbl074.pdf\">source paper</a>). The observed differences between infected people and non-infected people include:</p>\n<blockquote>\n<ul>\n<li>Decreased novelty seeking&nbsp;behavior</li>\n<li>Slower reactions</li>\n<li>Lower rule-consciousness and greater jealousy (in men)</li>\n<li>Greater warmth, conscientiousness and moralistic behavior (in women)</li>\n</ul>\n</blockquote>\n<div>It's also suspected by some of being <a href=\"http://en.wikipedia.org/wiki/Toxoplasmosis#Toxoplasma.27s_role_in_schizophrenia\">a cause of Schizophrenia</a>.</div>\n<div>Obviously some or all these may be due to unobserved 3rd causes.</div>\n<div>There haven't been any randomized studies yet, as far as I know. It seems like such studies would be easy to conduct rigorously since a high fraction of the population is already infected. For example, by finding people who are already infected and randomly cure some of them. This kind of experiment is even pretty close to how you would expect the information to be used.&nbsp;</div>\n<div>I've been around cats a fair amount and the base rate is high in the US, so my chances of having latent Toxoplasmosis seem fairly high. Thus I am curious whether this is worth doing something about. Diagnosis sounds like it is fairly simple (PCR on blood samples). It's easy enough so that it can be done in large scale studies at least. <a href=\"http://en.wikipedia.org/wiki/Toxoplasmosis#Latent\">Treatment</a> is done with atovaquone and clindamycin, which appear to be relatively inexpensive.</div>\n<div>I'd expect the effects to be net negatives (most random changes are detrimental) and even if the behavioral effects are smallish, the effects over a lifetime will add up. Has anyone else gotten tested and/or treated for latent Toxoplasmosis? Is it worth it?</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TnEjvi2bhTD3ocJie", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 37, "extendedScore": null, "score": 8.010044831471962e-07, "legacy": true, "legacyId": "10979", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T17:30:56.946Z", "modifiedAt": null, "url": null, "title": "Presentation on Learning", "slug": "presentation-on-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "datadataeverywhere", "createdAt": "2010-08-06T03:09:35.035Z", "isAdmin": false, "displayName": "datadataeverywhere"}, "userId": "wKbyzrGvXH77dFfZX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XbBLuPxMqd4DaDRrh/presentation-on-learning", "pageUrlRelative": "/posts/XbBLuPxMqd4DaDRrh/presentation-on-learning", "linkUrl": "https://www.lesswrong.com/posts/XbBLuPxMqd4DaDRrh/presentation-on-learning", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Presentation%20on%20Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APresentation%20on%20Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbBLuPxMqd4DaDRrh%2Fpresentation-on-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Presentation%20on%20Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbBLuPxMqd4DaDRrh%2Fpresentation-on-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXbBLuPxMqd4DaDRrh%2Fpresentation-on-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 327, "htmlBody": "<p>In order to do a better job putting together my thoughts and knowledge on the subject, I precommitted myself to giving a presentation on learning. My specific goal for the presentation is to inform audience members about how humans actually learn and teach them how to leverage this knowledge to efficiently learn and maintain factual and procedural knowledge and create desired habits.</p>\n<p>I will be focusing a little on background neuroscience, borrowing especially from <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">A Crash Course in the Neuroscience of Human Motivation</a>. I will heavily discuss spaced repetition, and I will also talk about the relevance of System 1 and System 2 thinking. I will not be talking about research, or about how to discover what to learn; for the purposes of my presentation, people already know what they want or need to learn, and have a fairly accurate picture of what that knowledge or those behaviors look like.</p>\n<p>Given that I will only have an hour to speak, I will be unable to explore everything I might like to in depth. Less Wrong (both the site and the community) are my most valuable resource here, so I am asking two things:</p>\n<ol>\n<li>In one hour, what would you cover if you earnestly wanted to improve people's ability to learn?</li>\n<li>What background material do I need to ensure fluency with? This should be material that I need to have adequate familiarity with or else risk presenting an error, even if I don't need to present the material itself in any depth.</li>\n</ol>\n<div>The audience will be students and faculty in a Computer Science department. In decreasing order of number of members, the audience will be Masters students, seniors, Ph.D candidates, professors; no Junior or lower-level undergraduates, so I will probably use computing analogies that wouldn't make sense in other contexts. Because of the audience, I'm also comfortable giving a fairly information-dense presentation, but since I intend to persuade as well as inform the presentation will not be a report.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XbBLuPxMqd4DaDRrh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 8.010137586477502e-07, "legacy": true, "legacyId": "10980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hN2aRnu798yas5b2k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T17:51:59.638Z", "modifiedAt": null, "url": null, "title": "Transcription and Summary of Nick Bostrom's Q&A", "slug": "transcription-and-summary-of-nick-bostrom-s-q-and-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.238Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eXHp9J4PXmQXzmBAj/transcription-and-summary-of-nick-bostrom-s-q-and-a", "pageUrlRelative": "/posts/eXHp9J4PXmQXzmBAj/transcription-and-summary-of-nick-bostrom-s-q-and-a", "linkUrl": "https://www.lesswrong.com/posts/eXHp9J4PXmQXzmBAj/transcription-and-summary-of-nick-bostrom-s-q-and-a", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Transcription%20and%20Summary%20of%20Nick%20Bostrom's%20Q%26A&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATranscription%20and%20Summary%20of%20Nick%20Bostrom's%20Q%26A%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXHp9J4PXmQXzmBAj%2Ftranscription-and-summary-of-nick-bostrom-s-q-and-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Transcription%20and%20Summary%20of%20Nick%20Bostrom's%20Q%26A%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXHp9J4PXmQXzmBAj%2Ftranscription-and-summary-of-nick-bostrom-s-q-and-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeXHp9J4PXmQXzmBAj%2Ftranscription-and-summary-of-nick-bostrom-s-q-and-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 9172, "htmlBody": "<p>INTRO: From the <a href=\"/r/discussion/lw/8fw/new_qa_by_nick_bostrom/\">original posting by Stuart_Armstrong</a>:</p>\n<blockquote>\n<p>Underground Q&amp;A session with Nick Bostrom (<a href=\"http://www.nickbostrom.com/\">http://www.nickbostrom.com</a>) on existential risks and artificial intelligence with the Oxford Transhumanists (recorded 10 October 2011).</p>\n<p><a href=\"http://www.youtube.com/watch?v=KQeijCRJSog\">http://www.youtube.com/watch?v=KQeijCRJSog</a></p>\n</blockquote>\n<p>Below I (will) have a summary of the Q&amp;A followed by the transcription. The transcription is slightly edited, mainly for readability. The numbers are minute markers. Anything followed by a (?) means I don't know quite what he said (example- attruing(?) program), but if you figure it out, let me know!</p>\n<hr />\n<p>SUMMARY: I'll have a summary here by end of the day, probably.</p>\n<hr />\n<p>TRANSCRIPTION:</p>\n<p>Nick: I wanted to just [interact with your heads]. Any questions, really, that you have. To discuss with you. I can say what I&rsquo;m working on right now which is this book on super-intelligence, not so much on the question of whether and how long it might take to develop machine intelligence that equals human intelligence, but rather what happens if and when that occurs. To forget human level machine intelligence, how quickly, how explosively will we get super-intelligenct, and how can you solve the control problem. If you build super-intelligence how can you make sure it will do what you want. That it will be safe and beneficial.</p>\n<p>Once one starts to pull on that problem, it turns out to be quite complicated and difficult. That it has many aspects to it that I would be happy to talk about. Or if you prefer to talk about other things; existential risks, or otherwise, I&rsquo;d be happy to do that as well. But no presentation, just Q&amp;A. So you all have to provide at least the stimulus. So should I take questions or do you want&hellip;</p>\n<p>[00:01]</p>\n<p><strong>Questioner: So what&rsquo;s your definition of machine intelligence or super-intellegence AI&hellip; Is there like a precise definition there?</strong></p>\n<p>Nick: There isn&rsquo;t. Now if you look at domain specific intelligence, there are already areas where machines surpass humans, such doing arithmetical calculations or chess. I think the interesting point is when machines equal humans in general intelligence or perhaps slightly more specifically in engineering intelligence. So if you had this general capability of being able to program creatively and design new systems... There is in a sense a point at which if you had sufficient capability of <em>that</em> sort, you have general capability.</p>\n<p>Because if you can build new systems, even if all it could initially do is this type engineering work, you can build yourself a poetry module or build yourself a social skills module, if you have that general ability to build . So it might be that general intelligence or it might be that slightly more narrow version of that engineering type of intelligence is the key variable to look at. That&rsquo;s the kind of thing that can unleash the rest. But &ldquo;human-level intelligence&rdquo;... that&rsquo;s a vague term, and I think it&rsquo;s important to understand that. It&rsquo;s not necessarily the natural kind.</p>\n<p>[00:03]</p>\n<p><strong>Questioner: Got a question that maybe should have waited til the end: There are two organizations, FHI and SIAI, working on this. Let's say I thought this was the most important problem in the world, and I should be donating money to this. Who should I give it to?</strong></p>\n<p>Nick:</p>\n<p>It's good. We've come to the chase!</p>\n<p>I think there is a sense that both organizations are synergistic. If one were about to go under or something like that, that would probably be the one. If both were doing well, it's... different people will have different opinions. We work quite closely with a lot of the folks from SIAI.</p>\n<p>There is an advantage to having one academic platform and one outside academia. There are different things these types of organizations give us. If you want to get academics to pay more attention to this, to get postdocs to work on this, that's much easier to do within academia; also to get the ear of policy-makers and media.</p>\n<p>On the other hand, for SIAI there might be things that are easier for them to do. More flexibility, they're not embedded in a big bureaucracy. So they can more easily hire people with non-standard backgrounds without the kind of credentials that we would usually need, and also more grass-roots stuff like the community blog Less Wrong, is easier to do.</p>\n<p>So yeah. I'll give the non-answer answer to that question.</p>\n<p>[00:05]</p>\n<p><strong>Questioner: Do you think a biological component is necessary for an artificial intelligence to achieve sentience or something equivalent?</strong></p>\n<p>Nick: It doesn&rsquo;t seem that that should be advantageous&hellip;If you go all the way back to atoms, it doesn&rsquo;t seem to matter that it&rsquo;s carbon rather than silicon atoms. Then you could wonder, instead of having the same atoms you run a simulation of everything that&rsquo;s going on. Would you have to simulate biological processes? I don&rsquo;t even think that&rsquo;s necessary.</p>\n<p>My guess (and Im not sure about this, I don&rsquo;t have an official position or even a theory about what exactly the criteria are that would make a system conscious)&hellip;But my intuition is that If you replicated computational processes that goes on in a human brain, at a sufficient level of detail, where that sufficient level of detail might be roughly on the level of individual neurons and synapses, I think you would likely have consciousness. And it might be that it&rsquo;s something weaker than that which would suffice. Maybe you wouldn&rsquo;t need every neuron. Maybe you could simplify things and still have consciousness. But at least at that level it seems likely.</p>\n<p>It&rsquo;s a lot harder to say if you had very alien types of mental architecture. Something that wasn&rsquo;t a big neural network but of normal machine intelligence that performs very well in a certain way, but using a very different method than a human brain. Whether that would be conscious as well? Much less sure. A limiting case would be a big lookup table that was physically impossible to realize, but you can imagine having every sort of situation possible described, and that program would run through until it found the situation that matched its current memory and observation and would read off which action it should perform. But that would be an extremely alien type of architecture. But would that have conscious experience or not? Even less clear. It might be that it would not have, but maybe the process of generating this giant look-up table would generate kinds of experiences that you wouldn&rsquo;t get from actually implementing it or something like that. (?)</p>\n<p>[00:07]</p>\n<p><strong>Questioner- This relates to AI being dangerous. It seems to me that while it would certainly be interesting if we were to get AI that were much more intelligent than a human being, its not necessarily dangerous. </strong></p>\n<p><strong></strong><strong>Even if the AI is very intelligent it might be hard for it to get resources for it to actually do anything to be able to manufacture extra hardware or anything like that. There are obviously situations where you can imagine intelligence or Creative thinking can get you out of or get you further capability . So..</strong></p>\n<p>Nick: I guess it&rsquo;s useful to identify two cases: One is sort of the default case unless we successfully implement some sort of safeguard or engineer it in a particular way in order to avoid dangers &hellip;So let&rsquo;s think of a default just a bit: You have something that is super intelligent and capable of improving itself to even more levels of super intelligence&hellip;. I guess one way to get initial possibility of why this is dangerous is to think about why humans are powerful.. Why are we dominant on this planet? It&rsquo;s not because we have stronger muscles or our teeth are sharper or we have special poison glands. It&rsquo;s all because of our brains, which have enabled us to develop a lot of other technologies that give us in effect muscles that are stronger than the other animals&hellip;We have bulldozers and external devices and all the other things. And also, it enables us to coordinate socially and build up complicated society so we can act as groups. And all of this makes us supreme on this planet. We can argue with the case of bacteria which have their own domains where they rule. But certainly in the case of the larger mammals we are unchallenged because of our brains.</p>\n<p>And the brains are not all that different from the brains of other animals. It might be that all these advantages we have are due to a few tweaks on some parameters that occurred in our ancestors a couple million years ago. And these tiny changes in the nature our intelligence that had these huge affects. So just prima facie it then seems possible that that if the system surpassed us by just a small amount that we surpass chimpanzees, it could lead to a similar kind of advantage in power. And if they exceeded our intelligence by a much greater margin, then all of that could happen in a more dramatic fashion</p>\n<p>It&rsquo;s true that you could have in principle an AI that was locked in a box, such that it would be incapable of affecting anything outside the box and in that sense it would be weak. That might be one of the safety methods one tries to apply that I've been thinking about.</p>\n<p>Broadly speaking you can distinguish between two different approaches to solving the control problem, of making sure that super-intelligence, if it&rsquo;s built wouldn&rsquo;t cause harm. On one hand you have capability control measures where you try to limit what the AI is able to do. The most obvious example would be lock it in a box and limit its ability to interact with the rest of the world.</p>\n<p>The other class of approach would be motivation selection methods where you would try to control what it wants to do. Where you build it in such a way that even if it has the power to do all this bad stuff, it would choose not to. But so far, there isn&rsquo;t one method or even a combination of methods that it seems we can currently be fully convinced would work. There&rsquo;s a lot more work needed...</p>\n<p>[00:11]</p>\n<p><strong>Questioner: Human beings have been very successful. One feature of that that has been very crucial are our hands that have enabled us to get a start on working tools and so on. Even if an AI is running on some computer somewhere, that would be more analogous to a very intelligent creature which doesn&rsquo;t have very good hands. It&rsquo;s very hard for it to actually DO anything. </strong></p>\n<p><strong>Maybe the in-the-box method is promising. Because if we just don&rsquo;t give the AI hands, some way to actually do something..If all it can do is alter its own code, and maybe communicate infomationally. That seems...</strong></p>\n<p>Nick: So let&rsquo;s be careful there&hellip; So clearly it&rsquo;s not &ldquo;hands&rdquo; per se. If it didn&rsquo;t have hands it could still be very dangerous, because there are other people with hands, that it could persuade to do its bidding. It might be that it has no direct effectors other than the ability to type very slowly, and then some human gatekeeper could read and choose to act on or not. Even that limited ability to affect the world might be sufficient if it had a super power in the domain of persuasion. So if it had an engineering super-power, it might then get all these other superpowers. And then if it were able to, in particular be a super skilled persuader, it could then get other accessories outside our system that could implement its designs.</p>\n<p>You might have heard of this guy, Eliezer Yudkowsky, about 5 years back who ran a series of role playing exercises...The idea was one person should play the AI, pretend to be in a box. The other should play the human gatekeeper whose job was to not let the AI out of the box, but he has to talk with the AI for a couple of hours over the internet chat. This experiment was run five times, with EY playing the AI and different people playing the human gatekeeper. And for the most part people, who were intitially convinced that they would never let the AI out of the box, but in 3 of 5 cases, the experiment ended with the gatekeepers announcing yes, they would let the AI out of the box.</p>\n<p>This experiment was run under conditions that neither party would be allowed to disclose the methods that were used, the main conversational sequence...sorta maintain a shroud of mystery. But this is where the human-level persuader has two hours to work on the human gatekeeper. It seems reasonable to be doubtful of the ability of humanity to keep the super-intelligent persuader in the box, indefinitely, for that reason.</p>\n<p>[00:15]</p>\n<p><strong>Questioner: How hard do you think the idea of controlling the mentality of intelligence is, with something at least as intelligent as us, considering how hard it is to convince humans to act in a certain civilized way of life?</strong></p>\n<p>Nick: So humans sort of start out with a motivation system and then you can try to persuade them or structure incentives to behave in a certain way. But they don&rsquo;t start out with a tabula rasa where you get to write in what a human&rsquo;s values should be. So that&rsquo;s made a difference. In the case of the super-intelligence of course once it already has unfriendly values and it has sufficient power, it will resist any attempt to corrupt its goal system as it would see it.</p>\n<p>[00:16]</p>\n<p><strong>Questioner: You don&rsquo;t think that like us, its experiences might cause it to question its core values as we do?</strong></p>\n<p>Nick: Well, I think that depends on how the goal system is structured. So with humans we don&rsquo;t have a simple declarative goal structure list. Not like a simple slot where we have super goal and everything else is derived from that</p>\n<p>Rather it&rsquo;s like many different little people inhabit our skull and have their debates and fight it out and make compromises. And in some situations, some of them get a boost like permutations and stuff like that. Then over time we have different things that change what we want like hormones kicking in, fading out, all kinds of processes.</p>\n<p>Another process that might affect us is what I call value accretion. The idea that we can have mechanisms that loads new values into us, as we go along. Like maybe falling in love is like that; Initially you might not value that person for their own sake above any other person. But once you undergo this process you start to value them for their own sake in a special way. So human have this mechanism that make us acquire values depending on our experiences.</p>\n<p>If you were building a machine super intelligence and trying to engineer its goal systems so that it will be reliably safe and human friendly, you might want to go with something, more transparent where you have an easier time seeing what is happening, rather than have a complex modular minds with a lot of different forces battling it out...you might want to have a more hierarchical structure.</p>\n<p><strong>Questioner: What do you think of the necessary&hellip;requisites for the conscious mind? What are the features?</strong></p>\n<p>Nick: Yes, I&rsquo;m not sure. We&rsquo;ve talked a little on that earlier. Suppose there is a certain kind of computation that is needed, that is really is the essence of mind. I&rsquo;m sympathetic to the idea that something in the vicinity of that view might be correct. You have to think about exactly how to develop it. Then there is this stage of what is a computation.</p>\n<p>So there is this challenge (I think it might go back to Hans Moravec but I think similar objections have been raised in philosophy against computationalism) where the idea is that if you have an arbitrary physical system that is sufficiently complicated, it could be a stone or a chair or just anything with a lot of molecules in it. And then you have this abstract computation that you think is what constitutes the implementation of the mind. Then there would be some mathematical mapping between all the parts in your computation and atoms in the chair so that you could artificially, through a very complicated mapping interpret the motions of the molecules in the chair in such a way that they would be seen as implementing the computation. It would not be any plausible mapping, not a useful mapping, but a bizarro mapping. Nonetheless if there were sufficiently limited parts there, you could just arbitrarily define some, by injection..</p>\n<p>And clearly we don&rsquo;t think that all these random physical objects implement the mind, or all possible minds.</p>\n<p>So the lesson to me is that it seems that we need some kind of account of what it means to implement a computation that is not trivial and this mapping function between the abstract entity that is a sort of Turing program, or whatever your model of a computation is and the physical entity that decides to implement it to be some sort of non-trivial representation of what this mapping can look like</p>\n<p>It might have to be reasonably simple. It might have to have certain counter-factual properties, so that the system would have implemented a related, but slightly different computation if you had scrambled the initial conditions of the system in a certain way, so something like that. But this is an open question in the philosophy of mind, to try to nail down what it means to implement the computation.</p>\n<p>[00:20]</p>\n<p><strong>Questioner: To bring back to the goal and motivation approach to making an AI friendly towards us, one of the most effective ways of controlling human behavior, quite aside from goals and motivations , is to train them by instilling neuroses. It&rsquo;s why 99.99% of us in this room couldn&rsquo;t pee in our pants right now even if he really, really wanted to. </strong></p>\n<p><strong>Is it possible to approach controlling an AI in that way or even would it be possible for an AI to develop in such a way that there is a developmental period in which a risk-reward system or some sort of neuroses instilment could be used to basically create these rules that an AI couldn&rsquo;t break?</strong></p>\n<p>Nick: It doesn&rsquo;t sound so promising because a neurosis is a complicated thing that might be a particular syndrome of a phenomenon that occurs in human- style mind, because of the way that humans&rsquo; minds are configured. It&rsquo;s not clear there would be something exactly analogous to that in a cognitive system with a very different architecture.</p>\n<p>Also, because neuroses, at least certain kinds of neuroses, are ones we would choose to get rid of if we could. So if you had a big phobia and there was a button that would remove the phobia, obviously you would press the button. And here we have this system that is presumably able to self-modify. So if it had this big hang up that it didn&rsquo;t like, then it could reprogram itself to get rid of that.</p>\n<p>This would be different than a top-level goal because top-level goal would be the criterion it produced to decide whether to take an action. In particular, like an action to remove the top level goal.</p>\n<p>So generally speaking with reasonable and coherent goal architecture you would get certain convergent instrumental values that would crop up in a wide range of situations. One might be self preservation, not necessarily because you value your own survival for its own sake, but because in many situations you can predict that if you are around in the future you can continue to act in the future according to your goals, and that will make it more likely that the world will then be implementing your goals.</p>\n<p>Another convergent instrumental value might be protection of your goal system from corruption (?) for very much the same reason. For even if you were around in the future but you have different goals from the ones you had now, you would now predict that that means in the future you will no longer be working towards realizing your current goals but maybe towards a completely different purpose, that would make it now less likely that your current goals would be realized. If your current goals are what you use as a criterion to choose an action, you would want to try to take actions that would prevent corruption of your goal system.</p>\n<p>One might list a couple of other of the convergent instrumental values like intelligence amplification, technology perfection and resource acquisition. So this relates to why generic super-intelligence might be dangerous. It&rsquo;s not so much that you have to worry that it would have human Unfriendliness in the sense of disliking human goals, that it would *hate* humans . The danger is that it wouldn&rsquo;t *care* about humans. It would care about something different, like paperclips. But then if you have almost any other goals, like paperclips, there would be these other convergent instrumental reasons that you discover. For while your goal is to make as many paperclips as possible you might want to a) prevent humans from switching you off or tampering with your goal system or b) you might want to acquire as much resources as possible, including planets, and the solar system, and the galaxy. All of that stuff could be made into paperclips. So even with pretty much a random goal, you would end up with these motivational tendencies which would be harmful to humans.</p>\n<p>[00:25]</p>\n<p><strong>Questioner: Appreciating the existential risks, what do you think about goals and motivations, and such drastic measures of control sort of a) ethically and b) as a basis of a working relationship?</strong></p>\n<p>Nick: Well, in terms of the working relationship one has to think about the differences with these kinds of the artificial being. I think there are a lot of (?) about how to relate to artificial agents that are conditioned on the fact that we are used to dealing with human agents, and there are a lot of things we can assume about the human.</p>\n<p>We can assume perhaps that they don&rsquo;t want to be enslaved. Even if they say that they want to be enslaved, we might think that deep inside of them, there is a sort of more genuine authentic self that doesn&rsquo;t want to be enslaved. Even if some prisoner has been brainwashed to do the bidding of their master, maybe we say it&rsquo;s not really good for them because it&rsquo;s in their nature, this will to be autonomous. And there are other things like that, that don&rsquo;t necessarily have to obtain for a completely artificial system which might not have any of that rich human nature that we have.</p>\n<p>So in terms of what the good working relationship is, just as what we think of a good relationship with our word processor or email program. Not in these terms, as if you&rsquo;re exploiting it for your ends, without giving it anything in return. If your email program had a will, presumably it would be the will to be a good and efficient email program that processed your emails properly. Maybe that was the only thing it wanted and cared about. So having a relationship with it would be a different thing.</p>\n<p>There was another part of your question, about whether this would be right and ethical. I think if you are operating a new agent from scratch, and there are many different possible agents you could create, some of those agents will have human style values; they want to be independent and respected. Other agents that you could create would have no greater desire than to be of service. Others would just want paperclips. So if you step back, and look at which of these options we should decide, then looking at the question of moral constraints on which of these are legitimate.</p>\n<p>And I&rsquo;m not saying that those are trivial, I think there are some deep ethical questions here. However in the particular scenario where we are considering the creation of a single super intelligence the more pressing concern would be to ensure that it doesn&rsquo;t destroy everything else, like humanity and its future. Now, if you have a different scenario, like instead of this one uber-mind rising ahead, you have many minds that become smarter and smarter that rival humans and then gradually exceed them</p>\n<p>Say an uploading scenario where you start with very slow software, where you have human like minds running very slowly. In that case, maybe how we should relate to these machine intellects morally becomes more pressing. Or indeed, even if you just have one, but in the process of figuring out what to do it creates &ldquo;thought crimes&rdquo;.</p>\n<p>If you have a sufficiently powerful mind maybe you have thoughts themselves would contain structures that are conscious. This sounds mystical, but imagine you are a very powerful computer and one of the things you are doing is you are trying to predict what would happen in the future under different scenarios, and so you might play out a future</p>\n<p>And if those simulations you are running inside of this program were sufficiently detailed, then they could be conscious. This comes back to our earlier discussion of what is conscious. But I think a sufficiently detailed computer simulation of the mind could be conscious</p>\n<p>You could then have a super intelligence that could process by thinking about things could create sentient beings, maybe millions or billions or trillions of them, and their welfare would then be a major ethical issue. They might be killed when it stops thinking about them, or they might be mistreated in different ways. And I think that would be an important ethical complication in this context</p>\n<p>[00:30]</p>\n<p><strong>Questioner: Eliezer suggests that one of the many problems with arbitrary stamps in AI space is that human values are very complex. So virtually any goal system will go horribly wrong because it will be doing things we don&rsquo;t quite care about, and that&rsquo;s as bad as paperclips. How complex do you think human values will be?</strong></p>\n<p>Nick: It looks like human values are very complicated. Even if they were very simple, even if it turned out its just pleasure say, which compared to other things of what has value, like democracy flourishing and art. As far as we can think of values that&rsquo;s one of the more simplistic possibilities. Even that if you start to think of it from a physicalistic view, and you have to now specify which atoms have to go how and where for there to be pleasure. It would be a pretty difficult thing to write down, Like the Schr&ouml;dinger Equation for pleasure.</p>\n<p>So in that sense it seems fair that our values are very complex. So there are two issues here. There is a kind of technical problem of figuring out that if you knew what our values are, in the sense that we think that we normally know what our values are, how we could get the AI to share those values, like pleasure or absence of pain or anything like that.</p>\n<p>And there is the additional philosophical problem which is if we are unsure of what are values are, if we are groping about in axiology trying to figure out how much to value different things, and maybe there are values we have been blind to today, then how do you also get all of that on board, on top of what we already think has value, that potential of moral growth? Both of those are very serious problems and difficult challenges.</p>\n<p>There are a number of different ways you can try to go. One approach that is interesting is what we might call is indirect normativity. Where the idea is rather than specifying explicitly what you want the AI to achieve, like maximizing pleasure while respecting individual autonomy and pay special attention to the poor. Rather than creating a list, what you try to do instead is specify a process or mechanism by which the AI could find out what it is supposed to do.</p>\n<p>One of these ideas that has come out is this idea Coherent Extrapolated Volition, where the idea is if you could try to tell the AI to do that which we would have asked it to do if we had thought about the problem longer, and if we had been smarter, and if we had some other qualifications. Basically, if you could describe some sort of idealized process whereby we at the end, if we underwent that process would be able to create a more detailed list, then maybe point the AI to that and make the AI&rsquo;s value to run this process and do what comes out of the end of that, rather than go with where our current list gets us about what we want to do and what has value.</p>\n<p>[00:33]</p>\n<p><strong>Questioner: Isn&rsquo;t there are risk that.. the AI would decide that if we thought about it for 1000 years really, really carefully, that we would just decide to just let the AIs to take over?</strong></p>\n<p>Nick: Yeah, that seems to be a possibility. And then that raises some interesting questions. Like if that is really what our CEV would do. Let&rsquo;s assume that everything has been implemented in the right way, like there is no flaw on the realization of this. So how should we think about this?</p>\n<p>Well on the one hand, you might say if this is really what our wiser selves would want. What we would want if we were saved from these errors and illusions we are suffering under, then maybe we should go ahead with that. On the other hand, you could say, this is really a pretty tall order. That we&rsquo;re supposed to sacrifice not just a bit, but ourselves and everybody else, for this abstract idea that we don&rsquo;t really feel any strong connection to. I think that&rsquo;s one of the risks, but who knows what will be the outcome of this CEV?</p>\n<p>And there are further qualms one might have that need to be spelled out. Like exactly whose volition is it that is supposed to be extrapolated. Humanity&rsquo;s? Well then, who is humanity? Like does it include past generations for example? How far back? Does it include embryos that died?</p>\n<p>Who knows whether the core of humanity is nice? Maybe there are a lot of suppressed sadists out there, that we don&rsquo;t realize, because they know that they would be punished by society. Maybe if they went through this procedure, who knows what would come out?</p>\n<p>So it would be dangerous to run something like that, without some sort of safeguard check at the end. On the other hand, there is worry that if you put in too many of these checks, then in effect you move the whole thing back to what you want now. Because if you were allowed to look at an extrapolation, see whether you like it, or if you dislike it you run another one by changing the premises and you were allowed to keep going like that until you were happy with the result then basically it would be you now, making the decision. So, it&rsquo;s worth thinking about, whether there is some sort of compromise or blend that might be the most appealing.</p>\n<p>[00:36]</p>\n<p><strong>Questioner: You mentioned before about a computer producing sentience itself in running a scenario. What are the chances that that is the society that we live in today?</strong></p>\n<p>Nick: I don&rsquo;t know, so what exactly are the chances? I think significant. I don&rsquo;t know, it&rsquo;s a subjective judgment here. maybe less than 50%? Like 1 in 10?</p>\n<p>There&rsquo;s a whole different topic, maybe we should save that topic for a different time..</p>\n<p>[00:37]</p>\n<p><strong>Questioner: If I wanted to study this area generally, existential risk, what kind of subject would you recommend I pursue? We&rsquo;re all undergrads, so after our bachelors we will start on master or go into a job. If I wanted to study it, what kind of master would you recommend?</strong></p>\n<p><strong> </strong></p>\n<p>Nick: Well part of it would depend on your talent, like if you&rsquo;re a quantitative guy or a verbal guy. There isn&rsquo;t really an ideal sort of educational program anywhere, to deal with these things. You&rsquo;d want to get a fairly broad education, there are many fields that could be relevant. If one looks at where people are coming from so far that have had something useful to say, a fair chunk of them are philosophers, some computer scientists, some economists, maybe physics.</p>\n<p>Those fields have one thing in common in that they are fairly versatile. Like if you&rsquo;re doing Philosophy, you can do Philosophy of X, or of Y, or of almost anything. Economics as well. It gives you a general set of tools that you can use to analyze different things, and computer science has these ways of thinking and structuring a problem that is useful for many things</p>\n<p>So it&rsquo;s not obvious which of those disciplines would be best, generically. I think that would depend on the individual, but then what I would suggest is that while you were doing it, you also try to read in other areas other than the one you were studying. And try to do it at a place where there are a lot of other people around with a support group and advisor that encouraged you and gave you some freedom to pursue different things.</p>\n<p>[00:38]</p>\n<p><strong>Questioner: Would you consider AI created by human beings as some sort of consequence of evolutionary process? Like in a way that human beings tried to overcome their own limitations and as it&rsquo;s a really long time to get it on a dna level you just get it quicker on a more computational level?</strong></p>\n<p>Nick: So whether we would use evolutionary algorithms to produce super- intelligence or..?</p>\n<p><strong>Questioner: If AI itself is part of evolution..</strong></p>\n<p><strong> </strong></p>\n<p>Nick: So there&rsquo;s kind of a trivial sense in which if we evolved and we created&hellip;then obviously evolution had a part to play in the overall causal explanation of why we&rsquo;re going to get machine intelligence at the end. Now, for evolution to really to exert some shaping influence there have to be a number of factors at play. There would have to be a number of variants created that are different and then compete for resources and then there is a selection step. And for there to be significant evolution you have to enact this a lot of times.</p>\n<p>So whether that will happen or not in the future is not clear at all. If you have a signal tone for me, in that if a world order arises at a top level. Where there is only one decision making agency, which could be democratic world government or AI that rules everybody, or a self-enforcing moral code, or tyranny or a nice thing or bad thing</p>\n<p>But if you have that kind of structure there will at least be, in principal ability, for that unitary agent to control evolution within itself, like it could change selection pressures by taxing or subsidizing different kinds of life forms.</p>\n<p>If you don&rsquo;t have a singleton then you have different agencies that might be in competition with one another, and in principle in that scenario evolutionary pressures can come into play. But I think the way that it might pan out would be different from the way that we&rsquo;re used to seeing biological evolution, so for one thing you might have these potentially immortal life forms, that is they have software minds that don&rsquo;t naturally die, that could modify themselves.</p>\n<p>If they knew that their current type, if they continued to pursue their current strategy would be outcompeted and they didn&rsquo;t like that, they could change themselves immediately right away rather than wait to be eliminated.</p>\n<p>So you might get, if there were to be a long evolutionary process ahead and agents could anticipate that, you might get the effects of that instantaneously from anticipation.</p>\n<p>So I think you probably wouldn&rsquo;t see the evolutionary processes playing out but there might be some of the constraints that could be reflected more immediately by the fact that different agencies had to pursue strategies that they could see would be viable.</p>\n<p>[00:41]</p>\n<p><strong>Questioner: So do you think it&rsquo;s possible that our minds could be scanned and then be uploaded into a computer machine in some way and then could you create many copies of ourselves as those machines?</strong></p>\n<p>Nick: So this is what in technical terminology is &ldquo;whole brain emulation&rdquo; or in more popular terminology &ldquo;uploading&rdquo;. So obviously this is impossible now, but seems like it&rsquo;s consistent with everything we know about physics and chemistry and so forth. So I think that will become feasible barring some kind of catastrophic thing that puts a stop to scientific and technological progress.</p>\n<p>So the way I imagine it would work is that you take a particular brain, freeze it or vitrify it, and then slice it up into thin slices that would be fed through some array of microscopes that would scan each slice with sufficient resolution and then automated image analysis algorithms would work on this to reconstruct the 3 dimensional neural network that your own organic brain implemented and I have this sort of information structure in a computer.</p>\n<p>At this point you need computational neuroscience to tell you what each component does. So you need to have a good theory of what say a pyramidal cell does, what a different kind of&hellip;And then you would combine those little computational models of what each type of neuron does with this 3D map of the network and run it. And if everything went well you would have transferred the mind, with memories and personalities intact to the computer. And there is an open question of just how much resolution would you need to have, how much detail you would need to capture of the original mind in order to successfully do this. But I think there would be some level of detail which as I said before, might be on the level of synapses or thereabouts, possibly higher, that would suffice. So then you would be able to do this. And then after you&rsquo;re software , you could be copied, or speeded up or slowed down or paused or stuff like that</p>\n<p>[00:44]</p>\n<p><strong>Questioner: There has been a lot of talk of controlling the AI and evaluating the risk. My question would be assuming that we have created a far more perfect AI than ourselves is there a credible reason for human beings to continue existing?</strong></p>\n<p>Nick: Um, yeah, I certainly have the reason that if we value our own existence we seem to have a&hellip;Do you mean to say that there would be a moral reason to exist or if we would have a self interested reason to exist.</p>\n<p><strong>Questioner: Well I guess it would be your opinion..</strong></p>\n<p>Nick: My opinon is that I would rather not see the genocide of the entire human species. Rather that we all live happily ever after. If those are the only two alternatives, I think yeah! Let&rsquo;s all live happily ever after! Is where I would come down on that.</p>\n<p>[00:45]</p>\n<p><strong>Questioner: By keeping human species around You&rsquo;re going to have a situation presumably where you have extremely, extremely advanced AIs where they have few decades or few centuries or whatever and they will be far, far beyond our comprehension, and even if we still integrate to some degree with machines (mumble) biological humans then they&rsquo;ll just be completely inconceivable to us. So isn&rsquo;t there a danger that our stupidity will hamper their perfection?</strong></p>\n<p>Nick: Would hamper <em>their</em> perfection?? Well there&rsquo;s enough space for there to be many different kinds of perfection pursued. Like right now we have a bunch of dust mites crawling around everywhere, but not really hampering our pursuit of art or truth or beauty. They&rsquo;re going about their business and we&rsquo;re going about ours.</p>\n<p>I guess you could have a future where there would be a lot of room in the universe for planetary sized computers thinking their grand thoughts while&hellip;I&rsquo;m not making a prediction here, but if you wanted to have a nature preserve, with original nature or original human beings living like that, that wouldn&rsquo;t preclude the other thing from happening..</p>\n<p><strong>Questioner: Or a dust mite might not hamper us, but things like viruses or bacteria just by being so far below us (mumble). And if you leave humans on a nature preserve and they&rsquo;re aware of that, isn&rsquo;t there a risk that they&rsquo;ll be angry at the feeling of being irrelevant at the grand scheme of things?</strong></p>\n<p>Nick: I suppose. I don&rsquo;t think it would bother the AI that would be able to protect itself, or remain out of reach. Now it might demean the remaining humans if we were dethroned from this position of kings, the highest life forms around, that it would be a demotion, and one would have to deal with that I suppose.</p>\n<p>It&rsquo;s unclear how much value to place on that. I mean right now in this universe which looks like it&rsquo;s infinite somewhere out there are gonna be all kinds of things including god like intellects and everything in between that are already outstripping us in every possible way.</p>\n<p>It doesn&rsquo;t seem to upset us terribly; we just get on with it. So I think people will have to make some psychological..I&rsquo;m sure we can adjust to it easily. Now it might be from some particular theory of value that this might be a sad thing for humanity. That we are not even locally at the top of the ladder.</p>\n<p><strong>Questioner: If rationalism was true, that is if it were irrational to perform wrong acts. Would we still have to worry about super-intelligence? It seems to me that we wouldn&rsquo;t have.</strong></p>\n<p>Nick: Well you might have a system that doesn&rsquo;t care about being rational, according to that definition of rationality. So I think that we would still have to worry</p>\n<p>[00:48]</p>\n<p><strong>Questioner: Regarding trying to program AI without values, (mumbles) But as I understand it, what&rsquo;s considered one of the most promising approach in AI now is more statistical learning type approaches.. And the problem with that is if we were to produce an AI with that, we might not understand its inner workings enough to be able to dive in and modify it in precisely the right way to give it an unalterable list of terminal values. </strong></p>\n<p><strong>So if we were to end up with some big neural network that we trained in some way and ended up with something that could perform as well as humans in some particular task or something. We might be able to do that without knowing how to alter it to have some particular set of goals.</strong></p>\n<p>Nick: Yeah, so there are some things there to think about. One general worry that one needs to bear in mind if one tries that kinds of approach is we might give it various examples like this is a good action and this is a bad action in this context, and maybe it would learn all those examples then the question is how would it generalize to other examples outside this class?</p>\n<p>So we could test it we could divide our examples initially into classes and train it on one and test its performance on the other, the way you would do to cross-validate. And then we think that means other cases that it hasn&rsquo;t seen it would have the same kind of performance. But all the cases that we could test it on would be cases that would apply to its current level of intelligence. So presumably we&rsquo;re going to do this while it&rsquo;s still at human or less than human intelligence. We don&rsquo;t want to wait to do this until it&rsquo;s already super-intelligent.</p>\n<p>So then the worry is that even if it were able to analyze what to do in a certain way in all of these cases, it&rsquo;s only dealing with all of these cases in the training case, when it&rsquo;s still at a human level of intelligence. Now maybe once it becomes smarter it will realize that there are different ways of classifying these cases that will have radically different implications for humans.</p>\n<p>So suppose that you try to train it to&hellip; this was one of the classic example of a bad idea of how to solve the control problem: Lets train the AI to want to make people smile, what can go wrong with that? So we train it on different people and if they smile when it does something that&rsquo;s like a kind of reward; it gets strength in those positions that led to the behavior that made people smile. And frowning would move the AI away from that kind of behavior. And you can imagine that this would work pretty well at a primitive state where the AI will engage in more pleasing and useful behavior because the user will smile at it and it will all work very well. But then once the AI reaches a certain level of intellectual sophistication it might realize that It could get people to smile not just by being nice but also by paralyzing their facial muscles in that constant beaming smile.</p>\n<p>And then you would have this perverse instantiation of the constant values all along the value that it wants to make people smile, but the kinds of behaviors it would pursue to achieve this goal would suddenly radically change at a certain point once the new set of strategies became available to it, and you would get this treacherous turn, which would be dangerous. So that&rsquo;s not to dismiss that whole category of approaches altogether. One would have to think through quite carefully, exactly how one would go about that.</p>\n<p>[00:52]</p>\n<p>There&rsquo;s also the issue of, a lot of the things we would want it to learn, if we think of human values and goals and ambitions. We think of them using human concepts, not using basic physical..like place atom A to zed in a certain order, But we think like promote peace, encourage people to develop and achieve&hellip;These are things that to understand them we really need to have human concept, which a sub-human AI will not have, it&rsquo;s too dumb at that stage to have that. Now once it&rsquo;s super-intelligent it might easily understand all human concepts but then it&rsquo;s too late. It already needs to be friendly before that. So there might only be this brief window of opportunity where its roughly human leve,l where its still safe enough not to resist our attempt to indoctrinate it but smart enough that it can actually understand what we are trying to tell it.</p>\n<p>And again were going to have to be very careful to make sure that we can bring the system up to that interval and then freeze its development there and try to load the values in before boot strapping it farther.</p>\n<p>And maybe(this was one of the first questions) its intelligence will not be human level in the sense of being similar to a human at any one point. Maybe it will immediately be very good at chess but very bad at poetry and then it has to reach radically superhuman levels of capability in some domains before other domains even reach human level. And in that case it&rsquo;s not even clear that there will be this window of opportunity where you can load in the values. So I don&rsquo;t want to dismiss that, but that&rsquo;s like some additional things that one needs to think about, if one tries to develop that.</p>\n<p>[00:54]</p>\n<p><strong>Questioner: How likely is it that we will have the opportunity in our lifetimes to become immortal by mind uploading?</strong></p>\n<p>Nick: Well first of all, by immortal here we mean living for a very long time, rather than literally never dying, which is a very different thing that would require our best theories of cosmology to turn out to be false for something like that.</p>\n<p>So living for a very long time: Im not going to give you a probability in the end. But I can say some of the things that&hellip;Like first we would have to avoid most kinds of things like existential catastrophe that could put an end to this.</p>\n<p>So, if you start with 100% and you remove all the things that could go wrong, so first you would have to throw away whatever total level of existential risk is, integrated over all time. Then there is the obvious risk that you will die before any of this happens, which seems to be a very substantial risk. Now you can reduce that by signing up for cryonics, but that&rsquo;s of course an uncertain business as well. And there could be sub-existential catastrophes that would put an end to a lot of things like a big nuclear war or pandemics.</p>\n<p>And then I guess there are all these situations in which not everybody who is still around gets the opportunity to participate in what came after. Even though what came after doesn&rsquo;t count as an existential catastrophe&hellip; And [it can get] even more complicated, like if you took into account the simulation hypothesis, which we decided not to talk about today.</p>\n<p>[00:56]</p>\n<p><strong>Q: Is there a particular year we should aim for?</strong></p>\n<p>Nick: As for the timelines, truth is we don&rsquo;t know. So you need to think about a very smeared out probability distribution. And really smear it, because things could happen surprisingly sooner like some probability 10 years from now or 20 years now but probably more probable at 30, 40, 50 years but some probability at 80 years or 200 years..</p>\n<p>There is just not good evidence that human beings are very good at predicting with precision these kinds of things far out in the future.</p>\n<p><strong>Questioner: (hard to understand) How intelligent can we really get. &hellip; we already have this complexity class of problems that we can solve or not&hellip;</strong></p>\n<p><strong>Is it fair to believe that a super-intelligent machine can be actually be that exponentially intelligent... this is very close to what we could achieve &hellip;A literal definition of intelligence also, but..</strong></p>\n<p>Nick: Well in a sort of cheater sense we could solve all problems, sort of like everything a Turing Machine could..it could take like a piece of paper and..</p>\n<p>a) It would take too long to actually do it, and if we tried to do it, there are things that would probably throw us off before we have completed any sort of big Turing machine simulation</p>\n<p>There is a less figurative sense in which our abilities are already indirectly unlimited. That is, if we have the ability to create super intelligence, then in a sense we can do everything because we can create this thing that then solves the thing that we want solved. So there is this sequence of steps that we have to go through, but in the end it is solved.</p>\n<p>So there is this level of capability that means that once you have that level of capability your indirect reach is universal, like anything that could be done, you could indirectly achieve, and we might have already surpassed that level a long time ago, save for the fact that we are sort of uncoordinated on a global level and maybe a little bit unwise.</p>\n<p>But if you had a wise singleton then certainly you could imagine us plotting a very safe course, taking it very slowly and in the end we could be pretty confident that we would get to the end result. But maybe neither of those ideas are what you had in mind. Maybe you had more in mind The question of just how smart, in everyday sort of smart could a machine be,. So just how much more effective at social persuasion, to take one particular thing, than the most persuasive human.</p>\n<p>So that we don&rsquo;t really know. If one has a distribution of human abilities, and it seems like the best humans can do a lot better, in our intuitive sense of a lot, than the average humans. Then it would seem very surprising if the best humans like the top tenth of a percent had reached the upper limit of what was technologically feasible, that would seem to be an amazing coincidence. So one would then expect for the maximum achievable to be a lot higher. But exactly how high we don&rsquo;t know.</p>\n<p>So two more questions:</p>\n<p>[00:59]</p>\n<p><strong>Q: Just like we are wondering about super-intelligent being, is it possible that that super-intelligent will worry about another super-intelligent being that it will create? Isn&rsquo;t that also recursive?</strong></p>\n<p>Nick: So you consider where one AI designs another AI that&rsquo;s smarter and then that designs another.</p>\n<p>But it might not be clearly distinguishable from the scenario where we have one AI that modifies itself so that <em>it</em> ends up smarter. Whether you call it the same or different, it might be an unimportant difference.</p>\n<p>Last question. This has to be super profound question.</p>\n<p>[01:00]</p>\n<p><strong>Q: So my question is why should we even try to build a super-intelligence?</strong></p>\n<p>Nick: I don&rsquo;t think we should now, do that. If you took a step back and thought what would a sane species do, well they would first figure out how to solve the control problem, and then they would think about it for a while to make sure that they really had the solution right and they hadn&rsquo;t just deluded themselves to how to solve it, and then maybe they would build a super-intelligence.</p>\n<p>So that&rsquo;s what the sane species will do, now what humanity will do is try to do everything they can as soon as possible, so there are people who have tried to build it as we speak, in a number of different places on earth, and fortunately it looks very difficult to build it with current technology. But of course it&rsquo;s getting easier over time, computers get better, computer science, the state of the art advances, we learn more about how the human brain works.</p>\n<p>So every year it gets a little bit easier, from some unknown very difficult level, it gets easier and easier. So at some point it seems someone will probably succeed at doing it. If the world remains sort of uncoordinated and uncontrolled as it is now, it&rsquo;s bound to happen soon after it becomes feasible. But we have no reason to accelerate that even more than its already happening ...</p>\n<p>So we were thinking about what would a powerful AI thing do that had just come into existence and it didn&rsquo;t know very much yet, but it had a lot of clever algorithms and a lot of processing power. Someone was suggesting maybe it would move around randomly, like a human baby does, to figure out how things move, how it can move its actuators.</p>\n<p>Then we had a discussion if that was a wise thing or not.</p>\n<p>But if you think about how the human species behave, we are really behaving very much like a baby were sort of moving and shaking everything that moves, just to see what happens. And the risk is that we are not in the nursery with a kind mother who has put us in a cradle, but that we are out in the jungle somewhere screaming at the top of our lungs, and maybe just alerting the lions to their supper.</p>\n<p>So let&rsquo;s wrap up. I enjoyed this a great deal, so thank you for your questions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DigEmY3RrF3XL5cwe": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eXHp9J4PXmQXzmBAj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 53, "extendedScore": null, "score": 0.000126, "legacy": true, "legacyId": "10981", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>INTRO: From the <a href=\"/r/discussion/lw/8fw/new_qa_by_nick_bostrom/\">original posting by Stuart_Armstrong</a>:</p>\n<blockquote>\n<p>Underground Q&amp;A session with Nick Bostrom (<a href=\"http://www.nickbostrom.com/\">http://www.nickbostrom.com</a>) on existential risks and artificial intelligence with the Oxford Transhumanists (recorded 10 October 2011).</p>\n<p><a href=\"http://www.youtube.com/watch?v=KQeijCRJSog\">http://www.youtube.com/watch?v=KQeijCRJSog</a></p>\n</blockquote>\n<p>Below I (will) have a summary of the Q&amp;A followed by the transcription. The transcription is slightly edited, mainly for readability. The numbers are minute markers. Anything followed by a (?) means I don't know quite what he said (example- attruing(?) program), but if you figure it out, let me know!</p>\n<hr>\n<p>SUMMARY: I'll have a summary here by end of the day, probably.</p>\n<hr>\n<p>TRANSCRIPTION:</p>\n<p>Nick: I wanted to just [interact with your heads]. Any questions, really, that you have. To discuss with you. I can say what I\u2019m working on right now which is this book on super-intelligence, not so much on the question of whether and how long it might take to develop machine intelligence that equals human intelligence, but rather what happens if and when that occurs. To forget human level machine intelligence, how quickly, how explosively will we get super-intelligenct, and how can you solve the control problem. If you build super-intelligence how can you make sure it will do what you want. That it will be safe and beneficial.</p>\n<p>Once one starts to pull on that problem, it turns out to be quite complicated and difficult. That it has many aspects to it that I would be happy to talk about. Or if you prefer to talk about other things; existential risks, or otherwise, I\u2019d be happy to do that as well. But no presentation, just Q&amp;A. So you all have to provide at least the stimulus. So should I take questions or do you want\u2026</p>\n<p>[00:01]</p>\n<p><strong id=\"Questioner__So_what_s_your_definition_of_machine_intelligence_or_super_intellegence_AI__Is_there_like_a_precise_definition_there_\">Questioner: So what\u2019s your definition of machine intelligence or super-intellegence AI\u2026 Is there like a precise definition there?</strong></p>\n<p>Nick: There isn\u2019t. Now if you look at domain specific intelligence, there are already areas where machines surpass humans, such doing arithmetical calculations or chess. I think the interesting point is when machines equal humans in general intelligence or perhaps slightly more specifically in engineering intelligence. So if you had this general capability of being able to program creatively and design new systems... There is in a sense a point at which if you had sufficient capability of <em>that</em> sort, you have general capability.</p>\n<p>Because if you can build new systems, even if all it could initially do is this type engineering work, you can build yourself a poetry module or build yourself a social skills module, if you have that general ability to build . So it might be that general intelligence or it might be that slightly more narrow version of that engineering type of intelligence is the key variable to look at. That\u2019s the kind of thing that can unleash the rest. But \u201chuman-level intelligence\u201d... that\u2019s a vague term, and I think it\u2019s important to understand that. It\u2019s not necessarily the natural kind.</p>\n<p>[00:03]</p>\n<p><strong id=\"Questioner__Got_a_question_that_maybe_should_have_waited_til_the_end__There_are_two_organizations__FHI_and_SIAI__working_on_this__Let_s_say_I_thought_this_was_the_most_important_problem_in_the_world__and_I_should_be_donating_money_to_this__Who_should_I_give_it_to_\">Questioner: Got a question that maybe should have waited til the end: There are two organizations, FHI and SIAI, working on this. Let's say I thought this was the most important problem in the world, and I should be donating money to this. Who should I give it to?</strong></p>\n<p>Nick:</p>\n<p>It's good. We've come to the chase!</p>\n<p>I think there is a sense that both organizations are synergistic. If one were about to go under or something like that, that would probably be the one. If both were doing well, it's... different people will have different opinions. We work quite closely with a lot of the folks from SIAI.</p>\n<p>There is an advantage to having one academic platform and one outside academia. There are different things these types of organizations give us. If you want to get academics to pay more attention to this, to get postdocs to work on this, that's much easier to do within academia; also to get the ear of policy-makers and media.</p>\n<p>On the other hand, for SIAI there might be things that are easier for them to do. More flexibility, they're not embedded in a big bureaucracy. So they can more easily hire people with non-standard backgrounds without the kind of credentials that we would usually need, and also more grass-roots stuff like the community blog Less Wrong, is easier to do.</p>\n<p>So yeah. I'll give the non-answer answer to that question.</p>\n<p>[00:05]</p>\n<p><strong id=\"Questioner__Do_you_think_a_biological_component_is_necessary_for_an_artificial_intelligence_to_achieve_sentience_or_something_equivalent_\">Questioner: Do you think a biological component is necessary for an artificial intelligence to achieve sentience or something equivalent?</strong></p>\n<p>Nick: It doesn\u2019t seem that that should be advantageous\u2026If you go all the way back to atoms, it doesn\u2019t seem to matter that it\u2019s carbon rather than silicon atoms. Then you could wonder, instead of having the same atoms you run a simulation of everything that\u2019s going on. Would you have to simulate biological processes? I don\u2019t even think that\u2019s necessary.</p>\n<p>My guess (and Im not sure about this, I don\u2019t have an official position or even a theory about what exactly the criteria are that would make a system conscious)\u2026But my intuition is that If you replicated computational processes that goes on in a human brain, at a sufficient level of detail, where that sufficient level of detail might be roughly on the level of individual neurons and synapses, I think you would likely have consciousness. And it might be that it\u2019s something weaker than that which would suffice. Maybe you wouldn\u2019t need every neuron. Maybe you could simplify things and still have consciousness. But at least at that level it seems likely.</p>\n<p>It\u2019s a lot harder to say if you had very alien types of mental architecture. Something that wasn\u2019t a big neural network but of normal machine intelligence that performs very well in a certain way, but using a very different method than a human brain. Whether that would be conscious as well? Much less sure. A limiting case would be a big lookup table that was physically impossible to realize, but you can imagine having every sort of situation possible described, and that program would run through until it found the situation that matched its current memory and observation and would read off which action it should perform. But that would be an extremely alien type of architecture. But would that have conscious experience or not? Even less clear. It might be that it would not have, but maybe the process of generating this giant look-up table would generate kinds of experiences that you wouldn\u2019t get from actually implementing it or something like that. (?)</p>\n<p>[00:07]</p>\n<p><strong id=\"Questioner__This_relates_to_AI_being_dangerous__It_seems_to_me_that_while_it_would_certainly_be_interesting_if_we_were_to_get_AI_that_were_much_more_intelligent_than_a_human_being__its_not_necessarily_dangerous__\">Questioner- This relates to AI being dangerous. It seems to me that while it would certainly be interesting if we were to get AI that were much more intelligent than a human being, its not necessarily dangerous. </strong></p>\n<p><strong></strong><strong>Even if the AI is very intelligent it might be hard for it to get resources for it to actually do anything to be able to manufacture extra hardware or anything like that. There are obviously situations where you can imagine intelligence or Creative thinking can get you out of or get you further capability . So..</strong></p>\n<p>Nick: I guess it\u2019s useful to identify two cases: One is sort of the default case unless we successfully implement some sort of safeguard or engineer it in a particular way in order to avoid dangers \u2026So let\u2019s think of a default just a bit: You have something that is super intelligent and capable of improving itself to even more levels of super intelligence\u2026. I guess one way to get initial possibility of why this is dangerous is to think about why humans are powerful.. Why are we dominant on this planet? It\u2019s not because we have stronger muscles or our teeth are sharper or we have special poison glands. It\u2019s all because of our brains, which have enabled us to develop a lot of other technologies that give us in effect muscles that are stronger than the other animals\u2026We have bulldozers and external devices and all the other things. And also, it enables us to coordinate socially and build up complicated society so we can act as groups. And all of this makes us supreme on this planet. We can argue with the case of bacteria which have their own domains where they rule. But certainly in the case of the larger mammals we are unchallenged because of our brains.</p>\n<p>And the brains are not all that different from the brains of other animals. It might be that all these advantages we have are due to a few tweaks on some parameters that occurred in our ancestors a couple million years ago. And these tiny changes in the nature our intelligence that had these huge affects. So just prima facie it then seems possible that that if the system surpassed us by just a small amount that we surpass chimpanzees, it could lead to a similar kind of advantage in power. And if they exceeded our intelligence by a much greater margin, then all of that could happen in a more dramatic fashion</p>\n<p>It\u2019s true that you could have in principle an AI that was locked in a box, such that it would be incapable of affecting anything outside the box and in that sense it would be weak. That might be one of the safety methods one tries to apply that I've been thinking about.</p>\n<p>Broadly speaking you can distinguish between two different approaches to solving the control problem, of making sure that super-intelligence, if it\u2019s built wouldn\u2019t cause harm. On one hand you have capability control measures where you try to limit what the AI is able to do. The most obvious example would be lock it in a box and limit its ability to interact with the rest of the world.</p>\n<p>The other class of approach would be motivation selection methods where you would try to control what it wants to do. Where you build it in such a way that even if it has the power to do all this bad stuff, it would choose not to. But so far, there isn\u2019t one method or even a combination of methods that it seems we can currently be fully convinced would work. There\u2019s a lot more work needed...</p>\n<p>[00:11]</p>\n<p><strong id=\"Questioner__Human_beings_have_been_very_successful__One_feature_of_that_that_has_been_very_crucial_are_our_hands_that_have_enabled_us_to_get_a_start_on_working_tools_and_so_on__Even_if_an_AI_is_running_on_some_computer_somewhere__that_would_be_more_analogous_to_a_very_intelligent_creature_which_doesn_t_have_very_good_hands__It_s_very_hard_for_it_to_actually_DO_anything__\">Questioner: Human beings have been very successful. One feature of that that has been very crucial are our hands that have enabled us to get a start on working tools and so on. Even if an AI is running on some computer somewhere, that would be more analogous to a very intelligent creature which doesn\u2019t have very good hands. It\u2019s very hard for it to actually DO anything. </strong></p>\n<p><strong id=\"Maybe_the_in_the_box_method_is_promising__Because_if_we_just_don_t_give_the_AI_hands__some_way_to_actually_do_something__If_all_it_can_do_is_alter_its_own_code__and_maybe_communicate_infomationally__That_seems___\">Maybe the in-the-box method is promising. Because if we just don\u2019t give the AI hands, some way to actually do something..If all it can do is alter its own code, and maybe communicate infomationally. That seems...</strong></p>\n<p>Nick: So let\u2019s be careful there\u2026 So clearly it\u2019s not \u201chands\u201d per se. If it didn\u2019t have hands it could still be very dangerous, because there are other people with hands, that it could persuade to do its bidding. It might be that it has no direct effectors other than the ability to type very slowly, and then some human gatekeeper could read and choose to act on or not. Even that limited ability to affect the world might be sufficient if it had a super power in the domain of persuasion. So if it had an engineering super-power, it might then get all these other superpowers. And then if it were able to, in particular be a super skilled persuader, it could then get other accessories outside our system that could implement its designs.</p>\n<p>You might have heard of this guy, Eliezer Yudkowsky, about 5 years back who ran a series of role playing exercises...The idea was one person should play the AI, pretend to be in a box. The other should play the human gatekeeper whose job was to not let the AI out of the box, but he has to talk with the AI for a couple of hours over the internet chat. This experiment was run five times, with EY playing the AI and different people playing the human gatekeeper. And for the most part people, who were intitially convinced that they would never let the AI out of the box, but in 3 of 5 cases, the experiment ended with the gatekeepers announcing yes, they would let the AI out of the box.</p>\n<p>This experiment was run under conditions that neither party would be allowed to disclose the methods that were used, the main conversational sequence...sorta maintain a shroud of mystery. But this is where the human-level persuader has two hours to work on the human gatekeeper. It seems reasonable to be doubtful of the ability of humanity to keep the super-intelligent persuader in the box, indefinitely, for that reason.</p>\n<p>[00:15]</p>\n<p><strong id=\"Questioner__How_hard_do_you_think_the_idea_of_controlling_the_mentality_of_intelligence_is__with_something_at_least_as_intelligent_as_us__considering_how_hard_it_is_to_convince_humans_to_act_in_a_certain_civilized_way_of_life_\">Questioner: How hard do you think the idea of controlling the mentality of intelligence is, with something at least as intelligent as us, considering how hard it is to convince humans to act in a certain civilized way of life?</strong></p>\n<p>Nick: So humans sort of start out with a motivation system and then you can try to persuade them or structure incentives to behave in a certain way. But they don\u2019t start out with a tabula rasa where you get to write in what a human\u2019s values should be. So that\u2019s made a difference. In the case of the super-intelligence of course once it already has unfriendly values and it has sufficient power, it will resist any attempt to corrupt its goal system as it would see it.</p>\n<p>[00:16]</p>\n<p><strong id=\"Questioner__You_don_t_think_that_like_us__its_experiences_might_cause_it_to_question_its_core_values_as_we_do_\">Questioner: You don\u2019t think that like us, its experiences might cause it to question its core values as we do?</strong></p>\n<p>Nick: Well, I think that depends on how the goal system is structured. So with humans we don\u2019t have a simple declarative goal structure list. Not like a simple slot where we have super goal and everything else is derived from that</p>\n<p>Rather it\u2019s like many different little people inhabit our skull and have their debates and fight it out and make compromises. And in some situations, some of them get a boost like permutations and stuff like that. Then over time we have different things that change what we want like hormones kicking in, fading out, all kinds of processes.</p>\n<p>Another process that might affect us is what I call value accretion. The idea that we can have mechanisms that loads new values into us, as we go along. Like maybe falling in love is like that; Initially you might not value that person for their own sake above any other person. But once you undergo this process you start to value them for their own sake in a special way. So human have this mechanism that make us acquire values depending on our experiences.</p>\n<p>If you were building a machine super intelligence and trying to engineer its goal systems so that it will be reliably safe and human friendly, you might want to go with something, more transparent where you have an easier time seeing what is happening, rather than have a complex modular minds with a lot of different forces battling it out...you might want to have a more hierarchical structure.</p>\n<p><strong id=\"Questioner__What_do_you_think_of_the_necessary_requisites_for_the_conscious_mind__What_are_the_features_\">Questioner: What do you think of the necessary\u2026requisites for the conscious mind? What are the features?</strong></p>\n<p>Nick: Yes, I\u2019m not sure. We\u2019ve talked a little on that earlier. Suppose there is a certain kind of computation that is needed, that is really is the essence of mind. I\u2019m sympathetic to the idea that something in the vicinity of that view might be correct. You have to think about exactly how to develop it. Then there is this stage of what is a computation.</p>\n<p>So there is this challenge (I think it might go back to Hans Moravec but I think similar objections have been raised in philosophy against computationalism) where the idea is that if you have an arbitrary physical system that is sufficiently complicated, it could be a stone or a chair or just anything with a lot of molecules in it. And then you have this abstract computation that you think is what constitutes the implementation of the mind. Then there would be some mathematical mapping between all the parts in your computation and atoms in the chair so that you could artificially, through a very complicated mapping interpret the motions of the molecules in the chair in such a way that they would be seen as implementing the computation. It would not be any plausible mapping, not a useful mapping, but a bizarro mapping. Nonetheless if there were sufficiently limited parts there, you could just arbitrarily define some, by injection..</p>\n<p>And clearly we don\u2019t think that all these random physical objects implement the mind, or all possible minds.</p>\n<p>So the lesson to me is that it seems that we need some kind of account of what it means to implement a computation that is not trivial and this mapping function between the abstract entity that is a sort of Turing program, or whatever your model of a computation is and the physical entity that decides to implement it to be some sort of non-trivial representation of what this mapping can look like</p>\n<p>It might have to be reasonably simple. It might have to have certain counter-factual properties, so that the system would have implemented a related, but slightly different computation if you had scrambled the initial conditions of the system in a certain way, so something like that. But this is an open question in the philosophy of mind, to try to nail down what it means to implement the computation.</p>\n<p>[00:20]</p>\n<p><strong id=\"Questioner__To_bring_back_to_the_goal_and_motivation_approach_to_making_an_AI_friendly_towards_us__one_of_the_most_effective_ways_of_controlling_human_behavior__quite_aside_from_goals_and_motivations___is_to_train_them_by_instilling_neuroses__It_s_why_99_99__of_us_in_this_room_couldn_t_pee_in_our_pants_right_now_even_if_he_really__really_wanted_to__\">Questioner: To bring back to the goal and motivation approach to making an AI friendly towards us, one of the most effective ways of controlling human behavior, quite aside from goals and motivations , is to train them by instilling neuroses. It\u2019s why 99.99% of us in this room couldn\u2019t pee in our pants right now even if he really, really wanted to. </strong></p>\n<p><strong id=\"Is_it_possible_to_approach_controlling_an_AI_in_that_way_or_even_would_it_be_possible_for_an_AI_to_develop_in_such_a_way_that_there_is_a_developmental_period_in_which_a_risk_reward_system_or_some_sort_of_neuroses_instilment_could_be_used_to_basically_create_these_rules_that_an_AI_couldn_t_break_\">Is it possible to approach controlling an AI in that way or even would it be possible for an AI to develop in such a way that there is a developmental period in which a risk-reward system or some sort of neuroses instilment could be used to basically create these rules that an AI couldn\u2019t break?</strong></p>\n<p>Nick: It doesn\u2019t sound so promising because a neurosis is a complicated thing that might be a particular syndrome of a phenomenon that occurs in human- style mind, because of the way that humans\u2019 minds are configured. It\u2019s not clear there would be something exactly analogous to that in a cognitive system with a very different architecture.</p>\n<p>Also, because neuroses, at least certain kinds of neuroses, are ones we would choose to get rid of if we could. So if you had a big phobia and there was a button that would remove the phobia, obviously you would press the button. And here we have this system that is presumably able to self-modify. So if it had this big hang up that it didn\u2019t like, then it could reprogram itself to get rid of that.</p>\n<p>This would be different than a top-level goal because top-level goal would be the criterion it produced to decide whether to take an action. In particular, like an action to remove the top level goal.</p>\n<p>So generally speaking with reasonable and coherent goal architecture you would get certain convergent instrumental values that would crop up in a wide range of situations. One might be self preservation, not necessarily because you value your own survival for its own sake, but because in many situations you can predict that if you are around in the future you can continue to act in the future according to your goals, and that will make it more likely that the world will then be implementing your goals.</p>\n<p>Another convergent instrumental value might be protection of your goal system from corruption (?) for very much the same reason. For even if you were around in the future but you have different goals from the ones you had now, you would now predict that that means in the future you will no longer be working towards realizing your current goals but maybe towards a completely different purpose, that would make it now less likely that your current goals would be realized. If your current goals are what you use as a criterion to choose an action, you would want to try to take actions that would prevent corruption of your goal system.</p>\n<p>One might list a couple of other of the convergent instrumental values like intelligence amplification, technology perfection and resource acquisition. So this relates to why generic super-intelligence might be dangerous. It\u2019s not so much that you have to worry that it would have human Unfriendliness in the sense of disliking human goals, that it would *hate* humans . The danger is that it wouldn\u2019t *care* about humans. It would care about something different, like paperclips. But then if you have almost any other goals, like paperclips, there would be these other convergent instrumental reasons that you discover. For while your goal is to make as many paperclips as possible you might want to a) prevent humans from switching you off or tampering with your goal system or b) you might want to acquire as much resources as possible, including planets, and the solar system, and the galaxy. All of that stuff could be made into paperclips. So even with pretty much a random goal, you would end up with these motivational tendencies which would be harmful to humans.</p>\n<p>[00:25]</p>\n<p><strong id=\"Questioner__Appreciating_the_existential_risks__what_do_you_think_about_goals_and_motivations__and_such_drastic_measures_of_control_sort_of_a__ethically_and_b__as_a_basis_of_a_working_relationship_\">Questioner: Appreciating the existential risks, what do you think about goals and motivations, and such drastic measures of control sort of a) ethically and b) as a basis of a working relationship?</strong></p>\n<p>Nick: Well, in terms of the working relationship one has to think about the differences with these kinds of the artificial being. I think there are a lot of (?) about how to relate to artificial agents that are conditioned on the fact that we are used to dealing with human agents, and there are a lot of things we can assume about the human.</p>\n<p>We can assume perhaps that they don\u2019t want to be enslaved. Even if they say that they want to be enslaved, we might think that deep inside of them, there is a sort of more genuine authentic self that doesn\u2019t want to be enslaved. Even if some prisoner has been brainwashed to do the bidding of their master, maybe we say it\u2019s not really good for them because it\u2019s in their nature, this will to be autonomous. And there are other things like that, that don\u2019t necessarily have to obtain for a completely artificial system which might not have any of that rich human nature that we have.</p>\n<p>So in terms of what the good working relationship is, just as what we think of a good relationship with our word processor or email program. Not in these terms, as if you\u2019re exploiting it for your ends, without giving it anything in return. If your email program had a will, presumably it would be the will to be a good and efficient email program that processed your emails properly. Maybe that was the only thing it wanted and cared about. So having a relationship with it would be a different thing.</p>\n<p>There was another part of your question, about whether this would be right and ethical. I think if you are operating a new agent from scratch, and there are many different possible agents you could create, some of those agents will have human style values; they want to be independent and respected. Other agents that you could create would have no greater desire than to be of service. Others would just want paperclips. So if you step back, and look at which of these options we should decide, then looking at the question of moral constraints on which of these are legitimate.</p>\n<p>And I\u2019m not saying that those are trivial, I think there are some deep ethical questions here. However in the particular scenario where we are considering the creation of a single super intelligence the more pressing concern would be to ensure that it doesn\u2019t destroy everything else, like humanity and its future. Now, if you have a different scenario, like instead of this one uber-mind rising ahead, you have many minds that become smarter and smarter that rival humans and then gradually exceed them</p>\n<p>Say an uploading scenario where you start with very slow software, where you have human like minds running very slowly. In that case, maybe how we should relate to these machine intellects morally becomes more pressing. Or indeed, even if you just have one, but in the process of figuring out what to do it creates \u201cthought crimes\u201d.</p>\n<p>If you have a sufficiently powerful mind maybe you have thoughts themselves would contain structures that are conscious. This sounds mystical, but imagine you are a very powerful computer and one of the things you are doing is you are trying to predict what would happen in the future under different scenarios, and so you might play out a future</p>\n<p>And if those simulations you are running inside of this program were sufficiently detailed, then they could be conscious. This comes back to our earlier discussion of what is conscious. But I think a sufficiently detailed computer simulation of the mind could be conscious</p>\n<p>You could then have a super intelligence that could process by thinking about things could create sentient beings, maybe millions or billions or trillions of them, and their welfare would then be a major ethical issue. They might be killed when it stops thinking about them, or they might be mistreated in different ways. And I think that would be an important ethical complication in this context</p>\n<p>[00:30]</p>\n<p><strong id=\"Questioner__Eliezer_suggests_that_one_of_the_many_problems_with_arbitrary_stamps_in_AI_space_is_that_human_values_are_very_complex__So_virtually_any_goal_system_will_go_horribly_wrong_because_it_will_be_doing_things_we_don_t_quite_care_about__and_that_s_as_bad_as_paperclips__How_complex_do_you_think_human_values_will_be_\">Questioner: Eliezer suggests that one of the many problems with arbitrary stamps in AI space is that human values are very complex. So virtually any goal system will go horribly wrong because it will be doing things we don\u2019t quite care about, and that\u2019s as bad as paperclips. How complex do you think human values will be?</strong></p>\n<p>Nick: It looks like human values are very complicated. Even if they were very simple, even if it turned out its just pleasure say, which compared to other things of what has value, like democracy flourishing and art. As far as we can think of values that\u2019s one of the more simplistic possibilities. Even that if you start to think of it from a physicalistic view, and you have to now specify which atoms have to go how and where for there to be pleasure. It would be a pretty difficult thing to write down, Like the Schr\u00f6dinger Equation for pleasure.</p>\n<p>So in that sense it seems fair that our values are very complex. So there are two issues here. There is a kind of technical problem of figuring out that if you knew what our values are, in the sense that we think that we normally know what our values are, how we could get the AI to share those values, like pleasure or absence of pain or anything like that.</p>\n<p>And there is the additional philosophical problem which is if we are unsure of what are values are, if we are groping about in axiology trying to figure out how much to value different things, and maybe there are values we have been blind to today, then how do you also get all of that on board, on top of what we already think has value, that potential of moral growth? Both of those are very serious problems and difficult challenges.</p>\n<p>There are a number of different ways you can try to go. One approach that is interesting is what we might call is indirect normativity. Where the idea is rather than specifying explicitly what you want the AI to achieve, like maximizing pleasure while respecting individual autonomy and pay special attention to the poor. Rather than creating a list, what you try to do instead is specify a process or mechanism by which the AI could find out what it is supposed to do.</p>\n<p>One of these ideas that has come out is this idea Coherent Extrapolated Volition, where the idea is if you could try to tell the AI to do that which we would have asked it to do if we had thought about the problem longer, and if we had been smarter, and if we had some other qualifications. Basically, if you could describe some sort of idealized process whereby we at the end, if we underwent that process would be able to create a more detailed list, then maybe point the AI to that and make the AI\u2019s value to run this process and do what comes out of the end of that, rather than go with where our current list gets us about what we want to do and what has value.</p>\n<p>[00:33]</p>\n<p><strong id=\"Questioner__Isn_t_there_are_risk_that___the_AI_would_decide_that_if_we_thought_about_it_for_1000_years_really__really_carefully__that_we_would_just_decide_to_just_let_the_AIs_to_take_over_\">Questioner: Isn\u2019t there are risk that.. the AI would decide that if we thought about it for 1000 years really, really carefully, that we would just decide to just let the AIs to take over?</strong></p>\n<p>Nick: Yeah, that seems to be a possibility. And then that raises some interesting questions. Like if that is really what our CEV would do. Let\u2019s assume that everything has been implemented in the right way, like there is no flaw on the realization of this. So how should we think about this?</p>\n<p>Well on the one hand, you might say if this is really what our wiser selves would want. What we would want if we were saved from these errors and illusions we are suffering under, then maybe we should go ahead with that. On the other hand, you could say, this is really a pretty tall order. That we\u2019re supposed to sacrifice not just a bit, but ourselves and everybody else, for this abstract idea that we don\u2019t really feel any strong connection to. I think that\u2019s one of the risks, but who knows what will be the outcome of this CEV?</p>\n<p>And there are further qualms one might have that need to be spelled out. Like exactly whose volition is it that is supposed to be extrapolated. Humanity\u2019s? Well then, who is humanity? Like does it include past generations for example? How far back? Does it include embryos that died?</p>\n<p>Who knows whether the core of humanity is nice? Maybe there are a lot of suppressed sadists out there, that we don\u2019t realize, because they know that they would be punished by society. Maybe if they went through this procedure, who knows what would come out?</p>\n<p>So it would be dangerous to run something like that, without some sort of safeguard check at the end. On the other hand, there is worry that if you put in too many of these checks, then in effect you move the whole thing back to what you want now. Because if you were allowed to look at an extrapolation, see whether you like it, or if you dislike it you run another one by changing the premises and you were allowed to keep going like that until you were happy with the result then basically it would be you now, making the decision. So, it\u2019s worth thinking about, whether there is some sort of compromise or blend that might be the most appealing.</p>\n<p>[00:36]</p>\n<p><strong id=\"Questioner__You_mentioned_before_about_a_computer_producing_sentience_itself_in_running_a_scenario__What_are_the_chances_that_that_is_the_society_that_we_live_in_today_\">Questioner: You mentioned before about a computer producing sentience itself in running a scenario. What are the chances that that is the society that we live in today?</strong></p>\n<p>Nick: I don\u2019t know, so what exactly are the chances? I think significant. I don\u2019t know, it\u2019s a subjective judgment here. maybe less than 50%? Like 1 in 10?</p>\n<p>There\u2019s a whole different topic, maybe we should save that topic for a different time..</p>\n<p>[00:37]</p>\n<p><strong id=\"Questioner__If_I_wanted_to_study_this_area_generally__existential_risk__what_kind_of_subject_would_you_recommend_I_pursue__We_re_all_undergrads__so_after_our_bachelors_we_will_start_on_master_or_go_into_a_job__If_I_wanted_to_study_it__what_kind_of_master_would_you_recommend_\">Questioner: If I wanted to study this area generally, existential risk, what kind of subject would you recommend I pursue? We\u2019re all undergrads, so after our bachelors we will start on master or go into a job. If I wanted to study it, what kind of master would you recommend?</strong></p>\n<p><strong> </strong></p>\n<p>Nick: Well part of it would depend on your talent, like if you\u2019re a quantitative guy or a verbal guy. There isn\u2019t really an ideal sort of educational program anywhere, to deal with these things. You\u2019d want to get a fairly broad education, there are many fields that could be relevant. If one looks at where people are coming from so far that have had something useful to say, a fair chunk of them are philosophers, some computer scientists, some economists, maybe physics.</p>\n<p>Those fields have one thing in common in that they are fairly versatile. Like if you\u2019re doing Philosophy, you can do Philosophy of X, or of Y, or of almost anything. Economics as well. It gives you a general set of tools that you can use to analyze different things, and computer science has these ways of thinking and structuring a problem that is useful for many things</p>\n<p>So it\u2019s not obvious which of those disciplines would be best, generically. I think that would depend on the individual, but then what I would suggest is that while you were doing it, you also try to read in other areas other than the one you were studying. And try to do it at a place where there are a lot of other people around with a support group and advisor that encouraged you and gave you some freedom to pursue different things.</p>\n<p>[00:38]</p>\n<p><strong id=\"Questioner__Would_you_consider_AI_created_by_human_beings_as_some_sort_of_consequence_of_evolutionary_process__Like_in_a_way_that_human_beings_tried_to_overcome_their_own_limitations_and_as_it_s_a_really_long_time_to_get_it_on_a_dna_level_you_just_get_it_quicker_on_a_more_computational_level_\">Questioner: Would you consider AI created by human beings as some sort of consequence of evolutionary process? Like in a way that human beings tried to overcome their own limitations and as it\u2019s a really long time to get it on a dna level you just get it quicker on a more computational level?</strong></p>\n<p>Nick: So whether we would use evolutionary algorithms to produce super- intelligence or..?</p>\n<p><strong id=\"Questioner__If_AI_itself_is_part_of_evolution__\">Questioner: If AI itself is part of evolution..</strong></p>\n<p><strong> </strong></p>\n<p>Nick: So there\u2019s kind of a trivial sense in which if we evolved and we created\u2026then obviously evolution had a part to play in the overall causal explanation of why we\u2019re going to get machine intelligence at the end. Now, for evolution to really to exert some shaping influence there have to be a number of factors at play. There would have to be a number of variants created that are different and then compete for resources and then there is a selection step. And for there to be significant evolution you have to enact this a lot of times.</p>\n<p>So whether that will happen or not in the future is not clear at all. If you have a signal tone for me, in that if a world order arises at a top level. Where there is only one decision making agency, which could be democratic world government or AI that rules everybody, or a self-enforcing moral code, or tyranny or a nice thing or bad thing</p>\n<p>But if you have that kind of structure there will at least be, in principal ability, for that unitary agent to control evolution within itself, like it could change selection pressures by taxing or subsidizing different kinds of life forms.</p>\n<p>If you don\u2019t have a singleton then you have different agencies that might be in competition with one another, and in principle in that scenario evolutionary pressures can come into play. But I think the way that it might pan out would be different from the way that we\u2019re used to seeing biological evolution, so for one thing you might have these potentially immortal life forms, that is they have software minds that don\u2019t naturally die, that could modify themselves.</p>\n<p>If they knew that their current type, if they continued to pursue their current strategy would be outcompeted and they didn\u2019t like that, they could change themselves immediately right away rather than wait to be eliminated.</p>\n<p>So you might get, if there were to be a long evolutionary process ahead and agents could anticipate that, you might get the effects of that instantaneously from anticipation.</p>\n<p>So I think you probably wouldn\u2019t see the evolutionary processes playing out but there might be some of the constraints that could be reflected more immediately by the fact that different agencies had to pursue strategies that they could see would be viable.</p>\n<p>[00:41]</p>\n<p><strong id=\"Questioner__So_do_you_think_it_s_possible_that_our_minds_could_be_scanned_and_then_be_uploaded_into_a_computer_machine_in_some_way_and_then_could_you_create_many_copies_of_ourselves_as_those_machines_\">Questioner: So do you think it\u2019s possible that our minds could be scanned and then be uploaded into a computer machine in some way and then could you create many copies of ourselves as those machines?</strong></p>\n<p>Nick: So this is what in technical terminology is \u201cwhole brain emulation\u201d or in more popular terminology \u201cuploading\u201d. So obviously this is impossible now, but seems like it\u2019s consistent with everything we know about physics and chemistry and so forth. So I think that will become feasible barring some kind of catastrophic thing that puts a stop to scientific and technological progress.</p>\n<p>So the way I imagine it would work is that you take a particular brain, freeze it or vitrify it, and then slice it up into thin slices that would be fed through some array of microscopes that would scan each slice with sufficient resolution and then automated image analysis algorithms would work on this to reconstruct the 3 dimensional neural network that your own organic brain implemented and I have this sort of information structure in a computer.</p>\n<p>At this point you need computational neuroscience to tell you what each component does. So you need to have a good theory of what say a pyramidal cell does, what a different kind of\u2026And then you would combine those little computational models of what each type of neuron does with this 3D map of the network and run it. And if everything went well you would have transferred the mind, with memories and personalities intact to the computer. And there is an open question of just how much resolution would you need to have, how much detail you would need to capture of the original mind in order to successfully do this. But I think there would be some level of detail which as I said before, might be on the level of synapses or thereabouts, possibly higher, that would suffice. So then you would be able to do this. And then after you\u2019re software , you could be copied, or speeded up or slowed down or paused or stuff like that</p>\n<p>[00:44]</p>\n<p><strong id=\"Questioner__There_has_been_a_lot_of_talk_of_controlling_the_AI_and_evaluating_the_risk__My_question_would_be_assuming_that_we_have_created_a_far_more_perfect_AI_than_ourselves_is_there_a_credible_reason_for_human_beings_to_continue_existing_\">Questioner: There has been a lot of talk of controlling the AI and evaluating the risk. My question would be assuming that we have created a far more perfect AI than ourselves is there a credible reason for human beings to continue existing?</strong></p>\n<p>Nick: Um, yeah, I certainly have the reason that if we value our own existence we seem to have a\u2026Do you mean to say that there would be a moral reason to exist or if we would have a self interested reason to exist.</p>\n<p><strong id=\"Questioner__Well_I_guess_it_would_be_your_opinion__\">Questioner: Well I guess it would be your opinion..</strong></p>\n<p>Nick: My opinon is that I would rather not see the genocide of the entire human species. Rather that we all live happily ever after. If those are the only two alternatives, I think yeah! Let\u2019s all live happily ever after! Is where I would come down on that.</p>\n<p>[00:45]</p>\n<p><strong id=\"Questioner__By_keeping_human_species_around_You_re_going_to_have_a_situation_presumably_where_you_have_extremely__extremely_advanced_AIs_where_they_have_few_decades_or_few_centuries_or_whatever_and_they_will_be_far__far_beyond_our_comprehension__and_even_if_we_still_integrate_to_some_degree_with_machines__mumble__biological_humans_then_they_ll_just_be_completely_inconceivable_to_us__So_isn_t_there_a_danger_that_our_stupidity_will_hamper_their_perfection_\">Questioner: By keeping human species around You\u2019re going to have a situation presumably where you have extremely, extremely advanced AIs where they have few decades or few centuries or whatever and they will be far, far beyond our comprehension, and even if we still integrate to some degree with machines (mumble) biological humans then they\u2019ll just be completely inconceivable to us. So isn\u2019t there a danger that our stupidity will hamper their perfection?</strong></p>\n<p>Nick: Would hamper <em>their</em> perfection?? Well there\u2019s enough space for there to be many different kinds of perfection pursued. Like right now we have a bunch of dust mites crawling around everywhere, but not really hampering our pursuit of art or truth or beauty. They\u2019re going about their business and we\u2019re going about ours.</p>\n<p>I guess you could have a future where there would be a lot of room in the universe for planetary sized computers thinking their grand thoughts while\u2026I\u2019m not making a prediction here, but if you wanted to have a nature preserve, with original nature or original human beings living like that, that wouldn\u2019t preclude the other thing from happening..</p>\n<p><strong id=\"Questioner__Or_a_dust_mite_might_not_hamper_us__but_things_like_viruses_or_bacteria_just_by_being_so_far_below_us__mumble___And_if_you_leave_humans_on_a_nature_preserve_and_they_re_aware_of_that__isn_t_there_a_risk_that_they_ll_be_angry_at_the_feeling_of_being_irrelevant_at_the_grand_scheme_of_things_\">Questioner: Or a dust mite might not hamper us, but things like viruses or bacteria just by being so far below us (mumble). And if you leave humans on a nature preserve and they\u2019re aware of that, isn\u2019t there a risk that they\u2019ll be angry at the feeling of being irrelevant at the grand scheme of things?</strong></p>\n<p>Nick: I suppose. I don\u2019t think it would bother the AI that would be able to protect itself, or remain out of reach. Now it might demean the remaining humans if we were dethroned from this position of kings, the highest life forms around, that it would be a demotion, and one would have to deal with that I suppose.</p>\n<p>It\u2019s unclear how much value to place on that. I mean right now in this universe which looks like it\u2019s infinite somewhere out there are gonna be all kinds of things including god like intellects and everything in between that are already outstripping us in every possible way.</p>\n<p>It doesn\u2019t seem to upset us terribly; we just get on with it. So I think people will have to make some psychological..I\u2019m sure we can adjust to it easily. Now it might be from some particular theory of value that this might be a sad thing for humanity. That we are not even locally at the top of the ladder.</p>\n<p><strong id=\"Questioner__If_rationalism_was_true__that_is_if_it_were_irrational_to_perform_wrong_acts__Would_we_still_have_to_worry_about_super_intelligence__It_seems_to_me_that_we_wouldn_t_have_\">Questioner: If rationalism was true, that is if it were irrational to perform wrong acts. Would we still have to worry about super-intelligence? It seems to me that we wouldn\u2019t have.</strong></p>\n<p>Nick: Well you might have a system that doesn\u2019t care about being rational, according to that definition of rationality. So I think that we would still have to worry</p>\n<p>[00:48]</p>\n<p><strong id=\"Questioner__Regarding_trying_to_program_AI_without_values___mumbles__But_as_I_understand_it__what_s_considered_one_of_the_most_promising_approach_in_AI_now_is_more_statistical_learning_type_approaches___And_the_problem_with_that_is_if_we_were_to_produce_an_AI_with_that__we_might_not_understand_its_inner_workings_enough_to_be_able_to_dive_in_and_modify_it_in_precisely_the_right_way_to_give_it_an_unalterable_list_of_terminal_values__\">Questioner: Regarding trying to program AI without values, (mumbles) But as I understand it, what\u2019s considered one of the most promising approach in AI now is more statistical learning type approaches.. And the problem with that is if we were to produce an AI with that, we might not understand its inner workings enough to be able to dive in and modify it in precisely the right way to give it an unalterable list of terminal values. </strong></p>\n<p><strong id=\"So_if_we_were_to_end_up_with_some_big_neural_network_that_we_trained_in_some_way_and_ended_up_with_something_that_could_perform_as_well_as_humans_in_some_particular_task_or_something__We_might_be_able_to_do_that_without_knowing_how_to_alter_it_to_have_some_particular_set_of_goals_\">So if we were to end up with some big neural network that we trained in some way and ended up with something that could perform as well as humans in some particular task or something. We might be able to do that without knowing how to alter it to have some particular set of goals.</strong></p>\n<p>Nick: Yeah, so there are some things there to think about. One general worry that one needs to bear in mind if one tries that kinds of approach is we might give it various examples like this is a good action and this is a bad action in this context, and maybe it would learn all those examples then the question is how would it generalize to other examples outside this class?</p>\n<p>So we could test it we could divide our examples initially into classes and train it on one and test its performance on the other, the way you would do to cross-validate. And then we think that means other cases that it hasn\u2019t seen it would have the same kind of performance. But all the cases that we could test it on would be cases that would apply to its current level of intelligence. So presumably we\u2019re going to do this while it\u2019s still at human or less than human intelligence. We don\u2019t want to wait to do this until it\u2019s already super-intelligent.</p>\n<p>So then the worry is that even if it were able to analyze what to do in a certain way in all of these cases, it\u2019s only dealing with all of these cases in the training case, when it\u2019s still at a human level of intelligence. Now maybe once it becomes smarter it will realize that there are different ways of classifying these cases that will have radically different implications for humans.</p>\n<p>So suppose that you try to train it to\u2026 this was one of the classic example of a bad idea of how to solve the control problem: Lets train the AI to want to make people smile, what can go wrong with that? So we train it on different people and if they smile when it does something that\u2019s like a kind of reward; it gets strength in those positions that led to the behavior that made people smile. And frowning would move the AI away from that kind of behavior. And you can imagine that this would work pretty well at a primitive state where the AI will engage in more pleasing and useful behavior because the user will smile at it and it will all work very well. But then once the AI reaches a certain level of intellectual sophistication it might realize that It could get people to smile not just by being nice but also by paralyzing their facial muscles in that constant beaming smile.</p>\n<p>And then you would have this perverse instantiation of the constant values all along the value that it wants to make people smile, but the kinds of behaviors it would pursue to achieve this goal would suddenly radically change at a certain point once the new set of strategies became available to it, and you would get this treacherous turn, which would be dangerous. So that\u2019s not to dismiss that whole category of approaches altogether. One would have to think through quite carefully, exactly how one would go about that.</p>\n<p>[00:52]</p>\n<p>There\u2019s also the issue of, a lot of the things we would want it to learn, if we think of human values and goals and ambitions. We think of them using human concepts, not using basic physical..like place atom A to zed in a certain order, But we think like promote peace, encourage people to develop and achieve\u2026These are things that to understand them we really need to have human concept, which a sub-human AI will not have, it\u2019s too dumb at that stage to have that. Now once it\u2019s super-intelligent it might easily understand all human concepts but then it\u2019s too late. It already needs to be friendly before that. So there might only be this brief window of opportunity where its roughly human leve,l where its still safe enough not to resist our attempt to indoctrinate it but smart enough that it can actually understand what we are trying to tell it.</p>\n<p>And again were going to have to be very careful to make sure that we can bring the system up to that interval and then freeze its development there and try to load the values in before boot strapping it farther.</p>\n<p>And maybe(this was one of the first questions) its intelligence will not be human level in the sense of being similar to a human at any one point. Maybe it will immediately be very good at chess but very bad at poetry and then it has to reach radically superhuman levels of capability in some domains before other domains even reach human level. And in that case it\u2019s not even clear that there will be this window of opportunity where you can load in the values. So I don\u2019t want to dismiss that, but that\u2019s like some additional things that one needs to think about, if one tries to develop that.</p>\n<p>[00:54]</p>\n<p><strong id=\"Questioner__How_likely_is_it_that_we_will_have_the_opportunity_in_our_lifetimes_to_become_immortal_by_mind_uploading_\">Questioner: How likely is it that we will have the opportunity in our lifetimes to become immortal by mind uploading?</strong></p>\n<p>Nick: Well first of all, by immortal here we mean living for a very long time, rather than literally never dying, which is a very different thing that would require our best theories of cosmology to turn out to be false for something like that.</p>\n<p>So living for a very long time: Im not going to give you a probability in the end. But I can say some of the things that\u2026Like first we would have to avoid most kinds of things like existential catastrophe that could put an end to this.</p>\n<p>So, if you start with 100% and you remove all the things that could go wrong, so first you would have to throw away whatever total level of existential risk is, integrated over all time. Then there is the obvious risk that you will die before any of this happens, which seems to be a very substantial risk. Now you can reduce that by signing up for cryonics, but that\u2019s of course an uncertain business as well. And there could be sub-existential catastrophes that would put an end to a lot of things like a big nuclear war or pandemics.</p>\n<p>And then I guess there are all these situations in which not everybody who is still around gets the opportunity to participate in what came after. Even though what came after doesn\u2019t count as an existential catastrophe\u2026 And [it can get] even more complicated, like if you took into account the simulation hypothesis, which we decided not to talk about today.</p>\n<p>[00:56]</p>\n<p><strong id=\"Q__Is_there_a_particular_year_we_should_aim_for_\">Q: Is there a particular year we should aim for?</strong></p>\n<p>Nick: As for the timelines, truth is we don\u2019t know. So you need to think about a very smeared out probability distribution. And really smear it, because things could happen surprisingly sooner like some probability 10 years from now or 20 years now but probably more probable at 30, 40, 50 years but some probability at 80 years or 200 years..</p>\n<p>There is just not good evidence that human beings are very good at predicting with precision these kinds of things far out in the future.</p>\n<p><strong id=\"Questioner___hard_to_understand__How_intelligent_can_we_really_get____we_already_have_this_complexity_class_of_problems_that_we_can_solve_or_not_\">Questioner: (hard to understand) How intelligent can we really get. \u2026 we already have this complexity class of problems that we can solve or not\u2026</strong></p>\n<p><strong id=\"Is_it_fair_to_believe_that_a_super_intelligent_machine_can_be_actually_be_that_exponentially_intelligent____this_is_very_close_to_what_we_could_achieve__A_literal_definition_of_intelligence_also__but__\">Is it fair to believe that a super-intelligent machine can be actually be that exponentially intelligent... this is very close to what we could achieve \u2026A literal definition of intelligence also, but..</strong></p>\n<p>Nick: Well in a sort of cheater sense we could solve all problems, sort of like everything a Turing Machine could..it could take like a piece of paper and..</p>\n<p>a) It would take too long to actually do it, and if we tried to do it, there are things that would probably throw us off before we have completed any sort of big Turing machine simulation</p>\n<p>There is a less figurative sense in which our abilities are already indirectly unlimited. That is, if we have the ability to create super intelligence, then in a sense we can do everything because we can create this thing that then solves the thing that we want solved. So there is this sequence of steps that we have to go through, but in the end it is solved.</p>\n<p>So there is this level of capability that means that once you have that level of capability your indirect reach is universal, like anything that could be done, you could indirectly achieve, and we might have already surpassed that level a long time ago, save for the fact that we are sort of uncoordinated on a global level and maybe a little bit unwise.</p>\n<p>But if you had a wise singleton then certainly you could imagine us plotting a very safe course, taking it very slowly and in the end we could be pretty confident that we would get to the end result. But maybe neither of those ideas are what you had in mind. Maybe you had more in mind The question of just how smart, in everyday sort of smart could a machine be,. So just how much more effective at social persuasion, to take one particular thing, than the most persuasive human.</p>\n<p>So that we don\u2019t really know. If one has a distribution of human abilities, and it seems like the best humans can do a lot better, in our intuitive sense of a lot, than the average humans. Then it would seem very surprising if the best humans like the top tenth of a percent had reached the upper limit of what was technologically feasible, that would seem to be an amazing coincidence. So one would then expect for the maximum achievable to be a lot higher. But exactly how high we don\u2019t know.</p>\n<p>So two more questions:</p>\n<p>[00:59]</p>\n<p><strong id=\"Q__Just_like_we_are_wondering_about_super_intelligent_being__is_it_possible_that_that_super_intelligent_will_worry_about_another_super_intelligent_being_that_it_will_create__Isn_t_that_also_recursive_\">Q: Just like we are wondering about super-intelligent being, is it possible that that super-intelligent will worry about another super-intelligent being that it will create? Isn\u2019t that also recursive?</strong></p>\n<p>Nick: So you consider where one AI designs another AI that\u2019s smarter and then that designs another.</p>\n<p>But it might not be clearly distinguishable from the scenario where we have one AI that modifies itself so that <em>it</em> ends up smarter. Whether you call it the same or different, it might be an unimportant difference.</p>\n<p>Last question. This has to be super profound question.</p>\n<p>[01:00]</p>\n<p><strong id=\"Q__So_my_question_is_why_should_we_even_try_to_build_a_super_intelligence_\">Q: So my question is why should we even try to build a super-intelligence?</strong></p>\n<p>Nick: I don\u2019t think we should now, do that. If you took a step back and thought what would a sane species do, well they would first figure out how to solve the control problem, and then they would think about it for a while to make sure that they really had the solution right and they hadn\u2019t just deluded themselves to how to solve it, and then maybe they would build a super-intelligence.</p>\n<p>So that\u2019s what the sane species will do, now what humanity will do is try to do everything they can as soon as possible, so there are people who have tried to build it as we speak, in a number of different places on earth, and fortunately it looks very difficult to build it with current technology. But of course it\u2019s getting easier over time, computers get better, computer science, the state of the art advances, we learn more about how the human brain works.</p>\n<p>So every year it gets a little bit easier, from some unknown very difficult level, it gets easier and easier. So at some point it seems someone will probably succeed at doing it. If the world remains sort of uncoordinated and uncontrolled as it is now, it\u2019s bound to happen soon after it becomes feasible. But we have no reason to accelerate that even more than its already happening ...</p>\n<p>So we were thinking about what would a powerful AI thing do that had just come into existence and it didn\u2019t know very much yet, but it had a lot of clever algorithms and a lot of processing power. Someone was suggesting maybe it would move around randomly, like a human baby does, to figure out how things move, how it can move its actuators.</p>\n<p>Then we had a discussion if that was a wise thing or not.</p>\n<p>But if you think about how the human species behave, we are really behaving very much like a baby were sort of moving and shaking everything that moves, just to see what happens. And the risk is that we are not in the nursery with a kind mother who has put us in a cradle, but that we are out in the jungle somewhere screaming at the top of our lungs, and maybe just alerting the lions to their supper.</p>\n<p>So let\u2019s wrap up. I enjoyed this a great deal, so thank you for your questions.</p>", "sections": [{"title": "Questioner: So what\u2019s your definition of machine intelligence or super-intellegence AI\u2026 Is there like a precise definition there?", "anchor": "Questioner__So_what_s_your_definition_of_machine_intelligence_or_super_intellegence_AI__Is_there_like_a_precise_definition_there_", "level": 1}, {"title": "Questioner: Got a question that maybe should have waited til the end: There are two organizations, FHI and SIAI, working on this. Let's say I thought this was the most important problem in the world, and I should be donating money to this. Who should I give it to?", "anchor": "Questioner__Got_a_question_that_maybe_should_have_waited_til_the_end__There_are_two_organizations__FHI_and_SIAI__working_on_this__Let_s_say_I_thought_this_was_the_most_important_problem_in_the_world__and_I_should_be_donating_money_to_this__Who_should_I_give_it_to_", "level": 1}, {"title": "Questioner: Do you think a biological component is necessary for an artificial intelligence to achieve sentience or something equivalent?", "anchor": "Questioner__Do_you_think_a_biological_component_is_necessary_for_an_artificial_intelligence_to_achieve_sentience_or_something_equivalent_", "level": 1}, {"title": "Questioner- This relates to AI being dangerous. It seems to me that while it would certainly be interesting if we were to get AI that were much more intelligent than a human being, its not necessarily dangerous. ", "anchor": "Questioner__This_relates_to_AI_being_dangerous__It_seems_to_me_that_while_it_would_certainly_be_interesting_if_we_were_to_get_AI_that_were_much_more_intelligent_than_a_human_being__its_not_necessarily_dangerous__", "level": 1}, {"title": "Questioner: Human beings have been very successful. One feature of that that has been very crucial are our hands that have enabled us to get a start on working tools and so on. Even if an AI is running on some computer somewhere, that would be more analogous to a very intelligent creature which doesn\u2019t have very good hands. It\u2019s very hard for it to actually DO anything. ", "anchor": "Questioner__Human_beings_have_been_very_successful__One_feature_of_that_that_has_been_very_crucial_are_our_hands_that_have_enabled_us_to_get_a_start_on_working_tools_and_so_on__Even_if_an_AI_is_running_on_some_computer_somewhere__that_would_be_more_analogous_to_a_very_intelligent_creature_which_doesn_t_have_very_good_hands__It_s_very_hard_for_it_to_actually_DO_anything__", "level": 1}, {"title": "Maybe the in-the-box method is promising. Because if we just don\u2019t give the AI hands, some way to actually do something..If all it can do is alter its own code, and maybe communicate infomationally. That seems...", "anchor": "Maybe_the_in_the_box_method_is_promising__Because_if_we_just_don_t_give_the_AI_hands__some_way_to_actually_do_something__If_all_it_can_do_is_alter_its_own_code__and_maybe_communicate_infomationally__That_seems___", "level": 1}, {"title": "Questioner: How hard do you think the idea of controlling the mentality of intelligence is, with something at least as intelligent as us, considering how hard it is to convince humans to act in a certain civilized way of life?", "anchor": "Questioner__How_hard_do_you_think_the_idea_of_controlling_the_mentality_of_intelligence_is__with_something_at_least_as_intelligent_as_us__considering_how_hard_it_is_to_convince_humans_to_act_in_a_certain_civilized_way_of_life_", "level": 1}, {"title": "Questioner: You don\u2019t think that like us, its experiences might cause it to question its core values as we do?", "anchor": "Questioner__You_don_t_think_that_like_us__its_experiences_might_cause_it_to_question_its_core_values_as_we_do_", "level": 1}, {"title": "Questioner: What do you think of the necessary\u2026requisites for the conscious mind? What are the features?", "anchor": "Questioner__What_do_you_think_of_the_necessary_requisites_for_the_conscious_mind__What_are_the_features_", "level": 1}, {"title": "Questioner: To bring back to the goal and motivation approach to making an AI friendly towards us, one of the most effective ways of controlling human behavior, quite aside from goals and motivations , is to train them by instilling neuroses. It\u2019s why 99.99% of us in this room couldn\u2019t pee in our pants right now even if he really, really wanted to. ", "anchor": "Questioner__To_bring_back_to_the_goal_and_motivation_approach_to_making_an_AI_friendly_towards_us__one_of_the_most_effective_ways_of_controlling_human_behavior__quite_aside_from_goals_and_motivations___is_to_train_them_by_instilling_neuroses__It_s_why_99_99__of_us_in_this_room_couldn_t_pee_in_our_pants_right_now_even_if_he_really__really_wanted_to__", "level": 1}, {"title": "Is it possible to approach controlling an AI in that way or even would it be possible for an AI to develop in such a way that there is a developmental period in which a risk-reward system or some sort of neuroses instilment could be used to basically create these rules that an AI couldn\u2019t break?", "anchor": "Is_it_possible_to_approach_controlling_an_AI_in_that_way_or_even_would_it_be_possible_for_an_AI_to_develop_in_such_a_way_that_there_is_a_developmental_period_in_which_a_risk_reward_system_or_some_sort_of_neuroses_instilment_could_be_used_to_basically_create_these_rules_that_an_AI_couldn_t_break_", "level": 1}, {"title": "Questioner: Appreciating the existential risks, what do you think about goals and motivations, and such drastic measures of control sort of a) ethically and b) as a basis of a working relationship?", "anchor": "Questioner__Appreciating_the_existential_risks__what_do_you_think_about_goals_and_motivations__and_such_drastic_measures_of_control_sort_of_a__ethically_and_b__as_a_basis_of_a_working_relationship_", "level": 1}, {"title": "Questioner: Eliezer suggests that one of the many problems with arbitrary stamps in AI space is that human values are very complex. So virtually any goal system will go horribly wrong because it will be doing things we don\u2019t quite care about, and that\u2019s as bad as paperclips. How complex do you think human values will be?", "anchor": "Questioner__Eliezer_suggests_that_one_of_the_many_problems_with_arbitrary_stamps_in_AI_space_is_that_human_values_are_very_complex__So_virtually_any_goal_system_will_go_horribly_wrong_because_it_will_be_doing_things_we_don_t_quite_care_about__and_that_s_as_bad_as_paperclips__How_complex_do_you_think_human_values_will_be_", "level": 1}, {"title": "Questioner: Isn\u2019t there are risk that.. the AI would decide that if we thought about it for 1000 years really, really carefully, that we would just decide to just let the AIs to take over?", "anchor": "Questioner__Isn_t_there_are_risk_that___the_AI_would_decide_that_if_we_thought_about_it_for_1000_years_really__really_carefully__that_we_would_just_decide_to_just_let_the_AIs_to_take_over_", "level": 1}, {"title": "Questioner: You mentioned before about a computer producing sentience itself in running a scenario. What are the chances that that is the society that we live in today?", "anchor": "Questioner__You_mentioned_before_about_a_computer_producing_sentience_itself_in_running_a_scenario__What_are_the_chances_that_that_is_the_society_that_we_live_in_today_", "level": 1}, {"title": "Questioner: If I wanted to study this area generally, existential risk, what kind of subject would you recommend I pursue? We\u2019re all undergrads, so after our bachelors we will start on master or go into a job. If I wanted to study it, what kind of master would you recommend?", "anchor": "Questioner__If_I_wanted_to_study_this_area_generally__existential_risk__what_kind_of_subject_would_you_recommend_I_pursue__We_re_all_undergrads__so_after_our_bachelors_we_will_start_on_master_or_go_into_a_job__If_I_wanted_to_study_it__what_kind_of_master_would_you_recommend_", "level": 1}, {"title": "Questioner: Would you consider AI created by human beings as some sort of consequence of evolutionary process? Like in a way that human beings tried to overcome their own limitations and as it\u2019s a really long time to get it on a dna level you just get it quicker on a more computational level?", "anchor": "Questioner__Would_you_consider_AI_created_by_human_beings_as_some_sort_of_consequence_of_evolutionary_process__Like_in_a_way_that_human_beings_tried_to_overcome_their_own_limitations_and_as_it_s_a_really_long_time_to_get_it_on_a_dna_level_you_just_get_it_quicker_on_a_more_computational_level_", "level": 1}, {"title": "Questioner: If AI itself is part of evolution..", "anchor": "Questioner__If_AI_itself_is_part_of_evolution__", "level": 1}, {"title": "Questioner: So do you think it\u2019s possible that our minds could be scanned and then be uploaded into a computer machine in some way and then could you create many copies of ourselves as those machines?", "anchor": "Questioner__So_do_you_think_it_s_possible_that_our_minds_could_be_scanned_and_then_be_uploaded_into_a_computer_machine_in_some_way_and_then_could_you_create_many_copies_of_ourselves_as_those_machines_", "level": 1}, {"title": "Questioner: There has been a lot of talk of controlling the AI and evaluating the risk. My question would be assuming that we have created a far more perfect AI than ourselves is there a credible reason for human beings to continue existing?", "anchor": "Questioner__There_has_been_a_lot_of_talk_of_controlling_the_AI_and_evaluating_the_risk__My_question_would_be_assuming_that_we_have_created_a_far_more_perfect_AI_than_ourselves_is_there_a_credible_reason_for_human_beings_to_continue_existing_", "level": 1}, {"title": "Questioner: Well I guess it would be your opinion..", "anchor": "Questioner__Well_I_guess_it_would_be_your_opinion__", "level": 1}, {"title": "Questioner: By keeping human species around You\u2019re going to have a situation presumably where you have extremely, extremely advanced AIs where they have few decades or few centuries or whatever and they will be far, far beyond our comprehension, and even if we still integrate to some degree with machines (mumble) biological humans then they\u2019ll just be completely inconceivable to us. So isn\u2019t there a danger that our stupidity will hamper their perfection?", "anchor": "Questioner__By_keeping_human_species_around_You_re_going_to_have_a_situation_presumably_where_you_have_extremely__extremely_advanced_AIs_where_they_have_few_decades_or_few_centuries_or_whatever_and_they_will_be_far__far_beyond_our_comprehension__and_even_if_we_still_integrate_to_some_degree_with_machines__mumble__biological_humans_then_they_ll_just_be_completely_inconceivable_to_us__So_isn_t_there_a_danger_that_our_stupidity_will_hamper_their_perfection_", "level": 1}, {"title": "Questioner: Or a dust mite might not hamper us, but things like viruses or bacteria just by being so far below us (mumble). And if you leave humans on a nature preserve and they\u2019re aware of that, isn\u2019t there a risk that they\u2019ll be angry at the feeling of being irrelevant at the grand scheme of things?", "anchor": "Questioner__Or_a_dust_mite_might_not_hamper_us__but_things_like_viruses_or_bacteria_just_by_being_so_far_below_us__mumble___And_if_you_leave_humans_on_a_nature_preserve_and_they_re_aware_of_that__isn_t_there_a_risk_that_they_ll_be_angry_at_the_feeling_of_being_irrelevant_at_the_grand_scheme_of_things_", "level": 1}, {"title": "Questioner: If rationalism was true, that is if it were irrational to perform wrong acts. Would we still have to worry about super-intelligence? It seems to me that we wouldn\u2019t have.", "anchor": "Questioner__If_rationalism_was_true__that_is_if_it_were_irrational_to_perform_wrong_acts__Would_we_still_have_to_worry_about_super_intelligence__It_seems_to_me_that_we_wouldn_t_have_", "level": 1}, {"title": "Questioner: Regarding trying to program AI without values, (mumbles) But as I understand it, what\u2019s considered one of the most promising approach in AI now is more statistical learning type approaches.. And the problem with that is if we were to produce an AI with that, we might not understand its inner workings enough to be able to dive in and modify it in precisely the right way to give it an unalterable list of terminal values. ", "anchor": "Questioner__Regarding_trying_to_program_AI_without_values___mumbles__But_as_I_understand_it__what_s_considered_one_of_the_most_promising_approach_in_AI_now_is_more_statistical_learning_type_approaches___And_the_problem_with_that_is_if_we_were_to_produce_an_AI_with_that__we_might_not_understand_its_inner_workings_enough_to_be_able_to_dive_in_and_modify_it_in_precisely_the_right_way_to_give_it_an_unalterable_list_of_terminal_values__", "level": 1}, {"title": "So if we were to end up with some big neural network that we trained in some way and ended up with something that could perform as well as humans in some particular task or something. We might be able to do that without knowing how to alter it to have some particular set of goals.", "anchor": "So_if_we_were_to_end_up_with_some_big_neural_network_that_we_trained_in_some_way_and_ended_up_with_something_that_could_perform_as_well_as_humans_in_some_particular_task_or_something__We_might_be_able_to_do_that_without_knowing_how_to_alter_it_to_have_some_particular_set_of_goals_", "level": 1}, {"title": "Questioner: How likely is it that we will have the opportunity in our lifetimes to become immortal by mind uploading?", "anchor": "Questioner__How_likely_is_it_that_we_will_have_the_opportunity_in_our_lifetimes_to_become_immortal_by_mind_uploading_", "level": 1}, {"title": "Q: Is there a particular year we should aim for?", "anchor": "Q__Is_there_a_particular_year_we_should_aim_for_", "level": 1}, {"title": "Questioner: (hard to understand) How intelligent can we really get. \u2026 we already have this complexity class of problems that we can solve or not\u2026", "anchor": "Questioner___hard_to_understand__How_intelligent_can_we_really_get____we_already_have_this_complexity_class_of_problems_that_we_can_solve_or_not_", "level": 1}, {"title": "Is it fair to believe that a super-intelligent machine can be actually be that exponentially intelligent... this is very close to what we could achieve \u2026A literal definition of intelligence also, but..", "anchor": "Is_it_fair_to_believe_that_a_super_intelligent_machine_can_be_actually_be_that_exponentially_intelligent____this_is_very_close_to_what_we_could_achieve__A_literal_definition_of_intelligence_also__but__", "level": 1}, {"title": "Q: Just like we are wondering about super-intelligent being, is it possible that that super-intelligent will worry about another super-intelligent being that it will create? Isn\u2019t that also recursive?", "anchor": "Q__Just_like_we_are_wondering_about_super_intelligent_being__is_it_possible_that_that_super_intelligent_will_worry_about_another_super_intelligent_being_that_it_will_create__Isn_t_that_also_recursive_", "level": 1}, {"title": "Q: So my question is why should we even try to build a super-intelligence?", "anchor": "Q__So_my_question_is_why_should_we_even_try_to_build_a_super_intelligence_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 34}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["A9BZ6wEQqFbacR2E2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T17:57:42.468Z", "modifiedAt": null, "url": null, "title": "Three more classes coming from Stanford of interest here", "slug": "three-more-classes-coming-from-stanford-of-interest-here", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iTskH8btZGTWbz2TG/three-more-classes-coming-from-stanford-of-interest-here", "pageUrlRelative": "/posts/iTskH8btZGTWbz2TG/three-more-classes-coming-from-stanford-of-interest-here", "linkUrl": "https://www.lesswrong.com/posts/iTskH8btZGTWbz2TG/three-more-classes-coming-from-stanford-of-interest-here", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Three%20more%20classes%20coming%20from%20Stanford%20of%20interest%20here&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThree%20more%20classes%20coming%20from%20Stanford%20of%20interest%20here%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTskH8btZGTWbz2TG%2Fthree-more-classes-coming-from-stanford-of-interest-here%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Three%20more%20classes%20coming%20from%20Stanford%20of%20interest%20here%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTskH8btZGTWbz2TG%2Fthree-more-classes-coming-from-stanford-of-interest-here", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiTskH8btZGTWbz2TG%2Fthree-more-classes-coming-from-stanford-of-interest-here", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 13, "htmlBody": "<p><a href=\"http://www.pgm-class.org/\">http://www.pgm-class.org/</a>&nbsp;- Probabilistic Graphical Models</p>\n<p><a href=\"http://www.nlp-class.org/\">http://www.nlp-class.org/</a>&nbsp;- Natural Language Processing</p>\n<p><a href=\"http://www.game-theory-class.org/\">http://www.game-theory-class.org/</a>&nbsp;- Self explainatory<br /><br />ETA:</p>\n<p><a href=\"http://infotheory-class.org/\">http://infotheory-class.org/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iTskH8btZGTWbz2TG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 25, "extendedScore": null, "score": 8.010232511492729e-07, "legacy": true, "legacyId": "10982", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T19:28:49.359Z", "modifiedAt": null, "url": null, "title": "The curse of identity", "slug": "the-curse-of-identity", "viewCount": null, "lastCommentedAt": "2018-01-31T01:40:46.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tAXrD8Y6hcJ8dt6Nt/the-curse-of-identity", "pageUrlRelative": "/posts/tAXrD8Y6hcJ8dt6Nt/the-curse-of-identity", "linkUrl": "https://www.lesswrong.com/posts/tAXrD8Y6hcJ8dt6Nt/the-curse-of-identity", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20curse%20of%20identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20curse%20of%20identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAXrD8Y6hcJ8dt6Nt%2Fthe-curse-of-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20curse%20of%20identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAXrD8Y6hcJ8dt6Nt%2Fthe-curse-of-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtAXrD8Y6hcJ8dt6Nt%2Fthe-curse-of-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1903, "htmlBody": "<blockquote>\r\n<p>So what you probably mean is, \"I intend to do school to improve my chances on the market\". But this statement is still false, unless it is also true that \"I intend to improve my chances on the market\". Do you, in actual fact, intend to improve your chances on the market?<br /><br />I expect not. Rather, I expect that your motivation is to appear to be the sort of person who you think you would be if you were ambitiously attempting to improve your chances on the market... which is not really motivating enough to actually DO the work. However, by persistently trying to do so, and presenting yourself with enough suffering at your failure to do it, you get to feel as if you are that sort of person without having to actually do the work. This is actually a pretty optimal solution to the problem, if you think about it. (Or rather, if you DON'T think about it!) -- <a href=\"/lw/7s4/poll_results_lw_probably_doesnt_cause_akrasia/59hx\">PJ Eby</a></p>\r\n</blockquote>\r\n<p>I have become convinced that problems of this kind are the number one problem humanity has. I'm also pretty sure that most people here, no matter how much they've been reading about <a href=\"http://wiki.lesswrong.com/wiki/Signaling\">signaling</a>, still fail to appreciate the magnitude of the problem.</p>\r\n<p>Here are two major screw-ups and one narrowly averted screw-up that I've been guilty of. See if you can find the pattern.</p>\r\n<ul>\r\n<li>When I began my university studies back in 2006, I felt strongly motivated to do something about Singularity matters. I genuinely believed that this was the most important thing facing humanity, and that it needed to be urgently taken care of. So in order to become able to contribute, I tried to study as much as possible. I had had troubles with procrastination, and so, in what has to be one of the most idiotic and ill-thought-out acts of self-sabotage possible, I taught myself to feel guilty whenever I was relaxing and not working. Combine an inability to properly relax with an attempted course load that was twice the university's recommended pace, and you can guess the results: after a year or two, I had an extended burnout that I still haven't fully recovered from. I ended up completing my Bachelor's degree in five years, which is the official target time for doing both your Bachelor's and your Master's.</li>\r\n<li>A few years later, I became one of the founding members of the <a href=\"http://en.wikipedia.org/wiki/Piraattipuolue\">Finnish Pirate Party</a>, and on the basis of some writings the others thought were pretty good, got myself elected as the spokesman. Unfortunately &ndash; and as I should have known before taking up the post &ndash; I was a pretty bad choice for this job. I'm good at expressing myself in writing, and when I have the time to think. I hate talking with strangers on the phone, find it distracting to look people in the eyes when I'm talking with them, and have a tendency to start a sentence over two or three times before hitting on a formulation I like. I'm also bad at thinking quickly on my feet and coming up with snappy answers in live conversation. The spokesman task involved things like giving quick statements to reporters ten seconds after I'd been woken up by their phone call, and live interviews where I had to reply to criticisms so foreign to my thinking that they would never have occurred to me naturally. I was pretty terrible at the job, and finally delegated most of it to other people until my term ran out &ndash; though not before I'd already done noticeable damage to our cause.</li>\r\n<li><a href=\"http://xuenay.livejournal.com/332512.html\">Last year, I was a Visiting Fellow</a> at the Singularity Institute. At one point, I ended up helping Eliezer in writing his book. Mostly this involved me just sitting next to him and making sure he did get writing done while I surfed the Internet or played a computer game. Occasionally I would offer some suggestion if asked. Although I did not actually do much, the multitasking required still made me unable to spend this time productively myself, and for some reason it always left me tired the next day. I felt somewhat unhappy with this, in that I felt I was doing something that anyone could do. Eventually Anna Salamon pointed out to me that maybe this was something that I was more capable of doing than others, exactly because so many people would feel that &rdquo;anyone&rdquo; could do this and thus would prefer to do something else.</li>\r\n</ul>\r\n<p>It may not be immediately obvious, but all three examples have something in common. In each case, I thought I was working for a particular goal (become capable of doing useful Singularity work, advance the cause of a political party, do useful Singularity work). But as soon as I set that goal, my brain automatically and invisibly re-interpreted it as the goal of doing something that gave the impression of doing prestigious work for a cause (spending all my waking time working, being the spokesman of a political party, writing papers or doing something else few others could do). \"Prestigious work\" could also be translated as \"work that really convinces others that you are doing something valuable for a cause\".</p>\r\n<p><a id=\"more\"></a>We run on <a href=\"/lw/uv/ends_dont_justify_means_among_humans/\">corrupted hardware</a>: our minds are <a href=\"/tag/whyeveryonehypocrite\">composed of many modules</a>, and the modules that evolved to <a href=\"/lw/8ev/modularity_signaling_and_belief_in_belief/\">make us seem impressive and gather allies</a> are also evolved to subvert the ones holding our conscious beliefs. Even when we believe that we are working on something that may ultimately determine the fate of humanity, our signaling modules may hijack our goals so as to optimize for persuading outsiders that we are working on the goal, instead of optimizing for achieving the goal!<br /><br />You can see this all the time, everywhere:</p>\r\n<ul>\r\n<li>Charity groups often have difficulty attracting people to do much-needed but boring and unprestigious work, and even people who think they care about the cause may find it difficult to do such work. &nbsp;</li>\r\n<li> People may think that <a href=\"/lw/7s4/poll_results_lw_probably_doesnt_cause_akrasia/59hx\">they're motivated to study because they want to increase their earnings</a>, but then they don't actually achieve much in their studies. In reality, they might be only motivated to give the impression of being the kind of person who studies hard in order to increase their earnings, and looking like they work hard to study is enough to give this impression.&nbsp;</li>\r\n<li> Countless people intend to become a published author one day, but don't actually work to polish their writing to achieve this: they want to be writers, but they don't want to write. </li>\r\n<li> Self-help techniques may seem like really useful at first, but then the person loses the motivation to consistently use them, even if the techniques would help them achieve their goal. They don't actually want to achieve their goal, they just want to be seen working for the goal. Looking at various self-help techniques and trying out some for a couple of times can be enough to fulfill this goal. Not actually achieving it also lets people go buy more self-help books and therefore maintain that self-image. </li>\r\n<li> Likewise, some people try out lots of self-help techniques and think they're making great progress, or <a href=\"/lw/7s4/poll_results_lw_probably_doesnt_cause_akrasia/\">read Less Wrong and report it helping them with procrastination</a>, when they aren't actually any better than before and don't have any objective ways of measuring their progress. </li>\r\n<li>Likewise, some people only keep talking about solving problems all day and seem smart for having endlessly analyzed them, but never actually do anything about them. (Some people write posts like these and then comment on them, instead of solving their issues.)</li>\r\n<li>People commit altruistic acts, and <a href=\"/lw/1d9/doing_your_good_deed_for_the_day/\">then act selfishly and inconsiderately later in the day</a>, once they feel that they have been good enough that they've earned the right to be a little selfish. In other words, they estimate that they've been good enough at presenting an altruistic image that a few transgressions won't threaten that image.</li>\r\n<li>People often choose to not find out about ways of helping others, or <a href=\"/lw/72d/strategic_ignorance_and_plausible_deniability/\">attempt to remain purposefully ignorant</a> of the ways in which their actions hurt others. They are often uninterested in <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">optimal charity</a>, and prefer to just establish their nature as a good person by donating to some popular charity, regardless of its effectiveness. Groups that try to make others more aware of the consequences of their actions (e.g. animal rights activists presenting evidence of the way factory animals are treated, people talking about optimal charity) are often treated with scorn and derision. AGI researchers may purposefully avoid finding out about and thinking about the risks of AGI. All of these actions help <a href=\"/lw/72d/strategic_ignorance_and_plausible_deniability/\">establish plausible deniability</a>: it's easier for a person to claim and think that they're a good person if they can show that they didn't know about the negative consequences of their actions.&nbsp;</li>\r\n<li> The <a href=\"http://kriswrites.com/2009/04/16/freelancers-survival-guide-illness/\">freelancer's</a> <a href=\"http://kriswrites.com/2009/06/04/freelancers-survival-guide-discipline/\">curse</a>: for many people, working at home is much harder than working at an office, for there is no social environment pushing you to work full days. A freelancer may do a little bit of work and then feel too tired to continue, or they may be slightly sick and feel like they can't work today, or constantly have their mind claim that something else is more important for their productivity right now. \"<em><span>I need to figure out if I&rsquo;m really hungry or&mdash;catch this&mdash;bored with what I&rsquo;m doing.<span> </span>If I&rsquo;m bored, I think I&rsquo;m hungry, because that&rsquo;s one of the few things I will get up from my desk to deal with.<span> </span>If I need a meal, I eat.<span> </span>But my subconscious loves to trick me (and my hips) by convincing me to leave when I&rsquo;m not through.</span> <span>Often, the &ldquo;I&rsquo;m hungry&rdquo; reaction comes when I&rsquo;m working on something particularly difficult or something I don&rsquo;t want to do.<span> </span>Again, it took many months (and too many calories) to figure this one out.<span> </span>Now, before I get something to eat, I ask myself this:<span> </span>Do I like what I&rsquo;m working on?<span> </span>If the answer is no, I generally stay at my desk.</span></em>\" -- <a href=\"http://kriswrites.com/2009/06/04/freelancers-survival-guide-discipline/\">Kristine Kathryn Rusch</a> &nbsp;</li>\r\n<li> Skeptics, priding themselves on an ability to think clearly and debunk pseudoscience, may actually start engaging in <a href=\"/lw/1ww/undiscriminating_skepticism/\">undiscriminating skepticism</a>, attacking anything that feels vaguely pseudoscientific regardless of its actual merit. </li>\r\n<li>Intellectuals may want to have an identity that sets them apart from others, becoming <a href=\"/lw/2pv/intellectual_hipsters_and_metacontrarianism/\">intellectual hipsters and meta-contrarians</a> and question things just for the sake of questioning the accepted wisdom; more generally, people will do things just for the sake of being different. &nbsp;</li>\r\n<li>And many others, like ~all of Robin Hanson's posts on <a href=\"http://www.overcomingbias.com/tag/signaling\">signaling</a> or <a href=\"http://www.overcomingbias.com/tag/hypocrisy\">hypocrisy</a>.</li>\r\n</ul>\r\n<p>There's an additional caveat to be aware of: it is actually possible to fall prey to this problem while purposefully attempting to avoid it. You might realize that you have a tendency to only want to do particularly prestigeful work for a cause... so you decide to only do the least prestigeful work available, in order to prove that you are the kind of person who doesn't care about the prestige of the task! You are still optimizing your actions on the basis of expected prestige and being able to tell yourself and outsiders an impressive story, not on the basis of your marginal impact.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"x6evH6MyPK3nxsoff": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tAXrD8Y6hcJ8dt6Nt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 151, "baseScore": 192, "extendedScore": null, "score": 0.000379, "legacy": true, "legacyId": "10975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 192, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 305, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K9ZaZXDnL3SEmYZqB", "o6CuZk2oPtDXqeY5A", "q3rBapm2TjQ6tx9Td", "r8stxYL29NF9w53am", "fxgkYCbG5Hgy58TyC", "pC47ZTsPNAkjavkXs", "Jko7pt7MwwTBrfG3A", "9kcTNWopvXFncXgPy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T20:46:00.723Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ADxqmurNNxFeLBLzq/meetup-fort-collins-colorado-meetup-wedneday-7pm-0", "pageUrlRelative": "/posts/ADxqmurNNxFeLBLzq/meetup-fort-collins-colorado-meetup-wedneday-7pm-0", "linkUrl": "https://www.lesswrong.com/posts/ADxqmurNNxFeLBLzq/meetup-fort-collins-colorado-meetup-wedneday-7pm-0", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADxqmurNNxFeLBLzq%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADxqmurNNxFeLBLzq%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FADxqmurNNxFeLBLzq%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4x'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">23 November 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet  cool people. Talk about lots of interesting things. Have fun!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4x'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ADxqmurNNxFeLBLzq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.010829625290561e-07, "legacy": true, "legacyId": "10983", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/4x\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">23 November 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet  cool people. Talk about lots of interesting things. Have fun!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/4x\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-17T23:43:33.753Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne practical rationality meetup", "slug": "meetup-melbourne-practical-rationality-meetup-2", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WxA4iAsY3dT5nqw2n/meetup-melbourne-practical-rationality-meetup-2", "pageUrlRelative": "/posts/WxA4iAsY3dT5nqw2n/meetup-melbourne-practical-rationality-meetup-2", "linkUrl": "https://www.lesswrong.com/posts/WxA4iAsY3dT5nqw2n/meetup-melbourne-practical-rationality-meetup-2", "postedAtFormatted": "Thursday, November 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20practical%20rationality%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxA4iAsY3dT5nqw2n%2Fmeetup-melbourne-practical-rationality-meetup-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxA4iAsY3dT5nqw2n%2Fmeetup-melbourne-practical-rationality-meetup-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWxA4iAsY3dT5nqw2n%2Fmeetup-melbourne-practical-rationality-meetup-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4y'>Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, Victoria, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Practical rationality</em>, as distinct from <em>social</em> and <em>rationality outreach</em>. Look for a social meetup on the 3rd Friday of each month, and a rationality outreach meetup TBD.</p>\n\n<p><em>Discussion:</em> <br />\n<a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <br />\n<a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4y'>Melbourne practical rationality meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WxA4iAsY3dT5nqw2n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.011459628523417e-07, "legacy": true, "legacyId": "10984", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup\">Discussion article for the meetup : <a href=\"/meetups/4y\">Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne, Victoria, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><em>Practical rationality</em>, as distinct from <em>social</em> and <em>rationality outreach</em>. Look for a social meetup on the 3rd Friday of each month, and a rationality outreach meetup TBD.</p>\n\n<p><em>Discussion:</em> <br>\n<a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <br>\n<a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1\">Discussion article for the meetup : <a href=\"/meetups/4y\">Melbourne practical rationality meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T00:43:16.526Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-27", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:30.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Maelin", "createdAt": "2009-05-28T03:32:36.549Z", "isAdmin": false, "displayName": "Maelin"}, "userId": "CE5vuYfsSRTeG2KWd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zdw6Nm2AsGgNdAHv9/meetup-melbourne-social-meetup-27", "pageUrlRelative": "/posts/zdw6Nm2AsGgNdAHv9/meetup-melbourne-social-meetup-27", "linkUrl": "https://www.lesswrong.com/posts/zdw6Nm2AsGgNdAHv9/meetup-melbourne-social-meetup-27", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzdw6Nm2AsGgNdAHv9%2Fmeetup-melbourne-social-meetup-27%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzdw6Nm2AsGgNdAHv9%2Fmeetup-melbourne-social-meetup-27", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzdw6Nm2AsGgNdAHv9%2Fmeetup-melbourne-social-meetup-27", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/4z'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 November 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Charles Dickens Tavern, 290 Collins St, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's social meetup is at the Charles Dickens Tavern, on Collins St between Elizabeth and Swanston, starting at 7pm.</p>\n\n<p>We don't believe the place will get too loud, but if you show up and we aren't there, we may have moved somewhere else. In that case, call 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/4z'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zdw6Nm2AsGgNdAHv9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.011671527633888e-07, "legacy": true, "legacyId": "10985", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/4z\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 November 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Charles Dickens Tavern, 290 Collins St, Melbourne</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This month's social meetup is at the Charles Dickens Tavern, on Collins St between Elizabeth and Swanston, starting at 7pm.</p>\n\n<p>We don't believe the place will get too loud, but if you show up and we aren't there, we may have moved somewhere else. In that case, call 0421 231 789.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/4z\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T05:16:34.681Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Evaporative Cooling of Group Beliefs", "slug": "seq-rerun-evaporative-cooling-of-group-beliefs", "viewCount": null, "lastCommentedAt": "2018-01-30T20:01:18.620Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gfzYfKY7Mweoy3hEy/seq-rerun-evaporative-cooling-of-group-beliefs", "pageUrlRelative": "/posts/gfzYfKY7Mweoy3hEy/seq-rerun-evaporative-cooling-of-group-beliefs", "linkUrl": "https://www.lesswrong.com/posts/gfzYfKY7Mweoy3hEy/seq-rerun-evaporative-cooling-of-group-beliefs", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Evaporative%20Cooling%20of%20Group%20Beliefs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Evaporative%20Cooling%20of%20Group%20Beliefs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfzYfKY7Mweoy3hEy%2Fseq-rerun-evaporative-cooling-of-group-beliefs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Evaporative%20Cooling%20of%20Group%20Beliefs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfzYfKY7Mweoy3hEy%2Fseq-rerun-evaporative-cooling-of-group-beliefs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgfzYfKY7Mweoy3hEy%2Fseq-rerun-evaporative-cooling-of-group-beliefs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/lr/evaporative_cooling_of_group_beliefs/\">Evaporative Cooling of Group Beliefs</a> was originally published on 07 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When a cult encounters a blow to their own beliefs (a prediction fails to come true, their leader is caught in a scandal, etc) the cult will often become more fanatical. In the immediate aftermath, the cult members that leave will be the ones who were previously the voice of opposition, skepticism, and moderation. Without those members, the cult will slide further in the direction of fanaticism.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8gu/seq_rerun_fake_utility_functions/\">Fake Utility Functions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gfzYfKY7Mweoy3hEy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.01264150466663e-07, "legacy": true, "legacyId": "10994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZQG9cwKbct2LtmL3p", "PkuEBzJWA9LgKZPhd", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T07:00:04.417Z", "modifiedAt": null, "url": null, "title": "Draft of Muehlhauser & Helm, 'The Singularity and Machine Ethics'", "slug": "draft-of-muehlhauser-and-helm-the-singularity-and-machine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:29.227Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rPLvEQyTx6HnPijtC/draft-of-muehlhauser-and-helm-the-singularity-and-machine", "pageUrlRelative": "/posts/rPLvEQyTx6HnPijtC/draft-of-muehlhauser-and-helm-the-singularity-and-machine", "linkUrl": "https://www.lesswrong.com/posts/rPLvEQyTx6HnPijtC/draft-of-muehlhauser-and-helm-the-singularity-and-machine", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Draft%20of%20Muehlhauser%20%26%20Helm%2C%20'The%20Singularity%20and%20Machine%20Ethics'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADraft%20of%20Muehlhauser%20%26%20Helm%2C%20'The%20Singularity%20and%20Machine%20Ethics'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrPLvEQyTx6HnPijtC%2Fdraft-of-muehlhauser-and-helm-the-singularity-and-machine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Draft%20of%20Muehlhauser%20%26%20Helm%2C%20'The%20Singularity%20and%20Machine%20Ethics'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrPLvEQyTx6HnPijtC%2Fdraft-of-muehlhauser-and-helm-the-singularity-and-machine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrPLvEQyTx6HnPijtC%2Fdraft-of-muehlhauser-and-helm-the-singularity-and-machine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<p><a href=\"/user/Louie\">Louie</a> and I are sharing a draft of our chapter submission to <em><a href=\"http://singularityhypothesis.blogspot.com/p/about-singularity-hypothesis.html\">The Singularity Hypothesis</a></em>&nbsp;for feedback:</p>\n<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a></p>\n<p>Thanks in advance.</p>\n<p>Also, thanks to Kevin for <a href=\"/lw/49c/book_draft_ethics_and_superintelligence_part_1/3jkg\">suggesting in February</a>&nbsp;that I submit an abstract to the editors. Seems like a lifetime ago, now.</p>\n<p><strong>Edit: </strong>As of 3/31/2012, the link above now points to a preprint.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rPLvEQyTx6HnPijtC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 8.013008874707751e-07, "legacy": true, "legacyId": "10996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T07:37:59.513Z", "modifiedAt": null, "url": null, "title": "Might I ask for some advice? ", "slug": "might-i-ask-for-some-advice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AmagicalFishy", "createdAt": "2011-06-17T13:22:31.254Z", "isAdmin": false, "displayName": "AmagicalFishy"}, "userId": "77u6aqgcFfDiDNHMS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hj2yFSK7AMKrAp52k/might-i-ask-for-some-advice", "pageUrlRelative": "/posts/hj2yFSK7AMKrAp52k/might-i-ask-for-some-advice", "linkUrl": "https://www.lesswrong.com/posts/hj2yFSK7AMKrAp52k/might-i-ask-for-some-advice", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Might%20I%20ask%20for%20some%20advice%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMight%20I%20ask%20for%20some%20advice%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhj2yFSK7AMKrAp52k%2Fmight-i-ask-for-some-advice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Might%20I%20ask%20for%20some%20advice%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhj2yFSK7AMKrAp52k%2Fmight-i-ask-for-some-advice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fhj2yFSK7AMKrAp52k%2Fmight-i-ask-for-some-advice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1281, "htmlBody": "<p>If you're expecting anything but a long post by an LW lurking college student asking sincerely for some advice, you should read <a title=\"The Curse of Identity\" href=\"/lw/8gv/the_curse_of_identity/\" target=\"_blank\">The Curse of Identity</a>,&nbsp;the article that spurred this very post. It's a good read, regardless of my advice-seeking status. With that said: Hello. I'm an LW lurking college student in need of advice, and this is my long post asking for it. I hope this isn't inappropriate.</p>\n<p>Mainly, this comes down to my hardly having a satisfying direction in life. I'm ignorant as to the reasons behind my lack of some fully functional inner compass. Is it that I&nbsp;just haven't found my passion&mdash;my niche in life? Or am I just lazy? Are the goals I want to achieve products of genuine interest, or are they methods of preserving a reputation which I (admittedly) very much enjoy having? Is my discouragement something I must use instrumental rationality to overcome, a sign that I'm fooling myself; one I should listen to and change something, or just a natural feeling when a particular situation is difficult? Is my not having direction a reasonable, youth-related status (is 22 <em>that</em> young?), or a sign that I've been doing something horribly wrong?<a id=\"more\"></a></p>\n<p>I've always enjoyed, and associated myself with writing and literature&mdash;but not to the extent that I feel the need to pursue a formal academic degree for them. When I started school, I majored in Philosophy because I like different philosophies, philosophizing, and the philosophers who did so before me. Then I dropped out for a couple of years and lived half-way across the U.S. with (at the time) my girlfriend. Currently, I'm back in school majoring in Physics and Mathematics (I've always wanted to study sub-atomic particles&mdash;I also enjoy mathematics, mathematical thinking, etc., and the two compliment&nbsp;one another&nbsp;well).</p>\n<div>So now I'm in school for Physics and Mathematics, and, while I enjoy the subjects, when I'm actually <strong>doing</strong> things related to the fields, I'm consistently discouraged. I feel like I'm not learning anything, trucking through problems without an actual understanding of the material, and by the time I finish what needs to be done for one class, I have a load of work for another, leaving me little time to, say, work through actual proofs of <em>why</em> something works (or, more likely, my time-management skills are atrocious. This combined with a large work-load leaves me little time). My problem comes ultimately from my trying to figure out <em>what </em>other problems are. For example:</div>\n<div>I see homework as that which actually teaches me to use the concepts I learn in Physics or Calculus.&nbsp;The more I wade through the many online interfaces through which I do my homework, the less I like what I'm doing. If I like Mathematics, I should similarly like the ideas&nbsp;<em>in</em>&nbsp;Mathematics. If I'm more and more disliking doing the homework&mdash;that is, actually learning how to use the concepts&mdash;then do I really like Mathematics, or do I just like the idea of it?</div>\n<div>I've come to the conclusion that the online interfaces are what make working on a subject unenjoyable&mdash;I blame them on my growing discontent with whatever I'm doing. (MyMathLab, WebWork, WebAssign, MasteringPhysics, etc. are targets of wide-spread hatred&nbsp;among&nbsp;the student bodies of all 3 schools I've been to).&nbsp;<em>But I realized something:</em>&nbsp;When I genuinely understand the work, and when I'm able to do it well&mdash;I don't mind these interfaces in the slightest. Perhaps it's the lack of immediate feedback on&nbsp;<em>what</em>&nbsp;(out of the many potential things) I've done incorrect that annoys me. Maybe I just feel better doing things from a textbook. My lack of understanding in something is definitely correlated to how much, on a face-value level, I like something. It can't just be the online interfaces themselves.</div>\n<div>After some thought, I lose any grasp of what the problem actually is. I truly enjoy ideas in Physics, but do I want to&nbsp;<em>do</em>&nbsp;Physics? I&nbsp;<em>think&nbsp;</em>so.&nbsp;Is it just the way it's taught and the way I'm learning it that brings about my discouragement, which brings about these questions? Should I, instead of thinking of a different major, try to learn differently? Does&nbsp;<em>every</em>&nbsp;potential&nbsp;physicist&nbsp;ask themselves these questions? When I tell someone my major is Physics, they give me the \"Wow-I could-never-do-that-you've-got-something-wrong-with-you\" expression. I like that reaction. How much does that play into my thinking I want to major in Physics? It feels great when I manage time in such a way that I can sit down and actually understand particular concepts&mdash;does this mean I really <em>should </em>be doing physics because I genuinely like it? Should I suck it up, stop analyzing, and grind through&nbsp;<em>everything</em>&nbsp;until I'm a master at it? (Eat your heart out, MasteringPhysics) Etc., etc., etc.</div>\n<div>I think the reasonable answer to this line of questioning would be, \"If you have so many damned questions about this, you should change majors!\"<br /></div>\n<div><strong>The Actual Problem</strong></div>\n<div>The problem is that seemingly&nbsp;<strong>every&nbsp;</strong>endeavor ends in this way, regardless of academic major or goal: This sticky entanglement of questions and different approaches in the face of opposing force concludes in my having no idea where one problem starts and another one stops. My motivation to do anything fizzles in the obscurity that my (apparently inefficient)&nbsp;analytic&nbsp;mind becomes when spread over such a huge range of inquiries.</div>\n<p>And the only constant through all of this is that I'd rather sit down, shut off my brain, and play video games until my eyes redden and I can crawl into bed and sleep instantly. But whenever I have those days, I feel like I've wasted huge amounts of time when I could otherwise be doing something productive.&nbsp;<br /><br />In fact, I used to say I'd have a profession in the video game industry when I was younger; computers and video games have always been a huge part of my life, and I love programming (though I've never taken any formal classes, so I'm by no means an expert. My referring to it generally as \"programming\" is probably indicative of my being a novice). I considered whether or not I should change my major to something computer related&mdash;but, since this is what usually happens, I'm not sure whether or not it'd be a worthwhile thing to do. Will my love for programming fade away as I'm introduced to more&nbsp;rigorous&nbsp;methods and subject to various&nbsp;assignments&nbsp;and deadlines? Do I just dislike structure? Need I force myself into enjoying a more structured environment? Is this even a question of whether or not I&nbsp;<em>enjoy</em>&nbsp;a particular field?<br /><br />Oh, no. No, no, no . . . am I . . . am I a&nbsp;<em>free spirit!?</em></p>\n<p><strong>Potential Solution</strong></p>\n<p>Just writing this post has helped me in organizing my thoughts, and I'm considering this: Take all of my questions (they've got to be fininte) and provide counter-examples that would help me answer them. So, \"Is it just the way it's taught and the way I'm learning it that brings about my discouragement, which brings about these questions?\" Might be counter-questioned by my asking myself, \"Is there something enjoyable I can think of that, regardless of how it's taught or how I'm learning it, I'd still thoroughly enjoy?\"</p>\n<p>What is the better way of going about this? Where am I being particularly irrational or biased? How would you folks go about solving this?</p>\n<p>Apologies if this post is convoluted or confusing.</p>\n<p><strong>Edit:</strong>&nbsp;Thanks for all your responses, guys. They've been&nbsp;immensely&nbsp;helpful. The main points I've gathered are:</p>\n<p>a) There's nothing particularly irrational or unreasonably biased in what my position is&mdash;it's normal.&nbsp;<br />b) Passion and enjoyment aren't necessities for a satisfying, fulfilling direction (though they help). If anything, the idea of working for your \"passion\" is a kind of feel-good idiom.</p>\n<p>There've also been posted some excellent articles:<br /><a href=\"http://calnewport.com/blog/2011/07/15/how-to-cure-deep-procrastination/\">Curing Deep Procrastination</a><br /><a href=\"http://calnewport.com/blog/2010/01/23/beyond-passion-the-science-of-loving-what-you-do/\">The Science of Loving What You Do</a><br /><a href=\"/lw/6nz/approving_reinforces_loweffort_behaviors/\">Approval and Low-Effort Behaviors</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hj2yFSK7AMKrAp52k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 8.013143477628271e-07, "legacy": true, "legacyId": "10993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tAXrD8Y6hcJ8dt6Nt", "yDRX2fdkm3HqfTpav"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T09:58:27.327Z", "modifiedAt": null, "url": null, "title": "OPERA Confirms: Neutrinos Travel Faster Than Light", "slug": "opera-confirms-neutrinos-travel-faster-than-light", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MPq2heEvah2eA68Aw/opera-confirms-neutrinos-travel-faster-than-light", "pageUrlRelative": "/posts/MPq2heEvah2eA68Aw/opera-confirms-neutrinos-travel-faster-than-light", "linkUrl": "https://www.lesswrong.com/posts/MPq2heEvah2eA68Aw/opera-confirms-neutrinos-travel-faster-than-light", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20OPERA%20Confirms%3A%20Neutrinos%20Travel%20Faster%20Than%20Light&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOPERA%20Confirms%3A%20Neutrinos%20Travel%20Faster%20Than%20Light%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMPq2heEvah2eA68Aw%2Fopera-confirms-neutrinos-travel-faster-than-light%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=OPERA%20Confirms%3A%20Neutrinos%20Travel%20Faster%20Than%20Light%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMPq2heEvah2eA68Aw%2Fopera-confirms-neutrinos-travel-faster-than-light", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMPq2heEvah2eA68Aw%2Fopera-confirms-neutrinos-travel-faster-than-light", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<blockquote>\n<p><a href=\"http://news.sciencemag.org/scienceinsider/2011/11/faster-than-light-neutrinos-opera.html\" target=\"blank\">New high-precision tests carried out by the OPERA collaboration in Italy broadly confirm its claim, made in September,</a> to have detected neutrinos travelling at faster than the speed of light. The collaboration today submitted its results to a journal, but some members continue to insist that further checks are needed before the result can be considered sound.</p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://nextbigfuture.com/2011/11/faster-than-light-neutrinos-opera.html\">nextbigfuture.com/2011/11/faster-than-light-neutrinos-opera.html</a></p>\n<blockquote>\n<p>The OPERA Collaboration sent to the Cornell Arxiv an updated version of their preprint today, where they summarize the results of their analysis, expanded with additional statistical tests, and including the check performed with 20 additional neutrino interactions they collected in the last few weeks. These few extra timing measurements crucially allow the ruling out of some potential unaccounted sources of systematic uncertainty, notably ones connected to the knowledge of the proton spill time distribution.</p>\n<p>[...]</p>\n<p><strong>So what does OPERA find ?</strong> Their main result, based on the 15,233 neutrino interactions collected in three years of data taking, is unchanged from the September result. The most interesting part of the new publication is instead that the&nbsp; find that the 20 new neutrino events (where neutrino speeds are individually measured, as opposed to the combined measurement done with the three-year data published in September) <strong>confirm the earlier result: the arrival times appear to occur about 60 nanoseconds before they are expected. </strong></p>\n</blockquote>\n<p><strong>Link:</strong> <a href=\"http://www.science20.com/quantum_diaries_survivor/opera_confirms_neutrinos_travel_faster_light-84763\">science20.com/quantum_diaries_survivor/opera_confirms_neutrinos_travel_faster_light-84763</a></p>\n<p><strong>Paper:</strong> <a href=\"http://kruel.co/paper-neutrino-velocity-JHEP.pdf\">kruel.co/paper-neutrino-velocity-JHEP.pdf</a></p>\n<p><strong>Previously on LW:</strong> <a href=\"/lw/7rc/particles_break_lightspeed_limit/\">lesswrong.com/lw/7rc/particles_break_lightspeed_limit/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MPq2heEvah2eA68Aw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 12, "extendedScore": null, "score": 8.01364213253296e-07, "legacy": true, "legacyId": "10997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["znpwJw66QCSMQkk66"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T13:59:40.418Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta Less Wrong Meetup", "slug": "meetup-atlanta-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4fAfviJJDXBBaSBMh/meetup-atlanta-less-wrong-meetup", "pageUrlRelative": "/posts/4fAfviJJDXBBaSBMh/meetup-atlanta-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/4fAfviJJDXBBaSBMh/meetup-atlanta-less-wrong-meetup", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAfviJJDXBBaSBMh%2Fmeetup-atlanta-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAfviJJDXBBaSBMh%2Fmeetup-atlanta-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4fAfviJJDXBBaSBMh%2Fmeetup-atlanta-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/50'>Atlanta Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2011 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">537 West Howard Avenue, Decatur, GA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first Atlanta meetup will be this Saturday, November 19th at 7pm at Thinking Man Tavern in Decatur. There is no RSVP, so feel free to come by if you are interested.</p>\n\n<p>The tentative aims for the meetup are as follows:</p>\n\n<ol>\n<li>Introduce ourselves </li>\n<li>Discuss Less Wrong, rationality, etc </li>\n<li>Brainstorm some ideas on how to expand our group </li>\n<li>Anything else cool that comes up </li>\n</ol>\n\n<p>Also, do sign up for our mailing list:\n<a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p>\n\n<p>And feel free to send me a message if you have any questions/suggestions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/50'>Atlanta Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4fAfviJJDXBBaSBMh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.014498601191245e-07, "legacy": true, "legacyId": "10998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/50\">Atlanta Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2011 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">537 West Howard Avenue, Decatur, GA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first Atlanta meetup will be this Saturday, November 19th at 7pm at Thinking Man Tavern in Decatur. There is no RSVP, so feel free to come by if you are interested.</p>\n\n<p>The tentative aims for the meetup are as follows:</p>\n\n<ol>\n<li>Introduce ourselves </li>\n<li>Discuss Less Wrong, rationality, etc </li>\n<li>Brainstorm some ideas on how to expand our group </li>\n<li>Anything else cool that comes up </li>\n</ol>\n\n<p>Also, do sign up for our mailing list:\n<a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p>\n\n<p>And feel free to send me a message if you have any questions/suggestions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/50\">Atlanta Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta_Less_Wrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Atlanta Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Atlanta_Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T14:06:07.730Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Zurich, Paris, Seattle, Waterloo", "slug": "weekly-lw-meetups-zurich-paris-seattle-waterloo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:25.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uAT3wPrhTwNvN5Ajm/weekly-lw-meetups-zurich-paris-seattle-waterloo", "pageUrlRelative": "/posts/uAT3wPrhTwNvN5Ajm/weekly-lw-meetups-zurich-paris-seattle-waterloo", "linkUrl": "https://www.lesswrong.com/posts/uAT3wPrhTwNvN5Ajm/weekly-lw-meetups-zurich-paris-seattle-waterloo", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Zurich%2C%20Paris%2C%20Seattle%2C%20Waterloo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Zurich%2C%20Paris%2C%20Seattle%2C%20Waterloo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAT3wPrhTwNvN5Ajm%2Fweekly-lw-meetups-zurich-paris-seattle-waterloo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Zurich%2C%20Paris%2C%20Seattle%2C%20Waterloo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAT3wPrhTwNvN5Ajm%2Fweekly-lw-meetups-zurich-paris-seattle-waterloo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuAT3wPrhTwNvN5Ajm%2Fweekly-lw-meetups-zurich-paris-seattle-waterloo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/4h\">First Zurich LW Meetup:&nbsp;<span class=\"date\">12 November 2011 03:00PM</span></a></li>\n<li><a href=\"/meetups/4l\">Paris, Sunday November 13:&nbsp;<span class=\"date\">13 November 2011 02:00PM</span></a></li>\n<li><a href=\"/meetups/4q\">Seattle, The Planning Fallacy:&nbsp;<span class=\"date\">13 November 2011 04:00PM</span></a></li>\n<li><a href=\"/meetups/4p\">First(New?) Waterloo Meetup:&nbsp;<span class=\"date\">24 November 2011 08:00PM</span></a></li>\n</ul>\n<p><strong>Notice for the Mountain View/Tortuga meetup: </strong>The Mountain View meetup has moved to *Mondays* (still at 7pm, still at Tortuga); and over the next few weeks will present pieces of the \"Detaching from Sunk Costs\" rationality kata currently under development.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong>&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uAT3wPrhTwNvN5Ajm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.014521522704707e-07, "legacy": true, "legacyId": "10887", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T16:34:00.494Z", "modifiedAt": null, "url": null, "title": "Comprehensive List of All Singularity Summit Talks and Video Links", "slug": "comprehensive-list-of-all-singularity-summit-talks-and-video", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:29.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GCz9FcMd2khoqiq39/comprehensive-list-of-all-singularity-summit-talks-and-video", "pageUrlRelative": "/posts/GCz9FcMd2khoqiq39/comprehensive-list-of-all-singularity-summit-talks-and-video", "linkUrl": "https://www.lesswrong.com/posts/GCz9FcMd2khoqiq39/comprehensive-list-of-all-singularity-summit-talks-and-video", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Comprehensive%20List%20of%20All%20Singularity%20Summit%20Talks%20and%20Video%20Links&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComprehensive%20List%20of%20All%20Singularity%20Summit%20Talks%20and%20Video%20Links%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCz9FcMd2khoqiq39%2Fcomprehensive-list-of-all-singularity-summit-talks-and-video%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Comprehensive%20List%20of%20All%20Singularity%20Summit%20Talks%20and%20Video%20Links%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCz9FcMd2khoqiq39%2Fcomprehensive-list-of-all-singularity-summit-talks-and-video", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGCz9FcMd2khoqiq39%2Fcomprehensive-list-of-all-singularity-summit-talks-and-video", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1435, "htmlBody": "<p>Here is an index of all Singularity Summit speeches.&nbsp;</p>\n<p><br /> <strong>Summit 2006</strong></p>\n<ul>\n<li>Todd Davies, Tyler Emerson, and Peter Thiel. <a href=\"http://intelligence.org/media/introductiontothesingularitysummit\">Introduction to the Singularity Summit</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/thesingularity\">The Singularity: a hard or soft takeoff?</a></li>\n<li>Douglas Hofstadter. <a href=\"http://intelligence.org/media/tryingtomuserationally\">Trying to muse rationally about the Singularity scenario</a>. </li>\n<li>Nick Bostrom. <a href=\"http://intelligence.org/media/artificialintelligenceandexistentialrisks\">Artificial Intelligence and existential risks</a>. </li>\n<li>Sebastian Thrun. <a href=\"http://intelligence.org/media/towardhumanlevelintelligence\">Toward human-level intelligence in autonomous cars</a>. </li>\n<li>Cory Doctorow. <a href=\"http://intelligence.org/media/singularityordarkage\">Singularity or Dark Age?</a>. </li>\n<li>K. Eric Drexler. <a href=\"http://intelligence.org/media/productivenanosystems\">Productive Nanosystems: Toward a Super-Exponential Threshold in Physical Technology</a>. </li>\n<li>Max More. <a href=\"http://intelligence.org/media/cognitiveandemotionalsingularities\">Cognitive and Emotional Singularities: Will Superintelligence come with Superwisdom?</a> </li>\n<li>Christine Peterson. <a href=\"http://intelligence.org/media/bringinghumanity\">Bringing Humanity and the Biosphere through the Singularity</a></li>\n<li>John Smart. <a href=\"http://intelligence.org/media/searchingforthebigpicture\">Searching for the Big Picture: Systems Theories of Accelerating Change </a></li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/thehumanimportanceoftheintelligenceexplosion\">The Human Importance of the Intelligence Explosion</a>.</li>\n<li>Bill McKibben. <a href=\"http://intelligence.org/media/beinggoodenough\">Being good enough</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/followup\">Follow up</a>.</li>\n<li><a href=\"http://intelligence.org/media/paneldiscussion\">Panel Discussion and Q&amp;A</a>.</li>\n<li><a href=\"http://intelligence.org/media/paneldiscussion2\">Panel Discussion and Q&amp;A pt. 2</a>.</li>\n</ul>\n<p><br /> <strong>Summit 2007</strong></p>\n<ul>\n<li>Tyler Emerson &amp; Peter Thiel. <a href=\"http://intelligence.org/node/341\">Welcome and introduction</a>.</li>\n<li>Rodney Brooks. <a href=\"http://intelligence.org/media/singularity/rodneybrooks\">The Singularity: A Period Not An Event</a>.</li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/eliezeryudkowsky1\">Introducing the \"Singularity\": Three Major Schools of Thought</a>.</li>\n<li>Barney Pell. <a href=\"http://intelligence.org/media/singularitysummit2007/barneypell\">Pathways to Advanced General Intelligence: Architecture, Development, and Funding</a>.</li>\n<li>Wendell Wallach. <a href=\"http://intelligence.org/media/singularitysummit2007/wendellwallach%27%3EThe%20Road%20to%20Singularity:%20Comedic%20Complexity,%20Technological%20Thresholds,%20and%20Bioethical%20Broad%20Jumps%3C/a%3E.%3C/li%3E%0A%3Cli%3ESam%20Adams.%20%3Ca%20href=\">Superstition and Forgetfulness - Two Essentials for Artificial General Intelligence</a>.</li>\n<li>Barney Pell, Wendell Wallach, Sam Adams. <a href=\"http://intelligence.org/media/singularitysummit2007/firstpanel\">First panel discussion</a>.</li>\n<li>Jamais Cascio. <a href=\"http://intelligence.org/media/singularitysummit2007/jamaiscascio\">Metaverse Singularity</a>.</li>\n<li>Stephen M. Omohundro. <a href=\"http://intelligence.org/media/singularitysummit2007/stephenomohundro\">The Nature of Self-Improving Artificial Intelligence</a>.</li>\n<li>Peter Voss. <a href=\"http://intelligence.org/media/singularitysummit2007/petervoss\">Improved intelligence, improved life</a>.</li>\n<li>Stephen M. Omohundro, Peter Voss. <a href=\"http://intelligence.org/media/singularitysummit2007/secondpanel\">Second panel discussion</a>.</li>\n<li>Neil Jacobstein. <a href=\"http://intelligence.org/media/singularitysummit2007/neiljacobstein\">Innovative Applications of Early Stage AI</a>.</li>\n<li>Ben Goertzel. <a href=\"http://intelligence.org/media/singularitysummit2007/bengoertzel\">Nine Years to a Positive Singularity - If We Really, Really Try</a>.</li>\n<li>Paul Saffo. <a href=\"http://intelligence.org/media/singularitysummit2007/paulsaffo\">Machines of Loving Grace: Anticipating Advanced AI</a>.</li>\n<li>Neil Jacobstein, Ben Goertzel, Paul Saffo. <a href=\"http://intelligence.org/media/singularitysummit2007/thirdpanel\">Third panel discussion</a>.</li>\n<li>Peter Norvig. <a href=\"http://intelligence.org/media/singularitysummit2007/peternorvig\">The history and future of technological change</a>.</li>\n<li>J. Storrs Hall. <a href=\"http://intelligence.org/media/singularitysummit2007/jstorrshall\">Asimov's laws of robotics -- revised</a>.</li>\n<li>Peter Thiel. <a href=\"http://intelligence.org/media/singularitysummit2007/peterthiel\">Financial markets and the Singularity</a>.</li>\n<li>Charles L. Harper, Jr. <a href=\"http://intelligence.org/media/singularitysummit2007/charlesharper\">Superintelligence, the \"Dilemma of Power,\" and the transformation of desire</a>.</li>\n<li>J. Storrs Hall, Peter Thiel, Charles L. Harper, Jr. <a href=\"http://intelligence.org/media/singularitysummit2007/charlesharper\">Third panel discussion</a>.</li>\n<li>Steve Jurvetson. <a href=\"http://intelligence.org/media/singularitysummit2007/stevejurvetson\">Dichotomy of designed and evolutionary paths to AI futures</a>.</li>\n<li>Christine L. Peterson. <a href=\"http://intelligence.org/media/singularitysummit2007/christinepeterson\">Preparing for bizarreness: open source physical security</a>.</li>\n<li>James Hughes. <a href=\"http://intelligence.org/media/singularitysummit2007/jameshughes\">Waiting for the Great Leap...Forward?</a></li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/eliezeryudkowsky2\">The Challenge of Friendly AI</a>.</li>\n<li>Christine L. Peterson, James Hughes, Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/fifthpanel%27%3EFifth%20panel%20discussion%3C/a%3E.%3C/li%3E%0A%3Cli%3ERay%20Kurzweil.%20%3Ca%20href=\">A dialogue with Ray Kurzweil</a>.</li>\n</ul>\n<p><strong>Summit 2008</strong></p>\n<ul>\n<li>Vernor Vinge and Bob Pisani. <a href=\"http://intelligence.org/media/singularitysummit2008/vernorvinge-bobpisani\">Conversation on the Singularity</a>. </li>\n<li>Esther Dyson. <a href=\"http://intelligence.org/media/singularitysummit2008/estherdyson\">23andme and personal genomics</a>. </li>\n<li>James Miller. <a href=\"http://intelligence.org/media/singularitysummit2008/jamesmiller\">Societal reactions to the Singularity</a>. </li>\n<li>Eric Baum. <a href=\"http://intelligence.org/media/singularitysummit2008/ericbaum\">AI and the problem of understanding</a>. </li>\n<li>Dharmendra Modha. <a href=\"http://intelligence.org/media/singularitysummit2008/dharmendramodha\">IBM's research into Whole Brain Emulation</a>. </li>\n<li>Ben Goertzel. <a href=\"http://intelligence.org/media/singularitysummit2008/bengoertzel\">OpenCog -- an open source AGI project</a>. </li>\n<li>Marshall Brain. <a href=\"http://intelligence.org/media/singularitysummit2008/marshallbrain\">Robotics and structural unemployment</a>. </li>\n<li>Cynthia Breazeal. <a href=\"http://intelligence.org/media/singularitysummit2008/cynthiabreazeal\">Social robots</a>. </li>\n<li>Ray Kurzweil, Glen Zorpette and John Horgan. <a href=\"http://intelligence.org/media/singularitysummit2008/raykurzweiljohnhorgan\">Debate on the Singularity</a>. </li>\n<li>Pete Estep. <a href=\"http://intelligence.org/media/singularitysummit2008/peteestep\">The InnerSpace Foundation</a>. </li>\n<li>Neil Gershenfeld. <a href=\"http://intelligence.org/media/singularitysummit2008/neilgershenfeld\">Alternate models of computing</a>. </li>\n<li>Peter Diamandis. <a href=\"http://intelligence.org/media/singularitysummit2008/peterdiamandis\">History of the X Prize Foundation and future X Prizes</a>. </li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/singularitysummit2008/raykurzweil\">Exponential progress in information technologies</a>. </li>\n<li>Justin Rattner. <a href=\"http://intelligence.org/media/singularitysummit2008/justinrattner\">Intel and the continuous of Moore's law</a>. </li>\n<li>Nova Spivack. <a href=\"http://intelligence.org/media/singularitysummit2008/novaspivack\">Collective intelligence and the emerging global brain</a>. </li>\n</ul>\n<p><strong>Summit 2009</strong></p>\n<ul>\n<li>Michael Vassar. <a href=\"http://vimeo.com/7317738\">Introduction</a>.</li>\n<li>Anna Salamon. <a href=\"http://vimeo.com/7318055\">Shaping the intelligence explosion</a>.</li>\n<li>Anders Sandberg. <a href=\"http://vimeo.com/7318429\">Technical roadmap for whole brain emulation</a>.</li>\n<li>Randal Koene. <a href=\"http://vimeo.com/7318751\">The time is now: as a species we need whole brain emulation</a>.</li>\n<li>Itamar Arel. <a href=\"http://vimeo.com/7318781\">Technological convergence leading to artificial general intelligence</a>.</li>\n<li>Ben Goertzel. <a href=\"http://vimeo.com/7320152\">Pathways to beneficial artificial general intelligence</a>.</li>\n<li>Stuart Hameroff. <a href=\"http://vimeo.com/7320518\">Neural substrates of consciousness and the 'conscious pilot' model</a>.</li>\n<li>David Chalmers. <a href=\"http://vimeo.com/7320820\">Simulation and the singularity</a>.</li>\n<li>Gary Drescher. <a href=\"http://vimeo.com/7321259\">Choice machines, causality, and cooperation</a>.</li>\n<li>Ed Boyden. <a href=\"http://vimeo.com/7321578\">Synthetic neurobiology: optically engineering the brain to augment its function</a>.</li>\n<li>Marcus Hutter. <a href=\"http://vimeo.com/7321732\">Foundations of intelligent agents</a>.</li>\n<li>William Dickens. <a href=\"http://vimeo.com/7322293\">Cognitive ability: past and future enhancements and implications</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/7322310\">The ubiquity and predictability of the exponential growth of information technology</a>.</li>\n<li>Bela Nagy. <a href=\"http://vimeo.com/7335497\">More than Moore: comparing forecasts of technological progress</a>.</li>\n<li>Robin Hanson. <a href=\"http://vimeo.com/7336217\">How does society identify experts, and when does it work?</a></li>\n<li>Panel: <a href=\"http://vimeo.com/7336384\">Future of scientific method</a>.</li>\n<li>Gregory Benford. <a href=\"http://vimeo.com/7336479\">Artificial biological selection for longevity</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/7337535\">Critics of the singularity</a>.</li>\n<li>Brad Templeton. <a href=\"http://vimeo.com/7337628\">The finger of AI: Automated electrical vehicles and oil independence</a>.</li>\n<li>Gary Marcus. <a href=\"http://vimeo.com/7338930\">The fallibility and improvability of the human mind</a>.</li>\n<li>Peter Thiel. <a href=\"http://vimeo.com/7339317\">Macroeconomics and singularity</a>.</li>\n<li>Aubrey de Grey. <a href=\"http://vimeo.com/7339349\">The Singularity and the Methuselarity: similarities and differences</a>.</li>\n<li>Thiel, Yudkowsky &amp; de Grey panel: <a href=\"http://vimeo.com/7396024\">Changing the world</a>.</li>\n<li>Anna Salamon. <a href=\"http://vimeo.com/7397629\">How much it matters to know what matters: A back of the envelope calculation</a>.</li>\n<li>Gary Wolf. <a href=\"http://vimeo.com/7425764\">The petaflop macroscope</a>.</li>\n<li>Eliezer Yudkowsky. <a href=\"http://vimeo.com/7426357\">Cognitive biases and giant risks</a>.</li>\n<li>Jurgen Schmidhuber. <a href=\"http://vimeo.com/7441291\">Compression progress: The algorithmic principle behind curiosity and creativity</a>.</li>\n<li>Thiel, Rose &amp; Gorenberg Panel: <a href=\"http://vimeo.com/7443559\">Venture capitalism</a>.</li>\n<li>Michael Nielsen. <a href=\"http://vimeo.com/7447694\">Quantum computing: What it is, what it is not, what we have yet to learn</a>.</li>\n<li>Michael Nielsen. <a href=\"http://vimeo.com/7464017\">Collaborative networks in scientific discovery</a>.</li>\n<li>Stephen Wolfram. <a href=\"http://vimeo.com/7466113\">Conversation on the singularity</a>.</li>\n</ul>\n<p><br /> <strong>Summit 2010</strong></p>\n<ul>\n<li>Michael Vassar. <a href=\"http://vimeo.com/17512853\">The Darwinian method</a>.</li>\n<li>Gregory Stock. <a href=\"http://vimeo.com/18141173\">Evolution of post-human intelligence</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/18488795\">The mind and how to build one</a>.</li>\n<li>Ben Goertzel. <a href=\"http://vimeo.com/18143134\">AI against aging</a>.</li>\n<li>Steven Mann. <a href=\"http://vimeo.com/19362162\">Humanistic intelligence augmentation and mediation</a>.</li>\n<li>Mandayam Srinivasan. <a href=\"http://vimeo.com/18822577\">Enhancing our bodies and evolving our brains</a>.</li>\n<li>Brian Litt. <a href=\"http://vimeo.com/19358262\">The past, present and future of brain machine interfaces</a>.</li>\n<li>Demis Hassabis. <a href=\"http://vimeo.com/17513841\">Combining systems neuroscience and machine learning: a new approach to AGI</a>.</li>\n<li>Terry Sejnowski. <a href=\"http://vimeo.com/18824185\">Reverse-engineering brains is within reach</a>.</li>\n<li>Dennis Bray. <a href=\"http://vimeo.com/18143991\">What cells can do that robots can't</a>.</li>\n<li>Terry Sejnowski/Dennis Bray debate: <a href=\"http://vimeo.com/17700122\">Will we soon realistically emulate biological systems?</a>.</li>\n<li>Ramez Naam. <a href=\"http://vimeo.com/18141726\">The digital biome</a>.</li>\n<li>Lance Becker. <a href=\"http://vimeo.com/19098618\">Modifying the boundary between life and death</a><a>.</a></li>\n<li>Ellen Heber-Katz. <a href=\"http://vimeo.com/19096437\">The MRL mouse - how it regenerates and how we might do the same</a>.</li>\n<li>Shane Legg. <a href=\"http://vimeo.com/17553536\">Universal measures of intelligence</a>.</li>\n<li>John Tooby. <a href=\"http://vimeo.com/19097207\">Can discovering the design principles governing natural intelligence unleash breakthroughs in AI?</a>.</li>\n<li>Tooby, Goertzel, Yudkowsky &amp; Legg panel. <a href=\"http://vimeo.com/17702682\">Narrow and General Intelligence</a>.</li>\n<li>David Hanson. <a href=\"http://vimeo.com/18826257\">David Hanson: Why Characters Are Key to Friendly A.I.</a>.</li>\n<li>Irene Pepperberg. <a href=\"http://vimeo.com/19360766\">Irene Pepperberg: Nonhuman Intelligence: Where we are and where we're headed</a>.</li>\n<li>James Randi. <a href=\"http://vimeo.com/18653890\">Is there such a thing as scientific consensus?</a></li>\n</ul>\n<p><strong>Summit 2011</strong></p>\n<ul>\n<li>Ray Kurzweil. <a href=\"http://www.youtube.com/watch?v=WPqjYrLhDnk\">From Eliza to Watson to passing the Turing Test</a>. </li>\n<li>Stephen Badylak. <a href=\"http://www.youtube.com/watch?v=kC1MivfpT9o\">Regenerative medicine: possibilities and potential</a>. </li>\n<li>Sonia Arrison. <a href=\"http://www.youtube.com/watch?v=D0iUs2_TThA\">100 Plus: how the coming age of longevity will change everything, from careers and relationships to family and faith</a>. </li>\n<li>Peter Thiel. <a href=\"http://youtu.be/ROrUea0gLlY\">Back to the future</a>. </li>\n<li>James McLurkin. <a href=\"http://www.youtube.com/watch?v=9Cqv6xYk-uM\">The future of robotics is swarms: why a thousand robots are better than one</a>. </li>\n<li>Michael Shermer. <a href=\"http://www.youtube.com/watch?v=ZJW78HoNnX0\">Social Singularity: transitioning from civilization 1.0 to 2.0</a>. </li>\n<li>Jason Silva. <a href=\"http://www.youtube.com/watch?v=BzOxui9uw0o\">The 'Undivided Mind' &mdash; science and imagination</a>. </li>\n<li>Stephen Wolfram. <a href=\"http://www.youtube.com/watch?v=Pgw_nVqSTLw\">Computation and the future of mankind</a>. </li>\n<li>Dmitry Itskov. <a href=\"http://www.youtube.com/watch?v=zEi3ZAYheT0\">Project 'Immortality 2045' -- Russian experience</a>. </li>\n<li>Christof Koch. <a href=\"http://www.youtube.com/watch?v=6i9kE3Ne7as\">The neurobiology and mathematics of consciousness</a>. </li>\n<li>Eliezer Yudkowsky. <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Open problems in friendly artificial intelligence</a>. </li>\n<li>Max Tegmark. <a href=\"http://www.youtube.com/watch?v=GctnYAYcMhI\">The future of life: a cosmic perspective</a>. </li>\n<li>Alexander Wissner-Gross. <a href=\"http://www.youtube.com/watch?v=BS5ZtvNECMI\">Planetary-scale intelligence</a>. </li>\n<li>Sharon Bertsch McGrayne. <a href=\"http://www.youtube.com/watch?v=oTltncUkckQ\">A History of Bayes theorem.</a></li>\n<li>David Brin.&nbsp;<a href=\"http://www.youtube.com/watch?v=ryoqtB6H5nw\">So you want to make gods. Now why would that bother anybody?</a> </li>\n<li>Tyler Cowen. <a href=\"http://www.youtube.com/watch?v=ed6gNSZRawY\">The Great Stagnation</a>. </li>\n<li>Tyler Cowen &amp; Michael Vassar. <a href=\"http://youtu.be/ed6gNSZRawY\">Debate on the Great Stagnation</a>. </li>\n<li>John Mauldin. <a href=\"http://www.youtube.com/watch?v=xB-xjZQy_4M\">The endgame meets The millennium wave &mdash; why the economic crisis will be history as we create the future</a>. </li>\n<li>Riley Crane. <a href=\"http://www.youtube.com/watch?v=7vkhqeRno9M\">Rethinking communication</a>. </li>\n<li>Dileep George and Scott Brown. <a href=\"http://www.youtube.com/watch?v=wof47INDvdg\">From planes to brains: building AI the Wright way</a>. </li>\n<li>Jaan Tallinn. <a href=\"http://www.youtube.com/watch?v=84G6An1Ff2E\">Balancing the trichotomy: individual vs. society vs. universe</a>. </li>\n<li>David Ferrucci. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">Watson AI perceptions</a>. </li>\n<li>Dan Cerutti. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">Commercializing Watson. </a></li>\n<li>Ken Jennings. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">The human brain in Jeopardy: computers that 'think'</a>.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GCz9FcMd2khoqiq39", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 34, "extendedScore": null, "score": 8.015046665705256e-07, "legacy": true, "legacyId": "11001", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Here is an index of all Singularity Summit speeches.&nbsp;</p>\n<p><br> <strong>Summit 2006</strong></p>\n<ul>\n<li>Todd Davies, Tyler Emerson, and Peter Thiel. <a href=\"http://intelligence.org/media/introductiontothesingularitysummit\">Introduction to the Singularity Summit</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/thesingularity\">The Singularity: a hard or soft takeoff?</a></li>\n<li>Douglas Hofstadter. <a href=\"http://intelligence.org/media/tryingtomuserationally\">Trying to muse rationally about the Singularity scenario</a>. </li>\n<li>Nick Bostrom. <a href=\"http://intelligence.org/media/artificialintelligenceandexistentialrisks\">Artificial Intelligence and existential risks</a>. </li>\n<li>Sebastian Thrun. <a href=\"http://intelligence.org/media/towardhumanlevelintelligence\">Toward human-level intelligence in autonomous cars</a>. </li>\n<li>Cory Doctorow. <a href=\"http://intelligence.org/media/singularityordarkage\">Singularity or Dark Age?</a>. </li>\n<li>K. Eric Drexler. <a href=\"http://intelligence.org/media/productivenanosystems\">Productive Nanosystems: Toward a Super-Exponential Threshold in Physical Technology</a>. </li>\n<li>Max More. <a href=\"http://intelligence.org/media/cognitiveandemotionalsingularities\">Cognitive and Emotional Singularities: Will Superintelligence come with Superwisdom?</a> </li>\n<li>Christine Peterson. <a href=\"http://intelligence.org/media/bringinghumanity\">Bringing Humanity and the Biosphere through the Singularity</a></li>\n<li>John Smart. <a href=\"http://intelligence.org/media/searchingforthebigpicture\">Searching for the Big Picture: Systems Theories of Accelerating Change </a></li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/thehumanimportanceoftheintelligenceexplosion\">The Human Importance of the Intelligence Explosion</a>.</li>\n<li>Bill McKibben. <a href=\"http://intelligence.org/media/beinggoodenough\">Being good enough</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/followup\">Follow up</a>.</li>\n<li><a href=\"http://intelligence.org/media/paneldiscussion\">Panel Discussion and Q&amp;A</a>.</li>\n<li><a href=\"http://intelligence.org/media/paneldiscussion2\">Panel Discussion and Q&amp;A pt. 2</a>.</li>\n</ul>\n<p><br> <strong>Summit 2007</strong></p>\n<ul>\n<li>Tyler Emerson &amp; Peter Thiel. <a href=\"http://intelligence.org/node/341\">Welcome and introduction</a>.</li>\n<li>Rodney Brooks. <a href=\"http://intelligence.org/media/singularity/rodneybrooks\">The Singularity: A Period Not An Event</a>.</li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/eliezeryudkowsky1\">Introducing the \"Singularity\": Three Major Schools of Thought</a>.</li>\n<li>Barney Pell. <a href=\"http://intelligence.org/media/singularitysummit2007/barneypell\">Pathways to Advanced General Intelligence: Architecture, Development, and Funding</a>.</li>\n<li>Wendell Wallach. <a href=\"http://intelligence.org/media/singularitysummit2007/wendellwallach%27%3EThe%20Road%20to%20Singularity:%20Comedic%20Complexity,%20Technological%20Thresholds,%20and%20Bioethical%20Broad%20Jumps%3C/a%3E.%3C/li%3E%0A%3Cli%3ESam%20Adams.%20%3Ca%20href=\">Superstition and Forgetfulness - Two Essentials for Artificial General Intelligence</a>.</li>\n<li>Barney Pell, Wendell Wallach, Sam Adams. <a href=\"http://intelligence.org/media/singularitysummit2007/firstpanel\">First panel discussion</a>.</li>\n<li>Jamais Cascio. <a href=\"http://intelligence.org/media/singularitysummit2007/jamaiscascio\">Metaverse Singularity</a>.</li>\n<li>Stephen M. Omohundro. <a href=\"http://intelligence.org/media/singularitysummit2007/stephenomohundro\">The Nature of Self-Improving Artificial Intelligence</a>.</li>\n<li>Peter Voss. <a href=\"http://intelligence.org/media/singularitysummit2007/petervoss\">Improved intelligence, improved life</a>.</li>\n<li>Stephen M. Omohundro, Peter Voss. <a href=\"http://intelligence.org/media/singularitysummit2007/secondpanel\">Second panel discussion</a>.</li>\n<li>Neil Jacobstein. <a href=\"http://intelligence.org/media/singularitysummit2007/neiljacobstein\">Innovative Applications of Early Stage AI</a>.</li>\n<li>Ben Goertzel. <a href=\"http://intelligence.org/media/singularitysummit2007/bengoertzel\">Nine Years to a Positive Singularity - If We Really, Really Try</a>.</li>\n<li>Paul Saffo. <a href=\"http://intelligence.org/media/singularitysummit2007/paulsaffo\">Machines of Loving Grace: Anticipating Advanced AI</a>.</li>\n<li>Neil Jacobstein, Ben Goertzel, Paul Saffo. <a href=\"http://intelligence.org/media/singularitysummit2007/thirdpanel\">Third panel discussion</a>.</li>\n<li>Peter Norvig. <a href=\"http://intelligence.org/media/singularitysummit2007/peternorvig\">The history and future of technological change</a>.</li>\n<li>J. Storrs Hall. <a href=\"http://intelligence.org/media/singularitysummit2007/jstorrshall\">Asimov's laws of robotics -- revised</a>.</li>\n<li>Peter Thiel. <a href=\"http://intelligence.org/media/singularitysummit2007/peterthiel\">Financial markets and the Singularity</a>.</li>\n<li>Charles L. Harper, Jr. <a href=\"http://intelligence.org/media/singularitysummit2007/charlesharper\">Superintelligence, the \"Dilemma of Power,\" and the transformation of desire</a>.</li>\n<li>J. Storrs Hall, Peter Thiel, Charles L. Harper, Jr. <a href=\"http://intelligence.org/media/singularitysummit2007/charlesharper\">Third panel discussion</a>.</li>\n<li>Steve Jurvetson. <a href=\"http://intelligence.org/media/singularitysummit2007/stevejurvetson\">Dichotomy of designed and evolutionary paths to AI futures</a>.</li>\n<li>Christine L. Peterson. <a href=\"http://intelligence.org/media/singularitysummit2007/christinepeterson\">Preparing for bizarreness: open source physical security</a>.</li>\n<li>James Hughes. <a href=\"http://intelligence.org/media/singularitysummit2007/jameshughes\">Waiting for the Great Leap...Forward?</a></li>\n<li>Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/eliezeryudkowsky2\">The Challenge of Friendly AI</a>.</li>\n<li>Christine L. Peterson, James Hughes, Eliezer Yudkowsky. <a href=\"http://intelligence.org/media/singularitysummit2007/fifthpanel%27%3EFifth%20panel%20discussion%3C/a%3E.%3C/li%3E%0A%3Cli%3ERay%20Kurzweil.%20%3Ca%20href=\">A dialogue with Ray Kurzweil</a>.</li>\n</ul>\n<p><strong id=\"Summit_2008\">Summit 2008</strong></p>\n<ul>\n<li>Vernor Vinge and Bob Pisani. <a href=\"http://intelligence.org/media/singularitysummit2008/vernorvinge-bobpisani\">Conversation on the Singularity</a>. </li>\n<li>Esther Dyson. <a href=\"http://intelligence.org/media/singularitysummit2008/estherdyson\">23andme and personal genomics</a>. </li>\n<li>James Miller. <a href=\"http://intelligence.org/media/singularitysummit2008/jamesmiller\">Societal reactions to the Singularity</a>. </li>\n<li>Eric Baum. <a href=\"http://intelligence.org/media/singularitysummit2008/ericbaum\">AI and the problem of understanding</a>. </li>\n<li>Dharmendra Modha. <a href=\"http://intelligence.org/media/singularitysummit2008/dharmendramodha\">IBM's research into Whole Brain Emulation</a>. </li>\n<li>Ben Goertzel. <a href=\"http://intelligence.org/media/singularitysummit2008/bengoertzel\">OpenCog -- an open source AGI project</a>. </li>\n<li>Marshall Brain. <a href=\"http://intelligence.org/media/singularitysummit2008/marshallbrain\">Robotics and structural unemployment</a>. </li>\n<li>Cynthia Breazeal. <a href=\"http://intelligence.org/media/singularitysummit2008/cynthiabreazeal\">Social robots</a>. </li>\n<li>Ray Kurzweil, Glen Zorpette and John Horgan. <a href=\"http://intelligence.org/media/singularitysummit2008/raykurzweiljohnhorgan\">Debate on the Singularity</a>. </li>\n<li>Pete Estep. <a href=\"http://intelligence.org/media/singularitysummit2008/peteestep\">The InnerSpace Foundation</a>. </li>\n<li>Neil Gershenfeld. <a href=\"http://intelligence.org/media/singularitysummit2008/neilgershenfeld\">Alternate models of computing</a>. </li>\n<li>Peter Diamandis. <a href=\"http://intelligence.org/media/singularitysummit2008/peterdiamandis\">History of the X Prize Foundation and future X Prizes</a>. </li>\n<li>Ray Kurzweil. <a href=\"http://intelligence.org/media/singularitysummit2008/raykurzweil\">Exponential progress in information technologies</a>. </li>\n<li>Justin Rattner. <a href=\"http://intelligence.org/media/singularitysummit2008/justinrattner\">Intel and the continuous of Moore's law</a>. </li>\n<li>Nova Spivack. <a href=\"http://intelligence.org/media/singularitysummit2008/novaspivack\">Collective intelligence and the emerging global brain</a>. </li>\n</ul>\n<p><strong id=\"Summit_2009\">Summit 2009</strong></p>\n<ul>\n<li>Michael Vassar. <a href=\"http://vimeo.com/7317738\">Introduction</a>.</li>\n<li>Anna Salamon. <a href=\"http://vimeo.com/7318055\">Shaping the intelligence explosion</a>.</li>\n<li>Anders Sandberg. <a href=\"http://vimeo.com/7318429\">Technical roadmap for whole brain emulation</a>.</li>\n<li>Randal Koene. <a href=\"http://vimeo.com/7318751\">The time is now: as a species we need whole brain emulation</a>.</li>\n<li>Itamar Arel. <a href=\"http://vimeo.com/7318781\">Technological convergence leading to artificial general intelligence</a>.</li>\n<li>Ben Goertzel. <a href=\"http://vimeo.com/7320152\">Pathways to beneficial artificial general intelligence</a>.</li>\n<li>Stuart Hameroff. <a href=\"http://vimeo.com/7320518\">Neural substrates of consciousness and the 'conscious pilot' model</a>.</li>\n<li>David Chalmers. <a href=\"http://vimeo.com/7320820\">Simulation and the singularity</a>.</li>\n<li>Gary Drescher. <a href=\"http://vimeo.com/7321259\">Choice machines, causality, and cooperation</a>.</li>\n<li>Ed Boyden. <a href=\"http://vimeo.com/7321578\">Synthetic neurobiology: optically engineering the brain to augment its function</a>.</li>\n<li>Marcus Hutter. <a href=\"http://vimeo.com/7321732\">Foundations of intelligent agents</a>.</li>\n<li>William Dickens. <a href=\"http://vimeo.com/7322293\">Cognitive ability: past and future enhancements and implications</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/7322310\">The ubiquity and predictability of the exponential growth of information technology</a>.</li>\n<li>Bela Nagy. <a href=\"http://vimeo.com/7335497\">More than Moore: comparing forecasts of technological progress</a>.</li>\n<li>Robin Hanson. <a href=\"http://vimeo.com/7336217\">How does society identify experts, and when does it work?</a></li>\n<li>Panel: <a href=\"http://vimeo.com/7336384\">Future of scientific method</a>.</li>\n<li>Gregory Benford. <a href=\"http://vimeo.com/7336479\">Artificial biological selection for longevity</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/7337535\">Critics of the singularity</a>.</li>\n<li>Brad Templeton. <a href=\"http://vimeo.com/7337628\">The finger of AI: Automated electrical vehicles and oil independence</a>.</li>\n<li>Gary Marcus. <a href=\"http://vimeo.com/7338930\">The fallibility and improvability of the human mind</a>.</li>\n<li>Peter Thiel. <a href=\"http://vimeo.com/7339317\">Macroeconomics and singularity</a>.</li>\n<li>Aubrey de Grey. <a href=\"http://vimeo.com/7339349\">The Singularity and the Methuselarity: similarities and differences</a>.</li>\n<li>Thiel, Yudkowsky &amp; de Grey panel: <a href=\"http://vimeo.com/7396024\">Changing the world</a>.</li>\n<li>Anna Salamon. <a href=\"http://vimeo.com/7397629\">How much it matters to know what matters: A back of the envelope calculation</a>.</li>\n<li>Gary Wolf. <a href=\"http://vimeo.com/7425764\">The petaflop macroscope</a>.</li>\n<li>Eliezer Yudkowsky. <a href=\"http://vimeo.com/7426357\">Cognitive biases and giant risks</a>.</li>\n<li>Jurgen Schmidhuber. <a href=\"http://vimeo.com/7441291\">Compression progress: The algorithmic principle behind curiosity and creativity</a>.</li>\n<li>Thiel, Rose &amp; Gorenberg Panel: <a href=\"http://vimeo.com/7443559\">Venture capitalism</a>.</li>\n<li>Michael Nielsen. <a href=\"http://vimeo.com/7447694\">Quantum computing: What it is, what it is not, what we have yet to learn</a>.</li>\n<li>Michael Nielsen. <a href=\"http://vimeo.com/7464017\">Collaborative networks in scientific discovery</a>.</li>\n<li>Stephen Wolfram. <a href=\"http://vimeo.com/7466113\">Conversation on the singularity</a>.</li>\n</ul>\n<p><br> <strong>Summit 2010</strong></p>\n<ul>\n<li>Michael Vassar. <a href=\"http://vimeo.com/17512853\">The Darwinian method</a>.</li>\n<li>Gregory Stock. <a href=\"http://vimeo.com/18141173\">Evolution of post-human intelligence</a>.</li>\n<li>Ray Kurzweil. <a href=\"http://vimeo.com/18488795\">The mind and how to build one</a>.</li>\n<li>Ben Goertzel. <a href=\"http://vimeo.com/18143134\">AI against aging</a>.</li>\n<li>Steven Mann. <a href=\"http://vimeo.com/19362162\">Humanistic intelligence augmentation and mediation</a>.</li>\n<li>Mandayam Srinivasan. <a href=\"http://vimeo.com/18822577\">Enhancing our bodies and evolving our brains</a>.</li>\n<li>Brian Litt. <a href=\"http://vimeo.com/19358262\">The past, present and future of brain machine interfaces</a>.</li>\n<li>Demis Hassabis. <a href=\"http://vimeo.com/17513841\">Combining systems neuroscience and machine learning: a new approach to AGI</a>.</li>\n<li>Terry Sejnowski. <a href=\"http://vimeo.com/18824185\">Reverse-engineering brains is within reach</a>.</li>\n<li>Dennis Bray. <a href=\"http://vimeo.com/18143991\">What cells can do that robots can't</a>.</li>\n<li>Terry Sejnowski/Dennis Bray debate: <a href=\"http://vimeo.com/17700122\">Will we soon realistically emulate biological systems?</a>.</li>\n<li>Ramez Naam. <a href=\"http://vimeo.com/18141726\">The digital biome</a>.</li>\n<li>Lance Becker. <a href=\"http://vimeo.com/19098618\">Modifying the boundary between life and death</a><a>.</a></li>\n<li>Ellen Heber-Katz. <a href=\"http://vimeo.com/19096437\">The MRL mouse - how it regenerates and how we might do the same</a>.</li>\n<li>Shane Legg. <a href=\"http://vimeo.com/17553536\">Universal measures of intelligence</a>.</li>\n<li>John Tooby. <a href=\"http://vimeo.com/19097207\">Can discovering the design principles governing natural intelligence unleash breakthroughs in AI?</a>.</li>\n<li>Tooby, Goertzel, Yudkowsky &amp; Legg panel. <a href=\"http://vimeo.com/17702682\">Narrow and General Intelligence</a>.</li>\n<li>David Hanson. <a href=\"http://vimeo.com/18826257\">David Hanson: Why Characters Are Key to Friendly A.I.</a>.</li>\n<li>Irene Pepperberg. <a href=\"http://vimeo.com/19360766\">Irene Pepperberg: Nonhuman Intelligence: Where we are and where we're headed</a>.</li>\n<li>James Randi. <a href=\"http://vimeo.com/18653890\">Is there such a thing as scientific consensus?</a></li>\n</ul>\n<p><strong id=\"Summit_2011\">Summit 2011</strong></p>\n<ul>\n<li>Ray Kurzweil. <a href=\"http://www.youtube.com/watch?v=WPqjYrLhDnk\">From Eliza to Watson to passing the Turing Test</a>. </li>\n<li>Stephen Badylak. <a href=\"http://www.youtube.com/watch?v=kC1MivfpT9o\">Regenerative medicine: possibilities and potential</a>. </li>\n<li>Sonia Arrison. <a href=\"http://www.youtube.com/watch?v=D0iUs2_TThA\">100 Plus: how the coming age of longevity will change everything, from careers and relationships to family and faith</a>. </li>\n<li>Peter Thiel. <a href=\"http://youtu.be/ROrUea0gLlY\">Back to the future</a>. </li>\n<li>James McLurkin. <a href=\"http://www.youtube.com/watch?v=9Cqv6xYk-uM\">The future of robotics is swarms: why a thousand robots are better than one</a>. </li>\n<li>Michael Shermer. <a href=\"http://www.youtube.com/watch?v=ZJW78HoNnX0\">Social Singularity: transitioning from civilization 1.0 to 2.0</a>. </li>\n<li>Jason Silva. <a href=\"http://www.youtube.com/watch?v=BzOxui9uw0o\">The 'Undivided Mind' \u2014 science and imagination</a>. </li>\n<li>Stephen Wolfram. <a href=\"http://www.youtube.com/watch?v=Pgw_nVqSTLw\">Computation and the future of mankind</a>. </li>\n<li>Dmitry Itskov. <a href=\"http://www.youtube.com/watch?v=zEi3ZAYheT0\">Project 'Immortality 2045' -- Russian experience</a>. </li>\n<li>Christof Koch. <a href=\"http://www.youtube.com/watch?v=6i9kE3Ne7as\">The neurobiology and mathematics of consciousness</a>. </li>\n<li>Eliezer Yudkowsky. <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Open problems in friendly artificial intelligence</a>. </li>\n<li>Max Tegmark. <a href=\"http://www.youtube.com/watch?v=GctnYAYcMhI\">The future of life: a cosmic perspective</a>. </li>\n<li>Alexander Wissner-Gross. <a href=\"http://www.youtube.com/watch?v=BS5ZtvNECMI\">Planetary-scale intelligence</a>. </li>\n<li>Sharon Bertsch McGrayne. <a href=\"http://www.youtube.com/watch?v=oTltncUkckQ\">A History of Bayes theorem.</a></li>\n<li>David Brin.&nbsp;<a href=\"http://www.youtube.com/watch?v=ryoqtB6H5nw\">So you want to make gods. Now why would that bother anybody?</a> </li>\n<li>Tyler Cowen. <a href=\"http://www.youtube.com/watch?v=ed6gNSZRawY\">The Great Stagnation</a>. </li>\n<li>Tyler Cowen &amp; Michael Vassar. <a href=\"http://youtu.be/ed6gNSZRawY\">Debate on the Great Stagnation</a>. </li>\n<li>John Mauldin. <a href=\"http://www.youtube.com/watch?v=xB-xjZQy_4M\">The endgame meets The millennium wave \u2014 why the economic crisis will be history as we create the future</a>. </li>\n<li>Riley Crane. <a href=\"http://www.youtube.com/watch?v=7vkhqeRno9M\">Rethinking communication</a>. </li>\n<li>Dileep George and Scott Brown. <a href=\"http://www.youtube.com/watch?v=wof47INDvdg\">From planes to brains: building AI the Wright way</a>. </li>\n<li>Jaan Tallinn. <a href=\"http://www.youtube.com/watch?v=84G6An1Ff2E\">Balancing the trichotomy: individual vs. society vs. universe</a>. </li>\n<li>David Ferrucci. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">Watson AI perceptions</a>. </li>\n<li>Dan Cerutti. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">Commercializing Watson. </a></li>\n<li>Ken Jennings. <a href=\"http://www.youtube.com/watch?v=oFMeBId7vIM\">The human brain in Jeopardy: computers that 'think'</a>.</li>\n</ul>", "sections": [{"title": "Summit 2008", "anchor": "Summit_2008", "level": 1}, {"title": "Summit 2009", "anchor": "Summit_2009", "level": 1}, {"title": "Summit 2011", "anchor": "Summit_2011", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "29 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T18:13:08.645Z", "modifiedAt": null, "url": null, "title": "[Link] New paper: \"The quantum state cannot be interpreted statistically\"", "slug": "link-new-paper-the-quantum-state-cannot-be-interpreted", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "N6W7sAzCo3fGauM7i", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XtKB6myLdQ5hnuF2h/link-new-paper-the-quantum-state-cannot-be-interpreted", "pageUrlRelative": "/posts/XtKB6myLdQ5hnuF2h/link-new-paper-the-quantum-state-cannot-be-interpreted", "linkUrl": "https://www.lesswrong.com/posts/XtKB6myLdQ5hnuF2h/link-new-paper-the-quantum-state-cannot-be-interpreted", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20New%20paper%3A%20%22The%20quantum%20state%20cannot%20be%20interpreted%20statistically%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20New%20paper%3A%20%22The%20quantum%20state%20cannot%20be%20interpreted%20statistically%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtKB6myLdQ5hnuF2h%2Flink-new-paper-the-quantum-state-cannot-be-interpreted%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20New%20paper%3A%20%22The%20quantum%20state%20cannot%20be%20interpreted%20statistically%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtKB6myLdQ5hnuF2h%2Flink-new-paper-the-quantum-state-cannot-be-interpreted", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXtKB6myLdQ5hnuF2h%2Flink-new-paper-the-quantum-state-cannot-be-interpreted", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 822, "htmlBody": "<p>From a <a href=\"http://lanl.arxiv.org/abs/1111.3328\">recent paper</a> that is getting <a href=\"http://www.nature.com/news/quantum-theorem-shakes-foundations-1.9392\">non-trivial attention</a>...</p>\n<blockquote>\n<p>\"Quantum states are the key mathematical objects in quantum theory. It is therefore surprising that physicists have been unable to agree on what a quantum state represents. There are at least two opposing schools of thought, each almost as old as quantum theory itself. One is that a pure state is a physical property of system, much like position and momentum in classical mechanics. Another is that even a pure state has only a statistical significance, akin to a probability distribution in statistical mechanics. Here we show that, given only very mild assumptions, the statistical interpretation of the quantum state is inconsistent with the predictions of quantum theory. This result holds even in the presence of small amounts of experimental noise, and is therefore amenable to experimental test using present or near-future technology. If the predictions of quantum theory are confirmed, such a test would show that distinct quantum states must correspond to physically distinct states of reality.\"</p>\n</blockquote>\n<p>From my understanding, the result works by showing how, if a quantum state is determined only statistically by some true physical state of the universe, then it is possible for us to construct clever quantum measurements that put statistical probability on outcomes for which there is literally zero quantum amplitude, which is a contradiction of Born's rule. The assumptions required are very mild, and if this is confirmed in experiment it would give a lot of justification for a phyicalist / realist interpretation of the Many Worlds point of view.</p>\n<p>More from the paper:</p>\n<blockquote>\n<p>\"We conclude by outlining some consequences of the result. First, one motivation for the statistical view is the obvious parallel between the quantum process of instantaneous wave function collapse, and the (entirely non-mysterious) classical procedure of updating a probability distribution when new information is acquired. If the quantum state is a physical property of a system -- as it must be if one accepts the assumptions above -- then the quantum collapse must correspond to a real physical process. This is especially mysterious when two entangled systems are at separate locations, and measurement of one leads to an instantaneous collapse of the quantum state of the other.</p>\n<p>In some versions of quantum theory, on the other hand, there is no collapse of the quantum state. In this case, after a measurement takes place, the joint quantum state of the system and measuring apparatus will contain a component corresponding to each possible macroscopic measurement outcome. This is unproblematic if the quantum state merely reflects a lack of information about which outcome occurred. But if the quantum state is a physical property of the system and apparatus, it is hard to avoid the conclusion that each marcoscopically different component has a direct counterpart in reality.</p>\n<p>On a related, but more abstract note, the quantum state has the striking property of being an exponentially complicated object. Specifically, the number of real parameters needed to specify a quantum state is exponential in the number of systems n. This has a consequence for classical simulation of quantum systems. If a simulation is constrained by our assumptions -- that is, if it must store in memory a state for a quantum system, with independent preparations assigned uncorrelated states -- then it will need an amount of memory which is exponential in the number of quantum systems.</p>\n<p>For these reasons and others, many will continue to hold that the quantum state is not a real object. We have shown that this is only possible if one or more of the assumptions above is dropped. More radical approaches[14] are careful to avoid associating quantum systems with any physical properties at all. The alternative is to seek physically well motivated reasons why the other two assumptions might fail.\"</p>\n</blockquote>\n<p>On a related note, in one of David Deutsch's original arguments for why Many Worlds was <em>straightforwardly</em> obvious from quantum theory, he mentions Shor's quantum factoring algorithm. Essentially he asks any opponent of Many Worlds to give a real account, not just a parochial calculational account, of why the algorithm works when it is using exponentially more resources than could possibly be classically available to it. The way he put it was: \"where was the number factored?\"</p>\n<p>I was never convinced that regular quantum computation could really be used to convince someone of Many Worlds who did not already believe it, except possibly for bounded-error quantum computation where one must accept the fact that there are different worlds to find one's self in after the computation, namely some of the worlds where the computation had an error due to the algorithm itself (or else one must explain the measurement problem in some different way as per usual). But I think that in light of the paper mentioned above, Deutsch's \"where was the number factored\" argument may deserve more credence.<br /><br /><strong>Added: </strong>Scott Aaronson discusses the paper <a href=\"http://www.scottaaronson.com/blog/?p=822\">here</a>&nbsp;(the comments are also interesting).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XtKB6myLdQ5hnuF2h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 8.015398746585404e-07, "legacy": true, "legacyId": "11003", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T20:56:28.425Z", "modifiedAt": null, "url": null, "title": "Meetup : Skepticon IV meetup: Saturday night", "slug": "meetup-skepticon-iv-meetup-saturday-night", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.003Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t3Sqox4HczYwoN5YW/meetup-skepticon-iv-meetup-saturday-night", "pageUrlRelative": "/posts/t3Sqox4HczYwoN5YW/meetup-skepticon-iv-meetup-saturday-night", "linkUrl": "https://www.lesswrong.com/posts/t3Sqox4HczYwoN5YW/meetup-skepticon-iv-meetup-saturday-night", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Skepticon%20IV%20meetup%3A%20Saturday%20night&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Skepticon%20IV%20meetup%3A%20Saturday%20night%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft3Sqox4HczYwoN5YW%2Fmeetup-skepticon-iv-meetup-saturday-night%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Skepticon%20IV%20meetup%3A%20Saturday%20night%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft3Sqox4HczYwoN5YW%2Fmeetup-skepticon-iv-meetup-saturday-night", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft3Sqox4HczYwoN5YW%2Fmeetup-skepticon-iv-meetup-saturday-night", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/51'>Skepticon IV meetup: Saturday night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2011 09:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">University Plaza Hotel, 333 S John Q. Hammons Parkway, Springfield, Missouri 65806</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Following the last talk on Saturday, we'll meet up in the lobby of the University Plaza Hotel (where most conference participants are staying), and decide what to do and where (if anywhere) to go from there! Time is listed as 9:30 pm, to allow time to get from the conference location to the hotel. I will provide a \"Less Wrong\" sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/51'>Skepticon IV meetup: Saturday night</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t3Sqox4HczYwoN5YW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 8.015978870929314e-07, "legacy": true, "legacyId": "11004", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Skepticon_IV_meetup__Saturday_night\">Discussion article for the meetup : <a href=\"/meetups/51\">Skepticon IV meetup: Saturday night</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2011 09:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">University Plaza Hotel, 333 S John Q. Hammons Parkway, Springfield, Missouri 65806</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Following the last talk on Saturday, we'll meet up in the lobby of the University Plaza Hotel (where most conference participants are staying), and decide what to do and where (if anywhere) to go from there! Time is listed as 9:30 pm, to allow time to get from the conference location to the hotel. I will provide a \"Less Wrong\" sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Skepticon_IV_meetup__Saturday_night1\">Discussion article for the meetup : <a href=\"/meetups/51\">Skepticon IV meetup: Saturday night</a></h2>", "sections": [{"title": "Discussion article for the meetup : Skepticon IV meetup: Saturday night", "anchor": "Discussion_article_for_the_meetup___Skepticon_IV_meetup__Saturday_night", "level": 1}, {"title": "Discussion article for the meetup : Skepticon IV meetup: Saturday night", "anchor": "Discussion_article_for_the_meetup___Skepticon_IV_meetup__Saturday_night1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T21:07:51.461Z", "modifiedAt": null, "url": null, "title": "Raytheon to Develop Rationality-Training Games", "slug": "raytheon-to-develop-rationality-training-games", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.928Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Darmani", "createdAt": "2009-02-28T02:13:51.400Z", "isAdmin": false, "displayName": "Darmani"}, "userId": "MD49Fa8rRXChHwYoS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4PskshEWw5i56pLM/raytheon-to-develop-rationality-training-games", "pageUrlRelative": "/posts/H4PskshEWw5i56pLM/raytheon-to-develop-rationality-training-games", "linkUrl": "https://www.lesswrong.com/posts/H4PskshEWw5i56pLM/raytheon-to-develop-rationality-training-games", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Raytheon%20to%20Develop%20Rationality-Training%20Games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARaytheon%20to%20Develop%20Rationality-Training%20Games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4PskshEWw5i56pLM%2Fraytheon-to-develop-rationality-training-games%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Raytheon%20to%20Develop%20Rationality-Training%20Games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4PskshEWw5i56pLM%2Fraytheon-to-develop-rationality-training-games", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4PskshEWw5i56pLM%2Fraytheon-to-develop-rationality-training-games", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.networkworld.com/community/blog/raytheon-gets-105m-develop-serious-games</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4PskshEWw5i56pLM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 23, "extendedScore": null, "score": 8.016019307795804e-07, "legacy": true, "legacyId": "11005", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T21:55:35.720Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX NEW LOCATION!", "slug": "meetup-austin-tx-new-location", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MWxbZMeRuzMGoyNBy/meetup-austin-tx-new-location", "pageUrlRelative": "/posts/MWxbZMeRuzMGoyNBy/meetup-austin-tx-new-location", "linkUrl": "https://www.lesswrong.com/posts/MWxbZMeRuzMGoyNBy/meetup-austin-tx-new-location", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX%20NEW%20LOCATION!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%20NEW%20LOCATION!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWxbZMeRuzMGoyNBy%2Fmeetup-austin-tx-new-location%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20NEW%20LOCATION!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWxbZMeRuzMGoyNBy%2Fmeetup-austin-tx-new-location", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMWxbZMeRuzMGoyNBy%2Fmeetup-austin-tx-new-location", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/52'>Austin, TX NEW LOCATION!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2100 Speedway, Austin, Texas</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin LW meetup has moved (possibly temporarily, possibly permanently. Stay tuned)!</p>\n\n<p>We'll be meeting at 1:30, as normal, but in <a href=\"http://www.utexas.edu/maps/main/buildings/cba.html\" rel=\"nofollow\">CBA 4.338</a>. It's a seminar room with a projector, where we will be watching (and trying to make sense of) <a href=\"http://www.imdb.com/title/tt0390384/\" rel=\"nofollow\">Primer</a>.</p>\n\n<p>We hope to see you there!</p>\n\n<p>(Parking: You can either park wherever you parked for Caffe Medici and walk ~three blocks to the east, or you can part in the Brazos street garage (at Brazos and MLK) and walk west then north to CBA.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/52'>Austin, TX NEW LOCATION!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MWxbZMeRuzMGoyNBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.016188880938191e-07, "legacy": true, "legacyId": "11006", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX_NEW_LOCATION_\">Discussion article for the meetup : <a href=\"/meetups/52\">Austin, TX NEW LOCATION!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2100 Speedway, Austin, Texas</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin LW meetup has moved (possibly temporarily, possibly permanently. Stay tuned)!</p>\n\n<p>We'll be meeting at 1:30, as normal, but in <a href=\"http://www.utexas.edu/maps/main/buildings/cba.html\" rel=\"nofollow\">CBA 4.338</a>. It's a seminar room with a projector, where we will be watching (and trying to make sense of) <a href=\"http://www.imdb.com/title/tt0390384/\" rel=\"nofollow\">Primer</a>.</p>\n\n<p>We hope to see you there!</p>\n\n<p>(Parking: You can either park wherever you parked for Caffe Medici and walk ~three blocks to the east, or you can part in the Brazos street garage (at Brazos and MLK) and walk west then north to CBA.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX_NEW_LOCATION_1\">Discussion article for the meetup : <a href=\"/meetups/52\">Austin, TX NEW LOCATION!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX NEW LOCATION!", "anchor": "Discussion_article_for_the_meetup___Austin__TX_NEW_LOCATION_", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX NEW LOCATION!", "anchor": "Discussion_article_for_the_meetup___Austin__TX_NEW_LOCATION_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T22:34:21.633Z", "modifiedAt": null, "url": null, "title": "The punishment dilemma", "slug": "the-punishment-dilemma", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PuyaSharif", "createdAt": "2011-02-26T07:33:06.094Z", "isAdmin": false, "displayName": "PuyaSharif"}, "userId": "Kx2AumHK8eeJ4nHqt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tqaWDtNF26j63ZWg9/the-punishment-dilemma", "pageUrlRelative": "/posts/tqaWDtNF26j63ZWg9/the-punishment-dilemma", "linkUrl": "https://www.lesswrong.com/posts/tqaWDtNF26j63ZWg9/the-punishment-dilemma", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20punishment%20dilemma&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20punishment%20dilemma%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtqaWDtNF26j63ZWg9%2Fthe-punishment-dilemma%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20punishment%20dilemma%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtqaWDtNF26j63ZWg9%2Fthe-punishment-dilemma", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtqaWDtNF26j63ZWg9%2Fthe-punishment-dilemma", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 276, "htmlBody": "<p>Here is a thought experiment for you. There will be some bold assumptions here, and they may be regarded unrealistic. I am aware of that and the purpose of this query is not to propose some truths about society in general, but to isolate certain characteristics of preferences regarding the societal institutions of law enforcement and punishment.</p>\n<hr />\n<p>Assume that there existed a highly trustworthy model that showed beyond reasonable doubt that crime rates anti-correlated with harshness of punishments imposed on criminals. So basically, if policies changed towards shorter sentences, lower fines and lighter penalties, the number of criminal acts decreased (in every category).</p>\n<p>Further assume that this was empirically tested and each time penalties went down, fewer and fewer crimes was committed. But the dependence was not linear so if we would get rid of punishments all together - there would still be murders, rapes, robberies etc. But, the crime rates would be minimized in that case. To summarize: We knew that crime rates would be at minimum if there was no consequences at all.</p>\n<p>With no penalties, somebody could simply kill or rape your mother, sister or child and move in next door and live a nice and happy life in front of your very eyes, without society doing anything about it! Bare in mind now that this is the situation where the probability of your mother, sister or child being abused, robbed or killed is minimized!</p>\n<p>Would it be reasonable to go trough with this demobilization that would spare lots of innocent people all the pain of getting robbed and abused, given that those criminals still out there can do anything they want and go free?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tqaWDtNF26j63ZWg9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -3, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "11007", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-18T23:52:41.358Z", "modifiedAt": null, "url": null, "title": "Where do selfish values come from?", "slug": "where-do-selfish-values-come-from", "viewCount": null, "lastCommentedAt": "2021-12-06T23:26:57.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wei_Dai", "createdAt": "2009-03-06T19:59:52.096Z", "isAdmin": false, "displayName": "Wei_Dai"}, "userId": "4SHky5j2PNcRwBiZt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Nz62ZurRkGPigAxMK/where-do-selfish-values-come-from", "pageUrlRelative": "/posts/Nz62ZurRkGPigAxMK/where-do-selfish-values-come-from", "linkUrl": "https://www.lesswrong.com/posts/Nz62ZurRkGPigAxMK/where-do-selfish-values-come-from", "postedAtFormatted": "Friday, November 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20do%20selfish%20values%20come%20from%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20do%20selfish%20values%20come%20from%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNz62ZurRkGPigAxMK%2Fwhere-do-selfish-values-come-from%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20do%20selfish%20values%20come%20from%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNz62ZurRkGPigAxMK%2Fwhere-do-selfish-values-come-from", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNz62ZurRkGPigAxMK%2Fwhere-do-selfish-values-come-from", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 450, "htmlBody": "<p>Human values seem to be at least partly selfish. While it would probably be a bad idea to build AIs that are selfish, ideas from AI design can perhaps shed some light on the nature of selfishness, which we need to understand if we are to understand human values. (How does selfishness work in a decision theoretic sense? Do humans actually have selfish values?)&nbsp;Current theory suggest 3 possible ways to design a selfish agent:</p>\n<ol>\n<li>have a <a href=\"/lw/116/the_domain_of_your_utility_function/\">perception-determined utility function</a> (like AIXI)</li>\n<li>have a static (unchanging) world-determined utility function (like UDT) with a sufficiently detailed description of the agent embedded in the specification of its utility function at the time of the agent's creation</li>\n<li>have a world-determined utility function that changes (\"learns\") as the agent makes observations (for concreteness, let's assume a variant of UDT where you start out caring about everyone, and each time you make an observation, your utility function changes to no longer care about anyone who hasn't made that same observation)</li>\n</ol>\n<p>Note that 1 and 3 are not reflectively consistent (they both refuse to pay the <a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\">Counterfactual Mugger</a>), and 2 is not applicable to humans (since we are not born with detailed descriptions of ourselves embedded in our brains). Still, it seems plausible that humans do have selfish values, either because we are type 1 or type 3 agents, or because we were type 1 or type 3 agents at some time in the past, but have since self-modified into type 2 agents.</p>\n<p>But things aren't quite that simple. According to our current theories, an AI would judge its decision theory using that decision theory itself, and self-modify if it was found wanting under its own judgement. But humans do not actually work that way. Instead, we judge ourselves using <a href=\"/lw/2id/metaphilosophical_mysteries/\">something mysterious</a> called \"normativity\" or \"philosophy\". For example, a type 3 AI would just decide that its current values can be maximized by changing into a type 2 agent with a static copy of those values, but a human could perhaps think that changing values in response to observations is a mistake, and they ought to fix that mistake by rewinding their values back to before they were changed. Note that if you rewind your values all the way back to before you made the first observation, you're no longer selfish.</p>\n<p>So, should we freeze our selfish values, or rewind our values, or maybe even keep our \"irrational\" decision theory (which could perhaps be justified by saying that we intrinsically value having a decision theory that isn't too alien)? I don't know what conclusions to draw from this line of thought, except that on close inspection, selfishness may offer just as many difficult philosophical <a href=\"/lw/1mo/the_preference_utilitarians_time_inconsistency/\">problems</a> as altruism.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NLwTnsH9RSotqXYLw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Nz62ZurRkGPigAxMK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 58, "extendedScore": null, "score": 0.000117, "legacy": true, "legacyId": "10964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 58, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xgicQnkrdA5FehhnQ", "MAhueZtNz5SnDPhsy", "9bkY8etnLfGCzcYAu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T00:09:48.800Z", "modifiedAt": null, "url": null, "title": "[Link] Raytheon given $10.5 to develop 'serious games' for bias reduction", "slug": "link-raytheon-given-usd10-5-to-develop-serious-games-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.114Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tybTdafcYEBGLLAyT/link-raytheon-given-usd10-5-to-develop-serious-games-for", "pageUrlRelative": "/posts/tybTdafcYEBGLLAyT/link-raytheon-given-usd10-5-to-develop-serious-games-for", "linkUrl": "https://www.lesswrong.com/posts/tybTdafcYEBGLLAyT/link-raytheon-given-usd10-5-to-develop-serious-games-for", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Raytheon%20given%20%2410.5%20to%20develop%20'serious%20games'%20for%20bias%20reduction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Raytheon%20given%20%2410.5%20to%20develop%20'serious%20games'%20for%20bias%20reduction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtybTdafcYEBGLLAyT%2Flink-raytheon-given-usd10-5-to-develop-serious-games-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Raytheon%20given%20%2410.5%20to%20develop%20'serious%20games'%20for%20bias%20reduction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtybTdafcYEBGLLAyT%2Flink-raytheon-given-usd10-5-to-develop-serious-games-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtybTdafcYEBGLLAyT%2Flink-raytheon-given-usd10-5-to-develop-serious-games-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p><a href=\"http://www.networkworld.com/community/blog/raytheon-gets-105m-develop-serious-games\">http://www.networkworld.com/community/blog/raytheon-gets-105m-develop-serious-games</a></p>\n<blockquote>\n<p><a style=\"color: #0f7cc2; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 18px;\" href=\"https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;tab=core&amp;id=1793ab48906acabaf923c76486c29c0f&amp;_cview=0\">Under a contract from the government's cutting edge research group</a><span style=\"color: #333333; font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 18px;\">, the Intelligence Advanced Research Projects Activity (IARPA), Raytheon BBN will develop game-based training programs featuring an international detective theme developed by game designers, cognitive psychologists and experts in intelligence analysis and in measuring game-player engagement.</span></p>\n</blockquote>\n<p>\n<blockquote>\n<p style=\"font-size: 13px; margin-top: 10px; margin-right: 0px; margin-bottom: 10px; margin-left: 0px; line-height: 18px; color: #333333; font-family: Arial, Helvetica, sans-serif; padding: 0px;\">The gaming system will focus on certain types of bias that frequently hurt effective decision-making:</p>\n<ul style=\"list-style-type: square; list-style-position: initial; margin-top: 15px; margin-right: 15px; margin-bottom: 15px; margin-left: 30px; font-size: 13px; color: #333333; font-family: Arial, Helvetica, sans-serif; line-height: 15px; padding: 0px;\">\n<li style=\"padding-bottom: 8px;\">Confirmation bias -- the tendency to search for or interpret information in a way that confirms preconceptions.</li>\n<li style=\"padding-bottom: 8px;\">Blind spot bias -- being less aware of one's own cognitive biases than those of others.</li>\n<li style=\"padding-bottom: 8px;\">Fundamental attribution error -- over-emphasizing personality-based or character-based effects on behavior.</li>\n<li style=\"padding-bottom: 8px;\">Anchoring bias -- relying too heavily on one trait or one piece of information.</li>\n<li style=\"padding-bottom: 8px;\">Representative bias -- judging the likelihood of a hypothesis by its resemblance to immediately available data.</li>\n<li style=\"padding-bottom: 8px;\">Projection bias -- assuming others share one's current feelings, values or thinking</li>\n</ul>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tybTdafcYEBGLLAyT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "11008", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T02:20:33.325Z", "modifiedAt": null, "url": null, "title": "[Link] TED- Cynthia Kenyon: Experiments that Hint at Longer Lives", "slug": "link-ted-cynthia-kenyon-experiments-that-hint-at-longer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.739Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tE3eCNqAGHtRYt6RQ/link-ted-cynthia-kenyon-experiments-that-hint-at-longer", "pageUrlRelative": "/posts/tE3eCNqAGHtRYt6RQ/link-ted-cynthia-kenyon-experiments-that-hint-at-longer", "linkUrl": "https://www.lesswrong.com/posts/tE3eCNqAGHtRYt6RQ/link-ted-cynthia-kenyon-experiments-that-hint-at-longer", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20TED-%20Cynthia%20Kenyon%3A%20Experiments%20that%20Hint%20at%20Longer%20Lives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20TED-%20Cynthia%20Kenyon%3A%20Experiments%20that%20Hint%20at%20Longer%20Lives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtE3eCNqAGHtRYt6RQ%2Flink-ted-cynthia-kenyon-experiments-that-hint-at-longer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20TED-%20Cynthia%20Kenyon%3A%20Experiments%20that%20Hint%20at%20Longer%20Lives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtE3eCNqAGHtRYt6RQ%2Flink-ted-cynthia-kenyon-experiments-that-hint-at-longer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtE3eCNqAGHtRYt6RQ%2Flink-ted-cynthia-kenyon-experiments-that-hint-at-longer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 416, "htmlBody": "<p><strong>TEDGlobal 2011- Cynthia Kenyon: Experiments that Hint at Longer Lives</strong></p>\n<p>Video Link:&nbsp;<a href=\"http://www.ted.com/talks/cynthia_kenyon_experiments_that_hint_of_longer_lives.html\">http://www.ted.com/talks/cynthia_kenyon_experiments_that_hint_of_longer_lives.html</a></p>\n<p><strong>Speaker's Bio (from TED)</strong>:</p>\n<blockquote>\n<p class=\"MsoNormal\">Cynthia Kenyon is revolutionizing our understanding of aging. As an expert in biochemistry and biophysics at the University of California at San Francisco, she is particularly interested in the influence that genetics have on age-related diseases (from cancer to heart failure) in living things.<br /> <br /> Her biggest breakthrough was figuring out that there&rsquo;s a &ldquo;universal hormonal control for aging&rdquo;: carbohydrate intake, which can have a dramatic effect on how two critical genes behave, reducing insulin production and boosting repair and renovation activities. So far, her theory has proved true for worms, mice, rats, and monkeys &mdash; and she suspects it applies to humans, too.<br /> <br /> By studying aging, Kenyon believes that she and other scientists (many of whom have successfully duplicated her experiments) will be able to pinpoint the molecules responsible for the onset of age-related diseases in people and prevent them. She&rsquo;s co-founded a drug-development company called Elixir Pharmaceuticals to do just that.</p>\n<p class=\"MsoNormal\">She says: \"The link between aging and age-related disease suggests an entirely new way to combat many diseases all at once; namely, by going after their greatest risk factor: aging itself.\"</p>\n<p class=\"MsoNormal\">\"Ten years ago, we thought aging was probably the result of a slow decay, a sort of rusting. But Professor Kenyon has shown that it&rsquo;s ... controlled by genes. That opens the possibility of slowing it down with drugs.\"</p>\n<p class=\"MsoNormal\"><em>Jeff Holly, Bristol University<br /></em></p>\n</blockquote>\n<p>&nbsp;</p>\n<p><strong>Summary:</strong></p>\n<p>Different animals have different lifespans (i.e. mice- 2years, v. bat- 50 years) There must be something in the genes for aging, so you should be able to change the aging gene to expand life span.</p>\n<p class=\"MsoNormal\">They did experiments on a roundworm and found that mutations that damage a gene called daf-2 could double the worm&rsquo;s lifespan. The daf-2 gene encodes a hormone receptor which promotes aging. (similar to the ones that promote food uptake and growth.)</p>\n<p class=\"MsoNormal\">They did the same mutation in flies and mice, and it worked on them.</p>\n<p class=\"MsoNormal\">People who lived to 90-100 in a population of Ashkenazi Jews were more likely to have mutations to daf-2 which would make it work less well. They are also less susceptible to things like cancer and age related deseases.</p>\n<p class=\"MsoNormal\">She explains how and why it works.</p>\n<p class=\"MsoNormal\">Follow-up questions about- Certain animals that don&rsquo;t have aging. And can we do this by changing genes instead of developing a medication. (She says that would be a bad idea).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tE3eCNqAGHtRYt6RQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 8.017130184817158e-07, "legacy": true, "legacyId": "11013", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"TEDGlobal_2011__Cynthia_Kenyon__Experiments_that_Hint_at_Longer_Lives\">TEDGlobal 2011- Cynthia Kenyon: Experiments that Hint at Longer Lives</strong></p>\n<p>Video Link:&nbsp;<a href=\"http://www.ted.com/talks/cynthia_kenyon_experiments_that_hint_of_longer_lives.html\">http://www.ted.com/talks/cynthia_kenyon_experiments_that_hint_of_longer_lives.html</a></p>\n<p><strong>Speaker's Bio (from TED)</strong>:</p>\n<blockquote>\n<p class=\"MsoNormal\">Cynthia Kenyon is revolutionizing our understanding of aging. As an expert in biochemistry and biophysics at the University of California at San Francisco, she is particularly interested in the influence that genetics have on age-related diseases (from cancer to heart failure) in living things.<br> <br> Her biggest breakthrough was figuring out that there\u2019s a \u201cuniversal hormonal control for aging\u201d: carbohydrate intake, which can have a dramatic effect on how two critical genes behave, reducing insulin production and boosting repair and renovation activities. So far, her theory has proved true for worms, mice, rats, and monkeys \u2014 and she suspects it applies to humans, too.<br> <br> By studying aging, Kenyon believes that she and other scientists (many of whom have successfully duplicated her experiments) will be able to pinpoint the molecules responsible for the onset of age-related diseases in people and prevent them. She\u2019s co-founded a drug-development company called Elixir Pharmaceuticals to do just that.</p>\n<p class=\"MsoNormal\">She says: \"The link between aging and age-related disease suggests an entirely new way to combat many diseases all at once; namely, by going after their greatest risk factor: aging itself.\"</p>\n<p class=\"MsoNormal\">\"Ten years ago, we thought aging was probably the result of a slow decay, a sort of rusting. But Professor Kenyon has shown that it\u2019s ... controlled by genes. That opens the possibility of slowing it down with drugs.\"</p>\n<p class=\"MsoNormal\"><em>Jeff Holly, Bristol University<br></em></p>\n</blockquote>\n<p>&nbsp;</p>\n<p><strong id=\"Summary_\">Summary:</strong></p>\n<p>Different animals have different lifespans (i.e. mice- 2years, v. bat- 50 years) There must be something in the genes for aging, so you should be able to change the aging gene to expand life span.</p>\n<p class=\"MsoNormal\">They did experiments on a roundworm and found that mutations that damage a gene called daf-2 could double the worm\u2019s lifespan. The daf-2 gene encodes a hormone receptor which promotes aging. (similar to the ones that promote food uptake and growth.)</p>\n<p class=\"MsoNormal\">They did the same mutation in flies and mice, and it worked on them.</p>\n<p class=\"MsoNormal\">People who lived to 90-100 in a population of Ashkenazi Jews were more likely to have mutations to daf-2 which would make it work less well. They are also less susceptible to things like cancer and age related deseases.</p>\n<p class=\"MsoNormal\">She explains how and why it works.</p>\n<p class=\"MsoNormal\">Follow-up questions about- Certain animals that don\u2019t have aging. And can we do this by changing genes instead of developing a medication. (She says that would be a bad idea).</p>", "sections": [{"title": "TEDGlobal 2011- Cynthia Kenyon: Experiments that Hint at Longer Lives", "anchor": "TEDGlobal_2011__Cynthia_Kenyon__Experiments_that_Hint_at_Longer_Lives", "level": 1}, {"title": "Summary:", "anchor": "Summary_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T02:28:42.888Z", "modifiedAt": null, "url": null, "title": "Which is the the article that describes drawing the map of the city?", "slug": "which-is-the-the-article-that-describes-drawing-the-map-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.255Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CkYMRWba43aXptA2B/which-is-the-the-article-that-describes-drawing-the-map-of", "pageUrlRelative": "/posts/CkYMRWba43aXptA2B/which-is-the-the-article-that-describes-drawing-the-map-of", "linkUrl": "https://www.lesswrong.com/posts/CkYMRWba43aXptA2B/which-is-the-the-article-that-describes-drawing-the-map-of", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20is%20the%20the%20article%20that%20describes%20drawing%20the%20map%20of%20the%20city%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20is%20the%20the%20article%20that%20describes%20drawing%20the%20map%20of%20the%20city%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkYMRWba43aXptA2B%2Fwhich-is-the-the-article-that-describes-drawing-the-map-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20is%20the%20the%20article%20that%20describes%20drawing%20the%20map%20of%20the%20city%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkYMRWba43aXptA2B%2Fwhich-is-the-the-article-that-describes-drawing-the-map-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkYMRWba43aXptA2B%2Fwhich-is-the-the-article-that-describes-drawing-the-map-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 118, "htmlBody": "<p>I've searched a few related terms in Less Wrong, and I can't seem to find the article I'm looking for. It's an Eliezer post that describes a process of drawing a map of a city, and how you can't do it with the blinds closed, you have to actually go out and look at the city.</p>\n<p>I've found the wiki definition page, and I've found articles that look like they refer to the original in the way Eliezer often refers to his earlier metaphors, but I could have sworn there was a shortish article that specifically introduced it.</p>\n<p>(Is it possible that my map is simply wrong and the original parable was only a small portion of a larger post?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CkYMRWba43aXptA2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.017159175173987e-07, "legacy": true, "legacyId": "11014", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T03:39:40.850Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] When None Dare Urge Restraint", "slug": "seq-rerun-when-none-dare-urge-restraint", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bXz8PmHkWtZgEp2fi/seq-rerun-when-none-dare-urge-restraint", "pageUrlRelative": "/posts/bXz8PmHkWtZgEp2fi/seq-rerun-when-none-dare-urge-restraint", "linkUrl": "https://www.lesswrong.com/posts/bXz8PmHkWtZgEp2fi/seq-rerun-when-none-dare-urge-restraint", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20When%20None%20Dare%20Urge%20Restraint&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20When%20None%20Dare%20Urge%20Restraint%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbXz8PmHkWtZgEp2fi%2Fseq-rerun-when-none-dare-urge-restraint%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20When%20None%20Dare%20Urge%20Restraint%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbXz8PmHkWtZgEp2fi%2Fseq-rerun-when-none-dare-urge-restraint", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbXz8PmHkWtZgEp2fi%2Fseq-rerun-when-none-dare-urge-restraint", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/ls/when_none_dare_urge_restraint/\">When None Dare Urge Restraint</a> was originally published on 08 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The dark mirror to the happy death spiral is the spiral of hate. When everyone looks good for attacking someone, and anyone who disagrees with any attack must be a sympathizer to the enemy, the results are usually awful. It is too dangerous for there to be anyone in the world that we would prefer to say negative things about, over saying accurate things about.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8he/seq_rerun_evaporative_cooling_of_group_beliefs/\">Evaporative Cooling of Group Beliefs</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bXz8PmHkWtZgEp2fi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.017411325665401e-07, "legacy": true, "legacyId": "11016", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Tw9cLvzSKrkGjNHW3", "gfzYfKY7Mweoy3hEy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T05:05:41.399Z", "modifiedAt": null, "url": null, "title": "Drawing Less Wrong: Overview of Skills, and Relevance to Rationality", "slug": "drawing-less-wrong-overview-of-skills-and-relevance-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.027Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/shKSShMNtmjkgoBmQ/drawing-less-wrong-overview-of-skills-and-relevance-to", "pageUrlRelative": "/posts/shKSShMNtmjkgoBmQ/drawing-less-wrong-overview-of-skills-and-relevance-to", "linkUrl": "https://www.lesswrong.com/posts/shKSShMNtmjkgoBmQ/drawing-less-wrong-overview-of-skills-and-relevance-to", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Less%20Wrong%3A%20Overview%20of%20Skills%2C%20and%20Relevance%20to%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Less%20Wrong%3A%20Overview%20of%20Skills%2C%20and%20Relevance%20to%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FshKSShMNtmjkgoBmQ%2Fdrawing-less-wrong-overview-of-skills-and-relevance-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Less%20Wrong%3A%20Overview%20of%20Skills%2C%20and%20Relevance%20to%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FshKSShMNtmjkgoBmQ%2Fdrawing-less-wrong-overview-of-skills-and-relevance-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FshKSShMNtmjkgoBmQ%2Fdrawing-less-wrong-overview-of-skills-and-relevance-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1621, "htmlBody": "<p>So, you&#x27;ve considered your past experiences and your motivations, and you&#x27;ve got a decent idea of the effort required of you: Six to eight hours of solid work before you start showing improvement, and about twenty hours total before you start to exhaust the low hanging fruit. You want to learn to draw. What exactly does that entail?</p><p>A lot of things, really. There&#x27;s probably hundreds of subskills, techniques and bits of knowledge that go into creating a quality drawing. But I think they cluster into three main categories:</p><ul><li>Observation</li><li>Technical Skill</li><li>&quot;Instilling Energy and Weight&quot;</li></ul><p>Each of these skills is developed with different exercises, requiring different mindsets. Switching between those exercises can be difficult. Studying any of them can produce something that is interesting to look at, but ultimately you want to integrate them into a single, fluid mental process. You&#x27;ll need to develop some competence in each of them first. As you begin, the most important thing to remember is that *learning* to draw is not the same as actually drawing. To improve as quickly as possible you may have to set aside the reasons you wanted to draw in the first place. Don&#x27;t worry - you&#x27;ll achieve those terminal goals in time.</p><p>In this article I&#x27;ll briefly discuss each of those skill clusters, and how I believe they relate to rationality.</p><h1>Observation</h1><p>The ability to see reality, as it truly is. &quot;Reality&quot; refers both to the physical objects and light that you&#x27;re looking at, as well as to your perception of your own mental processes and how they should be interpreting those physical objects. If you have previous rationality training, this is where I expect it to benefit you the most. Your model of reality will be flawed, and you&#x27;ll need to fix it. Existing ability to notice biases and counter them should give you an advantage. </p><p>The advantage *may* come in reduced study time (I&#x27;d need a lot of data to confirm this) but mostly of the advantage will come from willingness to actually study effectively in the first place (this is a genuinely big deal). With minimal instruction, you could probably figure out where your biases lie and how to fix them. Some knowledge of cognitive science might even give you insight as to where and why they might be flawed. I&#x27;d be interested if someone without much drawing experience attempted to predict their biases in advance, once before having drawn at all in the recent past, and once again after one or two drawings. </p><p>It would still take a long time to do that, which is why in the next article I&#x27;m going to give away a lot of common answers. As a bonus rationality exercise you may want to predict those answers in advance. (For the benefit of others, leave your answers encrypted in the comments)</p><p>I&#x27;m not sure how to test for it, but I believe the skills here can transfer to other domains, if one deliberately made the attempt.</p><h1>Technical Skill:</h1><p>The ability to control your pencil, moving it in the direction you want it to go, applying different amounts of pressure to make it darker or lighter, thicker or thinner. As your observation and technical skill improve together, you&#x27;ll learn to identify important parts of reality, and use pencil techniques to emphasize them properly. It&#x27;s important to develop a minimal threshold of technical skill. But afterwards, it can wait until you&#x27;ve gotten a solid understanding of Observation, Energy and Weight. </p><p>Nine thousand hours practicing technical skill is what makes the difference between a competent amateur and a professional. A few of those hours will yield low hanging fruit, but not many. Rationality won&#x27;t be particularly relevant here. I&#x27;ll be doing one article that covers the basics, and later on I&#x27;ll link some online tutorials that may be useful *after* you&#x27;ve put in an initial twenty hours or so.</p><h1>Instilling Energy and Weight</h1><p>This is the most mysterious of the skill clusters. It&#x27;s (fairly) obvious to a lay person that they need to look at things and practice moving their pencil in order to improve at drawing. That&#x27;s what high school art students focus on. But their drawings feel flat, and lifeless. They&#x27;ll copy something from a photo and there won&#x27;t be anything obviously wrong. It&#x27;s just... boring somehow. </p><p>The problem was that the technical skill they developed focused on small, slow, precise movements. To instill energy and weight, you need to be able to draw big, bold lines, and to draw them quickly. They require a kind of hand-eye coordination (and more importantly, arm-eye coordination) that you&#x27;ve probably never developed. The first big, bold lines you draw will look hopelessly wrong against reality. You need to keep practicing, until your entire arm can do what your brain wants it to. Until then, much of the technical skill you acquire won&#x27;t be implemented correctly.</p><p>The harder part is the that to instill energy and weight, you may sometimes need to NOT draw the reality you see.[1] As this skill develops alongside technical knowledge, kinesthetic skill and observational ability, you&#x27;ll learn how to draw lines that differ from reality to make your drawing more interesting and creative, without sacrificing a realistic look that captures the original likeness. And you&#x27;ll need to start doing this with a part of your brain so small and underdeveloped that it doesn&#x27;t even show up on your model of yourself.</p><p>Your rationality training will probably not help you, because it doesn&#x27;t directly prepare for this sort of thing. The actual skills you&#x27;re developing here are kinesthetic, not high level cognition. But I think preparing for this sort of thing is a kind of instrumental rationality technique we *should* be working on. This weirdness you feel as you practice, the certainty that your teacher is screwing with you and that you should go back to the comfortable ways you&#x27;re familiar with... this is the feeling of your model being _wrong_, and butting up uncomfortably against reality of how your brain works. Not just getting a fact wrong. Not even failing to ask the right questions in the first place, but lacking the sense capabilities needed to even interact with the part of reality you needed to see in order to ask those questions. It&#x27;s having your decision-making algorithm *be* wrong on a gut level that can&#x27;t really be explained, only experienced.[2]</p><p>I think this is among the more important experiences that drawing offers[3], if the effort is made to understand it. Obviously, not every &quot;wrong&quot; feeling you ever have will stem from a model-reality mismatch. Sometimes when something feels wrong, it really is wrong. &quot;Wrong&quot; feelings are still evidence. But they are not absolute evidence. </p><p>Yes, I know you already &quot;knew&quot; that. But unless you&#x27;ve already had a similar experience, you probably didn&#x27;t really understand it on a gut level. Being able to recognize when you&#x27;re approaching things is a fundamentally wrong way, and be okay with it, is an incredibly important skill. And because each way of being fundamentally wrong is slightly different, we could use a variety of examples in order to better understand new ones in the future.</p><p>Existing Less Wrong literature has two areas that I consider to have helped build this skill for me. One was the overall &quot;woah&quot; moment when I realized I should actually incorporate the Singularity into my model of the world (Even this doesn&#x27;t really count, because the &quot;that can&#x27;t possibly be right&quot; feeling was accompanied by &quot;it would be<em> sooo cooool </em>if it were!&quot; instead of &quot;ugh this is painful to think about). The other was the emphasis that  <a href=\"https://www.lesserwrong.com/lw/pc/quantum_explanations/\">quantum physics isn&#x27;t weird, YOU are</a>. Those realizations may have more serious consequences on how I interpret the world, but the experiences themselves were weird on a similar level. Plenty of other content here <em>talks</em>  about this sort of thing, but there&#x27;s a limited number of ways to actually experience being wrong in this way. I think we need more of that. </p><p>Somewhere in your brain is an invisible mental-muscle that you never knew existed.[4] You&#x27;ll need to trust that it exists, and start practicing with it. Eventually you&#x27;ll be able to &quot;see&quot; that part of your mind, and you&#x27;ll incorporate it into your model of yourself.</p><p>And then you can learn how to make interesting depictions of reality.</p><p>Perhaps more importantly, you can learn to create Art\u2122.</p><hr class=\"dividerBlock\"/><p>[1] Many readers may notice that this statement... really makes no sense. That&#x27;s because it&#x27;s a simplification. I&#x27;ll explain the more complex rule that actually governs it in a later post. Feel free to speculate in the comments about why I chose to simplify it that way.</p><p>[2] I know that&#x27;s three posts in a row where I hint at the same mysterious thing. I&#x27;m building it up because it&#x27;s important and needs to be emphasized. You&#x27;re still not going to really understand it until a teacher makes you do it and helps explain some kinesthetic things that I&#x27;m going to have a hard time communicating through written text. Three posts from now I&#x27;ll attempt to explain it well enough that you&#x27;ll understand the teacher the first or second time e explains it, rather then after many frustrating hours.</p><p>[3] I do NOT recommend people learn to draw purely to experience this feeling of wrongness. In order for any of this to work you need to be instrinsically motivated.</p><p>[4] This may technically be untrue, if you HAVE already become aware of this mental-muscle and used it in some fashion. It may not even strike you as an odd thing, if you&#x27;ve had it for a while. But I&#x27;m willing to bet it&#x27;ll be a new thing for most people here. I actually think we&#x27;ll have a higher percentage of people who have a particularly hard time working with it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KDpqtN3MxHSmD4vcB": 2, "Ng8Gice9KNkncxqcj": 2, "fkABsGCJZ6y9qConW": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "shKSShMNtmjkgoBmQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 17, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "10933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "WPgA9x5ZvKu9oYvgB", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "drawing-less-wrong-observing-reality", "canonicalPrevPostSlug": "drawing-less-wrong-should-you-learn-to-draw", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>So, you've considered your past experiences and your motivations, and you've got a decent idea of the effort required of you: Six to eight hours of solid work before you start showing improvement, and about twenty hours total before you start to exhaust the low hanging fruit. You want to learn to draw. What exactly does that entail?</p><p>A lot of things, really. There's probably hundreds of subskills, techniques and bits of knowledge that go into creating a quality drawing. But I think they cluster into three main categories:</p><ul><li>Observation</li><li>Technical Skill</li><li>\"Instilling Energy and Weight\"</li></ul><p>Each of these skills is developed with different exercises, requiring different mindsets. Switching between those exercises can be difficult. Studying any of them can produce something that is interesting to look at, but ultimately you want to integrate them into a single, fluid mental process. You'll need to develop some competence in each of them first. As you begin, the most important thing to remember is that *learning* to draw is not the same as actually drawing. To improve as quickly as possible you may have to set aside the reasons you wanted to draw in the first place. Don't worry - you'll achieve those terminal goals in time.</p><p>In this article I'll briefly discuss each of those skill clusters, and how I believe they relate to rationality.</p><h1 id=\"Observation\">Observation</h1><p>The ability to see reality, as it truly is. \"Reality\" refers both to the physical objects and light that you're looking at, as well as to your perception of your own mental processes and how they should be interpreting those physical objects. If you have previous rationality training, this is where I expect it to benefit you the most. Your model of reality will be flawed, and you'll need to fix it. Existing ability to notice biases and counter them should give you an advantage. </p><p>The advantage *may* come in reduced study time (I'd need a lot of data to confirm this) but mostly of the advantage will come from willingness to actually study effectively in the first place (this is a genuinely big deal). With minimal instruction, you could probably figure out where your biases lie and how to fix them. Some knowledge of cognitive science might even give you insight as to where and why they might be flawed. I'd be interested if someone without much drawing experience attempted to predict their biases in advance, once before having drawn at all in the recent past, and once again after one or two drawings. </p><p>It would still take a long time to do that, which is why in the next article I'm going to give away a lot of common answers. As a bonus rationality exercise you may want to predict those answers in advance. (For the benefit of others, leave your answers encrypted in the comments)</p><p>I'm not sure how to test for it, but I believe the skills here can transfer to other domains, if one deliberately made the attempt.</p><h1 id=\"Technical_Skill_\">Technical Skill:</h1><p>The ability to control your pencil, moving it in the direction you want it to go, applying different amounts of pressure to make it darker or lighter, thicker or thinner. As your observation and technical skill improve together, you'll learn to identify important parts of reality, and use pencil techniques to emphasize them properly. It's important to develop a minimal threshold of technical skill. But afterwards, it can wait until you've gotten a solid understanding of Observation, Energy and Weight. </p><p>Nine thousand hours practicing technical skill is what makes the difference between a competent amateur and a professional. A few of those hours will yield low hanging fruit, but not many. Rationality won't be particularly relevant here. I'll be doing one article that covers the basics, and later on I'll link some online tutorials that may be useful *after* you've put in an initial twenty hours or so.</p><h1 id=\"Instilling_Energy_and_Weight\">Instilling Energy and Weight</h1><p>This is the most mysterious of the skill clusters. It's (fairly) obvious to a lay person that they need to look at things and practice moving their pencil in order to improve at drawing. That's what high school art students focus on. But their drawings feel flat, and lifeless. They'll copy something from a photo and there won't be anything obviously wrong. It's just... boring somehow. </p><p>The problem was that the technical skill they developed focused on small, slow, precise movements. To instill energy and weight, you need to be able to draw big, bold lines, and to draw them quickly. They require a kind of hand-eye coordination (and more importantly, arm-eye coordination) that you've probably never developed. The first big, bold lines you draw will look hopelessly wrong against reality. You need to keep practicing, until your entire arm can do what your brain wants it to. Until then, much of the technical skill you acquire won't be implemented correctly.</p><p>The harder part is the that to instill energy and weight, you may sometimes need to NOT draw the reality you see.[1] As this skill develops alongside technical knowledge, kinesthetic skill and observational ability, you'll learn how to draw lines that differ from reality to make your drawing more interesting and creative, without sacrificing a realistic look that captures the original likeness. And you'll need to start doing this with a part of your brain so small and underdeveloped that it doesn't even show up on your model of yourself.</p><p>Your rationality training will probably not help you, because it doesn't directly prepare for this sort of thing. The actual skills you're developing here are kinesthetic, not high level cognition. But I think preparing for this sort of thing is a kind of instrumental rationality technique we *should* be working on. This weirdness you feel as you practice, the certainty that your teacher is screwing with you and that you should go back to the comfortable ways you're familiar with... this is the feeling of your model being _wrong_, and butting up uncomfortably against reality of how your brain works. Not just getting a fact wrong. Not even failing to ask the right questions in the first place, but lacking the sense capabilities needed to even interact with the part of reality you needed to see in order to ask those questions. It's having your decision-making algorithm *be* wrong on a gut level that can't really be explained, only experienced.[2]</p><p>I think this is among the more important experiences that drawing offers[3], if the effort is made to understand it. Obviously, not every \"wrong\" feeling you ever have will stem from a model-reality mismatch. Sometimes when something feels wrong, it really is wrong. \"Wrong\" feelings are still evidence. But they are not absolute evidence. </p><p>Yes, I know you already \"knew\" that. But unless you've already had a similar experience, you probably didn't really understand it on a gut level. Being able to recognize when you're approaching things is a fundamentally wrong way, and be okay with it, is an incredibly important skill. And because each way of being fundamentally wrong is slightly different, we could use a variety of examples in order to better understand new ones in the future.</p><p>Existing Less Wrong literature has two areas that I consider to have helped build this skill for me. One was the overall \"woah\" moment when I realized I should actually incorporate the Singularity into my model of the world (Even this doesn't really count, because the \"that can't possibly be right\" feeling was accompanied by \"it would be<em> sooo cooool </em>if it were!\" instead of \"ugh this is painful to think about). The other was the emphasis that  <a href=\"https://www.lesserwrong.com/lw/pc/quantum_explanations/\">quantum physics isn't weird, YOU are</a>. Those realizations may have more serious consequences on how I interpret the world, but the experiences themselves were weird on a similar level. Plenty of other content here <em>talks</em>  about this sort of thing, but there's a limited number of ways to actually experience being wrong in this way. I think we need more of that. </p><p>Somewhere in your brain is an invisible mental-muscle that you never knew existed.[4] You'll need to trust that it exists, and start practicing with it. Eventually you'll be able to \"see\" that part of your mind, and you'll incorporate it into your model of yourself.</p><p>And then you can learn how to make interesting depictions of reality.</p><p>Perhaps more importantly, you can learn to create Art\u2122.</p><hr class=\"dividerBlock\"><p>[1] Many readers may notice that this statement... really makes no sense. That's because it's a simplification. I'll explain the more complex rule that actually governs it in a later post. Feel free to speculate in the comments about why I chose to simplify it that way.</p><p>[2] I know that's three posts in a row where I hint at the same mysterious thing. I'm building it up because it's important and needs to be emphasized. You're still not going to really understand it until a teacher makes you do it and helps explain some kinesthetic things that I'm going to have a hard time communicating through written text. Three posts from now I'll attempt to explain it well enough that you'll understand the teacher the first or second time e explains it, rather then after many frustrating hours.</p><p>[3] I do NOT recommend people learn to draw purely to experience this feeling of wrongness. In order for any of this to work you need to be instrinsically motivated.</p><p>[4] This may technically be untrue, if you HAVE already become aware of this mental-muscle and used it in some fashion. It may not even strike you as an odd thing, if you've had it for a while. But I'm willing to bet it'll be a new thing for most people here. I actually think we'll have a higher percentage of people who have a particularly hard time working with it.</p>", "sections": [{"title": "Observation", "anchor": "Observation", "level": 1}, {"title": "Technical Skill:", "anchor": "Technical_Skill_", "level": 1}, {"title": "Instilling Energy and Weight", "anchor": "Instilling_Energy_and_Weight", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FSwbFpDsca7uXpQ2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T05:11:20.102Z", "modifiedAt": null, "url": null, "title": "[Link] Cognitive bias modification as a treatment for depression", "slug": "link-cognitive-bias-modification-as-a-treatment-for", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.837Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/f77WkeyNFcxNgPjsB/link-cognitive-bias-modification-as-a-treatment-for", "pageUrlRelative": "/posts/f77WkeyNFcxNgPjsB/link-cognitive-bias-modification-as-a-treatment-for", "linkUrl": "https://www.lesswrong.com/posts/f77WkeyNFcxNgPjsB/link-cognitive-bias-modification-as-a-treatment-for", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Cognitive%20bias%20modification%20as%20a%20treatment%20for%20depression&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Cognitive%20bias%20modification%20as%20a%20treatment%20for%20depression%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff77WkeyNFcxNgPjsB%2Flink-cognitive-bias-modification-as-a-treatment-for%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Cognitive%20bias%20modification%20as%20a%20treatment%20for%20depression%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff77WkeyNFcxNgPjsB%2Flink-cognitive-bias-modification-as-a-treatment-for", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ff77WkeyNFcxNgPjsB%2Flink-cognitive-bias-modification-as-a-treatment-for", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>This seems relevant to LessWrong, both as an extreme example of how biases can hurt people and as a possible rationality technique. Depression is presumably at the outer end of some spectrum; to the extent that it's caused by cognitive mistakes, people in the middle of the spectrum should be able to benefit from undoing the same mistakes.&nbsp;</p>\n<p><a href=\"http://www.sciencedaily.com/releases/2011/11/111117202935.htm#.TsaYwil4AAg.reddit\">http://www.sciencedaily.com/releases/2011/11/111117202935.htm</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "f77WkeyNFcxNgPjsB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 8.01773700430994e-07, "legacy": true, "legacyId": "11018", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T19:26:25.777Z", "modifiedAt": null, "url": null, "title": "Don\u2019t Apply the Principle of Charity to Yourself", "slug": "don-t-apply-the-principle-of-charity-to-yourself", "viewCount": null, "lastCommentedAt": "2011-11-21T21:19:44.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "UnclGhost", "createdAt": "2010-09-11T07:31:23.816Z", "isAdmin": false, "displayName": "UnclGhost"}, "userId": "rqQJXhfF4TqhNnHiB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DwtsDbtPCzr9ArcxH/don-t-apply-the-principle-of-charity-to-yourself", "pageUrlRelative": "/posts/DwtsDbtPCzr9ArcxH/don-t-apply-the-principle-of-charity-to-yourself", "linkUrl": "https://www.lesswrong.com/posts/DwtsDbtPCzr9ArcxH/don-t-apply-the-principle-of-charity-to-yourself", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Don%E2%80%99t%20Apply%20the%20Principle%20of%20Charity%20to%20Yourself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADon%E2%80%99t%20Apply%20the%20Principle%20of%20Charity%20to%20Yourself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwtsDbtPCzr9ArcxH%2Fdon-t-apply-the-principle-of-charity-to-yourself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Don%E2%80%99t%20Apply%20the%20Principle%20of%20Charity%20to%20Yourself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwtsDbtPCzr9ArcxH%2Fdon-t-apply-the-principle-of-charity-to-yourself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwtsDbtPCzr9ArcxH%2Fdon-t-apply-the-principle-of-charity-to-yourself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 713, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-US</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:DontVertAlignCellWithSp /> <w:DontBreakConstrainedForcedTables /> <w:DontVertAlignInTxbx /> <w:Word11KerningPairs /> <w:CachedColBalance /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-qformat:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:\"Calibri\",\"sans-serif\"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-fareast-font-family:\"Times New Roman\"; mso-fareast-theme-font:minor-fareast; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin; mso-bidi-font-family:\"Times New Roman\"; mso-bidi-theme-font:minor-bidi;} --><!--[endif] --> In philosophy, the <a href=\"http://philosophy.lander.edu/oriental/charity.html\" target=\"_blank\">Principle of Charity</a> is a technique in which you evaluate your opponent&rsquo;s position as if it made the most amount of sense possible given the wording of the argument. That is, if you could interpret your opponent's argument in multiple ways, you would go for the most reasonable version. This is a good idea for several reasons. It counteracts the <a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\" target=\"_blank\">illusion of transparency</a> and <a href=\"/lw/hz/correspondence_bias/\" target=\"_blank\">correspondence bias</a>, it makes you look gracious, if your opponent really does believe a bad version of the argument sometimes he&rsquo;ll say so, and, most importantly, it helps you focus on getting to the truth, rather than just trying to win a debate.</p>\n<p class=\"MsoNormal\">Recently I was in a discussion online, and someone argued against a position I'd taken. Rather than evaluating his argument, I looked back at the posts I'd made. I realized that my previous posts would be just as coherent if I'd written them while believing a position that was slightly different from my real one, so I replied to my opponent <em>as if I had always believed the new position</em>. There was no textual evidence that showed that I hadn't. In essence, I got to accuse my opponent of using a strawman regardless of whether or not he actually was. It wasn't until much later that I realized I'd applied the Principle of Charity to myself.</p>\n<p class=\"MsoNormal\">Now, this is bad for basically every reason it's good to apply it to other people. You get undeserved status points for being good at arguing. You <em>exploit</em> the non-existence of transparency. It helps you <a href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\" target=\"_blank\">win a debate rather than trying to maintain consistent and true beliefs</a>. And maybe worst of all, if you're good enough at getting away with it, no one knows you're doing it but you... <em>and sometimes not even you</em>.</p>\n<p class=\"MsoNormal\">Like most bad argument techniques, I wasn't aware I was doing this at a conscious level. I've probably been doing it for a long time but just didn't recognize it. I'd <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\" target=\"_blank\">heard</a> about not giving yourself too much credit, and not just trying to \"win\" arguments, but I had no idea I was doing both of those in this particular way. I think it's likely that this habit started from realizing that posting your opinion doesn't give people a temporary flash of insight and the ability to look into your soul and see exactly what you mean<span class=\"st\">&mdash;</span><a href=\"/lw/ke/illusion_of_transparency_why_no_one_understands/\" target=\"_blank\">all they have to go by is the words</a>, and (what you hope are) connotations similar to your own. Once you've internalized this truth, be very careful not to abuse it and take advantage of the fact that people don't know that you <em>don't</em> always believe the best form of the argument.</p>\n<p class=\"MsoNormal\">It's also unfair to your opponent to make them think they've misunderstood your position when they haven't. If this happens enough, they could recalibrate their argument decoding techniques, when really they were accurate to start with, and you'll have made both of you that much worse at looking for the intended version of arguments.</p>\n<p class=\"MsoNormal\">Ideally, this would be frequently noticed, since you are in effect lying about a large construct of beliefs, and there's <a href=\"/lw/uw/entangled_truths_contagious_lies/\" target=\"_blank\">probably some inconsistency</a> between the new version and your past positions on the subject. Unfortunately though, most people aren't going to go back and check nearly as many of your past posts as you just did. If you suspect someone's doing this to you, and you're reasonably confident you don't just think so because of correspondence bias, read through their older posts (try not to go back too far though, in case they've just silently changed their mind). If that fails, it's risky, but you can try to call them on it by asking about their <a href=\"/lw/wj/is_that_your_true_rejection/\" target=\"_blank\">true rejection</a>.</p>\n<p class=\"MsoNormal\">How do you prevent yourself from doing this? If someone challenges your argument, <em>don't</em> look for ways by which you can (retroactively) have been right all along. Say \"Hm, I didn't think of that\", to both yourself and your opponent, and then suggest the new version of your argument <em>as</em> a new version. You'll be more transparent to both yourself and your opponent, which is vital for actually gaining something out of any debate.</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">tl;dr: If someone doesn't apply the Principle of Charity to you, <em>and they're right</em>, don't apply it to <em>yourself</em><span class=\"st\">&mdash;</span>realize that you might just have been wrong.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "MXcpQvaPGtXpB6vkM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DwtsDbtPCzr9ArcxH", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 73, "extendedScore": null, "score": 0.000162, "legacy": true, "legacyId": "11021", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["sSqoEw9eRP2kPKLCz", "DB6wbyrMugYMK5o6a", "dHQkDNMhj692ayx78", "AdYdLP2sRqPMoe8fb", "wyyfFfaRar2jEdeQK", "TGux5Fhcd7GmTfNGC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-11-19T19:26:25.777Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T20:01:36.818Z", "modifiedAt": null, "url": null, "title": "Political impasse as result of different odds ratios", "slug": "political-impasse-as-result-of-different-odds-ratios", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calcsam", "createdAt": "2011-04-30T17:07:18.622Z", "isAdmin": false, "displayName": "calcsam"}, "userId": "YpbtzJj8Qwi4PHGm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FbpsLiiCvGSy94zGb/political-impasse-as-result-of-different-odds-ratios", "pageUrlRelative": "/posts/FbpsLiiCvGSy94zGb/political-impasse-as-result-of-different-odds-ratios", "linkUrl": "https://www.lesswrong.com/posts/FbpsLiiCvGSy94zGb/political-impasse-as-result-of-different-odds-ratios", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Political%20impasse%20as%20result%20of%20different%20odds%20ratios&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitical%20impasse%20as%20result%20of%20different%20odds%20ratios%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbpsLiiCvGSy94zGb%2Fpolitical-impasse-as-result-of-different-odds-ratios%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Political%20impasse%20as%20result%20of%20different%20odds%20ratios%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbpsLiiCvGSy94zGb%2Fpolitical-impasse-as-result-of-different-odds-ratios", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFbpsLiiCvGSy94zGb%2Fpolitical-impasse-as-result-of-different-odds-ratios", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 447, "htmlBody": "<p><em>Related to: <a href=\"/lw/gw/politics_is_the_mindkiller\">Politics is the Mind-Killer</a></em></p>\n<p>Both sides seem to have a stake in the current budget supercommittee failing. Why?</p>\n<p>The NYTimes <a href=\"http://campaignstops.blogs.nytimes.com/2011/11/18/capitalizing-on-collapse/?hp\">reports</a>:</p>\n<blockquote>\n<p><a href=\"http://www.intrade.com/v4/home/\">Intrade</a>, the political futures market, currently puts the odds at just under three to one in favor of both a Republican takeover of the Senate and retention of the House &mdash; 74.4 to 21.5 for the Senate, 72.2 to 28 for the House.</p>\n<p>According to <a href=\"http://abcnews.go.com/blogs/politics/2011/10/majority-expects-obama-to-lose-re-election/\">an ABC poll</a>, a majority of Americans, by a margin of 55 to 37, believe that the Republican nominee will be victorious. Republican voters are overwhelmingly optimistic about their chances for the White House, 83-13. Democrats, by the far smaller margin of 58-33 percent, think President Obama will win re-election. Independents, by a 54-36 margin, believe that the Republicans will take the presidency.</p>\n<p>The chance of all three contests going the Republicans&rsquo; way is less than 50-50, but if they do, the payoff would be huge.*</p>\n<p>As a top Republican Congressional aide put it in a interview about the supercommittee&rsquo;s deliberations, &ldquo;Winning the trifecta &mdash; House, Senate and White House &mdash; in 2012 is a game changer. We would be in the driver&rsquo;s seat.&rdquo;</p>\n<p>In this scenario, Republicans in the 113th Congress would swiftly enact a version of the budget proposal put forward by Paul Ryan....</p>\n<p>Capitalizing on collapse is not the exclusive terrain of the right. There are some on the left who believe that simply taking no action whatsoever  before this year&rsquo;s November 23 and December 23 deadlines will force the expiration of the Bush tax cuts at the end of 2012. The expiration of these cuts will produce an estimated $3.8 trillion in new revenue between 2013 and 2022 &ndash; enough to maintain many of the key safety net programs with relatively minor tinkering.</p>\n<p>Of course, this strategy depends either on a Democratic chief executive to veto Republican legislation extending the Bush cuts or on the less likely event of Democratic retention of the Senate or a takeover of the House.</p>\n</blockquote>\n<p>In other words, the Republicans believe they can achieve complete victory so that they can enact their whole agenda, while Democrats believe they can block this victory.</p>\n<p>On a key component &mdash; Obama's re-election &mdash; Republicans believe they will defeat Obama with 1:6 odds, while Democrats believe this event has only 3:2 odds.**</p>\n<p>With a 9-fold difference in this key perception, it seems highly unlikely the two will reach a compromise.</p>\n<p>&nbsp;</p>\n<hr size=\"2\" />\n<p>* Not necessarily true &mdash; you can't just multiply 74%*72%*83% as these events have high positive correlations.</p>\n<p>** Yes, perhaps the people in power have different perceptions than the rank-and-file electorate &mdash; but they still must win their base's support to gain re-election.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FbpsLiiCvGSy94zGb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "11022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-19T22:24:36.269Z", "modifiedAt": null, "url": null, "title": "Meetup : Madison Monday Meetup", "slug": "meetup-madison-monday-meetup-3", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SFxFDYDyvoBDACHuM/meetup-madison-monday-meetup-3", "pageUrlRelative": "/posts/SFxFDYDyvoBDACHuM/meetup-madison-monday-meetup-3", "linkUrl": "https://www.lesswrong.com/posts/SFxFDYDyvoBDACHuM/meetup-madison-monday-meetup-3", "postedAtFormatted": "Saturday, November 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Madison%20Monday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Madison%20Monday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFxFDYDyvoBDACHuM%2Fmeetup-madison-monday-meetup-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Madison%20Monday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFxFDYDyvoBDACHuM%2Fmeetup-madison-monday-meetup-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSFxFDYDyvoBDACHuM%2Fmeetup-madison-monday-meetup-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2><span style=\"font-size: small; font-weight: normal;\"><strong>WHEN:</strong> <span class=\"date\">21 November 2011 06:30:00PM (-0600)</span></span></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHERE:</strong> <span class=\"address\">1831 Monroe St. Madison, WI 53711</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>This will be largely a social meetup - we'll get together at the Barriques on Monroe and chat. We'll probably play a game or two, and have an argument or two.</p>\n<p>We'll also chat about the upcoming Sequences reading group. If you have a strong opinion about what to read through, you should show up. ;)</p>\n<p>And, again, if you're in the area, you should sign up for the Madison <a href=\"https://groups.google.com/forum/#!forum/lesswrong-madison\">mailing list</a>.</p>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SFxFDYDyvoBDACHuM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 8.021410186860458e-07, "legacy": true, "legacyId": "11023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-20T03:21:16.068Z", "modifiedAt": null, "url": null, "title": "Meetup : Next Atlanta Less Wrong Meetup", "slug": "meetup-next-atlanta-less-wrong-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7Ghau5ctGmjxJWBYs/meetup-next-atlanta-less-wrong-meetup", "pageUrlRelative": "/posts/7Ghau5ctGmjxJWBYs/meetup-next-atlanta-less-wrong-meetup", "linkUrl": "https://www.lesswrong.com/posts/7Ghau5ctGmjxJWBYs/meetup-next-atlanta-less-wrong-meetup", "postedAtFormatted": "Sunday, November 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Next%20Atlanta%20Less%20Wrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Next%20Atlanta%20Less%20Wrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ghau5ctGmjxJWBYs%2Fmeetup-next-atlanta-less-wrong-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Next%20Atlanta%20Less%20Wrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ghau5ctGmjxJWBYs%2Fmeetup-next-atlanta-less-wrong-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7Ghau5ctGmjxJWBYs%2Fmeetup-next-atlanta-less-wrong-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/54'>Next Atlanta Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 December 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first Atlanta Less Wrong meetup was a great success... everyone who came was really cool, and we had twice as many people show up as expected.</p>\n\n<p>Our next meetup is Saturday, December 10th at 6pm at Chocolate' Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a>\n2094 North Decatur Road, Decatur, GA 30033-5367\n(404) 982-0790</p>\n\n<p>There is no RSVP, so feel free to come by if you are interested, and feel free to send me a message if you have any questions/suggestions.</p>\n\n<p>Also, do sign up for our mailing list: <a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/54'>Next Atlanta Less Wrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7Ghau5ctGmjxJWBYs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.02246534913647e-07, "legacy": true, "legacyId": "11024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Next_Atlanta_Less_Wrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/54\">Next Atlanta Less Wrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 December 2011 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2094 North Decatur Road, Decatur, GA 30033-5367</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Our first Atlanta Less Wrong meetup was a great success... everyone who came was really cool, and we had twice as many people show up as expected.</p>\n\n<p>Our next meetup is Saturday, December 10th at 6pm at Chocolate' Coffee in Decatur:</p>\n\n<p><a href=\"http://www.mychocolatecoffee.com/\" rel=\"nofollow\">http://www.mychocolatecoffee.com/</a>\n2094 North Decatur Road, Decatur, GA 30033-5367\n(404) 982-0790</p>\n\n<p>There is no RSVP, so feel free to come by if you are interested, and feel free to send me a message if you have any questions/suggestions.</p>\n\n<p>Also, do sign up for our mailing list: <a href=\"http://groups.google.com/group/atlanta-less-wrong-meetup-group\" rel=\"nofollow\">http://groups.google.com/group/atlanta-less-wrong-meetup-group</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Next_Atlanta_Less_Wrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/54\">Next Atlanta Less Wrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Next Atlanta Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Next_Atlanta_Less_Wrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Next Atlanta Less Wrong Meetup", "anchor": "Discussion_article_for_the_meetup___Next_Atlanta_Less_Wrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-20T04:23:44.233Z", "modifiedAt": null, "url": null, "title": "How to write a mathematical formula on the fear of death?", "slug": "how-to-write-a-mathematical-formula-on-the-fear-of-death", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:31.710Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheatreAddict", "createdAt": "2011-05-01T17:03:11.634Z", "isAdmin": false, "displayName": "TheatreAddict"}, "userId": "xtL2cZS7kaFnSNLus", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NfyJirMA33RCRFKhp/how-to-write-a-mathematical-formula-on-the-fear-of-death", "pageUrlRelative": "/posts/NfyJirMA33RCRFKhp/how-to-write-a-mathematical-formula-on-the-fear-of-death", "linkUrl": "https://www.lesswrong.com/posts/NfyJirMA33RCRFKhp/how-to-write-a-mathematical-formula-on-the-fear-of-death", "postedAtFormatted": "Sunday, November 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20write%20a%20mathematical%20formula%20on%20the%20fear%20of%20death%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20write%20a%20mathematical%20formula%20on%20the%20fear%20of%20death%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNfyJirMA33RCRFKhp%2Fhow-to-write-a-mathematical-formula-on-the-fear-of-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20write%20a%20mathematical%20formula%20on%20the%20fear%20of%20death%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNfyJirMA33RCRFKhp%2Fhow-to-write-a-mathematical-formula-on-the-fear-of-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNfyJirMA33RCRFKhp%2Fhow-to-write-a-mathematical-formula-on-the-fear-of-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p><span style=\"white-space: pre;\"> </span>I was reading the Methods of Rationality, and I was reading the part about how it's irrational to fear death. Well I came across \"<em style=\"font-family: Verdana; font-size: 12px; line-height: 15px; text-align: left;\">All x: Die(x) = Not Exist x: Not Die(x)\" </em><span style=\"font-family: Verdana; font-size: 12px; line-height: 15px; text-align: left;\">I really don't get this.. I'm sorry, I'm not good at math. But does \"x\" here represent an unknown variable? If so, is it being like, multiplied when it's put in parenthesis? Could this be put into a simpler equation?</span></p>\n<p style=\"text-align: left;\"><span style=\"font-family: Verdana;\"><span style=\"font-size: 12px; line-height: 15px;\"><span style=\"white-space: pre;\"> </span>Because I totally get the part where you either have to want to keep living, because I want to live right now, I'll want to live tomorrow, so therefore I'll want to live forever. And then if I want to not live forever, it would mean that I don't really want to live very much.. Right?</span></span></p>\n<p style=\"text-align: left;\"><span style=\"font-family: Verdana;\"><span style=\"font-size: 12px; line-height: 15px;\"><span style=\"white-space: pre;\"> </span>This is what happens when someone who hasn't a clue about math and science reads a smart fanfiction. But if someone could either verify the part about \"</span></span><em style=\"font-family: Verdana; font-size: 12px; line-height: 15px;\">All x: Die(x) = Not Exist x: Not Die(x)\" </em><span style=\"font-family: Verdana; font-size: 12px; line-height: 15px;\">being the correct formula, and then explaining why, that would be like, really cool.</span></p>\n<p style=\"text-align: left;\"><span style=\"font-family: Verdana; font-size: 12px; line-height: 15px;\"><span style=\"white-space: pre;\"> </span>Thanks! :D</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NfyJirMA33RCRFKhp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 8.022687569557785e-07, "legacy": true, "legacyId": "11025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-20T04:36:27.959Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Robbers Cave Experiment", "slug": "seq-rerun-the-robbers-cave-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.246Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/37Lyhqv37qGgkDMWa/seq-rerun-the-robbers-cave-experiment", "pageUrlRelative": "/posts/37Lyhqv37qGgkDMWa/seq-rerun-the-robbers-cave-experiment", "linkUrl": "https://www.lesswrong.com/posts/37Lyhqv37qGgkDMWa/seq-rerun-the-robbers-cave-experiment", "postedAtFormatted": "Sunday, November 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Robbers%20Cave%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Robbers%20Cave%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37Lyhqv37qGgkDMWa%2Fseq-rerun-the-robbers-cave-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Robbers%20Cave%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37Lyhqv37qGgkDMWa%2Fseq-rerun-the-robbers-cave-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F37Lyhqv37qGgkDMWa%2Fseq-rerun-the-robbers-cave-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>Today's post, <a href=\"/lw/lt/the_robbers_cave_experiment/\">The Robbers Cave Experiment</a> was originally published on 10 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The Robbers Cave Experiment, by Sherif, Harvey, White, Hood, and Sherif (1954/1961), was designed to investigate the causes and remedies of problems between groups. Twenty-two middle school aged boys were divided into two groups and placed in a summer camp. From the first time the groups learned of each other's existence, a brutal rivalry was started. The only way the counselors managed to bring the groups together was by giving the two groups a common enemy. Any resemblance to modern politics is just your imagination.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8i0/seq_rerun_when_none_dare_urge_restraint/\">When None Dare Urge Restraint</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "37Lyhqv37qGgkDMWa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 8.022732850473264e-07, "legacy": true, "legacyId": "11026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["MBpj3QKfPg9xKNeXW", "bXz8PmHkWtZgEp2fi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-20T05:18:08.193Z", "modifiedAt": null, "url": null, "title": "Connecting Your Beliefs (a call for help)", "slug": "connecting-your-beliefs-a-call-for-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.366Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kHL6qX9eArmvNWY99/connecting-your-beliefs-a-call-for-help", "pageUrlRelative": "/posts/kHL6qX9eArmvNWY99/connecting-your-beliefs-a-call-for-help", "linkUrl": "https://www.lesswrong.com/posts/kHL6qX9eArmvNWY99/connecting-your-beliefs-a-call-for-help", "postedAtFormatted": "Sunday, November 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Connecting%20Your%20Beliefs%20(a%20call%20for%20help)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AConnecting%20Your%20Beliefs%20(a%20call%20for%20help)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHL6qX9eArmvNWY99%2Fconnecting-your-beliefs-a-call-for-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Connecting%20Your%20Beliefs%20(a%20call%20for%20help)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHL6qX9eArmvNWY99%2Fconnecting-your-beliefs-a-call-for-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkHL6qX9eArmvNWY99%2Fconnecting-your-beliefs-a-call-for-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 749, "htmlBody": "<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/bayesian-belief-network.jpg\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/bayesian-belief-network-small.png\" alt=\"\" /></a>A couple weeks after meeting me, <a href=\"/user/Will_Newsome/\">Will Newsome</a> gave me one of the best compliments I&rsquo;ve ever received. He said: &ldquo;Luke seems to have two copies of the <a href=\"/lw/2l6/taking_ideas_seriously/\">Take Ideas Seriously</a> gene.&rdquo;</p>\n<p>What did Will mean? To take an idea seriously is &ldquo;to update a belief and then accurately and completely propagate that belief update through the entire web of beliefs in which it is embedded,&rdquo; as in a <a href=\"http://en.wikipedia.org/wiki/Belief_propagation\">Bayesian belief network</a> (see right).</p>\n<p>Belief propagation is what happened, for example, when I first encountered that thundering paragraph from <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\">I.J. Good (1965)</a>:</p>\n<blockquote>\n<p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind... Thus the first ultraintelligent machine is the last invention that man need ever make.</p>\n</blockquote>\n<p>Good&rsquo;s paragraph ran me over like a train. Not because it was absurd, but because it was clearly true. Intelligence explosion was a direct consequence of things I already believed, I just hadn&rsquo;t noticed! Humans do not automatically propagate their beliefs, so I hadn&rsquo;t noticed that my worldview already implied intelligence explosion.</p>\n<p>I spent a week looking for counterarguments, to check whether I was missing something, and then accepted intelligence explosion to be likely (so long as scientific progress continued). And though I hadn&rsquo;t read Eliezer on the <a href=\"/lw/y3/value_is_fragile/\">complexity of value</a>, I <em>had</em> read <a href=\"http://en.wikipedia.org/wiki/David_Hume#Ethics\">David Hume</a> and <a href=\"http://www.wjh.harvard.edu/~jgreene/GreeneWJH/Greene-Dissertation.pdf\">Joshua Greene</a>. So I already understood that an arbitrary artificial intelligence would almost certainly not share our values.</p>\n<p>Accepting my belief update about intelligence explosion, I propagated its implications throughout my web of beliefs. I realized that:</p>\n<ul>\n<li>Things can go <em>very</em> wrong, for we live in a world <a href=\"/lw/uk/beyond_the_reach_of_god/\">beyond the reach of God</a>.</li>\n<li>Scientific progress can destroy the world.</li>\n<li>Strong <a href=\"http://en.wikipedia.org/wiki/Technological_determinism\">technological determinism</a> is true; purely social factors will be swamped by technology.</li>\n<li>Writing about philosophy of religion was not important enough to consume any more of my time.</li>\n<li>My highest-utility actions are either those that work toward reducing AI risk, or those that work toward making lots of money so I can <em>donate</em> to AI risk reduction.</li>\n<li>Moral theory is not idle speculation but an <a href=\"http://commonsenseatheism.com/?p=14013\">urgent engineering problem</a>.</li>\n<li>Technological utopia is possible, but unlikely.</li>\n<li>The value of information concerning intelligence explosion scenarios is extremely high.</li>\n<li>Rationality is even <em>more</em> important than I already believed it was.</li>\n<li>and more.</li>\n</ul>\n<p>I had encountered the I.J. Good paragraph on Less Wrong, so I put my other projects on hold and spent the next month reading almost <a href=\"http://commonsenseatheism.com/?p=13052\">everything</a> Eliezer had written. I also found articles by <a href=\"http://nickbostrom.com/\">Nick Bostrom</a> and <a href=\"http://steveomohundro.com/\">Steve Omohundro</a>. I began writing articles for Less Wrong and learning from the community. I applied to <a href=\"http://intelligence.org/\">Singularity Institute</a>&rsquo;s Visiting Fellows program and was accepted. I quit my job in L.A., moved to Berkeley, worked my ass off, got hired, and started collecting research related to <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">rationality</a> and <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">intelligence explosion</a>.</p>\n<p>My story surprises people because it <em>is</em> unusual. Human brains don&rsquo;t usually propagate new beliefs so thoroughly.</p>\n<p>But this isn&rsquo;t just another post on taking ideas seriously. Will <a href=\"/lw/2l6/taking_ideas_seriously/\">already offered</a> some ideas on <em>how</em> to propagate beliefs. He also listed some ideas that most people probably aren&rsquo;t taking seriously enough. My purpose here is to examine one prerequisite of successful belief propagation: <em>actually making sure your beliefs are connected to each other in the first place</em>.</p>\n<p>If your beliefs aren&rsquo;t connected to each other, there may be no paths along which you <em>can</em> propagate a new belief update.</p>\n<p>I&rsquo;m not talking about the problem of <a href=\"http://wiki.lesswrong.com/wiki/Free-floating_belief\">free-floating beliefs</a> that don&rsquo;t <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">control your anticipations</a>. No, I&rsquo;m talking about &ldquo;<a href=\"http://wiki.lesswrong.com/wiki/Improper_belief\">proper</a>&rdquo; beliefs that <a href=\"http://wiki.lesswrong.com/wiki/Beliefs_require_observations\">require observation</a>, can be <a href=\"http://wiki.lesswrong.com/wiki/Belief_update\">updated</a> by evidence, and <a href=\"http://wiki.lesswrong.com/wiki/Beliefs_pay_rent\">pay rent</a> in anticipated experiences. The trouble is that even <em>proper</em> beliefs can be inadequately connected to other proper beliefs inside the human mind.</p>\n<p>I wrote this post because I'm not sure what the \"making sure your beliefs are actually connected in the first place\" skill <em>looks like</em> when broken down to the <a href=\"/lw/5kz/the_5second_level/\">5-second level</a>.</p>\n<p>I was chatting about this with <a href=\"/user/atucker/\">atucker</a>, who told me he noticed that successful businessmen may have this trait more often than others. But what are they doing, at the 5-second level? What are people like Eliezer and <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl</a> doing? How does one engage in the purposeful decompartmentalization of one's own mind?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qoTbWwaJtTSKosRCA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kHL6qX9eArmvNWY99", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 34, "extendedScore": null, "score": 7e-05, "legacy": true, "legacyId": "11027", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 73, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q8jyAdRYbieK8PtfT", "GNnHHmm8EzePmKzPk", "sYgv4eYH82JEsTD34", "xLm9mgJRPvmPGpo7Q", "hN2aRnu798yas5b2k", "a7n8GdKiAZRX86T5A", "JcpzFpPBSmzuksmWM", "4uQxZonCwCZtz39Hw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-20T09:45:47.784Z", "modifiedAt": null, "url": null, "title": "Practical debiasing", "slug": "practical-debiasing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:26.698Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "crazy88", "createdAt": "2011-09-02T04:50:28.550Z", "isAdmin": false, "displayName": "crazy88"}, "userId": "JjaJBbvGJE6D6FYvF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rrjCeQLopeHXicAZ6/practical-debiasing", "pageUrlRelative": "/posts/rrjCeQLopeHXicAZ6/practical-debiasing", "linkUrl": "https://www.lesswrong.com/posts/rrjCeQLopeHXicAZ6/practical-debiasing", "postedAtFormatted": "Sunday, November 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Practical%20debiasing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APractical%20debiasing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrjCeQLopeHXicAZ6%2Fpractical-debiasing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Practical%20debiasing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrjCeQLopeHXicAZ6%2Fpractical-debiasing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrrjCeQLopeHXicAZ6%2Fpractical-debiasing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1568, "htmlBody": "<p>Some of this post is an expansion of topics <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">covered by Lukeprog here</a> <br /><br /><strong>1. Knowing about biases (doesn't stop you being biased)</strong><br /><br />Imagine you had to teach a course that would help people to become less biased. What would you teach? A natural idea, tempting enough in theory, might be that you should teach the students about all of the biases that influence their decision making. Once someone knows that they suffer from overconfidence in their ability to predict future events, surely they will adjust their confidence accordingly.<br /><br />Readers of Less Wrong will be aware that <a title=\"Knowing about biases can hurt people\" href=\"/lw/he/knowing_about_biases_can_hurt_people/\">it's more complicated than that</a>.<br /><br />There is a mass of research showing that knowing about cognitive biases does not stop someone from being biased. Quattrone et. al. (1981) showed that <a title=\"Anchoring and adjustment\" href=\"/lw/j7/anchoring_and_adjustment/\">anchoring effects</a> are not decreased by instructing subjects to avoid the bias. Similarly, Pohl et. al. (1996) demonstrate that the same applies to the <a title=\"Hindsight bias\" href=\"/lw/il/hindsight_bias/\">hindsight bias</a>. Finally, Arzy et al (2009) showed that including a misleading detail in a description of a medical case significantly decreased diagnostic accuracy. Accuracy does not improve if doctors are warned that such information may be present.<br /><br /><strong>2. Consider the opposite (but not too much)</strong><br /><br />So what does lead to debiasing? As <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">Lukeprog mentioned</a> one well supported tactic is that of \"consider the opposite\", which involves simply considering some reasons that an initial judgment might be incorrect. This has been shown to help counter overconfidence and hindsight bias as well as anchoring. See, for example, Arkes (1991) or Mussweiler et. al. (2000) for studies along this line.<br /><br />There are two more things worth noting about this tactic. The first is that Soll and Klayman (2004) have demonstrated that a related tactic has positive results in relation to overconfidence. In their experiment, Soll and Klayman asked subjects to give an interval such that they are 80% sure that the answer to a question lay within this interval. So they asked for predictions of things like the birth year of Oliver Cromwell and the subjects would need to provide an early year and a late year such that they were 80% sure that Cromwell was born somewhere between there two years. These subjects exhibited substantial overconfidence - they were right far less than 80% of the time.<br /><br />However, another group of subjects were asked two questions. For the first, they were asked to pick a year such that they were 90% sure Cromwell wasn't born before this year. For the second, they were asked to pick a year such that they were 90% sure that Cromwell wasn't born after this year. Subjects still displayed overconfidence in response to this question but to a far more minor extent. But the two questions are equivalent (<em>eta: though see <a title=\"this comment\" href=\"/lw/7ep/practical_debiasing/5ah1\">this comment</a></em>)! Being forced to consider arguments for both ends of the interval seemed to lead to more accurate prediction. Further studies have attempted to improve on this result through more sophisticated tactics along the same lines (see, for example, Andrew Speirs-Bridge et. al., 2009)</p>\n<p>The second thing worth noting is that considering too many reasons that an initial judgement might be incorrect is counterproductive (see Roese, 2004 or Sanna et. al. 2002). After a certain point, it becomes increasingly difficult for a person to generate reasons they might have been incorrect. This then serves to convince them that their idea must be right, otherwise it would be easier to come up with reasons against the claim. At this point, the technique ceases to have a debiasing effect. While the exact number of reasons that one should consider is likely to differ from case to case, Sanna et. al. (2002) found a debiasing effect when subjects were asked to consider 2 reasons against their initial conclusion but not when they were asked to consider 10. Consequently, it seems plausible that the ideal number of arguments to consider will be closer to 2 than 10.</p>\n<p>So consider the opposite but not too much.</p>\n<p><br /><strong>3. Provide reasons</strong></p>\n<p>There is also evidence that providing reasons for your decision or judgement can help to mitigate biases. Arkes et. al. (1988) demonstrated that, in relation to hindsight bias, asking for a rationale for a judgement can help debias that judgement.</p>\n<p>Similar research has been demonstrated in relation to <a title=\"Framing Effects in Anthropology\" href=\"/lw/6k/framing_effects_in_anthropology/\">framing effects</a>. Miller and Fagley (1991) presented participants with a series of scenarios about how to respond to a disease outbreak. One group was then presented with a positive frame while one was presented with a negative frame. This framing influenced the program of response that the participants selected. In other words, those in the negative frame group selected responses with a different frequency to those in the positive frame group despite the scenario being the same. However, if the groups were asked to provide a reason for their decision, then both groups selected responses at about the same frequency (However, Sieck and Yates (1997) demonstrated that this approach does not work in relation to all types of framing questions).</p>\n<p>So provide reasons for your decisions.</p>\n<p><strong>4. Get some training</strong></p>\n<p>There is also evidence that some biases <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">can be trained away</a>. Specifically, Larrick et. al. (1990) has shown that the <a title=\"sunk cost fallacy\" href=\"/lw/at/sunk_cost_fallacy/\">sunk cost fallacy</a> can be avoided by training and Fong et. al. (1986) has presented similar research with regards to judgements about sample variability.</p>\n<p>Larrick (2004) claims that this training is most effective when an abstract principle is taught along with concrete examples. He also suggested that the training should involve examples showing how the principle works in context. The process of training involves not just learning the rule but also figuring out when to apply it and then (hopefully) coming to apply it automatically.</p>\n<p>This seems like the sort of thing that could potentially be run in the discussion section of Less Wrong or at face to face meetups.</p>\n<p><strong>5. Reference class forecasting</strong><br /><br />The final technique I want to discuss is reference class forecasting which has been discussed by both <a title=\"Beware the inside view\" href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\">Robin</a> and <a title=\"Outside view as conversation halter\" href=\"/lw/1p5/outside_view_as_conversationhalter/\">Eliezer</a>. On Less Wrong, this topic is often discussed in terms of the inside and the outside view. Reference class forecasting is basically the idea that in predicting how long a project should take, one should not try to figure out how long each component of the project will take but should instead ask how long it has taken you (or others) to complete similar tasks in the past.</p>\n<p>This approach has been shown to be effective in overcoming <a title=\"Planning fallacy\" href=\"/lw/jg/planning_fallacy/\">the planning fallacy</a>. For example, Osberg and Shrauger (1986) demonstrated that those instructed to consider their performance in similar cases in the past were better able to predict their performance in new projects.</p>\n<p>So in predicting how long a task will take, use the outside not the inside view.</p>\n<p><strong>6. Concluding remarks<br /></strong></p>\n<p>I'm sure there's nothing here that will surprise most Less Wrong readers but I hope that having it all together in one place is useful. For anyone who's interested, I got a lot of the information for this post from Richard P. Larrick's article, 'Debiasing' in the <em>Blackwell Handbook of Judgment and Decision Making</em> which is a good book all round.</p>\n<p><strong>References</strong><br /><br />Arkes, H.R. 1991, 'Costs and benefits of judgement errors: Implications for debiasing', <em>Psychological Bulletin</em>, vol. 110, no. 3, pp. 486-498</p>\n<p>Arkes, H.R., Faust, D., Guilmette, T.J., &amp; Hart, K. 1988, 'Eliminating the Hindsight Bias', <em>Journal of Applied Psychology</em>, vol. <span>73, pp. 305-307</span></p>\n<p>Fong, G. T., Krantz, D. H., &amp; Nisbett, R. E. 1986, '<a rel=\"nofollow\" href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/26118/1/0000194.pdf\">The effects of statistical training on thinking about everyday problems.</a>', <em>Cognitive Psychology</em>, 18, 253-292.</p>\n<p>Larrick, R.P. 2004, 'Debiasing', in <em>Blackwell Handbook of Judgment and Decision Making, </em>Blackwell Publishing, Oxford, pp. 316-337.</p>\n<p>Miller, P.M. &amp; Fagley, N.S. 1991, 'The Effects of Framing, Problem Variations, and Providing Rationale on Choice', Personality and Social Psychology Bulletin, vol. 17, no. 5, pp. 517-522.</p>\n<p>Mussweiler, T. Strack, F. &amp; Pfeiffer, T. 2000, 'Overcoming the Inevitable Anchoring Effect: Considering the Opposite Compensates for Selective Accessibility', <em>Personality and Social Psychology Bulletin</em>, vol. 26, no. 9, pp. 1142-1150</p>\n<p>Osberg, T. M., &amp; Shrauger, J. S. 1986, 'Self-prediction: Exploring the parameters of accuracy', <em>Journal of Personality<br />and Social Psychology</em>, vol. 51,no. 5, pp. 1044-1057.</p>\n<p>Pohl, R.F. &amp; Hell, W. 1996, 'No reduction in Hindsight Bias after Complete Information and repeated Testing', <em>Organizational Behaviour and Human Decision Processes</em>, vol. 67, no. 1, pp. 49-58.<br /><br />Quattrone, G.A. Lawrence, C.P. Finkel, S.E. &amp; Andrus, D.C. 1981, Explorations in anchoring: The effects of prior range, anchor extremity, and suggestive hints. Manuscript, Stanford University.</p>\n<p>Roese, N.J. 2004, 'Twisted Pair: Counterfactual Thinking and the Hindsight Bias', in Blackwell Handbook of Judgment and Decision Making, Blackwell Publishing, Oxford, pp. 258-273.</p>\n<p>Sanna, L.J., Schwarz, N., Stocker, S.L. 2002, 'When Debiasing Backfires: Accessible Content and Accessibility Experiences in Debiasing Hindsight', <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, vol. 28, no. 3, pp. 497-502.</p>\n<p>Soll, J.B. &amp; Klayman, J. 2004, 'Overconfidence in Interval Estimates', <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, vol. 30, no. 2, pp. 299-314</p>\n<p><span class=\"TextDarkGreySmall\">Speirs-Bridge, A., Fidler, F<strong>.</strong>, McBride, M., Flander, L., Cumming, G. &amp; Burgman, M. 2009, 'Reducing overconfidence in the interval judgements of experts', <em>Risk Analysis,</em></span><span class=\"TextDarkGreySmall\"> vol. 30, no. 3, pp. 512&nbsp;&ndash;&nbsp;523</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4R8JYu4QF2FqzJxE5": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rrjCeQLopeHXicAZ6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 38, "extendedScore": null, "score": 8.5e-05, "legacy": true, "legacyId": "9601", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Some of this post is an expansion of topics <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">covered by Lukeprog here</a> <br><br><strong>1. Knowing about biases (doesn't stop you being biased)</strong><br><br>Imagine you had to teach a course that would help people to become less biased. What would you teach? A natural idea, tempting enough in theory, might be that you should teach the students about all of the biases that influence their decision making. Once someone knows that they suffer from overconfidence in their ability to predict future events, surely they will adjust their confidence accordingly.<br><br>Readers of Less Wrong will be aware that <a title=\"Knowing about biases can hurt people\" href=\"/lw/he/knowing_about_biases_can_hurt_people/\">it's more complicated than that</a>.<br><br>There is a mass of research showing that knowing about cognitive biases does not stop someone from being biased. Quattrone et. al. (1981) showed that <a title=\"Anchoring and adjustment\" href=\"/lw/j7/anchoring_and_adjustment/\">anchoring effects</a> are not decreased by instructing subjects to avoid the bias. Similarly, Pohl et. al. (1996) demonstrate that the same applies to the <a title=\"Hindsight bias\" href=\"/lw/il/hindsight_bias/\">hindsight bias</a>. Finally, Arzy et al (2009) showed that including a misleading detail in a description of a medical case significantly decreased diagnostic accuracy. Accuracy does not improve if doctors are warned that such information may be present.<br><br><strong>2. Consider the opposite (but not too much)</strong><br><br>So what does lead to debiasing? As <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">Lukeprog mentioned</a> one well supported tactic is that of \"consider the opposite\", which involves simply considering some reasons that an initial judgment might be incorrect. This has been shown to help counter overconfidence and hindsight bias as well as anchoring. See, for example, Arkes (1991) or Mussweiler et. al. (2000) for studies along this line.<br><br>There are two more things worth noting about this tactic. The first is that Soll and Klayman (2004) have demonstrated that a related tactic has positive results in relation to overconfidence. In their experiment, Soll and Klayman asked subjects to give an interval such that they are 80% sure that the answer to a question lay within this interval. So they asked for predictions of things like the birth year of Oliver Cromwell and the subjects would need to provide an early year and a late year such that they were 80% sure that Cromwell was born somewhere between there two years. These subjects exhibited substantial overconfidence - they were right far less than 80% of the time.<br><br>However, another group of subjects were asked two questions. For the first, they were asked to pick a year such that they were 90% sure Cromwell wasn't born before this year. For the second, they were asked to pick a year such that they were 90% sure that Cromwell wasn't born after this year. Subjects still displayed overconfidence in response to this question but to a far more minor extent. But the two questions are equivalent (<em>eta: though see <a title=\"this comment\" href=\"/lw/7ep/practical_debiasing/5ah1\">this comment</a></em>)! Being forced to consider arguments for both ends of the interval seemed to lead to more accurate prediction. Further studies have attempted to improve on this result through more sophisticated tactics along the same lines (see, for example, Andrew Speirs-Bridge et. al., 2009)</p>\n<p>The second thing worth noting is that considering too many reasons that an initial judgement might be incorrect is counterproductive (see Roese, 2004 or Sanna et. al. 2002). After a certain point, it becomes increasingly difficult for a person to generate reasons they might have been incorrect. This then serves to convince them that their idea must be right, otherwise it would be easier to come up with reasons against the claim. At this point, the technique ceases to have a debiasing effect. While the exact number of reasons that one should consider is likely to differ from case to case, Sanna et. al. (2002) found a debiasing effect when subjects were asked to consider 2 reasons against their initial conclusion but not when they were asked to consider 10. Consequently, it seems plausible that the ideal number of arguments to consider will be closer to 2 than 10.</p>\n<p>So consider the opposite but not too much.</p>\n<p><br><strong>3. Provide reasons</strong></p>\n<p>There is also evidence that providing reasons for your decision or judgement can help to mitigate biases. Arkes et. al. (1988) demonstrated that, in relation to hindsight bias, asking for a rationale for a judgement can help debias that judgement.</p>\n<p>Similar research has been demonstrated in relation to <a title=\"Framing Effects in Anthropology\" href=\"/lw/6k/framing_effects_in_anthropology/\">framing effects</a>. Miller and Fagley (1991) presented participants with a series of scenarios about how to respond to a disease outbreak. One group was then presented with a positive frame while one was presented with a negative frame. This framing influenced the program of response that the participants selected. In other words, those in the negative frame group selected responses with a different frequency to those in the positive frame group despite the scenario being the same. However, if the groups were asked to provide a reason for their decision, then both groups selected responses at about the same frequency (However, Sieck and Yates (1997) demonstrated that this approach does not work in relation to all types of framing questions).</p>\n<p>So provide reasons for your decisions.</p>\n<p><strong id=\"4__Get_some_training\">4. Get some training</strong></p>\n<p>There is also evidence that some biases <a title=\"Is Rationality Teachable\" href=\"/lw/76x/is_rationality_teachable/\">can be trained away</a>. Specifically, Larrick et. al. (1990) has shown that the <a title=\"sunk cost fallacy\" href=\"/lw/at/sunk_cost_fallacy/\">sunk cost fallacy</a> can be avoided by training and Fong et. al. (1986) has presented similar research with regards to judgements about sample variability.</p>\n<p>Larrick (2004) claims that this training is most effective when an abstract principle is taught along with concrete examples. He also suggested that the training should involve examples showing how the principle works in context. The process of training involves not just learning the rule but also figuring out when to apply it and then (hopefully) coming to apply it automatically.</p>\n<p>This seems like the sort of thing that could potentially be run in the discussion section of Less Wrong or at face to face meetups.</p>\n<p><strong>5. Reference class forecasting</strong><br><br>The final technique I want to discuss is reference class forecasting which has been discussed by both <a title=\"Beware the inside view\" href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\">Robin</a> and <a title=\"Outside view as conversation halter\" href=\"/lw/1p5/outside_view_as_conversationhalter/\">Eliezer</a>. On Less Wrong, this topic is often discussed in terms of the inside and the outside view. Reference class forecasting is basically the idea that in predicting how long a project should take, one should not try to figure out how long each component of the project will take but should instead ask how long it has taken you (or others) to complete similar tasks in the past.</p>\n<p>This approach has been shown to be effective in overcoming <a title=\"Planning fallacy\" href=\"/lw/jg/planning_fallacy/\">the planning fallacy</a>. For example, Osberg and Shrauger (1986) demonstrated that those instructed to consider their performance in similar cases in the past were better able to predict their performance in new projects.</p>\n<p>So in predicting how long a task will take, use the outside not the inside view.</p>\n<p><strong id=\"6__Concluding_remarks\">6. Concluding remarks<br></strong></p>\n<p>I'm sure there's nothing here that will surprise most Less Wrong readers but I hope that having it all together in one place is useful. For anyone who's interested, I got a lot of the information for this post from Richard P. Larrick's article, 'Debiasing' in the <em>Blackwell Handbook of Judgment and Decision Making</em> which is a good book all round.</p>\n<p><strong>References</strong><br><br>Arkes, H.R. 1991, 'Costs and benefits of judgement errors: Implications for debiasing', <em>Psychological Bulletin</em>, vol. 110, no. 3, pp. 486-498</p>\n<p>Arkes, H.R., Faust, D., Guilmette, T.J., &amp; Hart, K. 1988, 'Eliminating the Hindsight Bias', <em>Journal of Applied Psychology</em>, vol. <span>73, pp. 305-307</span></p>\n<p>Fong, G. T., Krantz, D. H., &amp; Nisbett, R. E. 1986, '<a rel=\"nofollow\" href=\"http://deepblue.lib.umich.edu/bitstream/2027.42/26118/1/0000194.pdf\">The effects of statistical training on thinking about everyday problems.</a>', <em>Cognitive Psychology</em>, 18, 253-292.</p>\n<p>Larrick, R.P. 2004, 'Debiasing', in <em>Blackwell Handbook of Judgment and Decision Making, </em>Blackwell Publishing, Oxford, pp. 316-337.</p>\n<p>Miller, P.M. &amp; Fagley, N.S. 1991, 'The Effects of Framing, Problem Variations, and Providing Rationale on Choice', Personality and Social Psychology Bulletin, vol. 17, no. 5, pp. 517-522.</p>\n<p>Mussweiler, T. Strack, F. &amp; Pfeiffer, T. 2000, 'Overcoming the Inevitable Anchoring Effect: Considering the Opposite Compensates for Selective Accessibility', <em>Personality and Social Psychology Bulletin</em>, vol. 26, no. 9, pp. 1142-1150</p>\n<p>Osberg, T. M., &amp; Shrauger, J. S. 1986, 'Self-prediction: Exploring the parameters of accuracy', <em>Journal of Personality<br>and Social Psychology</em>, vol. 51,no. 5, pp. 1044-1057.</p>\n<p>Pohl, R.F. &amp; Hell, W. 1996, 'No reduction in Hindsight Bias after Complete Information and repeated Testing', <em>Organizational Behaviour and Human Decision Processes</em>, vol. 67, no. 1, pp. 49-58.<br><br>Quattrone, G.A. Lawrence, C.P. Finkel, S.E. &amp; Andrus, D.C. 1981, Explorations in anchoring: The effects of prior range, anchor extremity, and suggestive hints. Manuscript, Stanford University.</p>\n<p>Roese, N.J. 2004, 'Twisted Pair: Counterfactual Thinking and the Hindsight Bias', in Blackwell Handbook of Judgment and Decision Making, Blackwell Publishing, Oxford, pp. 258-273.</p>\n<p>Sanna, L.J., Schwarz, N., Stocker, S.L. 2002, 'When Debiasing Backfires: Accessible Content and Accessibility Experiences in Debiasing Hindsight', <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, vol. 28, no. 3, pp. 497-502.</p>\n<p>Soll, J.B. &amp; Klayman, J. 2004, 'Overconfidence in Interval Estimates', <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, vol. 30, no. 2, pp. 299-314</p>\n<p><span class=\"TextDarkGreySmall\">Speirs-Bridge, A., Fidler, F<strong>.</strong>, McBride, M., Flander, L., Cumming, G. &amp; Burgman, M. 2009, 'Reducing overconfidence in the interval judgements of experts', <em>Risk Analysis,</em></span><span class=\"TextDarkGreySmall\"> vol. 30, no. 3, pp. 512&nbsp;\u2013&nbsp;523</span></p>", "sections": [{"title": "4. Get some training", "anchor": "4__Get_some_training", "level": 1}, {"title": "6. Concluding remarks", "anchor": "6__Concluding_remarks", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "13 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["H2zKAfiSJR6WJQ8pn", "AdYdLP2sRqPMoe8fb", "bMkCEZoBNhgRBtzoj", "fkM9XsNvXdYH6PPAx", "655TmdcwAgryPGPWS", "tyMdPwd8x2RygcheE", "FsfnDfADftGDYeG4c", "CPm5LTwHrvBJCa9h5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T01:34:10.514Z", "modifiedAt": null, "url": null, "title": "Meetup : Sheridan College - Oakville, ON", "slug": "meetup-sheridan-college-oakville-on", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ataftoti", "createdAt": "2011-09-08T05:30:10.496Z", "isAdmin": false, "displayName": "ataftoti"}, "userId": "85QPpLm8F5FxshmwJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XZiC4q8SyjnTA8fvq/meetup-sheridan-college-oakville-on", "pageUrlRelative": "/posts/XZiC4q8SyjnTA8fvq/meetup-sheridan-college-oakville-on", "linkUrl": "https://www.lesswrong.com/posts/XZiC4q8SyjnTA8fvq/meetup-sheridan-college-oakville-on", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Sheridan%20College%20-%20Oakville%2C%20ON&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Sheridan%20College%20-%20Oakville%2C%20ON%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZiC4q8SyjnTA8fvq%2Fmeetup-sheridan-college-oakville-on%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Sheridan%20College%20-%20Oakville%2C%20ON%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZiC4q8SyjnTA8fvq%2Fmeetup-sheridan-college-oakville-on", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXZiC4q8SyjnTA8fvq%2Fmeetup-sheridan-college-oakville-on", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 104, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/55'>Sheridan College - Oakville, ON</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">25 November 2011 08:30:58PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">S-Wing, Sheridan College, 1430 Trafalgar Road, Oakville, Ontario</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I wish to do something to promote rationality on the campus...perhaps under the friendly name of a \"philosophy\" club. I do not expect a high chance of another LW reader on campus, but I called this meetup in any case, before I attempt to start any rationality clubs by myself. If you are in the area, please post below or send me an email: ataftoti at gmail dot com.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/55'>Sheridan College - Oakville, ON</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XZiC4q8SyjnTA8fvq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.027209222083962e-07, "legacy": true, "legacyId": "11028", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Sheridan_College___Oakville__ON\">Discussion article for the meetup : <a href=\"/meetups/55\">Sheridan College - Oakville, ON</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">25 November 2011 08:30:58PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">S-Wing, Sheridan College, 1430 Trafalgar Road, Oakville, Ontario</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I wish to do something to promote rationality on the campus...perhaps under the friendly name of a \"philosophy\" club. I do not expect a high chance of another LW reader on campus, but I called this meetup in any case, before I attempt to start any rationality clubs by myself. If you are in the area, please post below or send me an email: ataftoti at gmail dot com.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Sheridan_College___Oakville__ON1\">Discussion article for the meetup : <a href=\"/meetups/55\">Sheridan College - Oakville, ON</a></h2>", "sections": [{"title": "Discussion article for the meetup : Sheridan College - Oakville, ON", "anchor": "Discussion_article_for_the_meetup___Sheridan_College___Oakville__ON", "level": 1}, {"title": "Discussion article for the meetup : Sheridan College - Oakville, ON", "anchor": "Discussion_article_for_the_meetup___Sheridan_College___Oakville__ON1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T01:46:47.798Z", "modifiedAt": null, "url": null, "title": "Rare and Valuable skills worth mastering?", "slug": "rare-and-valuable-skills-worth-mastering", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.026Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CJa43wHM7AMuAKLMn/rare-and-valuable-skills-worth-mastering", "pageUrlRelative": "/posts/CJa43wHM7AMuAKLMn/rare-and-valuable-skills-worth-mastering", "linkUrl": "https://www.lesswrong.com/posts/CJa43wHM7AMuAKLMn/rare-and-valuable-skills-worth-mastering", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rare%20and%20Valuable%20skills%20worth%20mastering%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARare%20and%20Valuable%20skills%20worth%20mastering%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJa43wHM7AMuAKLMn%2Frare-and-valuable-skills-worth-mastering%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rare%20and%20Valuable%20skills%20worth%20mastering%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJa43wHM7AMuAKLMn%2Frare-and-valuable-skills-worth-mastering", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCJa43wHM7AMuAKLMn%2Frare-and-valuable-skills-worth-mastering", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 79, "htmlBody": "<p>Hey everyone I just read <a href=\"/r/discussion/lw/8hd/might_i_ask_for_some_advice/\">this</a>, which in turn lead me to <a href=\"http://calnewport.com/blog/2010/01/23/beyond-passion-the-science-of-loving-what-you-do/\">this&nbsp;</a>and I'm curious as to what are some skills and fields of study&nbsp;that people think fit the category of 'rare and valuable'.</p>\r\n<p>Not just in white collar environments either, but also considering the entertainment industry, hospitality, trades and so forth.<br /><br />I've noticed anything that involves interfacing tends to have good dividends, especially if you can freelance.<br /><br />Would love to hear your thoughts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CJa43wHM7AMuAKLMn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 8, "extendedScore": null, "score": 8.027254165657135e-07, "legacy": true, "legacyId": "11029", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hj2yFSK7AMKrAp52k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T02:27:26.456Z", "modifiedAt": null, "url": null, "title": "Meetup : Monthly Bay Area meetup", "slug": "meetup-monthly-bay-area-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.495Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SdS8ex2ppWhBtyQL6/meetup-monthly-bay-area-meetup", "pageUrlRelative": "/posts/SdS8ex2ppWhBtyQL6/meetup-monthly-bay-area-meetup", "linkUrl": "https://www.lesswrong.com/posts/SdS8ex2ppWhBtyQL6/meetup-monthly-bay-area-meetup", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Monthly%20Bay%20Area%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Monthly%20Bay%20Area%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdS8ex2ppWhBtyQL6%2Fmeetup-monthly-bay-area-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Monthly%20Bay%20Area%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdS8ex2ppWhBtyQL6%2Fmeetup-monthly-bay-area-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSdS8ex2ppWhBtyQL6%2Fmeetup-monthly-bay-area-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/56'>Monthly Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 November 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next monthly Less Wrong meetup will be Saturday, November 26, at\n7pm, in Berkeley. We'll gather in the Starbucks at 2128 Oxford Street,\nand then walk to the Free Speech Cafe on campus. See you there!</p>\n\n<p>-Nisan</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/56'>Monthly Bay Area meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SdS8ex2ppWhBtyQL6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.027398899831098e-07, "legacy": true, "legacyId": "11030", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup\">Discussion article for the meetup : <a href=\"/meetups/56\">Monthly Bay Area meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 November 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The next monthly Less Wrong meetup will be Saturday, November 26, at\n7pm, in Berkeley. We'll gather in the Starbucks at 2128 Oxford Street,\nand then walk to the Free Speech Cafe on campus. See you there!</p>\n\n<p>-Nisan</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup1\">Discussion article for the meetup : <a href=\"/meetups/56\">Monthly Bay Area meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Monthly Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup", "level": 1}, {"title": "Discussion article for the meetup : Monthly Bay Area meetup", "anchor": "Discussion_article_for_the_meetup___Monthly_Bay_Area_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T03:51:25.309Z", "modifiedAt": null, "url": null, "title": "Meetup : Ottawa Weekly Monday LessWrong Meetup", "slug": "meetup-ottawa-weekly-monday-lesswrong-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "amacfie", "createdAt": "2010-09-28T21:46:49.325Z", "isAdmin": false, "displayName": "amacfie"}, "userId": "wwoK3MFWTk9bmPJuW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MbuLanmQjPm3MHRLk/meetup-ottawa-weekly-monday-lesswrong-meetup-0", "pageUrlRelative": "/posts/MbuLanmQjPm3MHRLk/meetup-ottawa-weekly-monday-lesswrong-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/MbuLanmQjPm3MHRLk/meetup-ottawa-weekly-monday-lesswrong-meetup-0", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMbuLanmQjPm3MHRLk%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Ottawa%20Weekly%20Monday%20LessWrong%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMbuLanmQjPm3MHRLk%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMbuLanmQjPm3MHRLk%2Fmeetup-ottawa-weekly-monday-lesswrong-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/57'>Ottawa Weekly Monday LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">21 November 2011 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Pub Italia: 434 Preston St, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing more sections from Why Philosophers Should Care About Computational Complexity by Scott Aaronson.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/57'>Ottawa Weekly Monday LessWrong Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MbuLanmQjPm3MHRLk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.027697969954943e-07, "legacy": true, "legacyId": "11031", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup\">Discussion article for the meetup : <a href=\"/meetups/57\">Ottawa Weekly Monday LessWrong Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">21 November 2011 07:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Pub Italia: 434 Preston St, Ottawa, ON</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be discussing more sections from Why Philosophers Should Care About Computational Complexity by Scott Aaronson.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/57\">Ottawa Weekly Monday LessWrong Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Ottawa Weekly Monday LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Ottawa Weekly Monday LessWrong Meetup", "anchor": "Discussion_article_for_the_meetup___Ottawa_Weekly_Monday_LessWrong_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T05:10:57.685Z", "modifiedAt": null, "url": null, "title": "Drawing Less Wrong: Observing Reality", "slug": "drawing-less-wrong-observing-reality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:23.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3ve2pEZS2dBmxbZS2/drawing-less-wrong-observing-reality", "pageUrlRelative": "/posts/3ve2pEZS2dBmxbZS2/drawing-less-wrong-observing-reality", "linkUrl": "https://www.lesswrong.com/posts/3ve2pEZS2dBmxbZS2/drawing-less-wrong-observing-reality", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Less%20Wrong%3A%20Observing%20Reality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Less%20Wrong%3A%20Observing%20Reality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ve2pEZS2dBmxbZS2%2Fdrawing-less-wrong-observing-reality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Less%20Wrong%3A%20Observing%20Reality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ve2pEZS2dBmxbZS2%2Fdrawing-less-wrong-observing-reality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3ve2pEZS2dBmxbZS2%2Fdrawing-less-wrong-observing-reality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2142, "htmlBody": "<p><a href=\"https://www.lesserwrong.com/lw/gv/outside_the_laboratory/\">To draw a city, you must walk around that city and look at it.</a> </p><p>You can&#x27;t sit in a room with the blinds closed and create a map and expect it to be accurate. You cannot draw what you cannot see. To draw things, you need to look a things. This is surprisingly hard for a few reasons.</p><p>One is that you may want to be drawing imaginary things. I&#x27;ll talk about this at length in the a later post. For now, let me just say that you can&#x27;t *learn* to draw realistically (even realistic fantasy) by drawing things that aren&#x27;t real. </p><p>Another reason is that when people begin, they do not have very good hand-eye coordination. Your can&#x27;t trust your hand to move on its own - you feel like you must be watching it the entire time, staring intently at the pen and paper and making sure they&#x27;re doing what you want them to. Coupled with this is a gross overconfidence in how good your memory is.</p><p>The third, and most significant reason, is that you don&#x27;t know how to see in the first place.</p><h1>How you will probably draw</h1><h2>(even knowing this is how you will probably draw)</h2><p>Inexperienced artists will take a look at their subject, create a mental map of them in their brain, and then turn back to their paper. Slowly, carefully, they draw lines corresponding to that mental representation. Occasionally they may look up - but in the moment they look up, their finger slips and they draw a line that&#x27;s completely inaccurate. This reinforces the mistrust in their hand-eye coordination, so they spend more and more time focusing down on their paper.</p><p>At the same time, as their drawing takes shape, it starts to look interesting. They&#x27;ve already stopped looking at the reality in front of them, but now they start getting distracted from their mental model. If you&#x27;re drawing a human, you might get get focused on their eyes. You spend a lot of time on them, and they become the new map from which you navigate. You draw the nose or the lips based on where you drew eyes, the chin based on the lips. Your pencil journeys through a map by following a map which was following a map. You work from one small, interesting area to another, never considering how the drawing will work as a whole. Occasionally you&#x27;ll think back to the original mental model in your head, but it will have grown fuzzy by this point.</p><p>And all this time, you probably drew slowly, using small, careful lines. Because after all, your hand-eye coordination is untrustworthy, and you wouldn&#x27;t want to draw something too big and sloppy.</p><p>Minutes go by. Eventually you&#x27;ll look back up at the real territory that was in front of you, and all the lines will be off. You drew the legs crossed when they were wide apart. The elbow is pointed up rather than sideways. The lips, nose and eyes, rather than forming a straight line up the center of the face, are crooked - each one slightly off, and referencing the lines of the previous one until they had little relation to the actual face. </p><p>Your conscious mind won&#x27;t process most of this. The drawing will just feel &quot;off&quot;.</p><p>In that moment, you&#x27;re in the middle of a kinesthetic process that *felt* like the right way to do it, so you probably won&#x27;t realize the obvious problem: you should never have been drawing from a mental representation in the first place, you should have been drawing directly from the reality in front of you.</p><h1>How You Should Actually Draw</h1><p>An unfortunate truth for beginners is that you must spend most of your time looking at your subject, and that you must also spend most of your time actually drawing. But you don&#x27;t have the technical skill necessary to do both of those things at once without your drawings looking horrendous.</p><p>(First off, be okay with your drawings looking horrendous. You&#x27;re building new skills from scratch. Your drawings WILL look messy. That&#x27;s fine.)</p><p>To start with, develop a habit of spending at least half of your time looking at your subject, switching back and forth every 2-3 seconds. If you&#x27;ve spent more than 5 seconds looking at your drawing, it&#x27;s time to look back up and make sure the lips you&#x27;re drawing are in the right spot compared against the REAL chin and the REAL eyes. This actually isn&#x27;t too hard, except that you&#x27;ll forget a lot. Having a teacher to remind you to stop looking at the paper will be helpful. If you don&#x27;t have a teacher handy, find a person to draw and ask them to remind you to look at them if they notice you staring at the page too much.</p><p>A step up from this is to practice making pencil strokes *while* you&#x27;re not looking at the paper. Eventually you want to be able to do this for extended periods of time. For now, a good technique is to allow yourself to look at the page as often as you want, but *<em>only* </em>make marks while you&#x27;re looking at your subject. (In practice this also means drawing for 2-3 seconds at a time, and glancing down to make sure your pencil hasn&#x27;t gotten lost)</p><p>Again, your first several strokes following this technique may come out very off. Don&#x27;t worry about that. </p><h1>The problem with Mapmakers, and Territory</h1><p>So, one big problem is that you aren&#x27;t going to naturally look at things, and your underdeveloped coordination will reinforce this.</p><p>But there&#x27;s another problem - a huge problem. Which is that even when you&#x27;re looking at something, you&#x27;re usually not actually &quot;seeing&quot; it.</p><p>Human brains take a lot of shortcuts when they&#x27;re observing things. When you look at a person, what you perceive is not a series of shapes and colors that correspond to what&#x27;s there, but rather a bunch of hastily constructed symbols that convey the information that the brain thinks is important. If you haven&#x27;t rewired your brain for drawing, then &quot;important&quot; questions do not include <em>&quot;Is that elbow angled at 90 degrees or 75?&quot; </em>or <em>&quot;Where are the eyes in relation to the top of the head?&quot; </em>Instead, what you usually care about are things like &quot;is this person happy, or angry?&quot; and the information that gets recorded is a little tag that says &quot;Smiling&quot; with a vague curving-upwards-line symbol accompanying it.</p><p>A large chunk of the information we usually need has to do with the face. This plays a role in two common biases that are near-universal in inexperienced artists:</p><ul><li>Drawing the head much larger than it actually is, compared to the rest of the body</li><li>Drawing the &quot;face&quot; (i.e. everything between the eyebrows and mouth) as if they took up the entire head rather the bottom half. Practically everything above the eyebrows conveys no relevant information, so it&#x27;s just ignored.</li></ul><p>Your brain has a mental model of what a human is &quot;supposed&quot; to look like, and that model is wrong. You can see major gains in drawing capability just by learning the &quot;ideal&quot; proportions of a human being. Most humans are shaped pretty similarly. But I&#x27;m hesitant to just give you that information, because it can actually be damaging. A few reasons why:</p><ol><li>Not all people are shaped the same</li><li>&quot;Shapes&quot; change dramatically depending on how a person is posed. </li><li>You probably want to learn to draw things other than humans, at some point.</li></ol><p>It&#x27;s not good enough to create a more accurate model of what people are &quot;supposed&quot; to be. You need to look at a subject and discard all your preconceived notions of what they are &quot;supposed&quot; to look like, along with all the symbols and names that your verbal center assigns to the pieces. You need to ignore the little tags that say &quot;face&quot; and &quot;arms&quot; and &quot;hands&quot; and &quot;torso,&quot; and instead see the lines, shapes, colors and shadows that are there in reality.</p><h2>&quot;Drawing on the Right Side of the Brain&quot; (Again)</h2><p>Much of our knowledge of how this works and how to improve the process is relatively recent. In 1969, an art teacher named Betty Edwards was frustrated with her students, some of whom were having extreme difficulty. They could clearly see where things were, but that knowledge didn&#x27;t translate into the drawing. </p><blockquote><em>&quot;Can you see that the orange in this still life is clearly in front of the vase?&quot;</em></blockquote><blockquote><em>&quot;Yes.&quot;</em></blockquote><blockquote><em>&quot;Well, in your drawing, you have the orange and the vase occupying the same space.&quot;</em></blockquote><blockquote><em>&quot;Yeah I know. I didn&#x27;t know how to draw that.&quot;</em></blockquote><p>On a whim, she asked students to copy a painting while it was upside down. There was an immediate, dramatic improvement. </p><blockquote><em>&quot;How can you draw upside down when you can&#x27;t draw right side up!?&quot;</em></blockquote><blockquote><em>&quot;Upside down, we didn&#x27;t know what we were drawing!&quot;</em></blockquote><p>That experiment prompted a series of questions and investigations that led Edwards to the neuroscience of the time, which suggested that humans used two major processing centers, located in different hemispheres of the brain. The &quot;left brain&quot; dealt with verbal and analytic processing. The &quot;right brain&quot; dealt with visual and perceptual processing. Many people naturally draw using their left brain, which wants to name things and refer to existing knowledge about them. Edwards developed a series of techniques to suppress the left brain so that right brain processes can take over. She published a book in 1979 called &quot;Drawing on the Right Side of the Brain,&quot; that discussed her research and the accompanying exercises.</p><p>(Later on, neuroscientists learned that while the two processing centers are real, they are not neatly divided between brain hemispheres. The modern edition of the book uses the terms &quot;left mode&quot; and &quot;right mode&quot; to distinguish between the modes of thought)</p><p>The book has become extremely influential within the art field, and in recent years Edwards has worked to find ways to transfer &quot;right mode thinking&quot; into areas beyond drawing. In corporate seminars (typically lasting three days), she spends the first day and a half teaching employees how to observe and draw. The second day and half, she helps them outline workplace problems using drawn visual metaphors, which allow them see things from multiple perspectives and stumble upon solutions that seemed obvious in retrospect, but which had gone unnoticed (in one case, for decades).</p><p>Edwards breaks down drawing into five main subskills:</p><ul><li>The perception of edges</li><li>The perception of spaces</li><li>The perception of relationships</li><li>The perception of light and shadow</li><li>The perception of the whole, or gestalt</li></ul><p>She considers these the building blocks necessary to develop two final skills: Drawing from memory, and drawing from your imagination.</p><h2>Integrating Observation and &quot;Gestural Thought&quot;</h2><p>Eventually, your goal is to be able to do observe near instantly. The 30-second gesture drawings are important because they train you to evaluate and draw in one fluid stroke of thought and pencil. In less than five seconds you should be able to draw a line that describes the general size and shape of the body, and a small ovoid shape that describes the relative size of the head. </p><p>But that will be difficult, until you&#x27;ve practiced several slower-paced exercises that develop your ability to see reality and compare objects to each other. In my first workshop I realized that it&#x27;s difficult to develop &quot;gestural thought&quot; unless you can do one of the following:</p><ol><li>Construct a mental model of your subject in a matter of seconds (and be able to revise it on the fly)</li><li>Have a pre-existing model (i.e. &quot;ideal proportions&quot;) to use as your starting point, which you then fix as you have time to observe in more detail.</li><li>Spend time observing your subject, creating a specific model of them, before you begin drawing.</li></ol><p>I&#x27;m still experimenting with the ideal order to present various exercises, to develop sufficient observational skill as quickly as possible. As I noted in the last installment, observation and &quot;gestural thought&quot; require different types of thinking that initially will be hard to switch between. Giving them time to gel independently is important, but I believe students should learn to integrate them as quickly as possible. My students and I both made progress in second workshop (mostly by using the third technique listed above), and there is definitely room for improvement.</p><p>In a future post I&#x27;ll be outlining some specific exercises to develop observation. Several of which will be lifted directly from Edwards&#x27; book, others developed from my teaching experiences. Developing your observation will, initially, involve many slow-paced exercises. Some focus on helping you break down the barrier between &quot;left mode&quot; and &quot;right mode&quot; thinking, but typically won&#x27;t be used as often when creating a polished piece of art. Others specifically develop the skills you&#x27;ll use to observe during &quot;regular&quot; drawing. </p><p>The goal will be to develop a baseline competence in observation, so that you can continue to develop it simultaneously alongside your ability to work quickly and energetically.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KDpqtN3MxHSmD4vcB": 1, "fkABsGCJZ6y9qConW": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3ve2pEZS2dBmxbZS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 51, "extendedScore": null, "score": 0.000112, "legacy": true, "legacyId": "11017", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "WPgA9x5ZvKu9oYvgB", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "drawing-less-wrong-technical-skill", "canonicalPrevPostSlug": "drawing-less-wrong-overview-of-skills-and-relevance-to", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 51, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"https://www.lesserwrong.com/lw/gv/outside_the_laboratory/\">To draw a city, you must walk around that city and look at it.</a> </p><p>You can't sit in a room with the blinds closed and create a map and expect it to be accurate. You cannot draw what you cannot see. To draw things, you need to look a things. This is surprisingly hard for a few reasons.</p><p>One is that you may want to be drawing imaginary things. I'll talk about this at length in the a later post. For now, let me just say that you can't *learn* to draw realistically (even realistic fantasy) by drawing things that aren't real. </p><p>Another reason is that when people begin, they do not have very good hand-eye coordination. Your can't trust your hand to move on its own - you feel like you must be watching it the entire time, staring intently at the pen and paper and making sure they're doing what you want them to. Coupled with this is a gross overconfidence in how good your memory is.</p><p>The third, and most significant reason, is that you don't know how to see in the first place.</p><h1 id=\"How_you_will_probably_draw\">How you will probably draw</h1><h2 id=\"_even_knowing_this_is_how_you_will_probably_draw_\">(even knowing this is how you will probably draw)</h2><p>Inexperienced artists will take a look at their subject, create a mental map of them in their brain, and then turn back to their paper. Slowly, carefully, they draw lines corresponding to that mental representation. Occasionally they may look up - but in the moment they look up, their finger slips and they draw a line that's completely inaccurate. This reinforces the mistrust in their hand-eye coordination, so they spend more and more time focusing down on their paper.</p><p>At the same time, as their drawing takes shape, it starts to look interesting. They've already stopped looking at the reality in front of them, but now they start getting distracted from their mental model. If you're drawing a human, you might get get focused on their eyes. You spend a lot of time on them, and they become the new map from which you navigate. You draw the nose or the lips based on where you drew eyes, the chin based on the lips. Your pencil journeys through a map by following a map which was following a map. You work from one small, interesting area to another, never considering how the drawing will work as a whole. Occasionally you'll think back to the original mental model in your head, but it will have grown fuzzy by this point.</p><p>And all this time, you probably drew slowly, using small, careful lines. Because after all, your hand-eye coordination is untrustworthy, and you wouldn't want to draw something too big and sloppy.</p><p>Minutes go by. Eventually you'll look back up at the real territory that was in front of you, and all the lines will be off. You drew the legs crossed when they were wide apart. The elbow is pointed up rather than sideways. The lips, nose and eyes, rather than forming a straight line up the center of the face, are crooked - each one slightly off, and referencing the lines of the previous one until they had little relation to the actual face. </p><p>Your conscious mind won't process most of this. The drawing will just feel \"off\".</p><p>In that moment, you're in the middle of a kinesthetic process that *felt* like the right way to do it, so you probably won't realize the obvious problem: you should never have been drawing from a mental representation in the first place, you should have been drawing directly from the reality in front of you.</p><h1 id=\"How_You_Should_Actually_Draw\">How You Should Actually Draw</h1><p>An unfortunate truth for beginners is that you must spend most of your time looking at your subject, and that you must also spend most of your time actually drawing. But you don't have the technical skill necessary to do both of those things at once without your drawings looking horrendous.</p><p>(First off, be okay with your drawings looking horrendous. You're building new skills from scratch. Your drawings WILL look messy. That's fine.)</p><p>To start with, develop a habit of spending at least half of your time looking at your subject, switching back and forth every 2-3 seconds. If you've spent more than 5 seconds looking at your drawing, it's time to look back up and make sure the lips you're drawing are in the right spot compared against the REAL chin and the REAL eyes. This actually isn't too hard, except that you'll forget a lot. Having a teacher to remind you to stop looking at the paper will be helpful. If you don't have a teacher handy, find a person to draw and ask them to remind you to look at them if they notice you staring at the page too much.</p><p>A step up from this is to practice making pencil strokes *while* you're not looking at the paper. Eventually you want to be able to do this for extended periods of time. For now, a good technique is to allow yourself to look at the page as often as you want, but *<em>only* </em>make marks while you're looking at your subject. (In practice this also means drawing for 2-3 seconds at a time, and glancing down to make sure your pencil hasn't gotten lost)</p><p>Again, your first several strokes following this technique may come out very off. Don't worry about that. </p><h1 id=\"The_problem_with_Mapmakers__and_Territory\">The problem with Mapmakers, and Territory</h1><p>So, one big problem is that you aren't going to naturally look at things, and your underdeveloped coordination will reinforce this.</p><p>But there's another problem - a huge problem. Which is that even when you're looking at something, you're usually not actually \"seeing\" it.</p><p>Human brains take a lot of shortcuts when they're observing things. When you look at a person, what you perceive is not a series of shapes and colors that correspond to what's there, but rather a bunch of hastily constructed symbols that convey the information that the brain thinks is important. If you haven't rewired your brain for drawing, then \"important\" questions do not include <em>\"Is that elbow angled at 90 degrees or 75?\" </em>or <em>\"Where are the eyes in relation to the top of the head?\" </em>Instead, what you usually care about are things like \"is this person happy, or angry?\" and the information that gets recorded is a little tag that says \"Smiling\" with a vague curving-upwards-line symbol accompanying it.</p><p>A large chunk of the information we usually need has to do with the face. This plays a role in two common biases that are near-universal in inexperienced artists:</p><ul><li>Drawing the head much larger than it actually is, compared to the rest of the body</li><li>Drawing the \"face\" (i.e. everything between the eyebrows and mouth) as if they took up the entire head rather the bottom half. Practically everything above the eyebrows conveys no relevant information, so it's just ignored.</li></ul><p>Your brain has a mental model of what a human is \"supposed\" to look like, and that model is wrong. You can see major gains in drawing capability just by learning the \"ideal\" proportions of a human being. Most humans are shaped pretty similarly. But I'm hesitant to just give you that information, because it can actually be damaging. A few reasons why:</p><ol><li>Not all people are shaped the same</li><li>\"Shapes\" change dramatically depending on how a person is posed. </li><li>You probably want to learn to draw things other than humans, at some point.</li></ol><p>It's not good enough to create a more accurate model of what people are \"supposed\" to be. You need to look at a subject and discard all your preconceived notions of what they are \"supposed\" to look like, along with all the symbols and names that your verbal center assigns to the pieces. You need to ignore the little tags that say \"face\" and \"arms\" and \"hands\" and \"torso,\" and instead see the lines, shapes, colors and shadows that are there in reality.</p><h2 id=\"_Drawing_on_the_Right_Side_of_the_Brain___Again_\">\"Drawing on the Right Side of the Brain\" (Again)</h2><p>Much of our knowledge of how this works and how to improve the process is relatively recent. In 1969, an art teacher named Betty Edwards was frustrated with her students, some of whom were having extreme difficulty. They could clearly see where things were, but that knowledge didn't translate into the drawing. </p><blockquote><em>\"Can you see that the orange in this still life is clearly in front of the vase?\"</em></blockquote><blockquote><em>\"Yes.\"</em></blockquote><blockquote><em>\"Well, in your drawing, you have the orange and the vase occupying the same space.\"</em></blockquote><blockquote><em>\"Yeah I know. I didn't know how to draw that.\"</em></blockquote><p>On a whim, she asked students to copy a painting while it was upside down. There was an immediate, dramatic improvement. </p><blockquote><em>\"How can you draw upside down when you can't draw right side up!?\"</em></blockquote><blockquote><em>\"Upside down, we didn't know what we were drawing!\"</em></blockquote><p>That experiment prompted a series of questions and investigations that led Edwards to the neuroscience of the time, which suggested that humans used two major processing centers, located in different hemispheres of the brain. The \"left brain\" dealt with verbal and analytic processing. The \"right brain\" dealt with visual and perceptual processing. Many people naturally draw using their left brain, which wants to name things and refer to existing knowledge about them. Edwards developed a series of techniques to suppress the left brain so that right brain processes can take over. She published a book in 1979 called \"Drawing on the Right Side of the Brain,\" that discussed her research and the accompanying exercises.</p><p>(Later on, neuroscientists learned that while the two processing centers are real, they are not neatly divided between brain hemispheres. The modern edition of the book uses the terms \"left mode\" and \"right mode\" to distinguish between the modes of thought)</p><p>The book has become extremely influential within the art field, and in recent years Edwards has worked to find ways to transfer \"right mode thinking\" into areas beyond drawing. In corporate seminars (typically lasting three days), she spends the first day and a half teaching employees how to observe and draw. The second day and half, she helps them outline workplace problems using drawn visual metaphors, which allow them see things from multiple perspectives and stumble upon solutions that seemed obvious in retrospect, but which had gone unnoticed (in one case, for decades).</p><p>Edwards breaks down drawing into five main subskills:</p><ul><li>The perception of edges</li><li>The perception of spaces</li><li>The perception of relationships</li><li>The perception of light and shadow</li><li>The perception of the whole, or gestalt</li></ul><p>She considers these the building blocks necessary to develop two final skills: Drawing from memory, and drawing from your imagination.</p><h2 id=\"Integrating_Observation_and__Gestural_Thought_\">Integrating Observation and \"Gestural Thought\"</h2><p>Eventually, your goal is to be able to do observe near instantly. The 30-second gesture drawings are important because they train you to evaluate and draw in one fluid stroke of thought and pencil. In less than five seconds you should be able to draw a line that describes the general size and shape of the body, and a small ovoid shape that describes the relative size of the head. </p><p>But that will be difficult, until you've practiced several slower-paced exercises that develop your ability to see reality and compare objects to each other. In my first workshop I realized that it's difficult to develop \"gestural thought\" unless you can do one of the following:</p><ol><li>Construct a mental model of your subject in a matter of seconds (and be able to revise it on the fly)</li><li>Have a pre-existing model (i.e. \"ideal proportions\") to use as your starting point, which you then fix as you have time to observe in more detail.</li><li>Spend time observing your subject, creating a specific model of them, before you begin drawing.</li></ol><p>I'm still experimenting with the ideal order to present various exercises, to develop sufficient observational skill as quickly as possible. As I noted in the last installment, observation and \"gestural thought\" require different types of thinking that initially will be hard to switch between. Giving them time to gel independently is important, but I believe students should learn to integrate them as quickly as possible. My students and I both made progress in second workshop (mostly by using the third technique listed above), and there is definitely room for improvement.</p><p>In a future post I'll be outlining some specific exercises to develop observation. Several of which will be lifted directly from Edwards' book, others developed from my teaching experiences. Developing your observation will, initially, involve many slow-paced exercises. Some focus on helping you break down the barrier between \"left mode\" and \"right mode\" thinking, but typically won't be used as often when creating a polished piece of art. Others specifically develop the skills you'll use to observe during \"regular\" drawing. </p><p>The goal will be to develop a baseline competence in observation, so that you can continue to develop it simultaneously alongside your ability to work quickly and energetically.</p>", "sections": [{"title": "How you will probably draw", "anchor": "How_you_will_probably_draw", "level": 1}, {"title": "(even knowing this is how you will probably draw)", "anchor": "_even_knowing_this_is_how_you_will_probably_draw_", "level": 2}, {"title": "How You Should Actually Draw", "anchor": "How_You_Should_Actually_Draw", "level": 1}, {"title": "The problem with Mapmakers, and Territory", "anchor": "The_problem_with_Mapmakers__and_Territory", "level": 1}, {"title": "\"Drawing on the Right Side of the Brain\" (Again)", "anchor": "_Drawing_on_the_Right_Side_of_the_Brain___Again_", "level": 2}, {"title": "Integrating Observation and \"Gestural Thought\"", "anchor": "Integrating_Observation_and__Gestural_Thought_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["N2pENnTPB75sfc9kb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T05:17:33.159Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Every Cause Wants To Be A Cult", "slug": "seq-rerun-every-cause-wants-to-be-a-cult", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.226Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SuYucmPuZgryFXymv/seq-rerun-every-cause-wants-to-be-a-cult", "pageUrlRelative": "/posts/SuYucmPuZgryFXymv/seq-rerun-every-cause-wants-to-be-a-cult", "linkUrl": "https://www.lesswrong.com/posts/SuYucmPuZgryFXymv/seq-rerun-every-cause-wants-to-be-a-cult", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Every%20Cause%20Wants%20To%20Be%20A%20Cult&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Every%20Cause%20Wants%20To%20Be%20A%20Cult%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSuYucmPuZgryFXymv%2Fseq-rerun-every-cause-wants-to-be-a-cult%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Every%20Cause%20Wants%20To%20Be%20A%20Cult%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSuYucmPuZgryFXymv%2Fseq-rerun-every-cause-wants-to-be-a-cult", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSuYucmPuZgryFXymv%2Fseq-rerun-every-cause-wants-to-be-a-cult", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>Today's post, <a href=\"/lw/lv/every_cause_wants_to_be_a_cult/\">Every Cause Wants To Be A Cult</a> was originally published on 12 December 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Every_Cause_Wants_To_Be_A_Cult\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Simply having a good idea at the center of a group of people is not enough to prevent that group from becoming a cult. As long as the idea's adherents are human, they will be vulnerable to the flaws in reasoning that cause cults. Simply basing a group around the idea of being rational is not enough. You have to actually put in the work to oppose the slide into cultishness.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/8ia/seq_rerun_the_robbers_cave_experiment/\">The Robbers Cave Experiment</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SuYucmPuZgryFXymv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.02800471677422e-07, "legacy": true, "legacyId": "11032", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yEjaj7PWacno5EvWa", "37Lyhqv37qGgkDMWa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T14:56:47.207Z", "modifiedAt": null, "url": null, "title": "[Link] Walking Through Doors Causes Forgetting", "slug": "link-walking-through-doors-causes-forgetting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "khafra", "createdAt": "2009-03-20T14:27:07.210Z", "isAdmin": false, "displayName": "khafra"}, "userId": "TYxB9awGtAt3n4PfL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vtwX6HuG2Tve7eSFf/link-walking-through-doors-causes-forgetting", "pageUrlRelative": "/posts/vtwX6HuG2Tve7eSFf/link-walking-through-doors-causes-forgetting", "linkUrl": "https://www.lesswrong.com/posts/vtwX6HuG2Tve7eSFf/link-walking-through-doors-causes-forgetting", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Walking%20Through%20Doors%20Causes%20Forgetting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Walking%20Through%20Doors%20Causes%20Forgetting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtwX6HuG2Tve7eSFf%2Flink-walking-through-doors-causes-forgetting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Walking%20Through%20Doors%20Causes%20Forgetting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtwX6HuG2Tve7eSFf%2Flink-walking-through-doors-causes-forgetting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtwX6HuG2Tve7eSFf%2Flink-walking-through-doors-causes-forgetting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<blockquote>\r\n<p>We investigated the ability of people to retrieve information about objects as they moved through rooms in a virtual space. People were probed with object names that were either associated with the person (i.e., carried) or dissociated from the person (i.e., just set down). Also, people either did or did not shift spatial regions (i.e., go to a new room). Information about objects was less accessible when the objects were dissociated from the person. Furthermore, information about an object was also less available when there was a spatial shift. However, the spatial shift had a larger effect on memory for the currently associated object. These data are interpreted as being more supportive of a situation model explanation, following on work using narratives and film. Simpler memory-based accounts that do not take into account the context in which a person is embedded cannot adequately account for the results.</p>\r\n</blockquote>\r\n<p><a href=\"http://www.springerlink.com/content/m6lq80675m22232h/\">http://www.springerlink.com/content/m6lq80675m22232h/</a>&nbsp;</p>\r\n<p>There's probably some deep implications to this I'm not qualified to plumb.&nbsp; But next time I'm concentrating on something, and need to get up from the computer and walk around a bit, I'm going to try avoiding doorways.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vtwX6HuG2Tve7eSFf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.030068144176243e-07, "legacy": true, "legacyId": "11036", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T15:32:34.377Z", "modifiedAt": null, "url": null, "title": "How did you come to find LessWrong?", "slug": "how-did-you-come-to-find-lesswrong", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:33.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quanticle", "createdAt": "2009-12-02T01:39:50.714Z", "isAdmin": false, "displayName": "quanticle"}, "userId": "usztQFrTvM67pdcCq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DMpj9MnvcvrSJ3sBT/how-did-you-come-to-find-lesswrong", "pageUrlRelative": "/posts/DMpj9MnvcvrSJ3sBT/how-did-you-come-to-find-lesswrong", "linkUrl": "https://www.lesswrong.com/posts/DMpj9MnvcvrSJ3sBT/how-did-you-come-to-find-lesswrong", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20did%20you%20come%20to%20find%20LessWrong%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20did%20you%20come%20to%20find%20LessWrong%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMpj9MnvcvrSJ3sBT%2Fhow-did-you-come-to-find-lesswrong%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20did%20you%20come%20to%20find%20LessWrong%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMpj9MnvcvrSJ3sBT%2Fhow-did-you-come-to-find-lesswrong", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMpj9MnvcvrSJ3sBT%2Fhow-did-you-come-to-find-lesswrong", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<p>I was reflecting the other day about how I learned about LessWrong. As best as I can recall/retrace, I learned about LessWrong from gwern, who I met in the #wikipedia IRC channel via an essentially chance meeting. I'm wondering how typical my experience is. How did <em>you</em> come to LessWrong?</p>\n<p>EDIT: Optional follow-up question: Do you think that we (the community) are doing enough to bring in new users to LessWrong? If not, what do you think could be done to increase awareness of LessWrong amongst potential rationalists?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DMpj9MnvcvrSJ3sBT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 8.03019565730336e-07, "legacy": true, "legacyId": "11037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 76, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-21T16:24:33.744Z", "modifiedAt": null, "url": null, "title": "[LINK] Signalling and irrationality in Software Development", "slug": "link-signalling-and-irrationality-in-software-development", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "quanticle", "createdAt": "2009-12-02T01:39:50.714Z", "isAdmin": false, "displayName": "quanticle"}, "userId": "usztQFrTvM67pdcCq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8d9u2HvkAwC4sRnzM/link-signalling-and-irrationality-in-software-development", "pageUrlRelative": "/posts/8d9u2HvkAwC4sRnzM/link-signalling-and-irrationality-in-software-development", "linkUrl": "https://www.lesswrong.com/posts/8d9u2HvkAwC4sRnzM/link-signalling-and-irrationality-in-software-development", "postedAtFormatted": "Monday, November 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Signalling%20and%20irrationality%20in%20Software%20Development&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Signalling%20and%20irrationality%20in%20Software%20Development%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d9u2HvkAwC4sRnzM%2Flink-signalling-and-irrationality-in-software-development%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Signalling%20and%20irrationality%20in%20Software%20Development%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d9u2HvkAwC4sRnzM%2Flink-signalling-and-irrationality-in-software-development", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8d9u2HvkAwC4sRnzM%2Flink-signalling-and-irrationality-in-software-development", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 617, "htmlBody": "<p><a href=\"http://sealedabstract.com/rants/why-software-projects-are-terrible-and-how-not-to-fix-them/\" target=\"_blank\">Why Software Projects are terrible and how not to fix them</a> (by Drew Crawford):</p>\n<blockquote>\n<p>Unless you are having a meeting with the <strong>one person who is going to use the software that you&rsquo;re writing</strong>, you&rsquo;re not meeting with the <em>real</em>&nbsp;customer. &nbsp;You&rsquo;re meeting with a person who has to explain to someone who can explain to someone who can explain what you&rsquo;re saying to the <em>real</em> customer. &nbsp;It&rsquo;s not enough to convince the person you&rsquo;re sitting in the room with that Agile is a good idea. &nbsp;He has to convince <em>his</em>&nbsp;boss. &nbsp;That person has to convince <em>his</em>&nbsp;boss. &nbsp;That person has to convince the sales team. &nbsp;The sales team has to convince the customer. &nbsp;If the customer is b2b, your contact at the customer organization has to convince <em>his</em> boss. &nbsp;Who convinces <em>his</em> boss. &nbsp;Who convinces the real customer. &nbsp;Maybe. &nbsp;Unless that sale is also b2b. &nbsp;This is a <strong>very long game of telephone</strong>. &nbsp;If the guy you&rsquo;re talking too is thinking &ldquo;This sounds like a really good idea but I&rsquo;m concerned I can&rsquo;t sell this upstairs,&rdquo; you are <strong>dead in the water</strong>. &nbsp;At any point in the chain, if somebody thinks that, you are<strong> dead in the water</strong>. &nbsp;You can&rsquo;t just say &ldquo;It&rsquo;s objectively better,&rdquo; you have to show how he can turn around and sell the idea to someone else.</p>\n<p>Put yourself in the middle manager&rsquo;s shoes. &nbsp;If the project goes bad, he has to &ldquo;look busy&rdquo;. &nbsp;He has to put more developers on the project, call a meeting and yell at people, and other arbitrary bad ideas. &nbsp;Not because he thinks those will solve the problem. &nbsp;In fact, managers often do this in spite of the fact that they know it&rsquo;s bad. &nbsp;&nbsp;<strong>Because that&rsquo;s what will convince upper management that they&rsquo;re doing their best</strong>.</p>\n</blockquote>\n<p>In other words, it's all about signaling, isn't it? Managers will take actions that actively harm the continued progress of the project if that action makes them look \"decisive\" and \"in charge\".&nbsp; I've seen this on many projects I've been on, and it took me a while to realize that my managers weren't stupid or ignorant. It's just that the organization I was working in put a higher priority on process than on results. My managers, therefore quite rationally did things that maximized their apparent value in the eyes of <em>their</em> bosses, even if it meant that the project (and, as a result) the entire organization was hurt.</p>\n<p>Crawford then goes on to detail why organizations with such maladaptive practices survive:</p>\n<blockquote>\n<p>Yes, businesses are under pressure to gravitate toward bad engineering practices, but shouldn&rsquo;t they be under equal market pressure to compete against companies that are using actually good software engineering practices? &nbsp;Shouldn&rsquo;t, at some point, bad companies simply implode under their own weight? Why sure, in the long run. &nbsp;But as Keynes succinctly put it, &ldquo;In the long run, we&rsquo;ll all be dead.&rdquo; &nbsp;<strong>Eventually is a long time</strong>. &nbsp;It&rsquo;s months, years, or decades. &nbsp;A project can be failing a long time before management is clued in. &nbsp;And even longer before management&rsquo;s management is clued in. &nbsp;And it can be <strong>ages</strong> before it hits the user.</p>\n</blockquote>\n<p>I think this is something that we as rationalists sometimes forget about. <em>Irrationality has momentum</em>. Humans have been thinking intuitively for thousands (hundreds of thousands, even) of years before we figured out how to think with rigorous rationality. Even if rationality had a massive advantage of intuitive thinking in everyday situations (it doesn't, as far as I can tell) it would take a very long time for rational thought to propagate through society.</p>\n<p>So the next time you get frustrated at some bit of wanton irrationality, remind yourself, \"Momentum,\" before you get frustrated.</p>\n<p>&nbsp;</p>\n<p>EDIT: Fixed spelling as per <a href=\"/r/discussion/lw/8im/link_signalling_and_irrationality_in_software/5ant\">RolfAndreassen's</a> post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8d9u2HvkAwC4sRnzM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 12, "extendedScore": null, "score": 8.030380912196294e-07, "legacy": true, "legacyId": "11038", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T01:48:16.486Z", "modifiedAt": null, "url": null, "title": "Where do I most obviously still need to say \"oops\"?", "slug": "where-do-i-most-obviously-still-need-to-say-oops", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.086Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BgpnbaJMthXjDeHcE/where-do-i-most-obviously-still-need-to-say-oops", "pageUrlRelative": "/posts/BgpnbaJMthXjDeHcE/where-do-i-most-obviously-still-need-to-say-oops", "linkUrl": "https://www.lesswrong.com/posts/BgpnbaJMthXjDeHcE/where-do-i-most-obviously-still-need-to-say-oops", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20do%20I%20most%20obviously%20still%20need%20to%20say%20%22oops%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20do%20I%20most%20obviously%20still%20need%20to%20say%20%22oops%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgpnbaJMthXjDeHcE%2Fwhere-do-i-most-obviously-still-need-to-say-oops%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20do%20I%20most%20obviously%20still%20need%20to%20say%20%22oops%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgpnbaJMthXjDeHcE%2Fwhere-do-i-most-obviously-still-need-to-say-oops", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBgpnbaJMthXjDeHcE%2Fwhere-do-i-most-obviously-still-need-to-say-oops", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>Eliezer once told me:</p>\n<blockquote>\n<p>The most common error I see on Less Wrong is the failure to say \"<a href=\"/lw/i9/the_importance_of_saying_oops/\">Oops</a>.\"</p>\n</blockquote>\n<p>If there's one rationality skill I like to think I'm pretty good at, it's this one: the skill of saying \"Oops.\"</p>\n<p>In fact, I say \"Oops, fixed, thanks\" so often on Less Wrong I once <a href=\"/lw/63i/rational_romantic_relationships_part_1/55aj\">suggested</a> I should have a shortcut for it: \"OFT.\"</p>\n<p>And I don't just say \"oops\" for typos and <a href=\"/lw/78s/help_fund_lukeprog_at_siai/4q7f\">mistakes</a> in <a href=\"/lw/74f/are_deontological_moral_judgments_rationalizations/4nle\">tone</a>, but also for mistakes in my <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/4ogq\">facts</a> and <a href=\"http://commonsenseatheism.com/?p=1174\">arguments</a>.</p>\n<p>It's not that I say \"oops\" every time I'm <em>challenged at length</em>, either. I don't say \"oops\" until I <em>actually</em> think I was significantly wrong; otherwise, I <a href=\"/lw/899/great_explanations/55pc\">stand</a> my <a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qe4\">ground</a> and ask for better counter-arguments.</p>\n<p>But I'm sure I can improve.</p>\n<p>Wanna help me debug my own mind?</p>\n<p>Tell me: On which issues do you think I most obviously still need to say \"Oops\"?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BgpnbaJMthXjDeHcE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 9, "extendedScore": null, "score": 1.9e-05, "legacy": true, "legacyId": "11043", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wCqfCLs8z5Qw4GbKS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T04:26:42.713Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Reversed Stupidity Is Not Intelligence", "slug": "seq-rerun-reversed-stupidity-is-not-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.302Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qior9M4ay9G5tv7aA/seq-rerun-reversed-stupidity-is-not-intelligence", "pageUrlRelative": "/posts/qior9M4ay9G5tv7aA/seq-rerun-reversed-stupidity-is-not-intelligence", "linkUrl": "https://www.lesswrong.com/posts/qior9M4ay9G5tv7aA/seq-rerun-reversed-stupidity-is-not-intelligence", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Reversed%20Stupidity%20Is%20Not%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Reversed%20Stupidity%20Is%20Not%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqior9M4ay9G5tv7aA%2Fseq-rerun-reversed-stupidity-is-not-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Reversed%20Stupidity%20Is%20Not%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqior9M4ay9G5tv7aA%2Fseq-rerun-reversed-stupidity-is-not-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqior9M4ay9G5tv7aA%2Fseq-rerun-reversed-stupidity-is-not-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>Today's post, <a href=\"/lw/lw/reversed_stupidity_is_not_intelligence/\">Reversed Stupidity Is Not Intelligence</a> was originally published on 12 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The world's greatest fool may say the Sun is shining, but that doesn't make it dark out. Stalin also believed that 2 + 2 = 4. Stupidity or human evil do not anticorrelate with truth. Arguing against weaker advocates proves nothing, because even the strongest idea will attract weak advocates.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8ig/seq_rerun_every_cause_wants_to_be_a_cult/\">Every Cause Wants To Be A Cult</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qior9M4ay9G5tv7aA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.032954943209825e-07, "legacy": true, "legacyId": "11047", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qNZM3EGoE5ZeMdCRt", "SuYucmPuZgryFXymv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T05:00:08.722Z", "modifiedAt": null, "url": null, "title": "Yet another Sleeping Beauty", "slug": "yet-another-sleeping-beauty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.226Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielLC", "createdAt": "2009-12-26T17:34:50.257Z", "isAdmin": false, "displayName": "DanielLC"}, "userId": "3e6zTkDmDpNspRb8P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NbNq32qfCQrLTEkWx/yet-another-sleeping-beauty", "pageUrlRelative": "/posts/NbNq32qfCQrLTEkWx/yet-another-sleeping-beauty", "linkUrl": "https://www.lesswrong.com/posts/NbNq32qfCQrLTEkWx/yet-another-sleeping-beauty", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yet%20another%20Sleeping%20Beauty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYet%20another%20Sleeping%20Beauty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbNq32qfCQrLTEkWx%2Fyet-another-sleeping-beauty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yet%20another%20Sleeping%20Beauty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbNq32qfCQrLTEkWx%2Fyet-another-sleeping-beauty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNbNq32qfCQrLTEkWx%2Fyet-another-sleeping-beauty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p>leeping Beauty is put to sleep on Sunday. If the coin lands on heads, she is awakened only on Monday. If it lands on tails, she is awaken on Monday and Tuesday, and has her memory erased between them. Each time she is awoken, she is asked how likely it is the coin landed on tails.</p>\n<p>According to the one theory, she would figure it's twice as likely to be her if the coin landed on tails, so it's now twice as likely to be tales. According to another, she would figure that the world she's in isn't eliminated by heads or tails, so it's equally likely. I'd like to use the second possibility, and add a simple modification:</p>\n<p>The coin is tossed a second time. She's shown the result of this toss on Monday, and the opposite on Tuesday (if she's awake for it).&nbsp;She wakes up, and believes that there are four equally probable results: HH, HT, TH, and TT. She then is shown heads. This will happen at some point unless the coin has the result HT. In that case, she is only woken once, and is shown tails. She now spreads the probability between the remaining three outcomes: HH, TH, and TT. She is asked how likely it is that the coin landed on heads. She gives 1/3. Thanks to this modification, she got the same answer as if she had used SIA.</p>\n<p>Now suppose that, instead of being told the result of second coin toss, she had some other observation. Perhaps she observed how tired she was when she woke up, or how long it took to open her eyes, or something else. In any case, if it's an unlikely observation, it probably won't happen twice, so she's about twice as likely to make it if she wakes up twice.</p>\n<p>Edit: SIA and SSA don't seem to be what I thought they were. In both cases, you get approximately 1/3. As far as I can figure, the reason Wikipedia states that you get 1/2 with SIA is that it uses sleeping beauty during the course of this experiment as the entire reference class (rather than all&nbsp;existent&nbsp;observers). I've seen someone use this logic before (they only updated on the existence of such an observer). Does anyone know what it's called?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NbNq32qfCQrLTEkWx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 1, "extendedScore": null, "score": 8.03307414887353e-07, "legacy": true, "legacyId": "11048", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T08:15:23.901Z", "modifiedAt": null, "url": null, "title": "Metaethics: Where I'm Headed", "slug": "metaethics-where-i-m-headed", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tyGbG4XGkJzDmLTdh/metaethics-where-i-m-headed", "pageUrlRelative": "/posts/tyGbG4XGkJzDmLTdh/metaethics-where-i-m-headed", "linkUrl": "https://www.lesswrong.com/posts/tyGbG4XGkJzDmLTdh/metaethics-where-i-m-headed", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Metaethics%3A%20Where%20I'm%20Headed&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMetaethics%3A%20Where%20I'm%20Headed%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyGbG4XGkJzDmLTdh%2Fmetaethics-where-i-m-headed%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Metaethics%3A%20Where%20I'm%20Headed%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyGbG4XGkJzDmLTdh%2Fmetaethics-where-i-m-headed", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtyGbG4XGkJzDmLTdh%2Fmetaethics-where-i-m-headed", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 573, "htmlBody": "<p>In an <a href=\"/lw/7cl/my_intentions_for_my_metaethics_sequence/\">earlier post</a>, I explained that <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">Pluralistic Moral Reductionism</a> finished my application of the basic lessons of the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Major_Sequences\">first four sequences</a> to moral philosophy. I also explained that my next task was to fill in some <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distances</a> by summarizing lots more cognitive science for LessWrong (e.g. <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Neuroscience of Human Motivation</a>, <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">Concepts Don't Work That Way</a>).</p>\n<p>Progress has been slow because I'm simultaneously working on many other projects. But I might as well let ya'll know where I'm headed:</p>\n<p>&nbsp;</p>\n<p>\"Philosophy for Humans, 2: Living Metaphorically\" (cogsci summary)</p>\n<p style=\"padding-left: 30px;\">Human concepts and human thought are thoroughly metaphorical, and this has significant consequences for philosophical methodology. (A summary of the literature reviewed in chs. 4-5 of <em><a href=\"http://www.amazon.com/Philosophy-Flesh-Embodied-Challenge-Western/dp/0465056741/\">Philosophy in the Flesh</a></em>.)</p>\n<p>&nbsp;</p>\n<p>\"Philosophy for Humans, 3: Concepts Aren't Shared That Way (cogsci summary)</p>\n<p style=\"padding-left: 30px;\">Concepts are not shared between humans in the way required to justify some common philosophical practices. (A summary of the literature reviewed in several chapters of <a style=\"font-style: italic;\" href=\"http://www.amazon.com/Making-Concepts-Developmental-Cognitive-Neuroscience/dp/0199549222/\">The Making of Human Concepts</a>,&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Mahon-Caramazza-Concepts-and-categories-a-neuropsychological-perspective.pdf\">Mahon &amp; Caramazza 2009</a>, and&nbsp;<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Kourtzi-Connor-Neural-Representations-for-Object-Perception-Structure-Category-and-Adaptive-Coding.pdf\">Kourtzi &amp; Connor 2011</a>.)</p>\n<p>&nbsp;</p>\n<p>\"The Making of a Moral Judgment\" (cogsci summary)</p>\n<p style=\"padding-left: 30px;\">A summary of the emerging consensus view on how moral judgments are formed (e.g. see <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Cushman-et-al-Multi-system-moral-psychology.pdf\">Cushman et al. 2010</a>).</p>\n<p>&nbsp;</p>\n<p>\"Habits and Goals\" (cogsci summary)</p>\n<p style=\"padding-left: 30px;\">A sequel to <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Neuroscience of Human Motivation</a> that explains the (at least) three different systems that feed into the final choice mechanism that encodes expected utility and so on. (A summary of the literature reviewed in chapter 2 of <em><a href=\"/r/discussion/lw/8a7/review_of_sharot_dolan_neuroscience_of_preference/\">Neuroscience of Preference and Choice</a></em>.)</p>\n<p>&nbsp;</p>\n<p>\"Where Value Comes From\" (metaethics main sequence)</p>\n<p style=\"padding-left: 30px;\">The typical approach to metaethics analyzes the meanings of value terms, e.g. <a href=\"http://plato.stanford.edu/entries/value-theory/#VarGoo\">the meaning of \"good\"</a> or <a href=\"/lw/sm/the_meaning_of_right/\">the meaning of \"right.\"</a> Given persistent and motivated disagreement and confusion over the \"meanings\" of our value concepts (which are metaphorical and not shared between humans), I prefer to <a href=\"/lw/5u2/pluralistic_moral_reductionism/\">taboo and reduce value terms</a>. To explain value in a naturalistic universe, I like to tell the story of where value comes from. Our universe evolved for hundreds of thousands of years before the atom was built, and it existed for billions of years before value was built. Just like the atom, value is not necessary or eternal. Like the atom, it is made of smaller parts. And as with the atom, that is what makes value real.</p>\n<p>&nbsp;</p>\n<p>\"The Great Chasm of the Robot's Rebellion\" (metaethics main sequence)</p>\n<p style=\"padding-left: 30px;\">We are robots built for replicating genes, but waking up to this fact gives us the chance to <a href=\"/lw/8gc/stanovich_the_robots_rebellion_review/\">rebel against our genes</a>&nbsp;and consciously pursue explicit goals. Alas, when we ask \"What do I want?\" and look inside, we don't find any utility function to maximize (see 'Habits and Goals', 'Where Value Comes From'). There is a Great Chasm from the spaghetti code that produces human behavior to a utility function that represents what we \"want.\" Luckily, we've spent several decades developing tools that may help us cross this great chasm: the tools of <em>value extraction</em>&nbsp;('<a href=\"http://en.wikipedia.org/wiki/Choice_modelling\">choice modeling</a>' in economics, '<a href=\"http://en.wikipedia.org/wiki/Preference_elicitation\">preference elicitation</a>' in AI) and <em>value extrapolation</em>&nbsp;(known <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">to philosophers</a> as 'full information' or 'ideal preference' theories of value).</p>\n<p>&nbsp;</p>\n<p>\"Value Extraction\" (metaethics main sequence)</p>\n<p style=\"padding-left: 30px;\">A summary of the literature on choice modeling and preference elicitation, with suggestions for where to push on the boundaries of what is currently known to make these fields useful for metaethics rather than for their current, narrow applications.</p>\n<p>&nbsp;</p>\n<p>\"Value Extrapolation\" (metaethics main sequence)</p>\n<p style=\"padding-left: 30px;\">A summary of the literature on value extrapolation, showing mostly negative results (extrapolation algorithms that <em>won't</em>&nbsp;work), with a preliminary exploration of value extrapolation methods that <em>might</em>&nbsp;work.</p>\n<p>&nbsp;</p>\n<p>After this, there are many places I could go, and I'm not sure which I'll choose.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tyGbG4XGkJzDmLTdh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 13, "extendedScore": null, "score": 8.033770378175819e-07, "legacy": true, "legacyId": "11049", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bxPK3BF5ECsi7rnaA", "3zDX3f3QTepNeZHGc", "hN2aRnu798yas5b2k", "wHjpCxeDeuFadG3jF", "GpAHPC5MELJQfusfv", "fG3g3764tSubr6xvs", "PsvzbYxasvPPLBCZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T10:32:13.175Z", "modifiedAt": null, "url": null, "title": "Objections to Coherent Extrapolated Volition", "slug": "objections-to-coherent-extrapolated-volition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:39.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JhB9eqJDScjDNpWiS/objections-to-coherent-extrapolated-volition", "pageUrlRelative": "/posts/JhB9eqJDScjDNpWiS/objections-to-coherent-extrapolated-volition", "linkUrl": "https://www.lesswrong.com/posts/JhB9eqJDScjDNpWiS/objections-to-coherent-extrapolated-volition", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Objections%20to%20Coherent%20Extrapolated%20Volition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AObjections%20to%20Coherent%20Extrapolated%20Volition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhB9eqJDScjDNpWiS%2Fobjections-to-coherent-extrapolated-volition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Objections%20to%20Coherent%20Extrapolated%20Volition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhB9eqJDScjDNpWiS%2Fobjections-to-coherent-extrapolated-volition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJhB9eqJDScjDNpWiS%2Fobjections-to-coherent-extrapolated-volition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 870, "htmlBody": "<blockquote>\n<p>In poetic terms, our coherent extrapolated volition is  our wish if we knew more, thought faster, were more the people we wished  we were, had grown up farther together; where the extrapolation  converges rather than diverges, where our wishes cohere rather than  interfere; extrapolated as we wish that extrapolated, interpreted as we  wish that interpreted.</p>\n</blockquote>\n<p>&mdash; Eliezer Yudkowsky, May 2004, <a title=\"Coherent Extrapolated Volition\" href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a></p>\n<h3>Foragers versus industry era folks</h3>\n<p>Consider the difference between a <a href=\"http://en.wikipedia.org/wiki/Hunter-gatherer\">hunter-gatherer</a>, who cares about his hunting success and to become the new <a href=\"http://en.wikipedia.org/wiki/Tribal_chief\">tribal chief</a>, and a modern <a href=\"http://en.wikipedia.org/wiki/Computer_science\">computer scientist</a> who wants to determine if a &ldquo;sufficiently large randomized <a href=\"http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\">Conway board</a> could turn out to converge to a barren &lsquo;all off&rsquo; state.&rdquo;</p>\n<p>The utility of the success in hunting down animals and proving abstract conjectures about <a href=\"http://en.wikipedia.org/wiki/Cellular_automaton\">cellular automata</a> is largely <a href=\"http://www.overcomingbias.com/2010/06/layers-of-delusion.html\">determined by factors</a> such as your education, culture and <a href=\"http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html\">environmental circumstances</a>. The same <a href=\"http://www.overcomingbias.com/2010/10/two-types-of-people.html\">forager</a> who cared to kill a lot of animals, to get the best ladies in its clan,  might have under different circumstances turned out to be a <a href=\"http://en.wikipedia.org/wiki/Vegetarianism\">vegetarian</a> <a href=\"http://en.wikipedia.org/wiki/Mathematics\">mathematician</a> solely caring about his understanding of <a href=\"http://www.simulation-argument.com/\">the nature of reality</a>. Both sets of values are to some extent <a href=\"http://en.wikipedia.org/wiki/Mutually_exclusive_events\">mutually exclusive</a> or at least <a href=\"http://en.wikipedia.org/wiki/Disjoint_sets\">disjoint</a>.  Yet both sets of values are what the person wants, given the  circumstances. Change the circumstances dramatically and you change the  persons values.</p>\n<h3>What do you really want?</h3>\n<p>You might conclude that what the hunter-gatherer really wants is to  solve abstract mathematical problems, he just doesn&rsquo;t know it. But there  is no set of values that a person &ldquo;really&rdquo; wants. Humans are largely  defined by the circumstances they reside in.</p>\n<ul>\n<li> If you already knew a  movie, <a href=\"http://en.wikipedia.org/wiki/Spoiler_%28media%29\">you wouldn&rsquo;t watch it</a>. </li>\n<li>To be able to get your meat from the supermarket changes the value of hunting.</li>\n</ul>\n<p>If &ldquo;we knew more, thought faster, were more the people we wished we  were, and had grown up closer together&rdquo; then we would stop to desire  what we learnt, wish to think even faster, become even different people  and get bored of and rise up from the people similar to us.</p>\n<h3>A singleton is an attractor</h3>\n<p>A <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a> will inevitably change everything by causing a <a href=\"http://en.wikipedia.org/wiki/Feedback\">feedback loop</a> between itself as an <a href=\"http://en.wikipedia.org/wiki/Attractor\">attractor</a> and humans and their values.</p>\n<p>Much of our values and goals, what we want, are culturally induced or  the result of our ignorance. Reduce our ignorance and you change our  values. One trivial example is our intellectual curiosity. If we don&rsquo;t  need to figure out what we want on our own, our curiosity is impaired.</p>\n<p>A singleton won&rsquo;t extrapolate human volition but implement an  artificial set values as a result of abstract high-order contemplations  about rational conduct.</p>\n<h3>With knowledge comes responsibility, with wisdom comes sorrow</h3>\n<p>Knowledge changes and introduces terminal goals. The toolkit that is called &lsquo;<a href=\"http://wiki.lesswrong.com/wiki/Rationality\">rationality</a>&rsquo;,  the rules and heuristics developed to help us to achieve our terminal  goals are also altering and deleting them. A stone age hunter-gatherer  seems to possess very different values than we do. Learning about  rationality and various ethical theories such as&nbsp;<a href=\"http://www.hedweb.com/abolitionist-project/index.html\">Utilitarianism</a> would alter those values considerably.</p>\n<p>Rationality was meant to help us achieve our goals, e.g. become a  better hunter. Rationality was designed to tell us what we ought to do  (instrumental goals) to achieve what we want to do (terminal goals). Yet  what actually happens is that we are told, that we will learn, <a href=\"http://youtu.be/lC4FnfNKwUo\">what we <em>ought</em> to want</a>.</p>\n<p>If an agent becomes more knowledgeable and smarter then this does not  leave its goal-reward-system intact if it is not especially designed to  be stable. An agent who originally wanted to become a better hunter and  feed his tribe would end up wanting to <a href=\"http://www.overcomingbias.com/2011/07/the-great-charity-storm.html\">eliminate poverty in Obscureistan</a>.  The question is, how much of this new &ldquo;wanting&rdquo; is the result of using  rationality to achieve terminal goals and how much is a side-effect of  using rationality, how much is left of the original values versus the  values induced by a feedback loop between the toolkit and its user?</p>\n<p>Take for example an agent that is facing the <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">Prisoner&rsquo;s dilemma</a>. Such an agent might originally tend to cooperate and only after learning about <a href=\"http://en.wikipedia.org/wiki/Game_theory\">game theory</a> decide to defect and gain a greater payoff. Was it rational for the  agent to learn about game theory, in the sense that it helped the agent  to achieve its goal or in the sense that it deleted one of its goals in  exchange for a allegedly more &ldquo;valuable&rdquo; goal?</p>\n<h3>Beware rationality as a purpose in and of itself</h3>\n<p>It seems to me that becoming more knowledgeable and smarter is  gradually altering our utility functions. But what is it that we are  approaching if the extrapolation of our volition becomes a purpose in  and of itself? <a href=\"http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/\">Extrapolating our coherent volition</a> will distort or alter what we really value by installing a new  cognitive toolkit designed to achieve an equilibrium between us and  other agents with the same toolkit.</p>\n<p>Would a <a href=\"http://wiki.lesswrong.com/wiki/Singleton\">singleton</a> be a tool that we can use to get what we want or would the tool use us  to do what it does, would we be modeled or would it create models, would  we be extrapolating our volition or rather follow our extrapolations?</p>\n<p><sub><em>(This post is a write-up of a previous comment designated to receive feedback from a larger audience.)</em></sub></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JhB9eqJDScjDNpWiS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 45, "baseScore": 12, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "11050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>In poetic terms, our coherent extrapolated volition is  our wish if we knew more, thought faster, were more the people we wished  we were, had grown up farther together; where the extrapolation  converges rather than diverges, where our wishes cohere rather than  interfere; extrapolated as we wish that extrapolated, interpreted as we  wish that interpreted.</p>\n</blockquote>\n<p>\u2014 Eliezer Yudkowsky, May 2004, <a title=\"Coherent Extrapolated Volition\" href=\"http://intelligence.org/upload/CEV.html\">Coherent Extrapolated Volition</a></p>\n<h3 id=\"Foragers_versus_industry_era_folks\">Foragers versus industry era folks</h3>\n<p>Consider the difference between a <a href=\"http://en.wikipedia.org/wiki/Hunter-gatherer\">hunter-gatherer</a>, who cares about his hunting success and to become the new <a href=\"http://en.wikipedia.org/wiki/Tribal_chief\">tribal chief</a>, and a modern <a href=\"http://en.wikipedia.org/wiki/Computer_science\">computer scientist</a> who wants to determine if a \u201csufficiently large randomized <a href=\"http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\">Conway board</a> could turn out to converge to a barren \u2018all off\u2019 state.\u201d</p>\n<p>The utility of the success in hunting down animals and proving abstract conjectures about <a href=\"http://en.wikipedia.org/wiki/Cellular_automaton\">cellular automata</a> is largely <a href=\"http://www.overcomingbias.com/2010/06/layers-of-delusion.html\">determined by factors</a> such as your education, culture and <a href=\"http://www.overcomingbias.com/2009/09/this-is-the-dream-time.html\">environmental circumstances</a>. The same <a href=\"http://www.overcomingbias.com/2010/10/two-types-of-people.html\">forager</a> who cared to kill a lot of animals, to get the best ladies in its clan,  might have under different circumstances turned out to be a <a href=\"http://en.wikipedia.org/wiki/Vegetarianism\">vegetarian</a> <a href=\"http://en.wikipedia.org/wiki/Mathematics\">mathematician</a> solely caring about his understanding of <a href=\"http://www.simulation-argument.com/\">the nature of reality</a>. Both sets of values are to some extent <a href=\"http://en.wikipedia.org/wiki/Mutually_exclusive_events\">mutually exclusive</a> or at least <a href=\"http://en.wikipedia.org/wiki/Disjoint_sets\">disjoint</a>.  Yet both sets of values are what the person wants, given the  circumstances. Change the circumstances dramatically and you change the  persons values.</p>\n<h3 id=\"What_do_you_really_want_\">What do you really want?</h3>\n<p>You might conclude that what the hunter-gatherer really wants is to  solve abstract mathematical problems, he just doesn\u2019t know it. But there  is no set of values that a person \u201creally\u201d wants. Humans are largely  defined by the circumstances they reside in.</p>\n<ul>\n<li> If you already knew a  movie, <a href=\"http://en.wikipedia.org/wiki/Spoiler_%28media%29\">you wouldn\u2019t watch it</a>. </li>\n<li>To be able to get your meat from the supermarket changes the value of hunting.</li>\n</ul>\n<p>If \u201cwe knew more, thought faster, were more the people we wished we  were, and had grown up closer together\u201d then we would stop to desire  what we learnt, wish to think even faster, become even different people  and get bored of and rise up from the people similar to us.</p>\n<h3 id=\"A_singleton_is_an_attractor\">A singleton is an attractor</h3>\n<p>A <a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a> will inevitably change everything by causing a <a href=\"http://en.wikipedia.org/wiki/Feedback\">feedback loop</a> between itself as an <a href=\"http://en.wikipedia.org/wiki/Attractor\">attractor</a> and humans and their values.</p>\n<p>Much of our values and goals, what we want, are culturally induced or  the result of our ignorance. Reduce our ignorance and you change our  values. One trivial example is our intellectual curiosity. If we don\u2019t  need to figure out what we want on our own, our curiosity is impaired.</p>\n<p>A singleton won\u2019t extrapolate human volition but implement an  artificial set values as a result of abstract high-order contemplations  about rational conduct.</p>\n<h3 id=\"With_knowledge_comes_responsibility__with_wisdom_comes_sorrow\">With knowledge comes responsibility, with wisdom comes sorrow</h3>\n<p>Knowledge changes and introduces terminal goals. The toolkit that is called \u2018<a href=\"http://wiki.lesswrong.com/wiki/Rationality\">rationality</a>\u2019,  the rules and heuristics developed to help us to achieve our terminal  goals are also altering and deleting them. A stone age hunter-gatherer  seems to possess very different values than we do. Learning about  rationality and various ethical theories such as&nbsp;<a href=\"http://www.hedweb.com/abolitionist-project/index.html\">Utilitarianism</a> would alter those values considerably.</p>\n<p>Rationality was meant to help us achieve our goals, e.g. become a  better hunter. Rationality was designed to tell us what we ought to do  (instrumental goals) to achieve what we want to do (terminal goals). Yet  what actually happens is that we are told, that we will learn, <a href=\"http://youtu.be/lC4FnfNKwUo\">what we <em>ought</em> to want</a>.</p>\n<p>If an agent becomes more knowledgeable and smarter then this does not  leave its goal-reward-system intact if it is not especially designed to  be stable. An agent who originally wanted to become a better hunter and  feed his tribe would end up wanting to <a href=\"http://www.overcomingbias.com/2011/07/the-great-charity-storm.html\">eliminate poverty in Obscureistan</a>.  The question is, how much of this new \u201cwanting\u201d is the result of using  rationality to achieve terminal goals and how much is a side-effect of  using rationality, how much is left of the original values versus the  values induced by a feedback loop between the toolkit and its user?</p>\n<p>Take for example an agent that is facing the <a href=\"http://en.wikipedia.org/wiki/Prisoner%27s_dilemma\">Prisoner\u2019s dilemma</a>. Such an agent might originally tend to cooperate and only after learning about <a href=\"http://en.wikipedia.org/wiki/Game_theory\">game theory</a> decide to defect and gain a greater payoff. Was it rational for the  agent to learn about game theory, in the sense that it helped the agent  to achieve its goal or in the sense that it deleted one of its goals in  exchange for a allegedly more \u201cvaluable\u201d goal?</p>\n<h3 id=\"Beware_rationality_as_a_purpose_in_and_of_itself\">Beware rationality as a purpose in and of itself</h3>\n<p>It seems to me that becoming more knowledgeable and smarter is  gradually altering our utility functions. But what is it that we are  approaching if the extrapolation of our volition becomes a purpose in  and of itself? <a href=\"http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/\">Extrapolating our coherent volition</a> will distort or alter what we really value by installing a new  cognitive toolkit designed to achieve an equilibrium between us and  other agents with the same toolkit.</p>\n<p>Would a <a href=\"http://wiki.lesswrong.com/wiki/Singleton\">singleton</a> be a tool that we can use to get what we want or would the tool use us  to do what it does, would we be modeled or would it create models, would  we be extrapolating our volition or rather follow our extrapolations?</p>\n<p><sub><em>(This post is a write-up of a previous comment designated to receive feedback from a larger audience.)</em></sub></p>", "sections": [{"title": "Foragers versus industry era folks", "anchor": "Foragers_versus_industry_era_folks", "level": 1}, {"title": "What do you really want?", "anchor": "What_do_you_really_want_", "level": 1}, {"title": "A singleton is an attractor", "anchor": "A_singleton_is_an_attractor", "level": 1}, {"title": "With knowledge comes responsibility, with wisdom comes sorrow", "anchor": "With_knowledge_comes_responsibility__with_wisdom_comes_sorrow", "level": 1}, {"title": "Beware rationality as a purpose in and of itself", "anchor": "Beware_rationality_as_a_purpose_in_and_of_itself", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "55 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T11:31:07.392Z", "modifiedAt": null, "url": null, "title": "One pixel electronic contact lens tested on rabbits [link]", "slug": "one-pixel-electronic-contact-lens-tested-on-rabbits-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.518Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ntb9spus6SyoaMHDR/one-pixel-electronic-contact-lens-tested-on-rabbits-link", "pageUrlRelative": "/posts/ntb9spus6SyoaMHDR/one-pixel-electronic-contact-lens-tested-on-rabbits-link", "linkUrl": "https://www.lesswrong.com/posts/ntb9spus6SyoaMHDR/one-pixel-electronic-contact-lens-tested-on-rabbits-link", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20pixel%20electronic%20contact%20lens%20tested%20on%20rabbits%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20pixel%20electronic%20contact%20lens%20tested%20on%20rabbits%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fntb9spus6SyoaMHDR%2Fone-pixel-electronic-contact-lens-tested-on-rabbits-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20pixel%20electronic%20contact%20lens%20tested%20on%20rabbits%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fntb9spus6SyoaMHDR%2Fone-pixel-electronic-contact-lens-tested-on-rabbits-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fntb9spus6SyoaMHDR%2Fone-pixel-electronic-contact-lens-tested-on-rabbits-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://www.newscientist.com/blogs/onepercent/2011/11/electronic-contact-lens-displa.html\">http://www.newscientist.com/blogs/onepercent/2011/11/electronic-contact-lens-displa.html</a></p>\n<p>&nbsp;</p>\n<p>http://iopscience.iop.org/0960-1317/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ntb9spus6SyoaMHDR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 8.034468397205576e-07, "legacy": true, "legacyId": "11052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T12:04:33.041Z", "modifiedAt": null, "url": null, "title": "How to prove anything with a review article", "slug": "how-to-prove-anything-with-a-review-article", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RichardKennaway", "createdAt": "2009-03-09T13:46:28.196Z", "isAdmin": false, "displayName": "RichardKennaway"}, "userId": "unnmqpwtrwhyDt6q5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hBTp6YZ5xqb5Cm8mZ/how-to-prove-anything-with-a-review-article", "pageUrlRelative": "/posts/hBTp6YZ5xqb5Cm8mZ/how-to-prove-anything-with-a-review-article", "linkUrl": "https://www.lesswrong.com/posts/hBTp6YZ5xqb5Cm8mZ/how-to-prove-anything-with-a-review-article", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20prove%20anything%20with%20a%20review%20article&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20prove%20anything%20with%20a%20review%20article%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhBTp6YZ5xqb5Cm8mZ%2Fhow-to-prove-anything-with-a-review-article%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20prove%20anything%20with%20a%20review%20article%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhBTp6YZ5xqb5Cm8mZ%2Fhow-to-prove-anything-with-a-review-article", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhBTp6YZ5xqb5Cm8mZ%2Fhow-to-prove-anything-with-a-review-article", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>Thus the subtitle of <a href=\"http://blogs.plos.org/obesitypanacea/2011/11/21/cigarettes-may-be-useful-for-distance-runners-or-how-to-prove-anything-with-a-review-article\">this blog posting</a> at PLoS, referencing <a href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001541/\">this article</a> on \"Cigarette smoking: an underused tool in high-performance endurance training\". The point being that you can write a review article to argue anything you want, with sufficient cherry-picking and chains of links.</p>\n<p>If you are doing actual experiments and making observations or proving theorems, then to a large extent -- larger in some sciences than in others -- you are constrained by the brute facts. But when writing secondary literature, especially in areas where data is generally fuzzier, it is easy, whether deliberately or not, to write to a bottom line, including findings you like and excluding those you don't.</p>\n<p>Something to bear in mind when reading or writing any review article.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hBTp6YZ5xqb5Cm8mZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 26, "extendedScore": null, "score": 8.034587621207399e-07, "legacy": true, "legacyId": "11053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T12:44:02.772Z", "modifiedAt": null, "url": null, "title": "[Infographic] A reminder as to how far the rationality waterline can climb (at least, for the US).", "slug": "infographic-a-reminder-as-to-how-far-the-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:28.456Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xa5HA9idHygDK86KJ/infographic-a-reminder-as-to-how-far-the-rationality", "pageUrlRelative": "/posts/Xa5HA9idHygDK86KJ/infographic-a-reminder-as-to-how-far-the-rationality", "linkUrl": "https://www.lesswrong.com/posts/Xa5HA9idHygDK86KJ/infographic-a-reminder-as-to-how-far-the-rationality", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BInfographic%5D%20A%20reminder%20as%20to%20how%20far%20the%20rationality%20waterline%20can%20climb%20(at%20least%2C%20for%20the%20US).&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BInfographic%5D%20A%20reminder%20as%20to%20how%20far%20the%20rationality%20waterline%20can%20climb%20(at%20least%2C%20for%20the%20US).%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXa5HA9idHygDK86KJ%2Finfographic-a-reminder-as-to-how-far-the-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BInfographic%5D%20A%20reminder%20as%20to%20how%20far%20the%20rationality%20waterline%20can%20climb%20(at%20least%2C%20for%20the%20US).%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXa5HA9idHygDK86KJ%2Finfographic-a-reminder-as-to-how-far-the-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXa5HA9idHygDK86KJ%2Finfographic-a-reminder-as-to-how-far-the-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3, "htmlBody": "<p><img src=\"data:image/png;base64,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\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<p>Encountered at: https://whyevolutionistrue.wordpress.com/2011/03/20/click-and-weep/scientific-literacy/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xa5HA9idHygDK86KJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 10, "extendedScore": null, "score": 3.3e-05, "legacy": true, "legacyId": "11054", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T13:28:44.070Z", "modifiedAt": null, "url": null, "title": "FAI-relevant XKCD", "slug": "fai-relevant-xkcd", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:32.833Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eALz6zmhiQHbSgdwn/fai-relevant-xkcd", "pageUrlRelative": "/posts/eALz6zmhiQHbSgdwn/fai-relevant-xkcd", "linkUrl": "https://www.lesswrong.com/posts/eALz6zmhiQHbSgdwn/fai-relevant-xkcd", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI-relevant%20XKCD&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI-relevant%20XKCD%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeALz6zmhiQHbSgdwn%2Ffai-relevant-xkcd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI-relevant%20XKCD%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeALz6zmhiQHbSgdwn%2Ffai-relevant-xkcd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeALz6zmhiQHbSgdwn%2Ffai-relevant-xkcd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://xkcd.com/962/\">http://xkcd.com/962/</a></p>\n<p><a href=\"http://imgs.xkcd.com/comics/the_corliss_resolution.png\"><img src=\"http://imgs.xkcd.com/comics/the_corliss_resolution.png\" alt=\"\" width=\"599\" height=\"273\" /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eALz6zmhiQHbSgdwn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -6, "extendedScore": null, "score": 8.034887889074577e-07, "legacy": true, "legacyId": "11055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T13:44:37.817Z", "modifiedAt": null, "url": null, "title": "5-second level case study: Value of information", "slug": "5-second-level-case-study-value-of-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.205Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xDiqYyqeqPo92PojS/5-second-level-case-study-value-of-information", "pageUrlRelative": "/posts/xDiqYyqeqPo92PojS/5-second-level-case-study-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/xDiqYyqeqPo92PojS/5-second-level-case-study-value-of-information", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%205-second%20level%20case%20study%3A%20Value%20of%20information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A5-second%20level%20case%20study%3A%20Value%20of%20information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDiqYyqeqPo92PojS%2F5-second-level-case-study-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=5-second%20level%20case%20study%3A%20Value%20of%20information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDiqYyqeqPo92PojS%2F5-second-level-case-study-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxDiqYyqeqPo92PojS%2F5-second-level-case-study-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2098, "htmlBody": "<p><em>This post started off as a comment to Vaniver's post <a href=\"/lw/85x/value_of_information_four_examples/\">Value of Information: Four Examples</a>. This post also heavily builds on Eliezer's post <a href=\"/lw/5kz/the_5second_level/\">The 5-Second Level</a>. The five second level is the idea that to develop a rationality skill, you need to automatically recognize a problem and then apply a stored, actionable procedural skill to deal with it, all in about five seconds or so. In here, I take the value of information concept and develop it into a five second skill, summarizing my thought process as I do so. Hopefully this will help others develop things into five second skills.</em></p>\r\n<p><em></em>So upon reading this, I thought \"the value of information seems like a valuable concept\", but didn't do much more. A little later, I thought, \"I want to make sure that I actually <em>apply</em> this concept when it is warranted. How do I make sure of that?\" In other words, \"how do I get this concept to the five second level?\" Then I decided to document my thought process in the hopes of it being useful to others. This is quite stream-of-consciousness, but I hope that seeing my thought process helps to learn from it. (Or to offer me valuable criticism on how I should have thought.)</p>\r\n<p>First off, \"how do I apply this concept?\" is too vague to be useful. A better question would be, \"in what kinds of situations might this concept be useful?\". With a bit of thought, it was easy to find at least three situations, ones where I am:</p>\r\n<p>1. ...tempted to act now without gathering more information, despite the VoI being high.<br />2. ...tempted to gather more information, despite the VoI being low.<br />3. ...not sure of whether I should seek information or not.</p>\r\n<p>#3 implies that I'm already reflecting on the situation, and am therefore relatively likely to remember VoI as a possible mental tool anyway. So developing a five-second level reaction for that one isn't as important. But in #1 and #2 I might just proceed by default, never realizing that I could do better. So I'll leave #3 aside, concentrating on #1 and #2.</p>\r\n<p>Now in these situations, the relevant thing is that the VoI might be \"high\" or \"low\". Time to get more concrete - what does that mean? Looking at Vaniver's post, the VoI is high if 1) extra information is likely to make me choose B when I had intended on choosing A, and 2) there's a high payoff in choosing correctly between A and B. If at least 2 is false, VoI is low. The intermediate case is the one where 2 is true but 1 is false, in which case it depends on how extreme the values are. E.g. only a 1% chance of changing my mind given extra information might sometimes imply a high VoI, if the difference between the correct and incorrect choice is a million euros, say.</p>\r\n<p>So sticking just to #1 for simplicity, and because I think that's a worse problem for me, I'd need to train myself to immediately notice and react if:<a id=\"more\"></a><br /><br />(I'm about to do something at once) AND [(Extra information might change my mind AND changing my mind matters) OR (extra information is pretty unlikely to change my mind AND changing my mind matters a lot)].</p>\r\n<p>That's a relatively complicated conjunction to evaluate in a second, without conscious notice. But my mind does carry out far more complicated evaluations without my conscious notice, all the time. So the question is not \"is it possible to learn this\", but \"how do I learn this\"?</p>\r\n<p>Fortunately, having gotten to this point of analysis, it's pretty clear that I already have some mental sensations corresponding to some of those criteria. Noticing the (extra information is pretty unlikely to change my mind AND changing my mind matters a lot) part comes naturally: decisions with big consequences naturally make everyone hesitate. That natural hesitation helps in recognizing both #1 and #2 situations: whenever I feel the hesitation involved in a big decision, I can ask myself, \"what's the VoI involved in this, am I hesitating too little or too much?\". So there's the initial trigger for those kinds of situations. I had already learned the \"if you can't decide, flip a coin\" heuristic from earlier, and sometimes applied it, so in effect I already had one VoI-related 5-second procedure in place.</p>\r\n<p>What if the consequences for an action are big, but I don't realize that? Recognizing that probably requires its own set of 5-second procedures, so I'll let that be for now.</p>\r\n<p>Then there's the (Extra information might change my mind AND changing my mind matters) condition. That's trickier, but I also have a related trigger for that - that little nagging doubt, a feeling of <a href=\"/lw/if/your_strength_as_a_rationalist/\">something not being quite right</a>. That's also something that mind automatically activates in some situations, without me needing to consciously think about it. It might be good idea to develop a five-second procedure for training this instinct. Perhaps I could train it by stopping each time when I realize I've been wrong in something that matters, and asking whether I felt the doubt and whether I should have. But for now, it's enough to know that I have such a natural reaction that I can use.</p>\r\n<p>So now I have two triggers for a five-second reaction - the hesitation upon a big decision, and the little nagging doubt. Great. What's the <em>procedure</em> I need to automatically associate with these feelings?</p>\r\n<p>The first one that comes to mind is \"trigger a conscious VoI evaluation\". I'm strongly tempted to just leave it at that. I've already written a lot but, in retrospect, only said things that feel obvious. And there's a certain cogsci/compsci geek charm in putting it that way. System 1 notices a possible problem and flags it for System 2 to evaluate. A computer device driver fails to fix a read error, and passes the issue one layer up to the operating system. That's how it's supposed to go, right?</p>\r\n<p>But no. How likely am I to actually \"do a conscious VoI evaluation\"? I'm lazy, just like everybody else. Just realizing that I'm supposed to do something doesn't mean that I'll actually do it. If I can have a lower layer of my mind fix the problem without bothering the higher layers, I'm much more likely to actually get the problem fixed. Besides, I don't even know what a \"conscious VoI evaluation\" means, yet. I need to put it into explicit steps, and if I can do that, I might be able to automate it, too.</p>\r\n<p>So. I was going to do something, but it's a big decision so I hesitate, or it isn't that big of a decision, but I feel a little doubt anyway. What do I do next?</p>\r\n<p>I skim Vaniver's original post, looking for direction, and then realize that I'm being too abstract again. I need a concrete example, and then I can try to generalize a pattern from that. When have I felt that hesitation or doubt lately?</p>\r\n<p>The answer: in various online and offline discussions. I've been writing something that reveals possibly embarrassing personal details about myself, or opinions that I might not want everyone to know that I hold. I've felt the doubt of the question, <em>is this something that I should say in the open, or should I hold it back</em>?</p>\r\n<p>This looks like a good candidate to analyze, especially since I haven't really analyzed the question in very much detail in those situations. I've just gone on a gut instinct and held back, or decided to just act anyway despite my doubts. What <em>should</em> I do in that situation?</p>\r\n<p>Again, I realize that I'm maybe too abstract, and should go for more concreteness. Let's suppose that I had a sexual interest in sheep. (I do not, for the record - this is just an example.) Further suppose I was having a public conversation with somebody over at Google Plus, and I was going to mention this, but then I hesitated. Do I really want the whole world to know about me and sheep? What would be the right course of action, here?</p>\r\n<p>First to evaluate the possible consequences of me saying this versus me not saying this. Maybe all the people who I need to care about are enlightened enough that they don't fault a person for his sexual interests, neither on an intellectual nor an emotional level. And maybe getting to tell others of this makes me relieved. Now I don't need to feel that this is something shameful that must be kept as a secret anymore.</p>\r\n<p>On the other hand, it could be that this knowledge spreads and I'm widely ridiculed and made fun of. Or maybe it won't be that bad, but the person who I'm talking with will feel awkward about this unexpected revelation and that will damage our relationship. Or maybe the outcome is ambiguous, and I'll spend the rest of my life worrying whether the knowledge will spread and embarrass me one day. The stakes are clearly high here. I might rid myself of the feeling of carrying a shameful secret with me and realize that nobody really finds it to be an issue after all, or I might pretty much ruin my life forever. So choosing correctly matters.</p>\r\n<p>If I found out more, is that likely to alter my estimates of the probability of the various consequences? Well, that depends on how confident I'm feeling in my model of other people and the average repulsiveness of sheep fantasies. So I have to evaluate that. Humans are often surprising, groups of humans are even more surprising, and the typical mind fallacy is often strong. So I don't think I have high confidence in my model of the consequences. The VoI thus seems high here. So I hold back the revelation for now, perhaps discreetly gauging my conversational partner's reactions to such topics, and starting to collect such information from other people as well.</p>\r\n<p>Concrete example analyzed. Time to go back to the abstract. What did I do here? First, I stopped to consider the various potential consequences. Then I asked myself how sure I felt in my model of those different consequences happening. When I realized that I was both unsure, and that a wrong decision might be costly, I decided to hold off for now and gather more information. This, as well as recalling the definition of VoI, suggests the following procedure:</p>\r\n<p>1. Notice a momentary doubt or hesitation when about to do something.<br />2. Pause to visualize various consequences of the action.<br />3. Contrast the most important consequences and ask, \"would it be a big deal for me if one of these happened and the other wouldn't\"?<br />4. If yes, then for any such major consequence, ask \"how surprised would I be if this actually happened\"?<br />5. If there are several possible consequences that wouldn't be very surprising, and it matters which one of them happens, ask \"is gathering more information likely to make at least one of these outcomes seem more surprising\"?<br />6. If yes, hold off and gather more information. If not, just do what the current expected utility estimate says is the best action, since it isn't likely to change. If the expected utilities are close, just flip a coin.</p>\r\n<p>I'm still not entirely sure if that's close enough to the five-second level - it seems like there's still a lot of conscious evaluation involved. But at least I now know the steps needed for the conscious operation. Now that I've written this out, it seems so obvious as to not be worth posting about... but then, no matter how obvious it feels like now, <em>I haven't been consistently doing that</em>. So this was probably worth writing down anyway.</p>\r\n<p>This exercise also suggested two new five-second techniques that I might want to look into. First, recognizing big decisions that don't seem like big decisions at first. Second, if I've screwed up when I should have felt that hesitation, teaching myself to feel that hesitation the next time. Also, writing this down as I went along was <em>really useful</em> - I don't think I would have gotten anywhere this far if I hadn't. A final lesson was that moving towards concrete examples was crucial in developing the skill. I made pretty much no progress with questions like \"what sort of a reaction should my triggering sensation actually trigger\" or \"what should I do in the general situation of being unsure of whether to say something\", but when I moved into concrete examples, the answers were quick to come to me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DbMQGrxbhLxtNkmca": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xDiqYyqeqPo92PojS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 36, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "11056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vADtvr9iDeYsCDfxd", "JcpzFpPBSmzuksmWM", "5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T16:17:24.431Z", "modifiedAt": null, "url": null, "title": "[META] Can't post comments", "slug": "meta-can-t-post-comments", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.129Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dYckw5zDQQukMqjQT/meta-can-t-post-comments", "pageUrlRelative": "/posts/dYckw5zDQQukMqjQT/meta-can-t-post-comments", "linkUrl": "https://www.lesswrong.com/posts/dYckw5zDQQukMqjQT/meta-can-t-post-comments", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Can't%20post%20comments&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Can't%20post%20comments%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYckw5zDQQukMqjQT%2Fmeta-can-t-post-comments%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Can't%20post%20comments%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYckw5zDQQukMqjQT%2Fmeta-can-t-post-comments", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdYckw5zDQQukMqjQT%2Fmeta-can-t-post-comments", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>I tried in FF 8.0 and Chrome 15.0, and couldn't post, but it worked in IE9. Anyone else having this problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dYckw5zDQQukMqjQT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "11057", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T16:31:31.508Z", "modifiedAt": null, "url": null, "title": "The ethics of randomized computation in the multiverse", "slug": "the-ethics-of-randomized-computation-in-the-multiverse", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C8GiEbpEpogJS3k9A/the-ethics-of-randomized-computation-in-the-multiverse", "pageUrlRelative": "/posts/C8GiEbpEpogJS3k9A/the-ethics-of-randomized-computation-in-the-multiverse", "linkUrl": "https://www.lesswrong.com/posts/C8GiEbpEpogJS3k9A/the-ethics-of-randomized-computation-in-the-multiverse", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20ethics%20of%20randomized%20computation%20in%20the%20multiverse&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20ethics%20of%20randomized%20computation%20in%20the%20multiverse%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8GiEbpEpogJS3k9A%2Fthe-ethics-of-randomized-computation-in-the-multiverse%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20ethics%20of%20randomized%20computation%20in%20the%20multiverse%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8GiEbpEpogJS3k9A%2Fthe-ethics-of-randomized-computation-in-the-multiverse", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC8GiEbpEpogJS3k9A%2Fthe-ethics-of-randomized-computation-in-the-multiverse", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>From David Deutsch's <em><a href=\"http://www.amazon.com/Beginning-Infinity-Explanations-Transform-World/dp/0670022756/\">The Beginning of Infinity</a></em>:</p>\n<blockquote>\n<p>\n<p>Take a powerful computer&nbsp;and set each bit randomly to 0 or 1 using a quantum randomizer. (That&nbsp;means that 0 and 1 occur in histories of equal measure.) At that point&nbsp;all possible contents of the computer&rsquo;s memory exist in the multiverse.&nbsp;So there are necessarily histories present in which the computer&nbsp;contains an AI program &ndash; indeed, all possible AI programs in all&nbsp;possible states, up to the size that the computer&rsquo;s memory can hold.&nbsp;Some of them are fairly accurate representations of you, living in a&nbsp;virtual-reality environment crudely resembling your actual environment.&nbsp;(Present-day computers do not have enough memory to simulate a&nbsp;realistic environment accurately, but, as I said in Chapter 7, I am sure&nbsp;that they have more than enough to simulate a person.) There are also&nbsp;people in every possible state of suffering. So my question is: is it&nbsp;wrong to switch the computer on, setting it executing all those&nbsp;programs simultaneously in different histories? Is it, in fact, the worst&nbsp;crime ever committed? Or is it merely inadvisable, because the&nbsp;combined measure of all the histories containing suffering is very tiny?&nbsp;Or is it innocent and trivial?</p>\n</p>\n</blockquote>\n<p>I'm not so sure we have the computing power to \"simulate a person,\" but suppose we did. (Perhaps we will soon.) How would you respond to this worry?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C8GiEbpEpogJS3k9A", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 8.035539937451503e-07, "legacy": true, "legacyId": "11058", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T17:42:06.036Z", "modifiedAt": null, "url": null, "title": "Criticisms of intelligence explosion", "slug": "criticisms-of-intelligence-explosion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iGRpMWxhTmdoyA8Er/criticisms-of-intelligence-explosion", "pageUrlRelative": "/posts/iGRpMWxhTmdoyA8Er/criticisms-of-intelligence-explosion", "linkUrl": "https://www.lesswrong.com/posts/iGRpMWxhTmdoyA8Er/criticisms-of-intelligence-explosion", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Criticisms%20of%20intelligence%20explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACriticisms%20of%20intelligence%20explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGRpMWxhTmdoyA8Er%2Fcriticisms-of-intelligence-explosion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Criticisms%20of%20intelligence%20explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGRpMWxhTmdoyA8Er%2Fcriticisms-of-intelligence-explosion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiGRpMWxhTmdoyA8Er%2Fcriticisms-of-intelligence-explosion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1790, "htmlBody": "<p>On this page I will collect criticisms of (1) the claim that intelligence explosion is plausible, (2) the claim that intelligence explosion is likely to occur within the next 150 years, and (3) the claim that intelligence explosion would have a massive impact on civilization. Please suggest your own, citing the original source when possible.</p>\n<p>[<strong>Under construction</strong>.]</p>\n<p>&nbsp;</p>\n<p>\"AGI won't be a big deal; we already have 6 billion general intelligences on Earth.\"</p>\n<p style=\"padding-left: 30px; \">Example: \"I see no reason to single out AI as a mould-breaking technology: we already have billions of humans.\" (Deutsch, <em><a href=\"http://www.amazon.com/Beginning-Infinity-Explanations-Transform-World/dp/0670022756/\">The Beginning of Infinity</a></em>, p. 456.)</p>\n<p style=\"padding-left: 30px; \">Response: The advantages of mere digitality (speed, copyability, goal coordination) alone are transformative, and will increase the odds of rapid recursive self-improvement in intelligence. Meat brains are badly constrained in ways that non-meat brains need not be.</p>\n<p>&nbsp;</p>\n<p>\"Intelligence requires experience and learning, so there is a limit to the speed at which even a machine can improve its own intelligence.\"</p>\n<p style=\"padding-left: 30px; \">Example: \"If you define the singularity as a point in time when intelligent machines are designing intelligent machines in such a way that machines get extremely intelligent in a short period of time--an exponential increase in intelligence--then it will never happen. Intelligence is largely defined by experience and training, not just by brain size or algorithms. It isn't a matter of writing software. Intelligent machines, like humans, will need to be trained in particular domains of expertise. This takes time and deliberate attention to the kind of knowledge you want the machine to have.\" (Hawkins, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px; \">Response: Intelligence defined as <a href=\"/lw/va/measuring_optimization_power/\">optimization power</a> doesn't necessarily need experience or learning from the external world. Even if it did, a superintelligent machine spread throughout the internet could gain experience and learning from billions of sub-agents all around the world simultaneously, while near-instantaneously propagating these updates to its other sub-agents.&nbsp;</p>\n<p>&nbsp;</p>\n<p>\"There are hard limits to how intelligent a machine can get.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"The term 'singularity' applied to intelligent machines refers to the idea that when intelligent machines can design intelligent machines smarter than themselves, it will cause an exponential growth in machine intelligence leading to a singularity of infinite (or at least extremely large) intelligence. Belief in this idea is based on a naive understanding of what intelligence is. As an analogy, imagine we had a computer that could design new computers (chips, systems, and software) faster than itself. Would such a computer lead to infinitely fast computers or even computers that were faster than anything humans could ever build? No. It might accelerate the rate of improvements for a while, but in the end there are limits to how big and fast computers can run... Exponential growth requires the exponential consumption of resources (matter, energy, and time), and there are always limits to this.\" (Hawkins, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: There <em>are</em>&nbsp;physical limits to how intelligent something can get, but they easily allow the intelligence required to transform the solar system.</p>\n<p>&nbsp;</p>\n<p>\"AGI won't be malevolent.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"No intelligent machine will 'wake up' one day and say 'I think I will enslave my creators.'\" (Hawkins, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Example: \"...it's more likely than not in my view that the two species will comfortably and more or less peacefully coexist--unless human interests start to interfere with those of the machines.\" (Casti, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: True. But most runaway machine superintelligence designs would kill us inadvertently. \"The AI does not love you, nor does it hate you, but you are made of atoms it can use for something else.\"</p>\n<p>&nbsp;</p>\n<p>\"If intelligence explosion was possible, we would have seen it by now.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"I don't believe in technological singularities. It's like extraterrestrial life--if it were there, we would have seen it by now.\" (Rodgers, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: Not true.</p>\n<p>&nbsp;</p>\n<p>\"Humanity will destroy itself before AGI arrives.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"the population will destroy itself before the technological singularity.\" (Bell, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: This is plausible, though there are many reasons to think that AGI will arrive before other global catastrophic risks do.</p>\n<p>&nbsp;</p>\n<p>\"The Singularity belongs to the genre of science fiction.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible. Look at domed cities, jet-pack commuting, underwater cities, mile-high buildings, and nuclear-powered automobiles--all staples of futuristic fantasies when I was a child that have never arrived.\" (Pinker, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: This is not an issue of literary genre, but of probability and prediction. Science fiction becomes science fact several times every year. In the case of technological singularity, there are good scientific and philosophical reasons to expect it.</p>\n<p>&nbsp;</p>\n<p>\"Intelligence isn't enough; a machine would also need to manipulate objects.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"The development of humans, what evolution has come up with, involves a lot more than just the intellectual capability. You can manipulate your fingers and other parts of your body. I don't see how machines are going to overcome that overall gap, to reach that level of complexity, even if we get them so they're intellectually more capable than humans.\" (Moore, <a href=\"http://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity\">Tech Luminaries Address Singularity</a>)</p>\n<p style=\"padding-left: 30px;\">Response: Robotics is making strong progress in addition to AI.</p>\n<p>&nbsp;</p>\n<p>\"Human intelligence or cognitive ability can never be achieved by a machine.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"Goedel's theorem must apply to cybernetical machines, because it is of the essence of being a machine, that it should be a concrete instantiation of a formal system. It follows that given any machine which is consistent and capable of doing simple arithmetic, there is a formula which it is incapable of producing as being true---i.e., the formula is unprovable-in-the-system-but which we can see to be true. It follows that no machine can be a complete or adequate model of the mind, that minds are essentially different from machines.\" (Lucas, <a href=\"http://cogprints.org/356/1/lucas.html\">Minds, Machines and Goedel</a>)</p>\n<p style=\"padding-left: 30px;\">Example: \"Instantiating a computer program is never by itself a sufficient condition of [human-liked]&nbsp;intentionality.\" (Searle, <a href=\"http://cogprints.org/7150/1/10.1.1.83.5248.pdf\">Minds, Brains, and Programs</a>)</p>\n<p style=\"padding-left: 30px;\">Response: \"...nothing in&nbsp;the singularity idea requires that an AI be a classical computational&nbsp;system or even that it be a computational system at all. For example,&nbsp;Penrose (like Lucas) holds that the brain is not an algorithmic system&nbsp;in the ordinary sense, but he allows that it is a mechanical system that&nbsp;relies on certain nonalgorithmic quantum processes. Dreyfus holds&nbsp;that the brain is not a rule-following symbolic system, but he allows&nbsp;that it may nevertheless be a mechanical system that relies on&nbsp;subsymbolic processes (for example, connectionist processes). If so,&nbsp;then these arguments give us no reason to deny that we can build artificial systems that exploit the relevant nonalgorithmic quantum processes, or the relevant subsymbolic processes, and that thereby allow&nbsp;us to simulate the human brain...&nbsp;As for the Searle and Block objections, these rely on the thesis that&nbsp;even if a system duplicates our behaviour, it might be missing important &lsquo;internal&rsquo; aspects of mentality: consciousness, understanding,&nbsp;intentionality, and so on....&nbsp;we&nbsp;can set aside these objections by stipulating that for the purposes of&nbsp;the argument, intelligence is to be measured wholly in terms of behaviour and behavioural dispositions, where behaviour is construed operationally in terms of the physical outputs that a system produces.\" (Chalmers, <a href=\"http://consc.net/papers/singularityjcs.pdf\">The Singularity: A Philosophical Analysis</a>)</p>\n<p>&nbsp;</p>\n<p>\"It might make sense in theory, but where's the evidence?\"</p>\n<p style=\"padding-left: 30px;\">Example: \"Too much theory, not enough empirical evidence.\" (MileyCyrus, <a href=\"/r/discussion/lw/8j7/criticisms_of_intelligence_explosion/5auu\">LW comment</a>)</p>\n<p style=\"padding-left: 30px;\">Response: \"Papers like <a href=\"http://www.nickbostrom.com/superintelligence.html\">How Long Before Superintelligence</a> contain some of the relevant evidence, but it is old and incomplete. Upcoming works currently in progress by Nick Bostrom and by SIAI researchers contain additional argument and evidence, but even this is not enough. More researchers should be assessing the state of the evidence.\"</p>\n<p>&nbsp;</p>\n<p>\"Humans will be able to keep up with AGI by using AGI's advancements themselves.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"...an essential part of what we mean by foom in the first place... is that it involves a small group accelerating in power away from the rest of the world. But the reason why that happened in human evolution is that genetic innovations mostly don't transfer across species. [But] human engineers carry out exactly this sort of technology transfer on a routine basis.\" (rwallace, <a href=\"/lw/312/the_curve_of_capability/\">The Curve of Capability</a>)</p>\n<p style=\"padding-left: 30px;\">Response: Human engineers cannot take a powerful algorithm from AI and implement it in their own neurobiology. Moreover, once an AGI is improving its own intelligence, it's not clear that it would share the 'secrets' of these improvements with humans.</p>\n<p>&nbsp;</p>\n<p>\"A discontinuous break with the past requires lopsided capabilities development.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"a chimpanzee could make an almost discontinuous jump to human level intelligence because it wasn't developing across the board. It was filling in a missing capability - symbolic intelligence - in an otherwise already very highly developed system. In other words, its starting point was staggeringly lopsided... [But]&nbsp;the lopsidedness is not occurring [in computers]. Obviously computer technology hasn't lagged in symbol processing - quite the contrary.\" (rwallace, <a href=\"/lw/312/the_curve_of_capability/\">The Curve of Capability</a>)</p>\n<p style=\"padding-left: 30px;\">Example: \"Some species, such as humans, have mostly taken over the worlds of other species. The seeming reason for this is that there was virtually no sharing of the relevant information between species. In human society there is a lot of information sharing.\" (Katja Grace, <a href=\"http://meteuphoric.wordpress.com/2009/10/16/how-far-can-ai-jump/\">How Far Can AI Jump?</a>)</p>\n<p style=\"padding-left: 30px;\">Response: It doesn't seem that symbol processing was the missing capability that made humans so powerful. Calculators have superior symbol processing, but have no power to rule the world. Also: many kinds of lopsidedness are occurring in computing technology that may allow a sudden discontinuous jump in AI abilities. In particular, we are amassing vast computational capacities without yet understanding the algorithmic keys to general intelligence.</p>\n<p>&nbsp;</p>\n<p>\"No small set of insights will lead to massive intelligence boost in AI.\"</p>\n<p style=\"padding-left: 30px;\">Example: \"...if there were a super mind theory that allowed vast mental efficiency gains all at once, but there isn&rsquo;t. &nbsp;Minds are vast complex structures full of parts that depend intricately on each other, much like the citizens of a city. &nbsp;Minds, like cities, best improve gradually, because you just never know enough to manage a vast redesign of something with such complex inter-dependent adaptations.\" (Robin Hanson, <a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\">Is the City-ularity Near?</a>)</p>\n<p style=\"padding-left: 30px;\">Example: \"Now if you artificially hobble something so as to simultaneously reduce many of its capacities, then when you take away that limitation you may simultaneously improve a great many of its capabilities... But beyond removing artificial restrictions, it is very hard to simultaneously improve many diverse capacities. Theories that help you improve capabilities are usually focused on a relatively narrow range of abilities &ndash; very general and useful theories are quite rare.\" (Robin Hanson, <a href=\"http://www.overcomingbias.com/2011/06/the-betterness-explosion.html\">The Betterness Explosion</a>)</p>\n<p style=\"padding-left: 30px;\">Response: An intelligence explosion doesn't require a breakthrough that improves all capabilities at once. Rather, it requires an AI capable of improving its intelligence in a variety of ways. Then it can use the advantages of mere digitality (speed, copyability, goal coordination, etc.) to improve its intelligence in dozens or thousands of ways relatively quickly.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>To be added:</p>\n<p>\n<ul>\n<li>Massimo Pigliucci on <a href=\"http://www.psychologytoday.com/blog/rationally-speaking/200911/david-chalmers-and-the-singularity-will-probably-not-come\">Chalmers' Singularity talk</a></li>\n<li>XiXiDu on <a href=\"/r/discussion/lw/8fa/is_an_intelligence_explosion_a_disjunctive_or/\">intelligence explosion as a disjunctive or conjunctive event</a>, on <a href=\"/r/discussion/lw/8fb/why_an_intelligence_explosion_might_be_a/\">intelligence explosion as a low-priority global risk</a>, on <a href=\"/lw/8dw/no_basic_ai_drives/\">basic AI drives</a></li>\n<li><a href=\"/lw/8j7/criticisms_of_intelligence_explosion/5ayj\">Diminishing returns from intelligence amplification</a></li>\n</ul>\n</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iGRpMWxhTmdoyA8Er", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 8.03579171836565e-07, "legacy": true, "legacyId": "11059", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Q4hLMDrFd8fbteeZ8", "7grfN4xNvLpEQoiyb", "j52uErqofDiJZCo76", "ZbgQ3HtPu2cLKGmy8", "RQWMDhhRPEf5AXtg3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T19:51:17.068Z", "modifiedAt": null, "url": null, "title": "[Link] The Decision Education Foundation", "slug": "link-the-decision-education-foundation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.065Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a9tMEyHdkm3aw64hb/link-the-decision-education-foundation", "pageUrlRelative": "/posts/a9tMEyHdkm3aw64hb/link-the-decision-education-foundation", "linkUrl": "https://www.lesswrong.com/posts/a9tMEyHdkm3aw64hb/link-the-decision-education-foundation", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Decision%20Education%20Foundation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Decision%20Education%20Foundation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9tMEyHdkm3aw64hb%2Flink-the-decision-education-foundation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Decision%20Education%20Foundation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9tMEyHdkm3aw64hb%2Flink-the-decision-education-foundation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa9tMEyHdkm3aw64hb%2Flink-the-decision-education-foundation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 240, "htmlBody": "<p>Among <a href=\"http://www.acceleratingfuture.com/michael/blog\">Michael Anissimov's</a> recent <a href=\"http://delicious.com/anissimov\">Delicious links</a>, I spotted&nbsp;<a href=\"http://www.decisioneducation.org/\">The Decision Education Foundation</a>. &nbsp;It seems to be relevant to our interest in spreading rationality and bringing up young rationalists, but I hadn't seen it shared or discussed before and just thought I'd share it.</p>\n<blockquote>\n<p>There is an urgent need for young people to understand how to make better decisions. High school dropout numbers, substance abuse, and violence are some of the visible signs of poor decision making among many young people in the U.S. In addition, mainstream students are failing to reach their full academic and personal potential because they lack good decision-making skills. They're facing an increasingly complex, risky, and uncertain future in which these skills will be more important than ever.</p>\n<p><br /><strong>Yet decision skills training is missing in the majority of today's school curricula.</strong></p>\n<p><br />To answer this urgent and critical need and to provide this missing link in schools and youth programs, the Decision Education Foundation (DEF) was created in 2001 by educators, decision scientists, and management consultants with a passion for education and a commitment to bring decision education to today's youth.&nbsp;<br />Since then, DEF&rsquo;s founders, staff, and volunteers have been designing and delivering training opportunities and teaching materials for educators and youth mentors who desire to increase self-esteem, build better critical thinking skills, and give students more power over their academic and personal lives through better decision skills.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a9tMEyHdkm3aw64hb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 8.036252623502574e-07, "legacy": true, "legacyId": "11060", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T20:06:10.340Z", "modifiedAt": null, "url": null, "title": "New website on careers for optimal philanthropy", "slug": "new-website-on-careers-for-optimal-philanthropy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.401Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xrNDqL8SpHpeXZdw7/new-website-on-careers-for-optimal-philanthropy", "pageUrlRelative": "/posts/xrNDqL8SpHpeXZdw7/new-website-on-careers-for-optimal-philanthropy", "linkUrl": "https://www.lesswrong.com/posts/xrNDqL8SpHpeXZdw7/new-website-on-careers-for-optimal-philanthropy", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20website%20on%20careers%20for%20optimal%20philanthropy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20website%20on%20careers%20for%20optimal%20philanthropy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxrNDqL8SpHpeXZdw7%2Fnew-website-on-careers-for-optimal-philanthropy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20website%20on%20careers%20for%20optimal%20philanthropy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxrNDqL8SpHpeXZdw7%2Fnew-website-on-careers-for-optimal-philanthropy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxrNDqL8SpHpeXZdw7%2Fnew-website-on-careers-for-optimal-philanthropy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 66, "htmlBody": "<p><strong>80,000 hours</strong> (<a href=\"http://eightythousand.org/\">eightythousand.org</a>) is a new website associated with <a href=\"http://highimpactcareers.org/\">High Impact Careers</a>, a <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a>-associated effort to inform the public about \"professional philanthropy\" and the fact that you can do more good as a banker or entrepreneur than as an aid worker. It recently got some <a href=\"http://www.bbc.co.uk/news/education-15820786\">BBC press</a>, and there's a neat new <a href=\"http://vimeo.com/29954976\">video</a>.</p>\n<p>Related to <a href=\"/lw/37f/efficient_charity/\">efficient</a> <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">charity</a> and <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy</a>. Also see <a href=\"/lw/hw/scope_insensitivity/\">scope insensitivity</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xrNDqL8SpHpeXZdw7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 8.0363057436654e-07, "legacy": true, "legacyId": "11061", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FCxHgPsDScx4C3H8n", "pC47ZTsPNAkjavkXs", "hEqsWLm5zQtsPevd3", "2ftJ38y9SRBCBsCzy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T22:13:23.977Z", "modifiedAt": null, "url": null, "title": "FAI FAQ draft: What is nanotechnology?", "slug": "fai-faq-draft-what-is-nanotechnology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:00.972Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2un4e7nxedhMsCT5h/fai-faq-draft-what-is-nanotechnology", "pageUrlRelative": "/posts/2un4e7nxedhMsCT5h/fai-faq-draft-what-is-nanotechnology", "linkUrl": "https://www.lesswrong.com/posts/2un4e7nxedhMsCT5h/fai-faq-draft-what-is-nanotechnology", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20FAQ%20draft%3A%20What%20is%20nanotechnology%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20FAQ%20draft%3A%20What%20is%20nanotechnology%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2un4e7nxedhMsCT5h%2Ffai-faq-draft-what-is-nanotechnology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20FAQ%20draft%3A%20What%20is%20nanotechnology%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2un4e7nxedhMsCT5h%2Ffai-faq-draft-what-is-nanotechnology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2un4e7nxedhMsCT5h%2Ffai-faq-draft-what-is-nanotechnology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p>\n<p>I invite your feedback on this snippet from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. This one is an answer to the question \"What is nanotechnology?\" For references, see <a href=\"/lw/8gh/fai_faq_draft_what_is_the_singularity/5awr\">here</a>.</p>\n<p>_____</p>\n</p>\n<p>&nbsp;</p>\n<p>Nanotechnology is the study of materials and devices built at the scale of 1-100 nanometers (&ldquo;nano-&rdquo; means &ldquo;one billionth of&rdquo;). A hydrogen atom is about 0.24nm across, so we&rsquo;re talking about materials and devices built <em>atom by atom</em>.</p>\n<p>One famous piece of nanotechnology is the <a href=\"http://en.wikipedia.org/wiki/Carbon_nanotube\">carbon nanotube</a>. A carbon nanotube is a one-atom-thick sheet of graphite that is rolled into a seamless tube. Because of their physical properties, carbon nanotubes usually allow <a href=\"http://en.wikipedia.org/wiki/Ballistic_conduction\">ballistic conduction</a>, meaning that electrons can flow through the tube without collisions (Lin &amp; Shung 1995), which allows the carbon nanotubes to conduct electricity without heat dissipation (Chico et al. 1996)! Carbon nanotubes are also much stronger than diamond or steel (Popov et al. 2002). <a href=\"http://www.eastonbellsports.com/\">Easton Bell Sports</a> uses carbon nanotubes to build tougher bicycles, doctors use carbon nanotubes as scaffolding for bone growth in tissue engineering applications (Zanello et al. 2006), and one company uses carbon nanotubes to produce a special kind of <a href=\"http://www.baytubes.com/news_and_services/pr_100223_heater.html\">high-conductance heater</a>.</p>\n<p>New nanomaterials are being developed every year, and may see applications in nearly every field of technology (Allhoff et al. 2010). Nanotechnology has already given us <a href=\"http://www.usatoday.com/tech/news/techinnovations/2003-07-23-robopants_x.htm\">stain-free pants</a>, <a href=\"http://www.hitachi.com/New/cnews/071015a.pdf\">larger-capacity hard drives</a>, <a href=\"http://www.nrc-cnrc.gc.ca/obj/irc/doc/pubs/nrcc46618/nrcc46618.pdf\">stronger cement</a>, <a href=\"http://www.azonano.com/article.aspx?ArticleID=319\">longer-lasting tennis balls</a>, the world&rsquo;s first sale of a <a href=\"http://www.dwavesys.com/en/pressreleases.html#lm_2011\">quantum computer</a>, a <a href=\"http://en.wikipedia.org/wiki/Nanoshell#Cancer_Treatment\">new method for fighting cancer</a>, and <a href=\"http://en.wikipedia.org/wiki/List_of_nanotechnology_applications\">much more</a>.</p>\n<p>An even more radical technology was described in Eric Drexler&rsquo;s (1987) <em><a href=\"http://www.amazon.com/Engines-Creation-Coming-Era-Nanotechnology/dp/0385199732/\">Engines of Creation</a></em>. As Allhoff et al. (2010, p. 7) explain, Drexler predicted</p>\n<blockquote>\n<p>a new form of technology based on molecular &ldquo;assemblers,&rdquo; which would be able to &ldquo;place atoms in almost any reasonable arrangement&rdquo; and thereby allow the formation of &ldquo;almost anything the laws of nature allow.&rdquo; This may sound like a fanciful and fantastical idea but, as Drexler points out, this is something that nature already does, unaided by human design, with the biologically based machines inside our own bodies (and those of any biological species).</p>\n</blockquote>\n<p>Tiny molecular machines called &ldquo;nanobots&rdquo; would be a particularly revolutionary invention. For example in nanomedicine they would allow us to intelligently access cancer cells and blood cells.</p>\n<p>It may also be possible to build self-replicating nanobots. These nanobots would use materials in their environment to manufacture copies of themselves. This would be an explosive technology, as Drexler (1987, p. 58) explains:</p>\n<blockquote>\n<p>[The] first replicator assembles a copy in one thousand seconds, the two replicators then build two more in the next thousand seconds, the four build another four, and the eight build another eight. At the end of ten hours, there are not thirty-six new replicators, but over 68 billion. In less than a day, they would weigh a ton; in less than two days, they would outweigh the Earth... if the bottle of chemicals hadn't run dry long before.</p>\n</blockquote>\n<p>But the massive production of nanobots does not require that they be self-replicating. They could also be produced by nanofactories (Phoenix 2005).</p>\n<p>Access to (or invention of) advanced molecular manufacturing is one thing that could make a machine superintelligence incredibly powerful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XJjvxWB68GYpts93N": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2un4e7nxedhMsCT5h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 3, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "11062", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["23odz5WbpEvXKw5HZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T22:28:03.868Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft: Why designing digital intelligence gets easier over time", "slug": "intelligence-explosion-analysis-draft-why-designing-digital", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.887Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YEkqxoWFCwAQBsMJ/intelligence-explosion-analysis-draft-why-designing-digital", "pageUrlRelative": "/posts/7YEkqxoWFCwAQBsMJ/intelligence-explosion-analysis-draft-why-designing-digital", "linkUrl": "https://www.lesswrong.com/posts/7YEkqxoWFCwAQBsMJ/intelligence-explosion-analysis-draft-why-designing-digital", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%3A%20Why%20designing%20digital%20intelligence%20gets%20easier%20over%20time&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%3A%20Why%20designing%20digital%20intelligence%20gets%20easier%20over%20time%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YEkqxoWFCwAQBsMJ%2Fintelligence-explosion-analysis-draft-why-designing-digital%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%3A%20Why%20designing%20digital%20intelligence%20gets%20easier%20over%20time%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YEkqxoWFCwAQBsMJ%2Fintelligence-explosion-analysis-draft-why-designing-digital", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YEkqxoWFCwAQBsMJ%2Fintelligence-explosion-analysis-draft-why-designing-digital", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2433, "htmlBody": "<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on. This section is less complete than the others; missing text is indicated with brackets: [].</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>Many predictions of human-level digital intelligence have been wrong.<sup>1</sup> On the other hand, machines surpass human ability at new tasks with some regularity (Kurzweil 2005). For example, machines recently achieved superiority at visually identifying traffic signs at low resolution (Sermanet and LeCun 2011), diagnosing cardiovascular problems from some types of MRI scan images (Li et al. 2009), and playing Jeopardy! (Markoff 2011). Below, we consider several factors that, considered together, appear to increase the odds that we will develop digital intelligence as the century progresses.</p>\n<p><em>More hardware</em>. For at least four decades, computing power<sup>2</sup> has increased exponentially, in accordance with Moore&rsquo;s law.<sup>3</sup> Experts disagree on how much longer Moore&rsquo;s law will hold (e.g. Mack 2011; Lundstrom 2003), but if it holds for two more decades then we may have enough computing power to emulate human brains by 2029.<sup>4</sup> Even if Moore&rsquo;s law fails to hold, our hardware should become much more powerful in the coming decades.<sup>5</sup> More hardware doesn&rsquo;t by itself give us digital intelligence, but it contributes to the development of digital intelligence in several ways:</p>\n<blockquote>\n<p>Powerful hardware may improve performance simply by allowing existing &ldquo;brute force&rdquo; solutions to run faster (Moravec, 1976). Where such solutions do not yet exist, researchers might be incentivized to quickly develop them given abundant hardware to exploit. Cheap computing may enable much more extensive experimentation in algorithm design, tweaking parameters or using methods such as genetic algorithms. Indirectly, computing may enable the production and processing of enormous datasets to improve AI performance (Halevi et al., 2009), or result in an expansion of the information technology industry and the quantity of researchers in the field.<sup>6</sup></p>\n</blockquote>\n<p><em>Massive datasets</em>. The greatest leaps forward in speech recognition and translation software have come not from faster hardware or smarter hand-coded algorithms, but from access to massive data sets of human-transcribed and human-translated words (Halevy, Norvig, and Pereira 2009). [add sentence about how datasets are expected to increase massively, or have been increasing massively and trends are expected to continue] [Possibly a sentence about Watson or usefulness of data for AI]</p>\n<p><em>Better algorithms</em>. Mathematical insights can reduce the computation time of a program by many orders of magnitude without additional hardware. For example, IBM&rsquo;s Deep Blue played chess at the level of world champion Garry Kasparov in 1997 using about 1.5 trillion instructions per second (TIPS), but a program called Deep Junior did it in 2003 using only 0.15 TIPS. Thus, the power of the chess algorithms increased by a factor of 100 in only six years, or 3.33 orders of magnitude per decade (Richard and Shaw 2004). [add sentence about how this sort of improvement is not uncommon, with citations]</p>\n<p><em>Progress in neuroscience</em>. [neuroscientists have figured out brain algorithms X, Y, and Z that are related to intelligence.] New insights into how the brain achieves human-level intelligence can inform our attempts to build human-level intelligence with silicon (van der Velde 2010; Koene 2011).&nbsp;</p>\n<p><em>Accelerated science</em>. A growing First World will mean that more researchers at well-funded universities will be available to do research relevant to digital intelligence. The world&rsquo;s scientific output (in publications) grew by a third from 2002 to 2007 alone, much of this driven by the rapid growth of scientific output in developing nations like China and India (Smith 2011). New tools can accelerate particular fields, just as fMRI accelerated neuroscience in the 1990s. Finally, the effectiveness of scientists themselves can potentially be increased with cognitive enhancement drugs (Sandberg and Bostrom 2009) and brain-computer interfaces that allow direct neural access to large databases (Gro&szlig; 2009). Better collaboration tools like blogs and Google scholar are already yielding results (Nielsen 2011).</p>\n<p><em>Automated science</em>. Early attempts at automated science &mdash; e.g., using data mining algorithms to make discoveries from existing data (Szalay and Gray 2006), or having a machine with no physics knowledge correctly infer natural laws from motion-tracking data (Schmidt and Lipson 2009) &mdash; were limited by the slowest part of the process: the human in the loop. Recently, the first &ldquo;closed-loop&rdquo; robot scientist successfully devised its own hypotheses (about yeast genomics), conducted experiments to test those hypotheses, assessed the results, and made novel scientific discoveries, all without human intervention (King et al. 2009). Current closed-loop robot scientists can only work on a narrow set of scientific problems, but future advances may allow for scalable, automated scientific discovery (Sparkes et al. 2010).</p>\n<p><em>Embryo selection for better scientists</em>. At age 8, Terrence Tao scored 760 on the math SAT, one of only [2?3?] children ever to do this at such an age; he later went on to [have a lot of impact on math]. Studies of similar kids convince researchers that there is a large &ldquo;aptitude&rdquo; component to mathematical achievement, even at the high end.<sup>7</sup> How rapidly would mathematics or AI progress if we could create hundreds of thousands of Terrence Tao&rsquo;s? This is a serious question because the creation of large numbers of exceptional scientists is an engineering project that we know in principle how to do. The plummeting costs of genetic sequencing [expected to go below AMOUNT per genome by SOONYEAR e.g. 2015] will soon make it feasible to compare the characteristics of an entire population of adults with those adults&rsquo; full genomes, and, thereby, to unravel the heritable components of intelligence, dilligence, and other contributors to scientific achievement. To make large numbers of babies with scientific abilities near the top of the current human range<sup>8</sup> would then require only the ability to combine known alleles onto a single genome; procedures that can do this have already been developed for mice. China, at least, appears interested in this prospect.<sup>9</sup></p>\n<p>It isn&rsquo;t clear which of these factors will ease progress toward digital intelligence, but it seems likely that &mdash; across a broad range of scenarios &mdash; some of these inputs will do so.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>____</p>\n<p><sup>1</sup>&nbsp;For example, Simon (1965, 96) predicted that &ldquo;machines will be capable, within twenty years, of doing any work a man can do.&rdquo;</p>\n<p><sup>2</sup>&nbsp;The technical measure predicted by Moore&rsquo;s law is the density of components on an integrated circuit, but this is closely tied to affordable computing power.</p>\n<p><sup>3</sup>&nbsp;For important qualifications, see Nagy et al. (2010); Mack (2011).</p>\n<p><sup>4</sup>&nbsp;This calculation depends on the &ldquo;level of emulation&rdquo; expected to be necessary for successful WBE. Sandberg and Bostrom (2008) report that attendees to a workshop on WBE tended to expect that emulation at the level of the brain&rsquo;s spiking neural network, perhaps including membrane states and concentrations of metabolites and neurotransmitters, would be required for successful WBE. They estimate that if Moore&rsquo;s law continues, we will have the computational capacity to emulate a human brain at the level of its spiking neural network by 2019, or at the level of metabolites and neurotransmitters by 2029.</p>\n<p><sup>5</sup>&nbsp;Quantum computing may also emerge during this period. Early worries that quantum computing may not be feasible have been overcome, but it is hard to predict whether quantum computing will contribute significantly to the development of digital intelligence because progress in quantum computing depends heavily on unpredictable insights in quantum algorithms (Rieffel and Polak 2011).</p>\n<p><sup>6</sup>&nbsp;Shulman and Sandberg (2010).</p>\n<p><sup>7</sup> [Benbow etc. on study of exceptional talent; genetics of g; genetics of conscientiousness and openness, pref. w/ any data linking conscientiousness or openness to scientific achievement. &nbsp;Try to frame in a way that highlights hard work type variables, so as to alienate people less.]</p>\n<p><sup>8</sup>&nbsp;[folks with very top scientific achievement likely had lucky circumstances as well as initial gifts (so that, say, new kids with Einstein&rsquo;s genome would be expected to average perhaps .8 times as exceptional). &nbsp;However, one could probably identify genomes better than Einstein&rsquo;s, both because these technologies would let genomes be combined that had unheard of, vastly statistically unlikely amounts of luck, and because e.g. there are likely genomes out there that are substantially better than Einstein (but on folks who had worse environmental luck).]</p>\n<p><sup>9</sup> [find source]</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>_____</p>\n<p>All references, including the ones used above:</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<ul>\n<li>Bainbridge 2006 managing nano-bio-info-cogno innovations</li>\n<li>Baum Goertzel Goertzel 2011 how long until human-level ai</li>\n<li>Bostrom 2003 ethical issues in advanced artificial intelligence</li>\n<li>Legg 2008 machine super intelligence</li>\n<li>Caplan 2008 the totalitarian threat</li>\n<li>Sandberg &amp; Bostrom 2011 machine intelligence survey</li>\n<li>Chalmers 2010 singularity philosophical analysis</li>\n<li>Turing 1950 machine intelligence</li>\n<li>Good 1965 speculations concerning...</li>\n<li>Von neumann 1966 theory of self-reproducing autonomata</li>\n<li>Solomonoff 1985 the time scale of artificial intelligence</li>\n<li>Vinge 1993 coming technological singularity</li>\n<li>Yudkowsky 2001 creating friendly ai</li>\n<li>Yudkowsky 2008a negative and positive factor in global risk</li>\n<li>Yudkowsky 2008b cognitive biases potentially affecting</li>\n<li>Russel Norvig 2010 artificial intelligence a modern approach 3e</li>\n<li>Nordman 2007 If and then: a critique of speculative nanoethics</li>\n<li>Moore and Healy the trouble with overconfidence</li>\n<li>Tversky Kahneman 2002 extensional versus intuitive reasoning, the conjunction fallacy</li>\n<li>Nickerson 1998 Confirmation Bias; A Ubiquitous Phenomenon in Many Guises</li>\n<li>Dreyfus 1972 what computers can't do</li>\n<li>Rhodes 1995 making of the atomic bomb</li>\n<li>Arrhenius 1896 On the Influence of Carbonic Acid in the Air Upon the Temperature</li>\n<li>Crawford 1997 &nbsp;Arrhenius' 1896 model of the greenhouse effect in context</li>\n<li>Rasmussen 1975 WASH-1400 report</li>\n<li>McGrayne 2011 theory that would not die</li>\n<li>Lundstrom 2003 Enhanced: Moore&rsquo;s law forever?&nbsp;</li>\n<li>Tversky and Kahneman 1974 Judgment under uncertainty: Heuristics and biases</li>\n<li>Horgan 1997 end of science</li>\n<li>Sutton and Barto 1998 reinforcement learning</li>\n<li>Hutter 2004 universal ai</li>\n<li>Schmidhuber 2007 godel machines</li>\n<li>Dewey 2011 learning what to value</li>\n<li>Simon 1965 The Shape of Automation for Men and Management</li>\n<li>Marcus 2008 kluge</li>\n<li>Sandberg Bostrom 2008 whole brain emulation</li>\n<li>Kurzweil 2005 singularity is near</li>\n<li>Sermanet Lecun 2011 traffic sign recognition with multi-scale convolutional networks</li>\n<li>Li et al. 2009 optimizing a medical image analysis system using</li>\n<li>Markoff 2011 watson trivial it's not</li>\n<li>Smith 2011 Knowledge networks and nations</li>\n<li>Sandberg Bostrom 2009 cognitive enhancement regulatory issues</li>\n<li>Gro&szlig; 2009 Blessing or Curse? Neurocognitive Enhancement by &ldquo;Brain Engineering&rdquo;</li>\n<li>Williams 2011 prediction markets theory and appilcations</li>\n<li>Nielsen 2011 reinventing discovery</li>\n<li>Tetlock 2005 expert judgment</li>\n<li>Green &amp; Armstrong 2007 The Ombudsman: Value of Expertise for Forecasting&nbsp;</li>\n<li>Weinberg et al. 2010 philosophers expert intuiters</li>\n<li>Szalay and gray 2006 science in an exponential world</li>\n<li>Schmidt Lipson 2009 distilling free-form natural laws from experimental data</li>\n<li>King et al. 2009 the automation of science</li>\n<li>Sparkes et al. 2010 Towards Robot Scientists for autonomous scientific discovery</li>\n<li>Stanovich 2010 rationality and the reflective mind</li>\n<li>Lillienfeld, Ammirati, and Landfield 2009 giving debiasing away</li>\n<li>Lipman 1983 Thinking Skills Fostered by Philosophy for Children</li>\n<li>Fong et al 1986 The effects of statistical training on thinking about everyday problems</li>\n<li>Shoemaker (1979). The role of statistical knowledge in gambling decisions</li>\n<li>Larrick 2004 debiasing</li>\n<li>Gordon 2007 reasoning about the future of nanotechnology</li>\n<li>Landeta 2006 Current validity of the delphi method in social sciences</li>\n<li>Maddison 2001 the world economy a millenial perspective</li>\n<li>Niparko 2009 cochlear implants principles and practices</li>\n<li>Bostrom 2002 existential risks</li>\n<li>Joyce 2007 moral anti-realism stanford encyclopedia of philosophy</li>\n<li>Portmore 2011 commonsense consequentialism</li>\n<li>Martin 1971 brief proposal on immortality</li>\n<li>Bostrom Cirkovic 2008 global catastrophic risks</li>\n<li>National Academy of Sciences 2010 presistent forecasting of disruptive technologies</li>\n<li>Donohoe and Needham 2009 Moving best practice forward, Delphi characteristics</li>\n<li>Gordon 1994 the delphi method</li>\n<li>Kesten, Armstrong, and Graefe 2007 Methods to Elicit Forecasts from Groups</li>\n<li>Woudenberg 1991 an evaluation of delphi</li>\n<li>Armstrong 2006 Findings from evidence-based forecasting</li>\n<li>Armstrong 1985 Long-Range Forecasting: From Crystal Ball to Computer, 2nd edition</li>\n<li>Anderson and Anderson-Parente 2011 A case study of long-term Delphi accuracy</li>\n<li>Bixby 2002 Solving real-world linear programs: A decade and more of progress</li>\n<li>Fox 2011 the limits of intelligence</li>\n<li>Friedman 1953 The Methodology of Positive Economics</li>\n<li>Schneider 2010 homo economicus, or more like Homer Simpsons</li>\n<li>Cartwright 2011 behavioral economics</li>\n<li>Bacon and Van Dam 2010 recent progress in quantum algorithms</li>\n<li>Rieffel Polak 2011 quantum computing a gentle introduction</li>\n<li>Mack 2011 fifty years of moore&rsquo;s law</li>\n<li>Nagy et al. 2010 testing laws of technological progress</li>\n<li>Lundstrom 2003 Moore&rsquo;s law forever</li>\n<li>Shulman Sandberg 2010 implications of a software-limited singularity</li>\n<li>Moravec 1976 The Role of raw power in intelligence</li>\n<li>Halevi et al. 2009 The Unreasonable effectiveness of data</li>\n<li>Alberth 2008 forecasting technology costs via the experience curve</li>\n<li>Omohundro 2007 the nature of self-improving AI</li>\n<li>Kurzban 2011 why everyone (else) is a hypocrite: evolution and the modular mind</li>\n<li>Richard Shaw 2004 chips architectures and algorithms</li>\n<li>Yudkowsky 2010 timeless decision theory</li>\n<li>De Blanc Ontological Crises in Artificial Agents' Value Systems</li>\n<li>Dewey 2011 learning what to value</li>\n<li>Halevy, Norvig, and Pereira 2009 the unreasonable effectiveness of data</li>\n<li>Ramachandran 2011 the tell-tale brain</li>\n<li>van der Velde 2010 Where Artificial Intelligence and Neuroscience Meet</li>\n<li>Koene 2011 AGI and neuroscience: Open sourcing the brain (in AGI-11 proceedings)</li>\n<li>Lichtenstein, Fischoff, and Phillips 1982 calibration of probabilities the state of the art to 1980</li>\n<li>Griffin and Tversky 1992 The weighing of evidence and the determinants of confidence</li>\n<li>Yates, Lee, Sieck, Choi, Price 2002 Probability judgment across cultures</li>\n<li>Murphy and Winkler 1984 probability forecasting in meteorology</li>\n<li>Grove and Meehl 1996 Comparative Efficiency of Informal...</li>\n<li>Grove et al. 2000 Clinical versus mechanical prediction: A meta-analysis</li>\n<li>Kandel et al. 2000 principles of neural science, 4th edition</li>\n<li>Shulman 2010 Omohundro's \"Basic AI Drives\" and Catastrophic Risks</li>\n<li>Friedman 1993 Problems of Coordination in Economic Activity</li>\n<li>Cooke 1991 experts on uncertainty</li>\n<li>Yampolskiy forthcoming Leakproofing the Singularity</li>\n<li>Lampson 1973 a note on the confinement problem</li>\n<li>Schaeffer 1997 one jump ahead</li>\n<li>Dolan and Sharot 2011 Neuroscience of preference and choice</li>\n</ul>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YEkqxoWFCwAQBsMJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 8.036812048165488e-07, "legacy": true, "legacyId": "11063", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-22T23:02:43.098Z", "modifiedAt": null, "url": null, "title": "Value of Information: Four Examples", "slug": "value-of-information-four-examples", "viewCount": null, "lastCommentedAt": "2017-06-17T04:34:37.056Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Vaniver", "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vADtvr9iDeYsCDfxd/value-of-information-four-examples", "pageUrlRelative": "/posts/vADtvr9iDeYsCDfxd/value-of-information-four-examples", "linkUrl": "https://www.lesswrong.com/posts/vADtvr9iDeYsCDfxd/value-of-information-four-examples", "postedAtFormatted": "Tuesday, November 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20of%20Information%3A%20Four%20Examples&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20of%20Information%3A%20Four%20Examples%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvADtvr9iDeYsCDfxd%2Fvalue-of-information-four-examples%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20of%20Information%3A%20Four%20Examples%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvADtvr9iDeYsCDfxd%2Fvalue-of-information-four-examples", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvADtvr9iDeYsCDfxd%2Fvalue-of-information-four-examples", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2966, "htmlBody": "<p>Value of Information (VoI) is a concept from <a href=\"/lw/8xr/decision_analysis_sequence/\">decision analysis</a>: how much answering a question allows a decision-maker to improve its decision. Like opportunity cost, it's easy to define but often hard to internalize; and so instead of belaboring the definition let's look at some examples.<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2>Gambling with Biased Coins<br /></h2>\n<p>Normal coins are approximately fair.<sup>1</sup> Suppose you and your friend want to gamble, and fair coins are boring, so he takes out a quarter and some gum and sticks the gum to the face of the quarter near the edge. He then offers to pay you $24 if the coin lands gum down, so long as you pay him $12 to play the game. Should you take that bet?</p>\n<p>First, let's assume risk neutrality for the amount of money you're wagering. Your expected profit is $24<em>p</em>-12<em>, </em>where <em>p</em> is the probability the coin lands gum down. This is a good deal if <em>p&gt;</em>.5, but a bad deal if <em>p&lt;</em>.5.&nbsp; So... what's <em>p</em>? More importantly, how much should you pay to figure out <em>p</em>?</p>\n<p>A Bayesian reasoner looking at this problem first tries to put a prior on <em>p</em>. An easy choice is a uniform distribution between 0 and 1, but there are a lot of reasons to be uncomfortable with that distribution. It might be that the gum will be more likely to be on the bottom- but it also might be more likely to be on the top. The gum might not skew the results very much- or it might skew them massively. You could choose a different prior, but you'd have trouble justifying it because you don't have any solid evidence to update on yet.<sup>2</sup></p>\n<p>If you had a uniform prior and no additional evidence, then the deal as offered is neutral. But before you choose to accept or reject, your friend offers you <em>another</em> deal- he'll flip the coin once and let you see the result before you choose to take the $12 deal, but you can't win anything on this first flip. How much should you pay to see one flip?</p>\n<p>Start by modeling yourself after you see one flip. It'll either come up gum or no gum, and you'll update and produce a posterior for each case. In the first case, your posterior on <em>p</em> is P(<em>p</em>)=2p; in the second, P(<em>p</em>)=2-2p. Your expected profit for playing in the first case is $4;<sup>3</sup> your expected profit for playing in the second case is <strong>negative </strong>$4. You think there's a half chance it'll land gum side up, and a half chance it'll land gum side down, and if it lands gum side down <em>you can choose not to play</em>. There's a half chance you get $4 from seeing the flip, and a half chance you get nothing (because you don't play) from seeing the flip, and so $2 is the <strong>VoI</strong> of seeing one flip of the biased coin, <em>given your original prior</em>.</p>\n<p>Notice that, even though it'd be impossible to figure out the 'true' chance that the coin will land gum down, you can model how much it would be worth it to you to figure that out. If I were able to tell you <em>p</em> directly, then you could choose to gamble only when <em>p</em>&gt;.5, and you would earn an average of $3.<sup>4</sup>&nbsp; One coin flip gives you two thirds of the value that perfect information would give you.</p>\n<p>Also notice that you need to <strong>change your decision</strong> to get any value out of more information. Suppose that, instead of letting you choose whether or not to gamble, your friend made you decide, flipped two coins, and then paid you if the second coin landed gum down and you paid him. The coin is flipped the same number of times, but you're worse off because you have to decide with less information.</p>\n<p>It's also worth noting that multimodal distributions- where there are strong clusters rather than smooth landscapes- tend to have higher VoI. If we knew the biased coin would either always come up heads or always come up tails, and expected each case were equally likely, then seeing one flip is worth $6, because it's a half chance of a guaranteed $12.</p>\n<p>&nbsp;</p>\n<h2>Choosing where to invest</h2>\n<p>Here's an example I came across in my research:</p>\n<p><a href=\"http://create.usc.edu/research/past_projects/KleinmuntzWillis2009-Risk-BasedAllocationofResourcestoCount.pdf\">Kleinmuntz and Willis</a> were trying to determine the value of doing detailed anti-terrorism assessments in the state of California for the Department of Homeland Security. There are hundreds of critical infrastructure sites across the state, and it's simply not possible to do a detailed analysis of each site. There are terrorism experts, though, who can quickly provide an estimate of the risk to various sites.</p>\n<p>They gave a carefully designed survey to those experts, asking them to rate the relative probability that a site would be attacked (conditioned on an attack occurring) and the probability that an attack would succeed on a scale from 0 to 10, and the scale of fatalities and economic loss on a logarithmic scale from 0 to 7. The experts were comfortable with the survey<sup>5</sup> and able to give meaningful answers.</p>\n<p>Now Kleinmutz and Willis were able to take the elicited vulnerability estimates and come up with an estimated score for each facility. This estimated score gave them a prior over detailed scores for each site- if the experts all agreed that a site was a (0, 1, 2, 3), then that still implies a range over actual values. The economic loss resulting from a successful attack (3) could be anywhere from $100 million to $1 billion. (Notice that having a panel of experts gave them a natural way to determine the spread of the prior beyond the range inherent in their answers- where the experts agreed, they could clump the probability mass together, with only a little on answers the experts didn't give, and where the experts disagreed they knew where to spread the probability out over.) They already had, from another source, data on the effectiveness of the risk reductions available at the various sites and the costs of those reductions.</p>\n<p>The highest actual consequence elicited was for $6 billion, assuming a value of $6 million per life. The highest VoI of getting a detailed site analysis, though, was only $1.1 <em>million</em>. From the definition, this shouldn't be that surprising- VoI is only large when you would be surprised or uncertainty is high. For some sites, it was obvious that DHS should invest in reducing risk; in others, it was obvious that DHS shouldn't invest in reducing risk. The detailed vulnerability analysis would just tell them what they already knew, and so wouldn't provide any value. Some sites were on the edge- it might be worthwhile to reduce risk, it might not. For those sites, a detailed vulnerability analysis would provide value- but because the site was on the edge, the expected value of learning more was necessarily small!<sup>6</sup> Remember, for VoI to be positive you have to <strong>change your decision</strong>, and if that doesn't happen there's no VoI.</p>\n<p>Distressingly, they went on to consider the case where risk reduction could not be performed without a detailed vulnerability analysis. Then, rather than measuring VoI, they were mostly measuring the value of risk reduction- and the maximum value shot up to $840 million. When Bayesian evidence is good enough, requiring <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">legal evidence</a> can be costly.<sup>7</sup></p>\n<p>&nbsp;</p>\n<h2>Medical Testing<br /></h2>\n<p>About two years ago, I was sitting at my computer and noticed a black dot on my upper arm. I idly scratched it, and then saw its little legs move.</p>\n<p>It was an tick engorged on my blood, which I had probably picked up walking through the woods earlier. I removed it, then looked up online the proper way to remove it. (That's the wrong order, by the way: you need the information before you make your decision for it to be of any use. I didn't do it the proper way, and thus increased my risk of disease transmission.)</p>\n<p>Some ticks carry <a href=\"http://www.ncbi.nlm.nih.gov/pubmedhealth/PMH0002296/\">Lyme disease</a>, and so I looked into getting tested. I was surprised to learn that if I didn't present any symptoms by 30 days, the recommendation was against testing. After a moment's reflection, this made sense- tests typically have false positive rates. If I didn't have any symptoms after 30 days, even if I took the test and got a positive result the EV could be higher for no treatment than for treatment. In that case, the VoI of the test would be 0- <strong>regardless of its outcome, I would have made the same decision.</strong> If I saw symptoms, though, then the test would be worthwhile, as it could distinguish Lyme disease from an unrelated rash, headache, or fever. \"Waiting for symptoms to appear\" was the test with positive VoI, not getting a blood test right away.</p>\n<p>One could argue that the blood test could have \"peace of mind\" value, but that's distinct from VoI. Even beyond that, it's not clear that you would get positive peace of mind on net. Suppose the test has a 2% false positive rate- what happens when you multiply the peace of mind from a true negative by .98, and subtract the costs of dealing with the false positives by .02? That could easily be negative.</p>\n<p>(I remain symptom-free; either the tick didn't have Lyme disease, didn't transfer it to me, or my immune system managed to destroy it.)</p>\n<p>&nbsp;</p>\n<h2>Choosing a Career<br /></h2>\n<p>Many careers have significant prerequisites: if you want to be a doctor, you're going to have to go to medical school. People often have to choose where to invest their time with limited knowledge- you can't know what the career prospects will be like when you graduate, how much you'll enjoy your chosen field, and so on. Many people just choose based on accumulated experience- lawyers were high-status and rich before, so they suspect becoming a lawyer now is a good idea.<sup>8</sup></p>\n<p>Reducing that uncertainty can help you make a better decision, and VoI helps decide what ways to reduce uncertainty are effective. But this example also helps show the limits of VoI: VoI is best suited to situations where you've done the background research and are now considering further experiments. With the biased coin, we started off with a uniform prior; with the defensive investments, we started off with estimated risks. Do we have a comparable springboard for careers?</p>\n<p>If we do, it'll take some building. There's a lot of different value functions we could build- it probably ought to include stress, income (both starting and lifetime)<sup>9</sup>, risk of unemployment, satisfaction, and status. It's not clear how to elicit weights on those, though. There's research on what makes people in general happy, but you might be uncomfortable just using those weights.<sup>10</sup></p>\n<p>There are also hundreds, if not thousands, of career options available. Prior distributions on income <a href=\"http://www.bls.gov/bls/wages.htm\">are easy to find</a>, but stress is harder to determine. Unemployment risk is hard to predict over a lifetime, especially as it relies on macroeconomic trends that may be hard to predict. (The BLS predicts employment numbers out 10 years from data that's a few years old. It seems unlikely that they're set up to see crashes coming, though.)</p>\n<p>Satisfaction is probably the easiest place to start: there are lots of career aptitude tests out there that can take self-reported personality factors and turn that into a list of careers you might be well-suited for. Now you have a manageable decision problem- probably somewhere between six and twenty options to research in depth.</p>\n<p>What does that look like from a VoI framework? You've done a first screening which has identified places where more information might <em>alter your decision</em>. If you faint at the sight of blood, it doesn't matter how much surgeons make, and so any time spent looking that up is wasted. If you do a quick scoring of the six value components I listed above (after brainstorming for other things relevant to you), just weighting them with those quick values may give you good preliminary results. Only once you know what comparisons are relevant- \"what tradeoff between status and unemployment risk am I willing to make?\"- would you spend a long time nailing down your weights.</p>\n<p>This is also a decision problem that could take a long, long time. (Even after you've selected a career, the option to switch is always present.) It can be useful to keep upper and lower bounds for your estimates and update those along with your estimates- their current values and their changes with the last few pieces of information you found can give you an idea of how much you can expect to get from more research, and so you can finish researching and make a decision at a carefully chosen time, rather than when you get fatigued.</p>\n<p>&nbsp;</p>\n<h1>Conclusion<br /></h1>\n<p>Let's take another look at the definition: how much<em></em> <em>answering</em> a question allows a decision-maker to <em>improve</em> its <em>decision</em>.</p>\n<p>The \"answering\" is important because we need to consider all possible answers.<sup>11</sup> We're replacing one random variable with two random variables- in the case of the biased coin, it replaced one unknown coin (one flip) with either the lucky coin and the unlucky coin (two flips- one to figure out which coin, one to bet on). When computing VoI, you can't just consider one possible answer, but all possible answers considering their relative likelihood.<sup>12</sup></p>\n<p>The \"improve\" is important because VoI isn't about sleeping better at night or covering your ass. If you don't expect to change your decision after receiving this information, or you think that the expected value of the information (the chance you change your decision times the relative value of the decisions) is lower than the cost of the information, just bite the bullet and don't run the test you were considering.</p>\n<p>The \"decision\" is important because this isn't just curiosity. Learning facts is often fun, but for it to fit into VoI some decision has to depend on that fact. When watching televised poker, you know what all the hands are- and while that may alter your enjoyment of the hand, it won't affect how any of the players play. You shouldn't pay much for that information, but the players would pay quite a bit for it.<sup>13</sup></p>\n<p>&nbsp;</p>\n<p>1. <a href=\"http://www-stat.stanford.edu/~cgates/PERSI/papers/thinking.pdf\">Persi Diaconis</a> predicts most human coin flips are fair to 2 decimals but not 3, and it's possible through training to bias coins you flip. With a machine, you can be precise enough to get the coin to come up the same way every time.</p>\n<p>2. There is one thing that isn't coin-related: your friend is offering you this gamble, and probably has information you don't. That suggests the deal favors him- but suppose that you and your friend just thought this up, and so neither of you has more information than the other.</p>\n<p>3. Your profit is 24<em>p-</em>12; your distribution on <em>p</em> is P(<em>p</em>)=2p, and so your distribution on profit is 48p<sup>2</sup>-24p integrated from 0 to 1, which is <a href=\"http://www.wolframalpha.com/input/?i=integrate+48x^2-24x+from+0+to+1\">4</a>.</p>\n<p>4. Again, your profit is 24<em>p-</em>12; you have a uniform distribution on what I will tell you about <em>p</em>, but you only care about the section where <em>p</em>&gt;.5. Integrated from .5 to 1, that's <a href=\"http://www.wolframalpha.com/input/?i=integrate+24x-12+from+0.5+to+1\">3</a>.</p>\n<p>5. Whenever eliciting information from experts, make sure to repeat back to them what you heard and ensure that they agree with it. You might know decision theory, but the reason you're talking to experts is because they know things you don't. Consistency can take a few iterations, and that's to be expected.</p>\n<p>6. A common trope in decision analysis is \"if a decision is hard, flip a coin.\" Most people balk at this because it seems arbitrary (and, more importantly, hard to justify to others)- but if a decision is hard, that typically means both options are roughly equally valuable, and so the loss from the coin flip coming up the wrong value is necessarily small.</p>\n<p>7. That said, recommendations for policy-makers are hard to make here. Legal evidence is designed to be hard to game; Bayesian evidence isn't, and so Bayesian evidence is only \"good enough\" if it's not being gamed. Checking your heuristic (i.e. the expert's estimates) to keep it honest can provide significant value. Performing detailed vulnerability analysis on some (how many?) randomly chosen sites for calibration is often a good choice. Beyond that, I can't do much besides point you to psychology to figure out good ways to diagnose and reduce bias.</p>\n<p>8. It doesn't appear that this is the case anymore. The supply of lawyers has dramatically increased, and so wages are declining; as well, law is a pretty soul-crushing field from a stress, work-life balance, and satisfaction perspective. If law looks like the best field for you and you're not in it for the money or status, the advice I hear is to specialize in a niche field that'll put food on the table but stay interesting and tolerably demanding.</p>\n<p>9. Both of these capture different information. A job with a high starting salary but no growth prospects might translate into more happiness than a job with a low starting salary but high growth prospects, for example.</p>\n<p>10. Most of the happiness/satisfaction literature I've seen has asked people about their attributes and their happiness/satisfaction. That's not a randomized trial, though, and so there could be massive selection effects. If we find that engineers are collectively less happy than waiters, does that mean engineering causes unhappiness, unhappiness causes engineering, that unhappiness and engineering are caused by the same thing, or none of those?</p>\n<p>11. Compare this with information theory, where bits are a property of answers, <em>not</em> questions. Here, VoI is a property of questions, <em>not</em> answers.</p>\n<p>12. If you already know the cost of the information, then you can stop computing as soon as you find a positive outcome good enough and likely enough that the VoI so far is higher than the cost.</p>\n<p>13. In high-stakes poker games, the VoI can get rather high, and the deceit / reading involved is why poker is a more interesting game than, say, the lottery.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DbMQGrxbhLxtNkmca": 2, "Ng8Gice9KNkncxqcj": 2, "KoXbd2HmbdRfqLngk": 2, "4kQXps8dYsKJgaayN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vADtvr9iDeYsCDfxd", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 89, "baseScore": 117, "extendedScore": null, "score": 0.00024, "legacy": true, "legacyId": "10581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 117, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Value of Information (VoI) is a concept from <a href=\"/lw/8xr/decision_analysis_sequence/\">decision analysis</a>: how much answering a question allows a decision-maker to improve its decision. Like opportunity cost, it's easy to define but often hard to internalize; and so instead of belaboring the definition let's look at some examples.<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h2 id=\"Gambling_with_Biased_Coins\">Gambling with Biased Coins<br></h2>\n<p>Normal coins are approximately fair.<sup>1</sup> Suppose you and your friend want to gamble, and fair coins are boring, so he takes out a quarter and some gum and sticks the gum to the face of the quarter near the edge. He then offers to pay you $24 if the coin lands gum down, so long as you pay him $12 to play the game. Should you take that bet?</p>\n<p>First, let's assume risk neutrality for the amount of money you're wagering. Your expected profit is $24<em>p</em>-12<em>, </em>where <em>p</em> is the probability the coin lands gum down. This is a good deal if <em>p&gt;</em>.5, but a bad deal if <em>p&lt;</em>.5.&nbsp; So... what's <em>p</em>? More importantly, how much should you pay to figure out <em>p</em>?</p>\n<p>A Bayesian reasoner looking at this problem first tries to put a prior on <em>p</em>. An easy choice is a uniform distribution between 0 and 1, but there are a lot of reasons to be uncomfortable with that distribution. It might be that the gum will be more likely to be on the bottom- but it also might be more likely to be on the top. The gum might not skew the results very much- or it might skew them massively. You could choose a different prior, but you'd have trouble justifying it because you don't have any solid evidence to update on yet.<sup>2</sup></p>\n<p>If you had a uniform prior and no additional evidence, then the deal as offered is neutral. But before you choose to accept or reject, your friend offers you <em>another</em> deal- he'll flip the coin once and let you see the result before you choose to take the $12 deal, but you can't win anything on this first flip. How much should you pay to see one flip?</p>\n<p>Start by modeling yourself after you see one flip. It'll either come up gum or no gum, and you'll update and produce a posterior for each case. In the first case, your posterior on <em>p</em> is P(<em>p</em>)=2p; in the second, P(<em>p</em>)=2-2p. Your expected profit for playing in the first case is $4;<sup>3</sup> your expected profit for playing in the second case is <strong>negative </strong>$4. You think there's a half chance it'll land gum side up, and a half chance it'll land gum side down, and if it lands gum side down <em>you can choose not to play</em>. There's a half chance you get $4 from seeing the flip, and a half chance you get nothing (because you don't play) from seeing the flip, and so $2 is the <strong>VoI</strong> of seeing one flip of the biased coin, <em>given your original prior</em>.</p>\n<p>Notice that, even though it'd be impossible to figure out the 'true' chance that the coin will land gum down, you can model how much it would be worth it to you to figure that out. If I were able to tell you <em>p</em> directly, then you could choose to gamble only when <em>p</em>&gt;.5, and you would earn an average of $3.<sup>4</sup>&nbsp; One coin flip gives you two thirds of the value that perfect information would give you.</p>\n<p>Also notice that you need to <strong>change your decision</strong> to get any value out of more information. Suppose that, instead of letting you choose whether or not to gamble, your friend made you decide, flipped two coins, and then paid you if the second coin landed gum down and you paid him. The coin is flipped the same number of times, but you're worse off because you have to decide with less information.</p>\n<p>It's also worth noting that multimodal distributions- where there are strong clusters rather than smooth landscapes- tend to have higher VoI. If we knew the biased coin would either always come up heads or always come up tails, and expected each case were equally likely, then seeing one flip is worth $6, because it's a half chance of a guaranteed $12.</p>\n<p>&nbsp;</p>\n<h2 id=\"Choosing_where_to_invest\">Choosing where to invest</h2>\n<p>Here's an example I came across in my research:</p>\n<p><a href=\"http://create.usc.edu/research/past_projects/KleinmuntzWillis2009-Risk-BasedAllocationofResourcestoCount.pdf\">Kleinmuntz and Willis</a> were trying to determine the value of doing detailed anti-terrorism assessments in the state of California for the Department of Homeland Security. There are hundreds of critical infrastructure sites across the state, and it's simply not possible to do a detailed analysis of each site. There are terrorism experts, though, who can quickly provide an estimate of the risk to various sites.</p>\n<p>They gave a carefully designed survey to those experts, asking them to rate the relative probability that a site would be attacked (conditioned on an attack occurring) and the probability that an attack would succeed on a scale from 0 to 10, and the scale of fatalities and economic loss on a logarithmic scale from 0 to 7. The experts were comfortable with the survey<sup>5</sup> and able to give meaningful answers.</p>\n<p>Now Kleinmutz and Willis were able to take the elicited vulnerability estimates and come up with an estimated score for each facility. This estimated score gave them a prior over detailed scores for each site- if the experts all agreed that a site was a (0, 1, 2, 3), then that still implies a range over actual values. The economic loss resulting from a successful attack (3) could be anywhere from $100 million to $1 billion. (Notice that having a panel of experts gave them a natural way to determine the spread of the prior beyond the range inherent in their answers- where the experts agreed, they could clump the probability mass together, with only a little on answers the experts didn't give, and where the experts disagreed they knew where to spread the probability out over.) They already had, from another source, data on the effectiveness of the risk reductions available at the various sites and the costs of those reductions.</p>\n<p>The highest actual consequence elicited was for $6 billion, assuming a value of $6 million per life. The highest VoI of getting a detailed site analysis, though, was only $1.1 <em>million</em>. From the definition, this shouldn't be that surprising- VoI is only large when you would be surprised or uncertainty is high. For some sites, it was obvious that DHS should invest in reducing risk; in others, it was obvious that DHS shouldn't invest in reducing risk. The detailed vulnerability analysis would just tell them what they already knew, and so wouldn't provide any value. Some sites were on the edge- it might be worthwhile to reduce risk, it might not. For those sites, a detailed vulnerability analysis would provide value- but because the site was on the edge, the expected value of learning more was necessarily small!<sup>6</sup> Remember, for VoI to be positive you have to <strong>change your decision</strong>, and if that doesn't happen there's no VoI.</p>\n<p>Distressingly, they went on to consider the case where risk reduction could not be performed without a detailed vulnerability analysis. Then, rather than measuring VoI, they were mostly measuring the value of risk reduction- and the maximum value shot up to $840 million. When Bayesian evidence is good enough, requiring <a href=\"/lw/in/scientific_evidence_legal_evidence_rational/\">legal evidence</a> can be costly.<sup>7</sup></p>\n<p>&nbsp;</p>\n<h2 id=\"Medical_Testing\">Medical Testing<br></h2>\n<p>About two years ago, I was sitting at my computer and noticed a black dot on my upper arm. I idly scratched it, and then saw its little legs move.</p>\n<p>It was an tick engorged on my blood, which I had probably picked up walking through the woods earlier. I removed it, then looked up online the proper way to remove it. (That's the wrong order, by the way: you need the information before you make your decision for it to be of any use. I didn't do it the proper way, and thus increased my risk of disease transmission.)</p>\n<p>Some ticks carry <a href=\"http://www.ncbi.nlm.nih.gov/pubmedhealth/PMH0002296/\">Lyme disease</a>, and so I looked into getting tested. I was surprised to learn that if I didn't present any symptoms by 30 days, the recommendation was against testing. After a moment's reflection, this made sense- tests typically have false positive rates. If I didn't have any symptoms after 30 days, even if I took the test and got a positive result the EV could be higher for no treatment than for treatment. In that case, the VoI of the test would be 0- <strong>regardless of its outcome, I would have made the same decision.</strong> If I saw symptoms, though, then the test would be worthwhile, as it could distinguish Lyme disease from an unrelated rash, headache, or fever. \"Waiting for symptoms to appear\" was the test with positive VoI, not getting a blood test right away.</p>\n<p>One could argue that the blood test could have \"peace of mind\" value, but that's distinct from VoI. Even beyond that, it's not clear that you would get positive peace of mind on net. Suppose the test has a 2% false positive rate- what happens when you multiply the peace of mind from a true negative by .98, and subtract the costs of dealing with the false positives by .02? That could easily be negative.</p>\n<p>(I remain symptom-free; either the tick didn't have Lyme disease, didn't transfer it to me, or my immune system managed to destroy it.)</p>\n<p>&nbsp;</p>\n<h2 id=\"Choosing_a_Career\">Choosing a Career<br></h2>\n<p>Many careers have significant prerequisites: if you want to be a doctor, you're going to have to go to medical school. People often have to choose where to invest their time with limited knowledge- you can't know what the career prospects will be like when you graduate, how much you'll enjoy your chosen field, and so on. Many people just choose based on accumulated experience- lawyers were high-status and rich before, so they suspect becoming a lawyer now is a good idea.<sup>8</sup></p>\n<p>Reducing that uncertainty can help you make a better decision, and VoI helps decide what ways to reduce uncertainty are effective. But this example also helps show the limits of VoI: VoI is best suited to situations where you've done the background research and are now considering further experiments. With the biased coin, we started off with a uniform prior; with the defensive investments, we started off with estimated risks. Do we have a comparable springboard for careers?</p>\n<p>If we do, it'll take some building. There's a lot of different value functions we could build- it probably ought to include stress, income (both starting and lifetime)<sup>9</sup>, risk of unemployment, satisfaction, and status. It's not clear how to elicit weights on those, though. There's research on what makes people in general happy, but you might be uncomfortable just using those weights.<sup>10</sup></p>\n<p>There are also hundreds, if not thousands, of career options available. Prior distributions on income <a href=\"http://www.bls.gov/bls/wages.htm\">are easy to find</a>, but stress is harder to determine. Unemployment risk is hard to predict over a lifetime, especially as it relies on macroeconomic trends that may be hard to predict. (The BLS predicts employment numbers out 10 years from data that's a few years old. It seems unlikely that they're set up to see crashes coming, though.)</p>\n<p>Satisfaction is probably the easiest place to start: there are lots of career aptitude tests out there that can take self-reported personality factors and turn that into a list of careers you might be well-suited for. Now you have a manageable decision problem- probably somewhere between six and twenty options to research in depth.</p>\n<p>What does that look like from a VoI framework? You've done a first screening which has identified places where more information might <em>alter your decision</em>. If you faint at the sight of blood, it doesn't matter how much surgeons make, and so any time spent looking that up is wasted. If you do a quick scoring of the six value components I listed above (after brainstorming for other things relevant to you), just weighting them with those quick values may give you good preliminary results. Only once you know what comparisons are relevant- \"what tradeoff between status and unemployment risk am I willing to make?\"- would you spend a long time nailing down your weights.</p>\n<p>This is also a decision problem that could take a long, long time. (Even after you've selected a career, the option to switch is always present.) It can be useful to keep upper and lower bounds for your estimates and update those along with your estimates- their current values and their changes with the last few pieces of information you found can give you an idea of how much you can expect to get from more research, and so you can finish researching and make a decision at a carefully chosen time, rather than when you get fatigued.</p>\n<p>&nbsp;</p>\n<h1 id=\"Conclusion\">Conclusion<br></h1>\n<p>Let's take another look at the definition: how much<em></em> <em>answering</em> a question allows a decision-maker to <em>improve</em> its <em>decision</em>.</p>\n<p>The \"answering\" is important because we need to consider all possible answers.<sup>11</sup> We're replacing one random variable with two random variables- in the case of the biased coin, it replaced one unknown coin (one flip) with either the lucky coin and the unlucky coin (two flips- one to figure out which coin, one to bet on). When computing VoI, you can't just consider one possible answer, but all possible answers considering their relative likelihood.<sup>12</sup></p>\n<p>The \"improve\" is important because VoI isn't about sleeping better at night or covering your ass. If you don't expect to change your decision after receiving this information, or you think that the expected value of the information (the chance you change your decision times the relative value of the decisions) is lower than the cost of the information, just bite the bullet and don't run the test you were considering.</p>\n<p>The \"decision\" is important because this isn't just curiosity. Learning facts is often fun, but for it to fit into VoI some decision has to depend on that fact. When watching televised poker, you know what all the hands are- and while that may alter your enjoyment of the hand, it won't affect how any of the players play. You shouldn't pay much for that information, but the players would pay quite a bit for it.<sup>13</sup></p>\n<p>&nbsp;</p>\n<p>1. <a href=\"http://www-stat.stanford.edu/~cgates/PERSI/papers/thinking.pdf\">Persi Diaconis</a> predicts most human coin flips are fair to 2 decimals but not 3, and it's possible through training to bias coins you flip. With a machine, you can be precise enough to get the coin to come up the same way every time.</p>\n<p>2. There is one thing that isn't coin-related: your friend is offering you this gamble, and probably has information you don't. That suggests the deal favors him- but suppose that you and your friend just thought this up, and so neither of you has more information than the other.</p>\n<p>3. Your profit is 24<em>p-</em>12; your distribution on <em>p</em> is P(<em>p</em>)=2p, and so your distribution on profit is 48p<sup>2</sup>-24p integrated from 0 to 1, which is <a href=\"http://www.wolframalpha.com/input/?i=integrate+48x^2-24x+from+0+to+1\">4</a>.</p>\n<p>4. Again, your profit is 24<em>p-</em>12; you have a uniform distribution on what I will tell you about <em>p</em>, but you only care about the section where <em>p</em>&gt;.5. Integrated from .5 to 1, that's <a href=\"http://www.wolframalpha.com/input/?i=integrate+24x-12+from+0.5+to+1\">3</a>.</p>\n<p>5. Whenever eliciting information from experts, make sure to repeat back to them what you heard and ensure that they agree with it. You might know decision theory, but the reason you're talking to experts is because they know things you don't. Consistency can take a few iterations, and that's to be expected.</p>\n<p>6. A common trope in decision analysis is \"if a decision is hard, flip a coin.\" Most people balk at this because it seems arbitrary (and, more importantly, hard to justify to others)- but if a decision is hard, that typically means both options are roughly equally valuable, and so the loss from the coin flip coming up the wrong value is necessarily small.</p>\n<p>7. That said, recommendations for policy-makers are hard to make here. Legal evidence is designed to be hard to game; Bayesian evidence isn't, and so Bayesian evidence is only \"good enough\" if it's not being gamed. Checking your heuristic (i.e. the expert's estimates) to keep it honest can provide significant value. Performing detailed vulnerability analysis on some (how many?) randomly chosen sites for calibration is often a good choice. Beyond that, I can't do much besides point you to psychology to figure out good ways to diagnose and reduce bias.</p>\n<p>8. It doesn't appear that this is the case anymore. The supply of lawyers has dramatically increased, and so wages are declining; as well, law is a pretty soul-crushing field from a stress, work-life balance, and satisfaction perspective. If law looks like the best field for you and you're not in it for the money or status, the advice I hear is to specialize in a niche field that'll put food on the table but stay interesting and tolerably demanding.</p>\n<p>9. Both of these capture different information. A job with a high starting salary but no growth prospects might translate into more happiness than a job with a low starting salary but high growth prospects, for example.</p>\n<p>10. Most of the happiness/satisfaction literature I've seen has asked people about their attributes and their happiness/satisfaction. That's not a randomized trial, though, and so there could be massive selection effects. If we find that engineers are collectively less happy than waiters, does that mean engineering causes unhappiness, unhappiness causes engineering, that unhappiness and engineering are caused by the same thing, or none of those?</p>\n<p>11. Compare this with information theory, where bits are a property of answers, <em>not</em> questions. Here, VoI is a property of questions, <em>not</em> answers.</p>\n<p>12. If you already know the cost of the information, then you can stop computing as soon as you find a positive outcome good enough and likely enough that the VoI so far is higher than the cost.</p>\n<p>13. In high-stakes poker games, the VoI can get rather high, and the deceit / reading involved is why poker is a more interesting game than, say, the lottery.</p>", "sections": [{"title": "Gambling with Biased Coins", "anchor": "Gambling_with_Biased_Coins", "level": 2}, {"title": "Choosing where to invest", "anchor": "Choosing_where_to_invest", "level": 2}, {"title": "Medical Testing", "anchor": "Medical_Testing", "level": 2}, {"title": "Choosing a Career", "anchor": "Choosing_a_Career", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "56 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 60, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "fhojYBGGiYAFcryHZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 38, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-23T04:06:53.362Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Argument Screens Off Authority", "slug": "seq-rerun-argument-screens-off-authority", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.728Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y4G8Jq2YzuTASsQRg/seq-rerun-argument-screens-off-authority", "pageUrlRelative": "/posts/Y4G8Jq2YzuTASsQRg/seq-rerun-argument-screens-off-authority", "linkUrl": "https://www.lesswrong.com/posts/Y4G8Jq2YzuTASsQRg/seq-rerun-argument-screens-off-authority", "postedAtFormatted": "Wednesday, November 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Argument%20Screens%20Off%20Authority&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Argument%20Screens%20Off%20Authority%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4G8Jq2YzuTASsQRg%2Fseq-rerun-argument-screens-off-authority%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Argument%20Screens%20Off%20Authority%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4G8Jq2YzuTASsQRg%2Fseq-rerun-argument-screens-off-authority", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY4G8Jq2YzuTASsQRg%2Fseq-rerun-argument-screens-off-authority", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/lx/argument_screens_off_authority/\">Argument Screens Off Authority</a> was originally published on 14 December 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are many cases in which we should take the authority of experts into account, when we decide whether or not to believe their claims. But, if there are technical arguments that are available, these can <em>screen off</em>&nbsp;the authority of experts.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/lw/8iv/seq_rerun_reversed_stupidity_is_not_intelligence/#comments\">Reversed Stupidity Is Not Intelligence</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y4G8Jq2YzuTASsQRg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 11, "extendedScore": null, "score": 8.038021284821223e-07, "legacy": true, "legacyId": "11074", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5yFRd3cjLpm3Nd6Di", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-23T18:14:16.546Z", "modifiedAt": null, "url": null, "title": "What mathematics to learn", "slug": "what-mathematics-to-learn", "viewCount": null, "lastCommentedAt": "2011-11-28T19:10:52.458Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Incorrect", "createdAt": "2011-05-19T06:36:41.700Z", "isAdmin": false, "displayName": "Incorrect"}, "userId": "YKd5C3yr2o6NtJgtP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LfFw5Lb7RkT4QzZcC/what-mathematics-to-learn", "pageUrlRelative": "/posts/LfFw5Lb7RkT4QzZcC/what-mathematics-to-learn", "linkUrl": "https://www.lesswrong.com/posts/LfFw5Lb7RkT4QzZcC/what-mathematics-to-learn", "postedAtFormatted": "Wednesday, November 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20mathematics%20to%20learn&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20mathematics%20to%20learn%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLfFw5Lb7RkT4QzZcC%2Fwhat-mathematics-to-learn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20mathematics%20to%20learn%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLfFw5Lb7RkT4QzZcC%2Fwhat-mathematics-to-learn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLfFw5Lb7RkT4QzZcC%2Fwhat-mathematics-to-learn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 46, "htmlBody": "<p>There is, of course, <a href=\"http://www.khanacademy.org/\">Kahn Academy</a> for fundamentals. We have already had a discussion on <a href=\"/lw/2ub/discuss_how_to_learn_math/\">How to learn math</a>.</p>\n<p>What resources exist detailing <em>which</em> mathematics to learn in what order? What resources exist that explain the utility of different mathematical subfields for the purpose of directing studies?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LfFw5Lb7RkT4QzZcC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.041046946841713e-07, "legacy": true, "legacyId": "11080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AttkaMkEGeMiaQnYJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-23T19:52:10.551Z", "modifiedAt": null, "url": null, "title": "Studying Psychology - Which path should I take to best help our cause? Suggestions please.", "slug": "studying-psychology-which-path-should-i-take-to-best-help", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Friendly-HI", "createdAt": "2011-04-18T18:19:01.357Z", "isAdmin": false, "displayName": "Friendly-HI"}, "userId": "nXA5fJiYcfJGaQkwG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JwYGn2eLMXwPoMdhJ/studying-psychology-which-path-should-i-take-to-best-help", "pageUrlRelative": "/posts/JwYGn2eLMXwPoMdhJ/studying-psychology-which-path-should-i-take-to-best-help", "linkUrl": "https://www.lesswrong.com/posts/JwYGn2eLMXwPoMdhJ/studying-psychology-which-path-should-i-take-to-best-help", "postedAtFormatted": "Wednesday, November 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Studying%20Psychology%20-%20Which%20path%20should%20I%20take%20to%20best%20help%20our%20cause%3F%20Suggestions%20please.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStudying%20Psychology%20-%20Which%20path%20should%20I%20take%20to%20best%20help%20our%20cause%3F%20Suggestions%20please.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwYGn2eLMXwPoMdhJ%2Fstudying-psychology-which-path-should-i-take-to-best-help%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Studying%20Psychology%20-%20Which%20path%20should%20I%20take%20to%20best%20help%20our%20cause%3F%20Suggestions%20please.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwYGn2eLMXwPoMdhJ%2Fstudying-psychology-which-path-should-i-take-to-best-help", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJwYGn2eLMXwPoMdhJ%2Fstudying-psychology-which-path-should-i-take-to-best-help", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 938, "htmlBody": "<p>If you solve the problem of human-friendly self-improving AI, you have indirectly solved every problem. After spending a decent amount of time on LW, I have been convinced of this premise and now I would like to devote my life to that cause.</p>\n<p>&nbsp;</p>\n<p>Currently I'm living in Germany and I'm studying psychology in the first semester. The university I'm studying at has a great reputation (even internationally if I can believe the rankings) for the quality of its scientific psychology research and it ranks about #2nd or #3rd place when it comes to various psy-science-related criteria out of about 55 German universities where one can study psychology. Five semesters of statistics in my Bachelor of Science might also hint at that.</p>\n<p>I want to finish my Bachelor of Science and then move on to my Master,&nbsp;so in about 5 years I might hit my&nbsp;\"phase of actual productivity\" in the working field. I'm flirting with cognitive neuroscience, but haven't made my decision yet - however, I am pretty sure that I want to move&nbsp;towards research and a scientific career rather than one in a therapeutic field.</p>\n<p>Before discovering lesswrong my most dominant personal interest in psychology has been in the field of \"positive psychology\" or plainly speaking the \"what makes humans happy\" field. This interest&nbsp;hasn't really changed through the discovery of LW, as much as it has evolved into: \"how can we distill what makes human life worthwhile and put it into terms a machine could execute for our benefit\"?</p>\n<p>&nbsp;</p>\n<p>As the title suggests, I'm writing all this because I want some creative input from you in order to expand my sense of possibilities concerning how I can&nbsp;help the development of friendly AI from the field of psychology most effectively.</p>\n<p>&nbsp;</p>\n<p>To give you a better idea of what might fit me, a bit more background-info about myself and my abilities seems in order:</p>\n<p>I like talking and writing a lot, mathematically I am a loser (whether due to early disgust or incompetence I can't really tell). I value and enjoy human contact and have constantly moved from being an introvert towards&nbsp;being an extrovert by several cognitive developments I can only speculate on. I would probably easily rank in the middle field of any positive&nbsp;extroversion&nbsp;scale nowadays. My IQ seems to be around 134 if one can trust the \"International High IQ Society\" (www.highiqsociety.org), but as mentioned my abilities probably lie more in the linguistic and to some extent&nbsp;analytic&nbsp;sphere than the mathematical. I understand Bayes' Theorem but haven't read the quantum mechanics sequence and many \"higher\" concepts here are still above my current level of comprehension. Although I haven't tried all that hard yet to be fair.</p>\n<p>I have programmed some primitive HTML and CSS once and&nbsp;didn't really like it. From that experiecne and my mathematical inability&nbsp;I take away, that programming wouldn't be the way that I could contribute most efficiently towards friendly AI-research. It is none of my strenghts or at least it would take a lot of time to develop that, which would probably be better spent somewhere else. Also I quite surely wouldn't enjoy it as much as work in the psychological realm with humans.</p>\n<p>My English is almost indistinguishable from that of a native speaker and I largely lack that (rightfully) despised and annoying German accent, so I could definitely see myself giving competent talks in English.</p>\n<p>Like many of you I have serious problems with akrasia (regardless of whether that's a rationalist phenomenon or whether we are just more aware of it and tend to do types of work that tempt it more readily). Before I learned of how to effectively combat it (thank you Piers Steel!), I had plenty of motivation to get rid of it and sunk insane efforts into overcoming it, although ultimately it was largely an unsuccessful undertaking due to half-assed pop-science and the lack of a real insight about what procrastination is caused by and how it actually functions. Now that I know how to fix&nbsp;procrastination (or rather now that I know that it can't be fixed, as much as it has to be managed in a similar fashion to any given drug-addition), my motivation to overcome it is almost gone and I feel myself slacking. Also, the high certainty that there is no such thing as \"free will\" may have played a serious part in my procrastination habits (interestingly, there are at least two papers I recall showing this correlation). In a nutshell: Procrastination is a problem that I need to address, since it is definitely the Achilles' heel of my performance and it's absolutely crippling my potential. I probably rank middle-high on the impulsiveness- (and thus also on the procrastination-) scale.</p>\n<p>That should be an adequate characterization of myself for now.</p>\n<p>&nbsp;</p>\n<p>I am absolutely open for suggestions that are not related to the neuroscience of \"what makes humans happy and how do I distill those goals and feelings into something a machine could work with\"-field, but currently I am definitely flirting with that idea, even though I have absolutely no clue how the heck this area of research could be sufficiently financed in a decade from now and how it could spit out findings precise enough to benefit the creation of FAI. Yet maybe it's just a lack of imagination.</p>\n<p>Trying to help set up and evolve a rationalist community in Germany would also be a decent task, but compared to specific research that actually directly aids our goals... I somehow feel it is less than what I could reasonably achieve if I really set my mind to it.</p>\n<p>&nbsp;</p>\n<p>So tell me, where does a German psychologist go nowadays to achieve the biggest possible positive impact in the field of friendly AI?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JwYGn2eLMXwPoMdhJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 3, "extendedScore": null, "score": 8.041396636864704e-07, "legacy": true, "legacyId": "11051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-23T19:52:32.175Z", "modifiedAt": null, "url": null, "title": "FAI FAQ draft: general intelligence and greater-than-human intelligence", "slug": "fai-faq-draft-general-intelligence-and-greater-than-human", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:50.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hqxSXSNisLqvSE49e/fai-faq-draft-general-intelligence-and-greater-than-human", "pageUrlRelative": "/posts/hqxSXSNisLqvSE49e/fai-faq-draft-general-intelligence-and-greater-than-human", "linkUrl": "https://www.lesswrong.com/posts/hqxSXSNisLqvSE49e/fai-faq-draft-general-intelligence-and-greater-than-human", "postedAtFormatted": "Wednesday, November 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%20FAQ%20draft%3A%20general%20intelligence%20and%20greater-than-human%20intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%20FAQ%20draft%3A%20general%20intelligence%20and%20greater-than-human%20intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqxSXSNisLqvSE49e%2Ffai-faq-draft-general-intelligence-and-greater-than-human%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%20FAQ%20draft%3A%20general%20intelligence%20and%20greater-than-human%20intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqxSXSNisLqvSE49e%2Ffai-faq-draft-general-intelligence-and-greater-than-human", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqxSXSNisLqvSE49e%2Ffai-faq-draft-general-intelligence-and-greater-than-human", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1528, "htmlBody": "<p>My thanks to everyone who has provided feedback on these drafts so far. It's been helpful, and I've been incorporating your suggestions into the document. Now, Iinvite your feedback on these two snippets from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. For references, see <a href=\"/lw/8gh/fai_faq_draft_what_is_the_singularity/5awr\">here</a>.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<h4 id=\"WhatIsGeneralIntelligence\">1.10. What is general intelligence?</h4>\n<p>There are many competing definitions and theories of intelligence (Davidson &amp; Kemp 2011; Niu &amp; Brass 2011; Legg &amp; Hutter 2007), and the term has seen its share of emotionally-laden controversy (Halpern et al. 2011; Daley &amp; Onwuegbuzie 2011).</p>\n<p>Legg (2008) collects dozens of definitions of intelligence, and finds that they loosely converge on the following idea:</p>\n<blockquote>\n<p>Intelligence measures an agent&rsquo;s ability to achieve goals in a wide range of environments.</p>\n</blockquote>\n<p>That will be our &lsquo;working definition&rsquo; for intelligence in this FAQ.</p>\n<p>There is a sense in which famous computers like <a href=\"http://is.gd/82KuU3\">Deep Blue</a> and <a href=\"http://en.wikipedia.org/wiki/IBM_Watson\">Watson</a> are &ldquo;intelligent.&rdquo; They can out-perform human competitors for a narrow range of goals (winning chess games or answers <em>Jeopardy!</em> questions), in a narrow range of environments. But drop them in a novel environment &mdash; a shallow pond or a New York taxicab &mdash; and they are dumb and helpless. In this sense their &ldquo;intelligence&rdquo; is not general.</p>\n<p>Human intelligence is general in that it allows us to achieve goals in a wide range of environments. We can solve new problems of survival, competition, and fun in a wide range of environments, including ones never before encountered. That is, after all, how humans came to dominate all the land and air on Earth, and what empowers us to explore more extreme environments &mdash; like the deep sea or outer space &mdash; when we choose to. Humans have invented languages, developed agriculture, domesticated other animals, created crafts and arts and architecture, written philosophy, explored the planet, discovered math and science, evolved new political and economic systems, built machines, developed medicine, and made plans for the distant future.</p>\n<p>Some other animals also have a slower but more general intelligence than Deep Blue and Watson. Apes, dolphins, elephants, and a few species of bird have demonstrated some ability to solve novel problems in novel environments (Zentall 2011).</p>\n<p>General intelligence in a machine is called artificial general intelligence (AGI). Nobody has developed AGI yet, though many approaches are being attempted. Goertzel &amp; Pennachin (2007) provides an overview of approaches to AGI.</p>\n<p>&nbsp;</p>\n<h4 id=\"WhatIsGreaterThan\">1.11. What is greater-than-human intelligence?</h4>\n<p>Humans gained dominance over Earth not because we had superior strength, speed, or durability, but because we had superior intelligence. It is our intelligence that makes us powerful. It is our intelligence that allows us to adapt to new environments. It is our intelligence that allows us to subdue animals or invent machines that surpass us in strength, speed, durability and other qualities.</p>\n<p>Humans do not operate at anywhere near the upper physical limit of general intelligence. Instead, humans are nearly the dumbest possible creature capable of developing a technological civilization. But our intelligence is still running on a mess of evolved mammalian modules built of meat. Our neurons communicate much slower than electric circuits. Our thinking is hobbled by comprehensive and deep-seated cognitive biases (Gilovich et al. 2002).</p>\n<p>It is easy to create machines that surpass our cognitive abilities in narrow domains (chess, etc.), and easy to imagine the creation of machines that eventually surpass our cognitive abilities in a general way. A greater-than-human machine intelligence would exhibit over us the kind of superiority we exhibit over our ancestors in the genus <em>Homo</em>, or chimpanzees, or dogs, or even snails.</p>\n<p>Some have argued that a machine cannot reach human-level general intelligence, for example see Lucas (1961); Dreyfus (1972); Penrose (1994); Searle (1980); Block (1981). But Chalmers (2010) points out that their arguments are irrelevant:</p>\n<blockquote>\n<p>To reply to the Lucas, Penrose, and Dreyfus objections, we can note that nothing in the singularity idea requires that an AI be a classical computational system or even that it be a computational system at all. For example, Penrose (like Lucas) holds that the brain is not an algorithmic system in the ordinary sense, but he allows that it is a mechanical system that relies on certain nonalgorithmic quantum processes. Dreyfus holds that the brain is not a rule-following symbolic system, but he allows that it may nevertheless be a mechanical system that relies on subsymbolic processes (for example, connectionist processes). If so, then these arguments give us no reason to deny that we can build artificial systems that exploit the relevant nonalgorithmic quantum processes, or the relevant subsymbolic processes, and that thereby allow us to simulate the human brain.</p>\n<p>As for the Searle and Block objections, these rely on the thesis that even if a system duplicates our behaviour, it might be missing important &lsquo;internal&rsquo; aspects of mentality: consciousness, understanding, intentionality, and so on... [But if] there are systems that produce apparently superintelligent outputs, then whether or not these systems are truly conscious or intelligent, they will have a transformative impact on the rest of the world.</p>\n</blockquote>\n<p>Chalmers (2010) summarizes two arguments suggesting that machines can reach human-level general intelligence:</p>\n<ul>\n<li>The emulation argument (see <a href=\"#WhatIsTheEmulation\">section 7.3</a>)</li>\n<li>The evolutionary argument (see <a href=\"#WhatIsTheEvolutionary\">section 7.4</a>)</li>\n</ul>\n<p>He also advances an argument for the conclusion that upon reaching human-level general intelligence, machines can be improved to reach greater-than-human intelligence: the extensibility argument (see <a href=\"#WhatIsTheExtendibility\">section 7.5</a>).</p>\n<p>We can also get a sense of how human cognition might be surpassed by examining the limits of human cognition. These include:</p>\n<ul>\n<li><em>Small scale</em>. The human brain contains 85-100 billion neurons (Azevedo et al. 2009; Williams &amp; Herrup 1988), but a computer need not be so limited. Legg (2008) writes:</li>\n</ul>\n<blockquote>\n<p style=\"padding-left: 60px;\">...a typical adult human brain weights about 1.4 kg and consumes just 25 watts of power (Kandel et al. 2000). This is ideal for a mobile intelligence, however an artificial intelligence need not be mobile and thus could be orders of magnitude larger and more energy intensive. At present a large supercomputer can fill a room twice the size of a basketball court and consume 10 megawatts of power. With a few billion dollars much larger machines could be built.</p>\n</blockquote>\n<p style=\"padding-left: 60px;\">With greater scale, a computer could far surpass human capacities for short-term memory, long-term memory, processing speed, and much more.</p>\n<ul>\n<li><em>Slow speed</em>. Again, here is Legg (2008):</li>\n</ul>\n<blockquote>\n<p style=\"padding-left: 60px;\">...brains use fairly large and slow components. Consider one of the simpler of these, axons... These are typically around 1 micrometre wide, carry spike signals at up to 75 metres per second at a frequency of at most a few hundred hertz (Kandel et al. 2000). Compare these characteristics with those of a wire that carries signals on a microchip. Currently these are 45 nanometres wide, propagate signals at 300 million metres per second and can easily operate at 4 billion hertz... Given that present day technology produces wires which are 20 times thinner, propagate signals 4 million times faster and operate at 20 million times the frequency, it is hard to believe that the performance of axons could not be improved by at least a few orders of magnitude.</p>\n</blockquote>\n<p><em> </em></p>\n<ul>\n<em>\n<li><em>Poor algorithms</em><span style=\"font-style: normal;\">. The brain&rsquo;s algorithms for making calculations are often highly inefficient. A cheap calculator beats the most impressive savant in mental calculation.</span></li>\n<li><em>Proneness to distraction</em><span style=\"font-style: normal;\">. Our brains are highly prone to distraction, loss of focus, and boredom. A machine intelligence need not suffer these deficiencies.</span></li>\n<li><em>Slow Learning speed</em><span style=\"font-style: normal;\">. Humans gain new skills and learn new material slowly, but a machine may be able to acquire new skills and knowledge at a rate more comparable to that of Neo in </span><em>The Matrix</em><span style=\"font-style: normal;\"> (&ldquo;I know kung-fu&rdquo;).</span></li>\n<li><em>Limited communication abilities</em><span style=\"font-style: normal;\">. Human tools for communication (the vibration of vocal chords, the movement of limbs, written words) are imprecise and noisy. Computers already communicate with each other much more quickly and accurately by using unambiguous languages (protocols) and direct electrical signaling.</span></li>\n<li><em>Limited self-reflection</em><span style=\"font-style: normal;\">. Only in the past few decades have humans been able to look inside the &ldquo;black box&rdquo; that produces their feelings, judgments, and behavior &mdash; and even still, most of how our brains work is a mystery. Because of this, we must often infer (and sometimes be mistaken about) our own </span><a style=\"font-style: normal;\" href=\"/lw/5sk/inferring_our_desires/\">desires</a><span style=\"font-style: normal;\"> and </span><a style=\"font-style: normal;\" href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">judgments</a><span style=\"font-style: normal;\">, and perhaps even our own </span><a style=\"font-style: normal;\" href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">subjective experiences</a><span style=\"font-style: normal;\">. In contrast, a machine could be made to have access to its own source code, and thereby know everything about its own operation and how to improve itself.</span></li>\n<li><em>Non-extensibility</em><span style=\"font-style: normal;\">. Humans cannot easily integrate with hardware or with other human minds. Machines could quickly gain the benefits of being able to integrate with a variety of hardware and substrates.</span></li>\n<li><em>Limited sensory data</em><span style=\"font-style: normal;\">. Humans have limited senses, and there are many more that could be had: ultraviolet vision (like bees have), infrared vision (like snakes), telescopic vision (like eagles), microscopic vision, infrasound hearing, ultrasound hearing, advanced chemical diagnosis (more sophisticated than the human tongue), super-smell, spectroscopy, and more.</span></li>\n<li><em>Cognitive biases</em><span style=\"font-style: normal;\">. Due to the haphazard evolutionary construction of the human mind (Marcus 2008), humans are subject to a long list of cognitive biases that distort our thinking (Gilovich et al. 2002; Stanovich 2010). This need not be the case in machines.</span></li>\n</em> \n</ul>\n<p>&nbsp;</p>\n<p>Thus, it seems that greater-than-human intelligence is possible for a long list of reasons.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hqxSXSNisLqvSE49e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 8.041397924111173e-07, "legacy": true, "legacyId": "11081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>My thanks to everyone who has provided feedback on these drafts so far. It's been helpful, and I've been incorporating your suggestions into the document. Now, Iinvite your feedback on these two snippets from the forthcoming <a href=\"/r/discussion/lw/8ew/friendly_ai_faq_drafts/\">Friendly AI FAQ</a>. For references, see <a href=\"/lw/8gh/fai_faq_draft_what_is_the_singularity/5awr\">here</a>.</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<h4 id=\"1_10__What_is_general_intelligence_\">1.10. What is general intelligence?</h4>\n<p>There are many competing definitions and theories of intelligence (Davidson &amp; Kemp 2011; Niu &amp; Brass 2011; Legg &amp; Hutter 2007), and the term has seen its share of emotionally-laden controversy (Halpern et al. 2011; Daley &amp; Onwuegbuzie 2011).</p>\n<p>Legg (2008) collects dozens of definitions of intelligence, and finds that they loosely converge on the following idea:</p>\n<blockquote>\n<p>Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.</p>\n</blockquote>\n<p>That will be our \u2018working definition\u2019 for intelligence in this FAQ.</p>\n<p>There is a sense in which famous computers like <a href=\"http://is.gd/82KuU3\">Deep Blue</a> and <a href=\"http://en.wikipedia.org/wiki/IBM_Watson\">Watson</a> are \u201cintelligent.\u201d They can out-perform human competitors for a narrow range of goals (winning chess games or answers <em>Jeopardy!</em> questions), in a narrow range of environments. But drop them in a novel environment \u2014 a shallow pond or a New York taxicab \u2014 and they are dumb and helpless. In this sense their \u201cintelligence\u201d is not general.</p>\n<p>Human intelligence is general in that it allows us to achieve goals in a wide range of environments. We can solve new problems of survival, competition, and fun in a wide range of environments, including ones never before encountered. That is, after all, how humans came to dominate all the land and air on Earth, and what empowers us to explore more extreme environments \u2014 like the deep sea or outer space \u2014 when we choose to. Humans have invented languages, developed agriculture, domesticated other animals, created crafts and arts and architecture, written philosophy, explored the planet, discovered math and science, evolved new political and economic systems, built machines, developed medicine, and made plans for the distant future.</p>\n<p>Some other animals also have a slower but more general intelligence than Deep Blue and Watson. Apes, dolphins, elephants, and a few species of bird have demonstrated some ability to solve novel problems in novel environments (Zentall 2011).</p>\n<p>General intelligence in a machine is called artificial general intelligence (AGI). Nobody has developed AGI yet, though many approaches are being attempted. Goertzel &amp; Pennachin (2007) provides an overview of approaches to AGI.</p>\n<p>&nbsp;</p>\n<h4 id=\"1_11__What_is_greater_than_human_intelligence_\">1.11. What is greater-than-human intelligence?</h4>\n<p>Humans gained dominance over Earth not because we had superior strength, speed, or durability, but because we had superior intelligence. It is our intelligence that makes us powerful. It is our intelligence that allows us to adapt to new environments. It is our intelligence that allows us to subdue animals or invent machines that surpass us in strength, speed, durability and other qualities.</p>\n<p>Humans do not operate at anywhere near the upper physical limit of general intelligence. Instead, humans are nearly the dumbest possible creature capable of developing a technological civilization. But our intelligence is still running on a mess of evolved mammalian modules built of meat. Our neurons communicate much slower than electric circuits. Our thinking is hobbled by comprehensive and deep-seated cognitive biases (Gilovich et al. 2002).</p>\n<p>It is easy to create machines that surpass our cognitive abilities in narrow domains (chess, etc.), and easy to imagine the creation of machines that eventually surpass our cognitive abilities in a general way. A greater-than-human machine intelligence would exhibit over us the kind of superiority we exhibit over our ancestors in the genus <em>Homo</em>, or chimpanzees, or dogs, or even snails.</p>\n<p>Some have argued that a machine cannot reach human-level general intelligence, for example see Lucas (1961); Dreyfus (1972); Penrose (1994); Searle (1980); Block (1981). But Chalmers (2010) points out that their arguments are irrelevant:</p>\n<blockquote>\n<p>To reply to the Lucas, Penrose, and Dreyfus objections, we can note that nothing in the singularity idea requires that an AI be a classical computational system or even that it be a computational system at all. For example, Penrose (like Lucas) holds that the brain is not an algorithmic system in the ordinary sense, but he allows that it is a mechanical system that relies on certain nonalgorithmic quantum processes. Dreyfus holds that the brain is not a rule-following symbolic system, but he allows that it may nevertheless be a mechanical system that relies on subsymbolic processes (for example, connectionist processes). If so, then these arguments give us no reason to deny that we can build artificial systems that exploit the relevant nonalgorithmic quantum processes, or the relevant subsymbolic processes, and that thereby allow us to simulate the human brain.</p>\n<p>As for the Searle and Block objections, these rely on the thesis that even if a system duplicates our behaviour, it might be missing important \u2018internal\u2019 aspects of mentality: consciousness, understanding, intentionality, and so on... [But if] there are systems that produce apparently superintelligent outputs, then whether or not these systems are truly conscious or intelligent, they will have a transformative impact on the rest of the world.</p>\n</blockquote>\n<p>Chalmers (2010) summarizes two arguments suggesting that machines can reach human-level general intelligence:</p>\n<ul>\n<li>The emulation argument (see <a href=\"#WhatIsTheEmulation\">section 7.3</a>)</li>\n<li>The evolutionary argument (see <a href=\"#WhatIsTheEvolutionary\">section 7.4</a>)</li>\n</ul>\n<p>He also advances an argument for the conclusion that upon reaching human-level general intelligence, machines can be improved to reach greater-than-human intelligence: the extensibility argument (see <a href=\"#WhatIsTheExtendibility\">section 7.5</a>).</p>\n<p>We can also get a sense of how human cognition might be surpassed by examining the limits of human cognition. These include:</p>\n<ul>\n<li><em>Small scale</em>. The human brain contains 85-100 billion neurons (Azevedo et al. 2009; Williams &amp; Herrup 1988), but a computer need not be so limited. Legg (2008) writes:</li>\n</ul>\n<blockquote>\n<p style=\"padding-left: 60px;\">...a typical adult human brain weights about 1.4 kg and consumes just 25 watts of power (Kandel et al. 2000). This is ideal for a mobile intelligence, however an artificial intelligence need not be mobile and thus could be orders of magnitude larger and more energy intensive. At present a large supercomputer can fill a room twice the size of a basketball court and consume 10 megawatts of power. With a few billion dollars much larger machines could be built.</p>\n</blockquote>\n<p style=\"padding-left: 60px;\">With greater scale, a computer could far surpass human capacities for short-term memory, long-term memory, processing speed, and much more.</p>\n<ul>\n<li><em>Slow speed</em>. Again, here is Legg (2008):</li>\n</ul>\n<blockquote>\n<p style=\"padding-left: 60px;\">...brains use fairly large and slow components. Consider one of the simpler of these, axons... These are typically around 1 micrometre wide, carry spike signals at up to 75 metres per second at a frequency of at most a few hundred hertz (Kandel et al. 2000). Compare these characteristics with those of a wire that carries signals on a microchip. Currently these are 45 nanometres wide, propagate signals at 300 million metres per second and can easily operate at 4 billion hertz... Given that present day technology produces wires which are 20 times thinner, propagate signals 4 million times faster and operate at 20 million times the frequency, it is hard to believe that the performance of axons could not be improved by at least a few orders of magnitude.</p>\n</blockquote>\n<p><em> </em></p>\n<ul>\n<em>\n<li><em>Poor algorithms</em><span style=\"font-style: normal;\">. The brain\u2019s algorithms for making calculations are often highly inefficient. A cheap calculator beats the most impressive savant in mental calculation.</span></li>\n<li><em>Proneness to distraction</em><span style=\"font-style: normal;\">. Our brains are highly prone to distraction, loss of focus, and boredom. A machine intelligence need not suffer these deficiencies.</span></li>\n<li><em>Slow Learning speed</em><span style=\"font-style: normal;\">. Humans gain new skills and learn new material slowly, but a machine may be able to acquire new skills and knowledge at a rate more comparable to that of Neo in </span><em>The Matrix</em><span style=\"font-style: normal;\"> (\u201cI know kung-fu\u201d).</span></li>\n<li><em>Limited communication abilities</em><span style=\"font-style: normal;\">. Human tools for communication (the vibration of vocal chords, the movement of limbs, written words) are imprecise and noisy. Computers already communicate with each other much more quickly and accurately by using unambiguous languages (protocols) and direct electrical signaling.</span></li>\n<li><em>Limited self-reflection</em><span style=\"font-style: normal;\">. Only in the past few decades have humans been able to look inside the \u201cblack box\u201d that produces their feelings, judgments, and behavior \u2014 and even still, most of how our brains work is a mystery. Because of this, we must often infer (and sometimes be mistaken about) our own </span><a style=\"font-style: normal;\" href=\"/lw/5sk/inferring_our_desires/\">desires</a><span style=\"font-style: normal;\"> and </span><a style=\"font-style: normal;\" href=\"/lw/531/how_you_make_judgments_the_elephant_and_its_rider/\">judgments</a><span style=\"font-style: normal;\">, and perhaps even our own </span><a style=\"font-style: normal;\" href=\"/lw/5ee/being_wrong_about_your_own_subjective_experience/\">subjective experiences</a><span style=\"font-style: normal;\">. In contrast, a machine could be made to have access to its own source code, and thereby know everything about its own operation and how to improve itself.</span></li>\n<li><em>Non-extensibility</em><span style=\"font-style: normal;\">. Humans cannot easily integrate with hardware or with other human minds. Machines could quickly gain the benefits of being able to integrate with a variety of hardware and substrates.</span></li>\n<li><em>Limited sensory data</em><span style=\"font-style: normal;\">. Humans have limited senses, and there are many more that could be had: ultraviolet vision (like bees have), infrared vision (like snakes), telescopic vision (like eagles), microscopic vision, infrasound hearing, ultrasound hearing, advanced chemical diagnosis (more sophisticated than the human tongue), super-smell, spectroscopy, and more.</span></li>\n<li><em>Cognitive biases</em><span style=\"font-style: normal;\">. Due to the haphazard evolutionary construction of the human mind (Marcus 2008), humans are subject to a long list of cognitive biases that distort our thinking (Gilovich et al. 2002; Stanovich 2010). This need not be the case in machines.</span></li>\n</em> \n</ul>\n<p>&nbsp;</p>\n<p>Thus, it seems that greater-than-human intelligence is possible for a long list of reasons.</p>", "sections": [{"title": "1.10. What is general intelligence?", "anchor": "1_10__What_is_general_intelligence_", "level": 1}, {"title": "1.11. What is greater-than-human intelligence?", "anchor": "1_11__What_is_greater_than_human_intelligence_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["23odz5WbpEvXKw5HZ", "2G7AH92pHyj3nC32T", "du395YvCnQXBPSJax", "J55XeCNeF7wNwgCj9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-24T04:40:51.834Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Hug the Query", "slug": "seq-rerun-hug-the-query", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:33.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PuyFrLXTHfiLB4zsn/seq-rerun-hug-the-query", "pageUrlRelative": "/posts/PuyFrLXTHfiLB4zsn/seq-rerun-hug-the-query", "linkUrl": "https://www.lesswrong.com/posts/PuyFrLXTHfiLB4zsn/seq-rerun-hug-the-query", "postedAtFormatted": "Thursday, November 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Hug%20the%20Query&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Hug%20the%20Query%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuyFrLXTHfiLB4zsn%2Fseq-rerun-hug-the-query%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Hug%20the%20Query%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuyFrLXTHfiLB4zsn%2Fseq-rerun-hug-the-query", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuyFrLXTHfiLB4zsn%2Fseq-rerun-hug-the-query", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 187, "htmlBody": "<p>Today's post, <a href=\"/lw/ly/hug_the_query/\">Hug the Query</a> was originally published on 14 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The more directly your arguments bear on a question, without intermediate inferences, the more powerful the evidence. We should try to observe evidence that is as near to the original question as possible, so that it screens off as many other arguments as possible.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8jm/seq_rerun_argument_screens_off_authority/\">Argument Screens Off Authority</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PuyFrLXTHfiLB4zsn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 5, "extendedScore": null, "score": 8.043285526144911e-07, "legacy": true, "legacyId": "11090", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2jp98zdLo898qExrr", "Y4G8Jq2YzuTASsQRg", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-24T08:30:48.552Z", "modifiedAt": null, "url": null, "title": "Review of Kurzweil, 'The Singularity is Near'", "slug": "review-of-kurzweil-the-singularity-is-near", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:54.397Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Np3fqBgnkcckKsZSn/review-of-kurzweil-the-singularity-is-near", "pageUrlRelative": "/posts/Np3fqBgnkcckKsZSn/review-of-kurzweil-the-singularity-is-near", "linkUrl": "https://www.lesswrong.com/posts/Np3fqBgnkcckKsZSn/review-of-kurzweil-the-singularity-is-near", "postedAtFormatted": "Thursday, November 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Review%20of%20Kurzweil%2C%20'The%20Singularity%20is%20Near'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReview%20of%20Kurzweil%2C%20'The%20Singularity%20is%20Near'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNp3fqBgnkcckKsZSn%2Freview-of-kurzweil-the-singularity-is-near%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Review%20of%20Kurzweil%2C%20'The%20Singularity%20is%20Near'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNp3fqBgnkcckKsZSn%2Freview-of-kurzweil-the-singularity-is-near", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNp3fqBgnkcckKsZSn%2Freview-of-kurzweil-the-singularity-is-near", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 771, "htmlBody": "<p><a href=\"http://en.wikipedia.org/wiki/Ray_Kurzweil\">Ray Kurzweil</a>'s writings are the best-known expression of Singularity memes, so I figured it's about time I read his 2005 best-seller <em><a href=\"http://www.amazon.com/Singularity-Near-Humans-Transcend-Biology/dp/0143037889/\">The Singularity is Near</a></em>.</p>\n<p>Though earlier users of the term \"technological Singularity\" used it to refer to the arrival of machine superintelligence (an event beyond which our ability to predict the future breaks down), Kurzweil's Singularity is more vaguely defined:</p>\n<blockquote>\n<p>What, then, is the Singularity? It's a future period during which the pace of technological change will be so rapid, its impact so deep, that human life will be irreversibly transformed.</p>\n</blockquote>\n<p>Kurzweil says that people don't expect the Singularity because they don't realize that technological progress is largely exponential, not linear:</p>\n<blockquote>\n<p>People intuitively assume that the current rate of progress will continue for future periods. Even for those who have been around long enough to experience how the pace of change increases, over time, unexamined intuition leaves one with the impression that change occurs at the same rate that we have experienced most recently. From the mathematician's perspective, the reason for this is that an exponential curve looks like a straight line when examined for only a brief duratio. As a result, even sophisticated commentators, when considering the future, typically extrapolate the current pace of change over the next ten years or one hundred years to determine their expectations...</p>\n<p>But a serious assessment of the history of technology reveals that technological change is exponential... You can examine the data in different ways, on different timescales, and for a wide variety of technologies, ranging from electronic to biological... the acceleration of progress and growth applies to each of them.</p>\n</blockquote>\n<p>Kurzweil has many examples:</p>\n<blockquote>\n<p>Consider Gary Kasparov, who scorned the pathetic state of computer chess in 1992. Yet the relentless doubling of computer power every year enabled a computer to defeat him only five years later...</p>\n<p>[Or] consider the biochemists who, in 1990, were skeptical of the goal of transcribing the entire human genome in a mere fifteen years. These scientists had just spent an entire year transcribing a mere one ten-thousandth of the genome. So... it seemed natural to them that it would take a century, if not longer, before the genome could be sequenced. [The complete genome was sequenced in 2003.]</p>\n</blockquote>\n<p>He emphasizes that people often fail to account for how progress in one field will feed on accelerating progress in another:</p>\n<blockquote>\n<p>Can the pace of technological progress continue to speed up indefinitely? Isn't there a point at which humans are unable to think fast enough to keep up? For unenhanced humans, clearly so. But what would 1,000 scientists, each 1,000 times more intelligent than human scientists today, and each operating 1,000 times faster that contemporary humans (because the information processing in their primarily nonbiological brains is faster) accomplish? One chronological year would be like a millennium for them... an hour would result in a century of progress (in today's terms).</p>\n</blockquote>\n<p>Kurzweil's second chapter aims to convince us that Moore's law of exponential growth in computing power is not an anomaly: the \"law of accelerating returns\" holds for a wide variety of technologies, evolutionary developments, and paradigm shifts. The chapter is full of logarithmic plots for bits of DRAM per dollar, microprocessor clock speed, processor performance in MIPS, growth in Genbank, hard drive bits per dollar, internet hosts, nanotech science citations, and more.</p>\n<p>The chapter is a wake-up call to those not used to thinking about exponential change, but one gets the sense that Kurzweil has cherry-picked his examples. Plenty of technologies have violated his law of accelerating returns, and Kurzweil doesn't mention them.</p>\n<p>This cherry-picking is one of the two persistent problems with <em>The Singularity is Near</em>. The second persistent problem is detailed storytelling. Kurzweil would make fewer false predictions if he made statements about the <em>kinds</em> of changes we can expect and then gave examples as illustrations, instead of giving detailed stories about the future as his actual predictions.</p>\n<p>My third major issue with the book is not a \"problem\" so much as it is a decision about the scope of the book. Human factors (sociology, psychology, politics) are largely ignored in the book , but would have been illuminating to include if done well &mdash; and certainly, they are important for technological forecasting.</p>\n<p>It's a big book with many specific claims, so there are hundreds of detailed criticisms I could make (e.g. about his handling of AI risks), but I prefer to keep this short. Kurzweil's vision of the future is more similar to what I expect is correct than most people's pictures of the future are, and he should be applauded for finding a way to bring transhumanist ideas to the mainstream culture.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Np3fqBgnkcckKsZSn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 7, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "11093", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-24T19:23:56.148Z", "modifiedAt": null, "url": null, "title": "Will the ems save us from the robots?", "slug": "will-the-ems-save-us-from-the-robots", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.954Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QqZcdAGDJFLnqpsmG/will-the-ems-save-us-from-the-robots", "pageUrlRelative": "/posts/QqZcdAGDJFLnqpsmG/will-the-ems-save-us-from-the-robots", "linkUrl": "https://www.lesswrong.com/posts/QqZcdAGDJFLnqpsmG/will-the-ems-save-us-from-the-robots", "postedAtFormatted": "Thursday, November 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Will%20the%20ems%20save%20us%20from%20the%20robots%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWill%20the%20ems%20save%20us%20from%20the%20robots%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQqZcdAGDJFLnqpsmG%2Fwill-the-ems-save-us-from-the-robots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Will%20the%20ems%20save%20us%20from%20the%20robots%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQqZcdAGDJFLnqpsmG%2Fwill-the-ems-save-us-from-the-robots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQqZcdAGDJFLnqpsmG%2Fwill-the-ems-save-us-from-the-robots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>At the FHI, we are currently working on a project around whole brain emulations (WBE), or uploads. One important question is if getting to whole brain emulations first would make subsequent AGI creation</p>\n<ol>\n<li>more or less likely to happen,</li>\n<li>more or less likely to be survivable.</li>\n</ol>\n<p>If you have any opinions or ideas on this, please submit them here. No need to present an organised overall argument; we'll be doing that. What would help most is any unusual suggestion, that we might not have thought of, for how WBE would affect AGI.</p>\n<p><strong>EDIT</strong>: Many thanks to everyone who suggested ideas here, they've been taken under consideration.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QqZcdAGDJFLnqpsmG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 8.046442305448556e-07, "legacy": true, "legacyId": "11095", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-24T20:56:28.969Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft: How long before digital intelligence?", "slug": "intelligence-explosion-analysis-draft-how-long-before", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.651Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KMzrqvoC7QGQnriLi/intelligence-explosion-analysis-draft-how-long-before", "pageUrlRelative": "/posts/KMzrqvoC7QGQnriLi/intelligence-explosion-analysis-draft-how-long-before", "linkUrl": "https://www.lesswrong.com/posts/KMzrqvoC7QGQnriLi/intelligence-explosion-analysis-draft-how-long-before", "postedAtFormatted": "Thursday, November 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%3A%20How%20long%20before%20digital%20intelligence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%3A%20How%20long%20before%20digital%20intelligence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMzrqvoC7QGQnriLi%2Fintelligence-explosion-analysis-draft-how-long-before%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%3A%20How%20long%20before%20digital%20intelligence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMzrqvoC7QGQnriLi%2Fintelligence-explosion-analysis-draft-how-long-before", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKMzrqvoC7QGQnriLi%2Fintelligence-explosion-analysis-draft-how-long-before", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1210, "htmlBody": "<p>\n<p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on. This section is less complete than the others; missing text is indicated with brackets: [].</p>\n<p>_____</p>\n</p>\n<p>&nbsp;</p>\n<p>\n<p>We do not know what it takes to build a digital intelligence. Because of this, we do not know what groundwork will be needed to understand intelligence, nor how long it may take to get there.</p>\n<p>Worse, it&rsquo;s easy to think we <em>do</em> know. Studies show that except for weather forecasters (Murphy and Winkler 1984), nearly all of us give inaccurate probability estimates when we try, and in particular we are overconfident of our predictions (Lichtenstein, Fischoff, and Phillips 1982; Griffin and Tversky 1992; Yates et al. 2002). Experts, too, often do little better than chance (Tetlock 2005), and are outperformed by crude computer algorithms (Grove and Meehl 1996; Grove et al. 2000; Tetlock 2005). So if you have a gut feeling about when digital intelligence will arrive, it is probably wrong.</p>\n<p>But uncertainty is not a &ldquo;get out of prediction free&rdquo; card. You either will or will not save for retirement or support AI risk mitigation. The outcomes of these choices will depend, among other things, on whether digital intelligence arrives in the near future. Should you plan as though there are 50/50 odds of reaching digital intelligence in the next 30 years? Are you 99% confident that digital intelligence won&rsquo;t arrive in the next 30 years? Or is it somewhere in between?</p>\n<p>Other than using one&rsquo;s guts for prediction or deferring to an expert, how might one estimate the time until digital intelligence? We consider several strategies below.</p>\n<p><em>Time since Dartmouth</em>. We have now seen 60 years of work toward digital intelligence since the seminal Dartmouth conference on AI, but digital intelligence has not yet arrived. This seems, intuitively, like strong evidence that digital intelligence won&rsquo;t arrive in the next minute, good evidence it won&rsquo;t arrive in the next year, and significant but far from airtight evidence that it won&rsquo;t arrive in the next few decades. Such intuitions can be formalized into models that, while simplistic, can form a useful starting point for estimating the time to digital intelligence.<sup>1</sup></p>\n<p><em>Simple hardware extrapolation</em>. Vinge (1993) wrote: &ldquo;Based on [hardware trends], I believe that the creation of greater-than-human intelligence will occur [between 2005 and 2030].&rdquo; Vinge seems to base his estimates on estimates of the &ldquo;raw hardware power that is present in organic brains.&rdquo; In a 2003 reprint of his article, Vinge notes the insufficiency of this reasoning: even if we have the hardware sufficient for AI, we may not have the software problem solved.</p>\n<p><em>Extrapolating the requirements for whole brain emulation</em>. One way to solve the software problem is to scan and emulate the human brain. Thus Ray Kurzweil (2005) extrapolates our progress in hardware, brain scanning, and our understanding of the brain to predict that (low resolution) whole brain emulation can be achieved by 2029. Many neuroscientists think this estimate is too optimistic, but the basic approach has promise.</p>\n<p><em>Tracking progress in machine intelligence</em>. Many folks intuitively estimate the time until digital intelligence by asking what proportion of human abilities today&rsquo;s software can match, and how quickly machines are catching up. However, it is not clear how to divide up the space of &ldquo;human abilities,&rdquo; nor how much each one matters. We also don&rsquo;t know whether machine progress will be linear or include a sudden jump. Watching an infant&rsquo;s progress in learning calculus might lead one to conclude the child will not learn it until the year 3000, until suddenly the child learns it in a spurt at age 17. Still, machine progress in chess performance has been regular,<sup>2</sup>&nbsp;and it may be worth checking whether a measure can be found for which both: (a) progress is smooth enough to extrapolate; and (b) when performance rises to a certain level, we can expect digital intelligence.<sup>3</sup></p>\n<p><em>Estimating progress in scientific research output</em>. Imagine a man digging a ten-kilometer ditch. If he digs 100 meters in one day, you might predict the ditch will be finished in 100 days. But what if 20 more diggers join him, and they are all given steroids? Now the ditch might not take so long. Analogously, when predicting progress toward digital intelligence it may be useful to consider not how much progress is made per year, but instead how much progress is made per unit of research effort. Thus, if we expect jumps in the amount of effective research effort (for reasons given in section 2.2.), we should expect analogous jumps in progress toward digital intelligence.</p>\n<p>Given the long history of confident false predictions within AI, and the human tendency toward overconfidence in general, it would seem misguided to be 90% confident that AI will succeed in the coming decade.<sup>4</sup> But 90% confidence that digital intelligence will <em>not</em> arrive before the end of the century also seems wrong, given that (a) many seemingly difficult AI benchmarks have been reached, (b) many factors, such as more hardware and automated science, may well accelerate progress toward digital intelligence, and (c) whole brain emulation may well be a relatively straightforward engineering problem that will succeed by 2070 if not 2030. There is a significant probability that digital intelligence will arrive within a century, and additional research can improve our estimates (as we discuss in section 5).</p>\n<div><br /></div>\n<div>________</div>\n<div><sup>1</sup>&nbsp;We can make a simple formal model of this by assuming (with much simplification) that every year a coin is tossed to determine whether we will get AI that year, and that we are initially unsure of the weighting on that coin. [add more: The 60 years of no AI that we&rsquo;ve had so far is then highly unlikely under models where the coin comes up &ldquo;AI&rdquo; on 90% of years (the probability of this would be 10^-60), or even that it comes up &ldquo;AI&rdquo; in 10% of all years (probability 0.18%, or one time in 500), whereas it&rsquo;s the expected case if the coin comes up &ldquo;AI&rdquo; in, say, 1% of all years, or for that matter in 0.0001% of all years. Thus, depending on one&rsquo;s prior over coin weightings, one should in this toy model update strongly against coin weightings in which AI would be likely in the next minute, or even year, while leaving the relative probabilities of &ldquo;AI in 200 years&rdquo; and &ldquo;AI in 2 million years&rdquo; more or less untouched.]</div>\n<div><sup>2</sup>&nbsp;See http://lukeprog.com/special/chess.pdf.</div>\n<div><sup>3</sup>&nbsp;It is probably also worth keeping crude track of &ldquo;progress in AI&rdquo; as a whole, even though there is no guarantee progress would be linear. [It would be nice to add a crude attempt to nevertheless quantify which areas of human intelligence have been substantially matched by machines. &nbsp;Ideal would be to take some canonical-sounding article from some decades ago that listed domains we haven&rsquo;t matched with computers, and then to note something like: of these domains, (3) has been solved, and (1) and (4) have seen substantial progress. GEB has a suitable listing, but might be better to use a more canonical article if we can find one.]</div>\n<div><sup>4</sup>&nbsp;Unless, that is, you have a kind of evidence that is strongly different from the kinds of evidence possessed by the many researchers since Dartmouth who incorrectly predicted that their particular AI paradigm, or human-level AI in general, was about to succeed.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KMzrqvoC7QGQnriLi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 8.046773266701311e-07, "legacy": true, "legacyId": "11096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T06:06:40.349Z", "modifiedAt": null, "url": null, "title": "Should LessWrong be Interested in the Occupy Movements?", "slug": "should-lesswrong-be-interested-in-the-occupy-movements", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.747Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DBreneman", "createdAt": "2011-04-25T20:53:22.008Z", "isAdmin": false, "displayName": "DBreneman"}, "userId": "yiqoBqkQCMPuhsFuY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qfiC7RmrQffCHAnAp/should-lesswrong-be-interested-in-the-occupy-movements", "pageUrlRelative": "/posts/qfiC7RmrQffCHAnAp/should-lesswrong-be-interested-in-the-occupy-movements", "linkUrl": "https://www.lesswrong.com/posts/qfiC7RmrQffCHAnAp/should-lesswrong-be-interested-in-the-occupy-movements", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20LessWrong%20be%20Interested%20in%20the%20Occupy%20Movements%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20LessWrong%20be%20Interested%20in%20the%20Occupy%20Movements%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqfiC7RmrQffCHAnAp%2Fshould-lesswrong-be-interested-in-the-occupy-movements%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20LessWrong%20be%20Interested%20in%20the%20Occupy%20Movements%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqfiC7RmrQffCHAnAp%2Fshould-lesswrong-be-interested-in-the-occupy-movements", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqfiC7RmrQffCHAnAp%2Fshould-lesswrong-be-interested-in-the-occupy-movements", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 461, "htmlBody": "<p>Since early October, I've been closely following Occupy Wall Street, and the other protests it spawned. &nbsp;At first I was interested in it as a sort of social experiment, I've never heard of long-term camping as a means of protest, and I was curious to see how it would work out. &nbsp;As it's grown though, I've been thinking that there might be a couple of things happening in the movements that might be of interest to rationalist communities. &nbsp;I've not seen much discussion of Occupy and its tactics on LessWrong, and I think that if nothing else, they're at least interesting, so I thought I'd open it up here.</p>\n<p>&nbsp;</p>\n<p>Each Occupy movement is a hotbed of community experimentation. &nbsp;Things like General Assemblies (horizontally democratic voting discussions to make policy decisions) and ad-hoc sanitation, fire, and security committees of all shapes and sizes are popping up all over. &nbsp;What's more, as the events grow in size, and as police pressure on the events rises, these constructs are going to be tested more and more. &nbsp;We have a wildly varied gene pool, strong environmental constraints, and a fast mutation rate. &nbsp;It's a big evolutionary experiment in community formation. &nbsp;And I think if we look closely, we can find a whole lot of useful hacks to make stronger communities.</p>\n<p>&nbsp;</p>\n<p>The whole thing's a great big ethical, emotional, and legal <em>mess</em>. &nbsp;There are issues with how private/public property laws intersect with freedom of speech, there are matters of what level of force is justifiable for police to keep peace in certain situations, there're issues of whether health and safety trump rights of protest, on and on and on. &nbsp;If nothing else, there's an interesting discussion there, about what a truly rational set of laws would look like, and whether or not the protesters or the police are justified in their actions. &nbsp;</p>\n<p>&nbsp;</p>\n<p>And at the risk of sounding like a James Bond villain, there are some serious options for us to <em>take over the world</em>&nbsp;here. &nbsp;In the sense at least that the Occupy movements' goal is lasting societal change, and they have a good deal of momentum already. &nbsp;If members of the rationalist community moved to help them, they might have a fair deal more. &nbsp;And if we introduce them to rational ways of thinking, if we inject those memes into the discussion, there's some serious opportunity here to help stop the world being <em>so insane.</em></p>\n<p>&nbsp;</p>\n<p>At least that's my take on the whole thing. &nbsp;And I'm not exactly strong in the ways of rationality yet, still reading and re-reading the Sequences (I keep getting lost somewhere halfway into the QM sequence, I think I need to practice mathematics more to understand it on a more instinctive level) and I'd certainly appreciate the view of those Stronger than me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qfiC7RmrQffCHAnAp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": -11, "extendedScore": null, "score": -1.6e-05, "legacy": true, "legacyId": "11106", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T14:14:52.389Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Houston, Atlanta, Eugene OR, San Diego, Fort Collins CO, Waterloo, Melbourne", "slug": "weekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FgTWYhDfEhsSPRY3X/weekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "pageUrlRelative": "/posts/FgTWYhDfEhsSPRY3X/weekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "linkUrl": "https://www.lesswrong.com/posts/FgTWYhDfEhsSPRY3X/weekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Houston%2C%20Atlanta%2C%20Eugene%20OR%2C%20San%20Diego%2C%20Fort%20Collins%20CO%2C%20Waterloo%2C%20Melbourne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Houston%2C%20Atlanta%2C%20Eugene%20OR%2C%20San%20Diego%2C%20Fort%20Collins%20CO%2C%20Waterloo%2C%20Melbourne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgTWYhDfEhsSPRY3X%2Fweekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Houston%2C%20Atlanta%2C%20Eugene%20OR%2C%20San%20Diego%2C%20Fort%20Collins%20CO%2C%20Waterloo%2C%20Melbourne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgTWYhDfEhsSPRY3X%2Fweekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFgTWYhDfEhsSPRY3X%2Fweekly-lw-meetups-houston-atlanta-eugene-or-san-diego-fort", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 370, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/4w\">Houston Meetup: Saturday, 11/19:&nbsp;<span class=\"date\">19 November 2011 02:25AM</span></a></li>\n<li><a href=\"/meetups/50\">Atlanta Less Wrong Meetup:&nbsp;<span class=\"date\">19 November 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/4u\">First Eugene Meetup:&nbsp;<span class=\"date\">20 November 2011 01:00PM</span></a></li>\n<li><a href=\"/meetups/4s\">San Diego meetup:&nbsp;<span class=\"date\">20 November 2011 01:00PM</span></a></li>\n<li><a href=\"/meetups/4x\">Fort Collins, Colorado Meetup Wedneday 7pm:&nbsp;<span class=\"date\">23 November 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/4p\">First(New?) Waterloo Meetup:&nbsp;<span class=\"date\">24 November 2011 08:00PM</span></a></li>\n<li><a href=\"/meetups/55\">Sheridan College - Oakville, ON:&nbsp;<span class=\"date\">25 November 2011 08:30PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/56\">Monthly Bay Area meetup:&nbsp;<span class=\"date\">26 November 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/4y\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">02 December 2011 07:00PM</span></a></li>\n</ul>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\"><strong></strong></a><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong>&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening:<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FgTWYhDfEhsSPRY3X", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.05048834887345e-07, "legacy": true, "legacyId": "10999", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T16:08:50.632Z", "modifiedAt": null, "url": null, "title": "LessWrong opinion of Nietzsche?", "slug": "lesswrong-opinion-of-nietzsche", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:36.639Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TheatreAddict", "createdAt": "2011-05-01T17:03:11.634Z", "isAdmin": false, "displayName": "TheatreAddict"}, "userId": "xtL2cZS7kaFnSNLus", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NxYgL5nPMTKZNGwDS/lesswrong-opinion-of-nietzsche", "pageUrlRelative": "/posts/NxYgL5nPMTKZNGwDS/lesswrong-opinion-of-nietzsche", "linkUrl": "https://www.lesswrong.com/posts/NxYgL5nPMTKZNGwDS/lesswrong-opinion-of-nietzsche", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20opinion%20of%20Nietzsche%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20opinion%20of%20Nietzsche%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxYgL5nPMTKZNGwDS%2Flesswrong-opinion-of-nietzsche%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20opinion%20of%20Nietzsche%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxYgL5nPMTKZNGwDS%2Flesswrong-opinion-of-nietzsche", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxYgL5nPMTKZNGwDS%2Flesswrong-opinion-of-nietzsche", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 68, "htmlBody": "<p>So, at the risk of starting controversy, I'm not exactly sure what the policy is about asking questions on philosophy..</p>\n<p>But would you mind giving your opinion on Nietzsche? I just bought Human, All Too Human. It's a tough read for me, and I'm sort of plowing through it, though it's interesting and stuff.</p>\n<p>So... what do you all think? :D</p>\n<p>Edit: I changed it from \"Rationalist opinion of Nietzsche\". Better?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NxYgL5nPMTKZNGwDS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "11109", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T17:52:56.404Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Guardians of the Truth", "slug": "seq-rerun-guardians-of-the-truth", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.349Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hrqfAJn56bY8djRvY/seq-rerun-guardians-of-the-truth", "pageUrlRelative": "/posts/hrqfAJn56bY8djRvY/seq-rerun-guardians-of-the-truth", "linkUrl": "https://www.lesswrong.com/posts/hrqfAJn56bY8djRvY/seq-rerun-guardians-of-the-truth", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Truth&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Truth%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhrqfAJn56bY8djRvY%2Fseq-rerun-guardians-of-the-truth%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Truth%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhrqfAJn56bY8djRvY%2Fseq-rerun-guardians-of-the-truth", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhrqfAJn56bY8djRvY%2Fseq-rerun-guardians-of-the-truth", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/lz/guardians_of_the_truth/\">Guardians of the Truth</a> was originally published on 15 December 2007. A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is an enormous psychological difference between believing that you absolutely, certainly, <em>have </em>the truth, versus trying to <em>discover </em>the truth. If you believe that you have the truth, and that it must be protected from heretics, torture and murder follow. Alternatively, if you believe that you are close to the truth, but perhaps not there yet, someone who disagrees with you is simply wrong, not a mortal enemy.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them. The previous post was <a href=\"/r/discussion/lw/8k2/seq_rerun_hug_the_query/\">Hug the Query</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort. You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hrqfAJn56bY8djRvY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.051268919496567e-07, "legacy": true, "legacyId": "11105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["etBrzxdfNop3DqJvA", "PuyFrLXTHfiLB4zsn", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T20:52:18.009Z", "modifiedAt": null, "url": null, "title": "Friendship and happiness generation", "slug": "friendship-and-happiness-generation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.683Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AlexMennen", "createdAt": "2009-11-27T18:24:19.500Z", "isAdmin": false, "displayName": "AlexMennen"}, "userId": "KgzPEGnYWvKDmWuNY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/knqzj6TXZyCdqWMuA/friendship-and-happiness-generation", "pageUrlRelative": "/posts/knqzj6TXZyCdqWMuA/friendship-and-happiness-generation", "linkUrl": "https://www.lesswrong.com/posts/knqzj6TXZyCdqWMuA/friendship-and-happiness-generation", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Friendship%20and%20happiness%20generation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFriendship%20and%20happiness%20generation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fknqzj6TXZyCdqWMuA%2Ffriendship-and-happiness-generation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Friendship%20and%20happiness%20generation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fknqzj6TXZyCdqWMuA%2Ffriendship-and-happiness-generation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fknqzj6TXZyCdqWMuA%2Ffriendship-and-happiness-generation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 445, "htmlBody": "<p>Happiness and utility are different things, with happiness (measured in hedons) generally referring to the desirability of an agent being in its current mental state, while utility (measured in utils) refers to the desirability, from the point of view of some agent, of the configuration of the universe.</p>\n<p>Naively, one could model caring about another person as having a portion of your utility function allocated to mimicking their utility (me.utility(universe) = caring_factor*friend.utility(universe) + me.utility(universe excluding value of friend's utility function)) or their happiness (me.utility(universe) = caring_factor*friend.happiness + me.utility(universe excluding friend's happiness)). However, I think these are bad models of how caring for people actually works in humans.</p>\n<p>I've noticed that I often gladly give up small amounts of hedons so that someone I care about can gain a similar amount of hedons. Extrapolating this, one might conclude that I care about plenty of other people nearly as much as I care about myself. However, I would be much less likely to give up a large amount of hedons for someone I care about unless the ratio of hedons that they could gain over the hedons I would have to give up is also fairly large.</p>\n<p>While trying to figure out why this is, I realized that whenever I think I'm sacrificing hedons for someone, I usually don't actually lose any hedons because I enjoy the feeling associated with knowing that I helped a friend. I expect that this reaction is fairly common. This implies that by doing small favors for each other, friends can generate happiness for both of them even when the amount of hedons sacrificed by one (not counting the friend-helping bonus) is similar to the amount of hedons gained by the other. However, this happiness bonus for helping a friend is bounded, and grows sublinearly with respect to the amount of good done to the friend. In terms of evolutionary psychology, this makes sense: seeking out cheap ways to signal loyalty sounds like a decent strategy for getting and keeping allies.</p>\n<p>I don't think that this tells the whole story. If a friend had enough at stake, I would sacrifice much more for them than could be reimbursed with the happiness bonus for helping a friend (plus happiness penalty that I would otherwise absorb for the feeling of knowing I had abandoned a friend), because I do actually care about people. Again, I would expect that most other people would act this way as well. But it seems likely that most favors that people do for each other are primarily motivated by pursuing personal happiness that they can get from knowing that they've helped a friend, rather than directly caring about how happy their friends are.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"mip7tdAN87Jarkcew": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "knqzj6TXZyCdqWMuA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 8.051911039267409e-07, "legacy": true, "legacyId": "11110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T22:29:07.219Z", "modifiedAt": null, "url": null, "title": "King Under The Mountain: Adventure Log + Soundtrack", "slug": "king-under-the-mountain-adventure-log-soundtrack", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:26.654Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ep2Z42hYqj68QZz6w/king-under-the-mountain-adventure-log-soundtrack", "pageUrlRelative": "/posts/Ep2Z42hYqj68QZz6w/king-under-the-mountain-adventure-log-soundtrack", "linkUrl": "https://www.lesswrong.com/posts/Ep2Z42hYqj68QZz6w/king-under-the-mountain-adventure-log-soundtrack", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20King%20Under%20The%20Mountain%3A%20Adventure%20Log%20%2B%20Soundtrack&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKing%20Under%20The%20Mountain%3A%20Adventure%20Log%20%2B%20Soundtrack%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEp2Z42hYqj68QZz6w%2Fking-under-the-mountain-adventure-log-soundtrack%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=King%20Under%20The%20Mountain%3A%20Adventure%20Log%20%2B%20Soundtrack%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEp2Z42hYqj68QZz6w%2Fking-under-the-mountain-adventure-log-soundtrack", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEp2Z42hYqj68QZz6w%2Fking-under-the-mountain-adventure-log-soundtrack", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6761, "htmlBody": "<p>With the help of many dedicated Less Wrongers (players muflax, Karl, Charlie, and Emile; musicians Mike Blume and Alicorn, technical support Ari Rahikkala) we have successfully completed what is, as far as I know, the first actual Dungeons and Discourse adventure anywhere. Except we're not calling it that, because I don't have the rights to use that name. Though it's not precisely rationality related, I hope it is all right if I post a summary of the adventure <a href=\"/lw/33b/bayesian_nights_rationalist_story_time/2yfz\">by popular demand</a>.<br /><br />Also, at some point it turned into a musical. The first half of the songs are only available as lyrics at the moment, but Alicorn and MBlume very kindly produced the second half as real music, which I've uploaded to YouTube and linked at the bottom of this post (<a href=\"#music\">skip to it now</a>).</p>\n<p><span style=\"text-decoration: underline;\"><strong>THE ADVENTURE</strong></span></p>\n<p><strong>BACKGROUND</strong><br /><br />The known world has many sects and religions, but all contain shadowy legends of two primeval deities: Sophia, Goddess of Wisdom; and Aleithos, God of Truth. When Sophia announced her plan to create thinking, rational beings, Aleithos objected, declaring that they would fall into error and produce endless falsehoods. Sophia ignored her brother's objections and created humankind, who named the world after their goddess-mother. But Aleithos' fears proved well-founded: humankind fell into error and produced endless falsehoods, and their clamor drove the God of Truth insane.<br /><br />The once mighty Aleithos fell from heaven, and all of his angelic servants turned into Paradox Beasts, arachnoid monstrosities that sought and devoured those who challenged the laws of logic. Over centuries, most of the Paradox Beasts were banished, but Aleithos himself remained missing. And though thousands of seekers set off to all the corners of the world in search of Truth, the Mad God keeps his own counsel, if He still even exists at all.<br /><br />The Truth God's madness had one other effect; the laws of physics, once inviolable, turned fluid, and those sufficiently advanced in the study of Truth gained apparently magical abilities. With knowledge literally being power, great philosophers and scientists built mighty cities and empires.<br /><br />In the middle of the Cartesian Plain at the confluence of the rivers Ordinate and Abcissa stands the mightiest of all, the imperial city of Origin. At the very center of the city stands the infinitely tall Z-Axis Tower, on whose bottom floor lives the all-seeing Wizard of 0=Z. Surrounding the Tower are a host of colleges and universities that attract the greatest scholars from all over Origin, all gathered in service to the great project to find Truth.<br /><br />Into the city comes Lady Cerune Russell, an exotic noblewoman from far-off parts seeking great thinkers to join her on a dangerous adventure. Four scholars flock to her banner. Nomophilos the Elder the Younger (Emile) is a political scientist studying the central role of laws in creating a just society. Phaidros (muflax) is a zealous Protestant theologian trying to meld strains of thought as disparate as Calvinism, Gnosticism, and W.L. Craig's apologetics. Ephraim (Charlie) is a Darwinian biologist with strong leftist sympathies and an experimental streak that sometimes gets him in trouble. And Macx (Karl) is a quiet but very precise logician with a talent for puzzles.<br /><br />Cerune explains to the Original scholars that she is the last living descendant of Good King Bertrand, historic ruler of the land of Russellia far to the west. Russellia was the greatest nation in the world until two hundred years ago, when a cataclysm destroyed the entire kingdom in a single day and night. Now the skies above Russellia are dark and filled with choking ash, monsters roam its plains, and the Good King is said to be locked in a magical undying sleep deep beneath the Golden Mountain in the kingdom's center. Though many have traveled to Russellia in search of answers, none have returned alive; Cerune, armed with secret information from the Turing Oracle which she refuses to share, thinks she can do better. The four Originals agree to protect her as she makes the dangerous journey to the Golden Mountain to investigate the mysterious disaster and perhaps lift the curse. Cerune gives them a day in Origin to prepare for the journey.<br /><br /><strong>CHAPTER ONE: ORIGIN</strong><br /><br />The party skip the city's major attractions, including the Z-Axis Tower and the Hagia Sophia, in favor of more academic preparations: a visit to the library to conduct research, and a shopping trip to Barnes &amp; Aristoi Booksellers, where they purchase reading material for the journey ahead. Here, they find a map of the lands on the road to Russellia, including the unpleasant-sounding Slough of Despotism and the Shadow City of Xar-Morgoloth, whose very name inexplicably chills the air when spoken aloud. After a long discussion on how this thermodynamic-defying effect could probably be used to produce unlimited free energy, they return to more immediate matters and head to the armory to pick up some weapons - a trusty isoceles triangle for Nomophilos, a bow for Macx - before the stores close for the evening. After a final night in Origin, they meet Cerune at the city gates and set off.<br /><br />They originally intend to stick to the course of the Abcissa, but it is flooding its banks and Cerune recommends crossing the river into Platonia at the Pons Asinorum. After being attacked by a Euclidean Elemental charged with letting no one enter who does not know geometry, they reach the other bank and find a strange old man, raving incomprehensibly. His turns of phrase start to make sense only after the party realizes that he is speaking as if he - and all objects - have no consistent identity. <br /><br />In his roundabout way, he identifies himself as Heraclitus, the Fire Mage, one of the four great Elemental Mages of Platonia. Many years ago, he crossed into Origin on some errand, only to be ambushed by his arch-enemy, the Water Mage Thales. Thales placed a curse on Heraclitus that he could never cross the same river twice, trapping him on the wrong side of the Abcissa and preventing his return to Platonia. In order to dispel the curse, Heraclitus finds a loophole in the curse: he convinces himself that objects have no permanent identity, and so he can never cross the same river twice since it is not the same river and he is not the same man. Accepting this thesis, he crosses the Abcissa without incident - only to find that his new metaphysics of identity prevents him from forming goals, executing long-term plans, or doing anything more complicated than sitting by the riverbank and eating the fish that swim by.<br /><br />This sets off a storm of conversation, as each member of the party tries to set Heraclitus right in their own way; Phaidros by appealing to God as a final arbiter of identity, Macx and Nomophilos by arguing that duty is independent of identity and that Heraclitus has a duty to his family and followers. Unfortunately, they make a logical misstep and end out convincing Heraclitus that it is illogical from his perspective to hold conversation; this ends the debate. And as the five philosophers stand around discussing what to do, they are ambushed by a party of assassins, who shoot poisoned arrows at them from a nearby knoll.<br /><br />Outnumbered and outflanked, the situation seems hopeless, until Macx notices several of the attackers confused and unwilling to attack. With this clue, he identifies them as Buridan's Assassins, who in the presence of two equally good targets will hesitate forever, unable to choose: he yells to his friends to stand with two or more adventurers equidistant from each assassin, and sure enough, this paralyzes the archers and allows the party some breathing space. <br /><br />But when a second group of assassins arrives to join the first, the end seems near - until Heraclitus, after much pondering, decides to accept his interlocutors' arguments for object permanence and joins in the battle. His fire magic makes short work of the remaining assassins, and when the battle is over, he thanks them and gives a powerful magic item as a gift to each. Then he disappears in a burst of flame after warning his new friends to beware the dangers ahead.<br /><br />The party searches the corpses of the assassins - who all carry obsidian coins marked PLXM - and then camp for the night on the fringe of the Slough of Despotism.<br /><strong><br />CHAPTER TWO: THE SLOUGH OF DESPOTISM</strong><br /><br />The Slough of Despotism is a swamp unfortunately filled with allegators, giant reptiles who thrive on moral superiority and on casting blame. They accuse our heroes of trespassing on their property; our heroes counter that the allegators, who do not have a state to enforce property rights, cannot have a meaningful concept of property. The allegators threaten to form a state, but before they can do so the party manages to turn them against each other by pointing out where their property rights conflict; while the allegators argue, the adventurers sneak off.<br /><br />They continue through the swamp, braving dense vegetation, giant snakes, and more allegators (who are working on the whole state thing; the party tells them that they're too small and disorganized to be a real state, and that they would have to unite the entire allegator nation under a mutually agreed system of laws) before arriving at an old barrow tomb. Though four of the five adventurers want to leave well enough alone, Ephraim's experimental spirits gets the better of him, and he enters the mound. Its local Barrow Wight has long since departed, but he has left behind a suit of Dead Wight Mail, which confers powerful bonuses on Conservatives and followers of the Right-Hand Path. Nomophilos, the party's Conservative, is all set to take the Mail when Phaidros objects that it is morally wrong to steal from the dead; this sparks a fight that almost becomes violent before Nomo finally backs down; with a sigh of remorse, he leaves the magic item where he found it.<br /><br />Beyond the barrow tomb lies the domain of the Hobbesgoblins, the mirror image of the Allegators in that they have a strong - some might say dictatorial - state under the rule of their unseen god-king, Lord-Over-All. They are hostile to any foreigners who refuse to swear allegiance to their ruler, but after seeing an idol of the god-king - a tentacled monstrosity bearing more than a passing resemblance to Cthulhu - our heroes are understandably reluctant to do so. As a result, the Hobbesgoblins try to refuse them passage through their capital city of Malmesbury on the grounds that, without being subordinated to Lord-Over-All or any other common ruler, the adventurers are in a state of nature relative to the Hobbesgoblins and may rob, murder, or otherwise exploit them. The Hobbesgoblins don't trust mere oaths or protestations of morality - but Nomophilos finally comes up with a compromise that satisfies them. He offers them a hostage in return for their good behavior, handing them his pet tortoise Xeno. This satisfies the Hobbesgoblins as assurance of their good behavior, and the party passes through Malmesbury without incident.<br /><br />On the far side of Malmesbury they come to a great lake, around which the miasmas of the swamp seem to swirl expectantly. On the shore of the lake lives Theseus with his two ships. Theseus tells his story: when he came of age, he set off on a trading expedition upon his father's favorite ship. His father made him swear to return the ship intact, but after many years of travel, Theseus realized that every part of the ship had been replaced and repaired, so that there was not a single piece of the ship that was the same as when it had left port. Mindful of his oath, he hunted down the old pieces he had replaced, and joined them together into a second ship. But now he is confused: is it the first or the second ship which he must return to his father?<br /><br />The five philosophers tell Theseus that it is the first ship: the ship's identity is linked to its causal history, not to the matter that composes it. Delighted with this answer, he offers the second ship to the adventurers, who sail toward the far shore.<br /><br />Halfway across the lake, they meet an old man sitting upon a small island. He introduces himself as Thomas Hobbes, and says that his spies and secret police have told him everything about the adventurers since they entered the Slough. Their plan to save Russellia is a direct threat to his own scheme to subordinate the entire world under one ruler, and so he will destroy them. When the party expresses skepticism, his \"island\" rises out of the water and reveals itself to be the back of the monstrous sea creature, Leviathan, the true identity of the Hobbesgoblins' Lord-Over-All. After explaining his theory of government (<a href=\"http://www.raikoth.net/Stuff/ddis/dsong_hobbes.html\">\"Let's Hear It For Leviathan\"</a>, lyrics only) Hobbes and the monster attack for the game's first boss battle. The fight is immediately plagued by mishaps, including one incident where Phaidros's \"Calvin's Predestined Hellfire\" spell causes Hobbes to briefly turn into a Dire Tiger. When one of Leviathan's tentacles grab Cerune, she manifests a battle-axe of magic fire called the Axe of Separation and hacks the creature's arm off. She refuses to explain this power, but inspired by the small victory the party defeat Hobbes and reduce Leviathan into a state of Cartesian doubt; the confused monster vanishes into the depths, and the adventurers hurry to the other side and out of the Slough.<br /><br /><strong>CHAPTER THREE: THE SHADOW CITY</strong><br /><br />Although our heroes make good time, they soon spot a detachment of Hobbesgoblins pursuing them. Afraid the goblins will be angry at the defeat of their god, the party hides; this turns out to be unnecessary, as the goblins only want Ephraim - the one who actually dealt the final blow against Leviathan - to be their new Lord-Over-All. Ephraim rejects the positions, and the party responds to the goblins' desperate pleading by suggesting a few pointers for creating a new society - punishing violence, promoting stability, reinforcing social behavior. The Hobbesgoblins grumble, but eventually depart - just in time for the party to be attacked by more of Buridan's Assassins. These killers' PLXM coins seem to suggest an origin in Xar-Morgoloth, the Shadow City, and indeed its jet-black walls now loom before them. But the city sits upon the only pass through the Central Mountains, so the party reluctantly enters.<br /><br />Xar-Morgoloth turns out to be a pleasant town of white-washed fences and laughing children. In search of an explanation for the incongruity the five seek out the town's spiritual leader, the Priest of Lies. The Priest explains that although Xar-Morgoloth is superficially a nice place, the town is evil by definition. He argues that all moral explanations must be grounded in base moral facts that cannot be explained, whether these be respect for others, preference of pleasure over pain, or simple convictions that murder and theft are wrong. One of these base level moral facts, he says, is that Xar-Morgoloth is evil. It is so evil, in fact, that it is a moral imperative to keep people out of the city - which is why he sent assassins to scare them off.<br /><br />Doubtful, the party seeks the mysterious visiting philosopher whom the Priest claimed originated these ideas: they find Immanuel Kant living alone on the outskirts of the city. Kant tells his story: he came from a parallel universe, but one day a glowing portal appeared in the sky, flinging him into the caves beyond Xar-Morgoloth. Wandering into Xar-Morgoloth, he tried to convince the citizens of his meta-ethical theories, but they insisted they could ground good and evil in basic moral intuitions instead. Kant proposed that Xar-Morgoloth was evil as a thought experiment to disprove them, but it got out of hand.<br /><br />When our heroes challenge Kant's story and blame him for the current state of the city, Kant gets angry and casts Parmenides' Stasis Hex, freezing them in place. Then he announces his intention to torture and kill them all. For although in this world Immanuel Kant is a moral philosopher, in his own world (he explains) Immanuel Kant is a legendary villain and figure of depravity (<a href=\"http://www.raikoth.net/Stuff/ddis/dsong_kant.html\">\"I'm Evil Immanuel Kant\"</a>, lyrics only). Cerune manifests a second magic weapon, the Axe of Choice, to break the Stasis Hex, and the party have their second boss battle, which ends in defeat for Evil Kant. Searching his home, they find an enchanted Parchment of Natural Law that causes the chill in the air whenever the city's name is spoken.<br /><br />Armed with this evidence, they return to the Priest of Lies and convince him that his moral theory is flawed. The Priest dispels the shadow over the city, recalls his assassins, and restores the town name to its previous non-evil transliteration of Summerglass. He then offers free passage through the caverns that form the only route through the Central Mountains.<br /><br /><strong>CHAPTER FOUR: THE CAVERNS OF ABCISSA</strong><br /><br />Inside the caverns, which are nearly flooded by the overflowing Abcissa River, the party encounter an army of Water Elementals, leading them to suspect that they may be nearing the headquarters of Heraclitus' arch-enemy, Thales. The Water Elementals are mostly busy mining the rock for gems and magic artifacts, but one of them is sufficiently spooked by Phaidros to cast a spell on him, temporarily turning him to water. This is not immediately a disaster - Phaidros assumes a new form as a water elemental but keeps his essential personality - except that in an Ephraimesque display of overexperimention, Phaidros wonders what would happen if he temporarily relaxed the morphogenic field that holds him in place - as a result, he loses his left hand, a wound which stays in place when he reverts back to his normal form a few hours later. A resigned Phaidros only quotes the Bible: (\"And if your hand offend you, cut it off: it is better for you to enter into life maimed, than having two hands to go into hell\" - Mark 9:43) and trusts in the Divine plan.<br /><br />The Caverns of Abcissa are labyrinthine and winding, but eventually the party encounters a trio who will reappear several times in their journey: Ruth (who tells the truth), Guy (who'll always lie) and Clancy (who acts on fancy). These three have a habit of hanging around branching caverns and forks in the road, and Ephraim solves their puzzle thoroughly enough to determine what route to take to the center of the cave system.<br /><br />Here, in a great cavern, lives a civilization of cave-men whose story sounds a lot like Evil Kant's - from another world, minding their own business until a glowing portal appeared in the sky and sucked them into the caves. The cave-men are currently on the brink of civil war after one of their number, Thag, claims to have visited the mythical \"outside\" and discovered a world of magic and beauty far more real than the shadows dancing on the walls of their cavern. Most of the other cave-men, led by the very practical Vur, have rejected his tale, saying that the true magic and beauty lies in accepting the real, in-cave world rather than chasing after some outside paradise - but a few of the youth have flocked to Thag's banner, including Antil, a girl with mysterious magic powers.<br /><br />Only the timely arrival of the adventurers averts a civil war; the party negotiates a truce and offers to solve the dispute empirically - they will escort Vur and Antil with them through the caverns so that representatives of both sides can see whether or not the \"outside\" really exists. This calms most of the cave-men down, and with Vur and Antil alongside, they head onward to the underground source of the Abcissa - which, according to their research, is the nerve center of Thales' watery empire.<br /><br />On the way, they encounter several dangers. First, they awake a family of hibernating bears, who are quickly dispatched but who manage to maul the frail Vur so severely that only some divine intervention mediated by Phaidros saves his life. Second, they come across a series of dimensional portals clearly linked to the stories related by Evil Kant and the cave-men. Some link directly to otherworldly seas, pouring their water into the Abcissa and causing the recent floods. Others lead to otherworldly mines and quarries, and are being worked by gangs of Water Elementals. After some discussion of the ethics of stranding the Water Elementals, the five philosophers decide to shut down as many of the portals as possible.<br /><br />They finally reach the source of the Abcissa, and expecting a battle, deck themselves out in magic armor that grants immunity to water magic. As expected, they encounter Thales, who reveals the full scale of his dastardly plot - to turn the entire world into water. But his exposition is marred by a series of incongruities, including his repeated mispronunciations of his own name (<a href=\"http://www.raikoth.net/Stuff/ddis/dsong_thales.html\">\"All is Water\"</a>, lyrics only). And when the battle finally begins, the party dispatches Thales with minimal difficulty, and the resulting corpse is not that of a Greek philosopher at all, but rather that of Davidson's Swampman, a Metaphysical summon that can take the form of any creature it encounters and imitate them perfectly.<br /><br />Before anyone has time to consider the implications of their discovery, they are attacked by the real Water Mage, who bombards them with powerful water spells to which their magic armor mysteriously offers no protection. Worse, the Mage is able to create dimensional portals at will, escaping attacks effortlessly. After getting battered by a series of magic Tsunamis that nearly kill several of the weaker party members, the adventurers are in dire straits.<br /><br />Then the tide begins to turn. Antil manifests the power to go invisible and attack the Water Mage from an unexpected vantage. Cerune manifests another magic weapon, the Axe of Extension, which gives her allies the same powers over space as the Water Mage seems to possess. And with a little prompting from Cerune, Phaidros and Nomophilos realize the Water Mage's true identity. Magic armor doesn't grant protection from his water spells because they are not water at all, but XYZ, a substance physically identical to but chemically different from H2O. And his mastery of dimensional portals arises from his own origin in a different dimension, Twin Earth. He is Hilary Putnam (<a href=\"http://www.raikoth.net/Stuff/ddis/dsong_putnam.html\">\"All is Water, Reprise\"</a>, lyrics only) who has crossed dimensions, defeated Thales, and assumed his identity in order to take over his watery empire and complete his world domination plot. With a last push of magic, the party manage to defeat Putnam, who is knocked into the raging Abcissa and drowned in the very element he sought to control.<br /><br />They tie up the loose ends of the chapter by evacuating the Water Elementals from Twin Earth, leading the cave-men to the promised land of the Outside, and confronting Antil about her mysterious magic. Antil gives them the source of her power to turn invisible: the Ring of Gyges, which she found on the cave floor after an earthquake. She warns them never to use it, as it presents a temptation which their ethics might be unable to overcome.<br /><br /><strong>CHAPTER FIVE: CLIMBING MOUNT IMPROBABLE</strong><br /><br />Now back on the surface, the party finds their way blocked by the towering Mount Improbable, which at first seems too tall to ever climb. But after some exploration, they find there is a gradual path sloping upward, and begin their ascent. They are blocked, however, by a regiment of uniformed apes: cuteness turns to fear when they get closer and find the apes have machine guns. They decide to negotiate, and the apes prove willing to escort them to their fortress atop the peak if they can prove their worth by answering a few questions about their religious beliefs.<br /><br />Satisfied, the ape army lead them to a great castle at the top of the mountain where Richard Dawkins (<a href=\"http://www.youtube.com/watch?v=eaGgpGLxLQw#t=1m04s\">\"Beware the Believers\"</a>, credit Michael Edmondson) and his snow leopard daemon plot their war against the gods themselves. Dawkins believes the gods to be instantiated memes - creations of human belief that have taken on a life of their own due to Aleithos' madness - and accuses them of causing disasters, poverty, and ignorance in order to increase humanity's dependence upon them and keep the belief that sustains their existence intact. With the help of his genetically engineered apes and a fleet of flying battleships, he has been waging war against all the major pantheons of polytheism simultaneously. Dawkins is now gearing up to attack his most implacable foe, Jehovah Himself, although he admits He has so far managed to elude him.<br /><br />Hoping the adventurers will join his forces, he takes them on a tour of the castle, showing them the towering battlements, the flotilla of flying battleships, and finally, the dungeons. In these last are imprisoned Fujin, Japanese god of storms; Meretseger, Egyptian goddess of the flood, and even Ares, the Greek god of war (whom Dawkins intends to try for war crimes: not any specific war crime, just war crimes in general). When the party reject Dawkins' offer to join his forces (most vocally Phaidros, most reluctantly Ephraim) Dawkins locks them in the dungeons themselves.<br /><br />They are rescued late at night by their old friend Theseus. Theseus lost his ship in a storm (caused by the Japanese storm god, Fujin) and joined Dawkins' forces to get revenge; he is now captain of the aerial battleships. Theseus loads the adventurers onto a flying battleship and deposits them on the far side of the mountain, where Dawkins and his apes will be unlikely to find them.<br /><br />Their troubles are not yet over, however, for they quickly encounter a three man crusade consisting of Blaise Pascal, Johann Tetzel, and St. Augustine of Hippo (mounted, cavalry-style, upon an actual hippopotamus). The three have come, led by a divine vision, to destroy Dawkins and his simian armies as an abomination unto the Lord, and upon hearing that the adventurers have themselves escaped Dawkins, invite them to come along. But the five, despite their appreciation for Pascal's expository fiddle music (<strong><a href=\"http://www.youtube.com/watch?v=8rNkTtJ35Xw\">\"The Devil and Blaise Pascal\"</a></strong>) are turned off by Tetzel's repeated attempts to sell them indulgences, and Augustine's bombastic preaching. After Phaidros gets in a heated debate with Augustine over the role of pacifism in Christian thinking, the two parties decide to go their separate ways, despite Augustine's fiery condemnations and Pascal's warning that there is a non-zero chance the adventurers' choice will doom them to Hell.<br /><br />After another encounter with Ruth, Guy, and Clancy, our heroes reach the base of Mount Improbable and at last find themselves in Russellia.<br /><br /><strong>CHAPTER SIX: THE PALL OVER RUSSELLIA</strong><br /><br />Russellia is, as the legends say, shrouded in constant darkness. The gloom and the shock of being back in her ancestral homeland are too much for Cerune, who breaks down and reveals her last few secrets. Before beginning the quest, she consulted the Turing Oracle in Cyberia, who told her to seek the aid of a local wizard, Zermelo the Magnificent. Zermelo gave her nine magic axes of holy fire, which he said possessed the power to break the curse over Russellia. But in desperation, she has already used three of the magic axes, and with only six left she is uncertain whether she will have the magic needed.<br /><br />At that moment, Heraclitus appears in a burst of flame, seeking a debriefing on the death of his old enemy Thales. After recounting the events of the past few weeks, our heroes ask Heraclitus whether, as a Fire Mage, he can reforge the axes of holy fire. Heraclitus admits the possibility, but says he would need to know more about the axes, their true purpose, and the enemy they were meant to fight. He gives the party an enchanted matchbook, telling them to summon him by striking a match when they gather the information he needs.<br /><br />Things continue going wrong when, in the midst of a discussion about large numbers, Phaidros makes a self-contradictory statement that summons a Paradox Beast. Our heroes stand their ground and manage to destroy the abomination, despite its habit of summoning more Paradox Beasts to its aid through its Principle of Explosion spell. Bruised and battered, they limp into the nearest Russellian city on their map, the town of Ravenscroft.<br /><br />The people of Ravenscroft tell their story: in addition to the eternal darkness, Russellia is plagued by vampire attacks and by a zombie apocalypse, which has turned the population of the entire country, save Ravenscroft, into ravenous brain-eating zombies. Despite the burghers claiming the zombie apocalypse had been confirmed by no less a figure than Thomas Nagel, who passed through the area a century ago, our heroes are unconvinced: for one thing, the Ravenscrofters are unable to present any evidence that the other Russellians are zombies except for their frequent attacks on Ravenscroft - and the Ravenscrofters themselves attack the other towns as a \"pre-emptive measure\". But the Ravenscrofters remain convinced, and even boast of their plan to launch a surprise attack on neighboring Brixton the next day.<br /><br />Suspicious, our heroes head to the encampment of the Ravenscroft army, where they are just in time to see Commander David Chalmers give a rousing oration against the zombie menace (<a href=\"http://www.emeraldrain.com/mp3/ERP_YZIL_04_Flee.mp3\">\"Flee! A History of Zombieism In Western Thought\"</a>, credit <a href=\"http://www.emeraldrain.com/\">Emerald Rain</a>). They decide to latch on to Chalmers' army, both because it is heading the same direction they are and because they hope they may be able to resolve the conflict between Ravenscroft and Brixton before it turns violent.<br /><br />They camp with the army in some crumbling ruins from the golden age of the Russellian Empire. Entering a ruined temple, they disarm a series of traps to enter a vault containing a legendary artifact, the Morningstar of Frege. They also encounter a series of statues and bas-reliefs of the Good King, in which he demonstrates his chivalry by swearing an oath to Aleithos that he will defend all those who cannot defend themselves. Before they can puzzle out the meaning of all they have seen, they are attacked by vampires, confirming the Ravenscrofters' tales; they manage to chase them away with their magic and a hare-brained idea of Phaidros' to bless their body water, turning it into holy water and burning them up from the inside.<br /><br />The next morning, they sneak into Brixton before the main army, and find their fears confirmed: the Brixtonites are normal people, no different from the Russellians, and they claim that Thomas Nagel told <em>them</em> that <em>they</em> were the only survivors of the zombie apocalypse. They manage to forge a truce between Ravenscroft and Brixton, but to their annoyance, the two towns make peace only to attack a third town, Mountainside, which they claim is definitely populated by zombies this time. In fact, they say, the people of Mountainside openly admit to being zombies and don't even claim to have souls.<br /><br />Once again, our heroes rush to beat the main army to Mountainside. There they find the town's leader, Daniel Dennett, who explains the theory of eliminative materialism (<strong><a href=\"http://www.youtube.com/watch?v=OFbE87P82-w\">\"The Zombies' Secret\"</a></strong>). The party tries to explain the subtleties of Dennett's position to a bloodthirsty Chalmers, and finally all sides agree to drop loaded terms like \"human\" and \"zombie\" and replace them with a common word that suggests a fundamental humanity but without an internal Cartesian theater (one of our heroes suggests \"NPC\", and it sticks). The armies of the three towns agree to ally against their true common enemy - the vampires who live upon the Golden Mountain and kidnap their friends and families in their nighttime raids.<br /><br />Before the attack, Nomophilos and Ephraim announce their intention to build an anti-vampire death ray. The theory is that places on the fringe of Russellia receive some sunlight, while places in the center are shrouded in endless darkness. If the towns of Russellia can set up a system of mirrors from their highest towers, they can reflect the sunlight from the borderlands into a central collecting mirror in Mountainside, which can be aimed at the vampires' hideout to flood it with daylight, turning them to ashes. Ephraim, who invested most of his skill points into techne, comes up with schematics for the mirror, and after constructing a successful prototype, Chalmers and Dennett sound the attack order.<br /><br />The death ray takes out many of the vampires standing guard, but within their castle they are protected from its light: our heroes volunteer to infiltrate the stronghold, but are almost immediately captured and imprisoned - the vampires intend to sacrifice Cerune in a ritual to use her royal blood to increase their power. But the adventurers make a daring escape: arch-conservative Nomophilos uses the invisible hand of the marketplace to steal the keys out of the jailer's pocket, and Phaidros summons a five hundred pound carnivorous Christ metaphor to maul the guards. Before the party can escape the castle, they are confronted by the vampire lord himself, who is revealed to be none other than Thomas Nagel (<a href=\"http://www.youtube.com/watch?v=nGDuXMu2MfE\"><strong>\"What Is It Like To Be A Bat?\"</strong></a>). In the resulting battle, Nagel is turned to ashes and the three allied cities make short work of the remaining vampires, capturing the castle.<br /><br />The next morning finds our heroes poring over the vampire lord's library. Inside, they find an enchanted copy of <em>Godel Escher Bach</em> (with the power to summon an identical enchanted copy of Godel Escher Bach) and a slew of books on Russellian history. Over discussion of these latter, they finally work out what curse has fallen over the land, and what role the magic axes play in its removal.<br /><br /><em>[spoiler alert; stop here if you want to figure it out for yourself]</em><br /><br />The Good King's oath to defend those who could not defend themselves was actually more complicated than that: he swore an oath to the god Aleithos to defend those <em>and only those</em> who could not defend themselves. His enemies, realizing the inherent contradiction, attacked him, trapping Russell in a contradiction - if he defended himself, he was prohibited from doing so; if he did not defend himself, he was obligated to do so. Trapped, he was forced to break his oath, and the Mad God punished him by casting his empire into eternal darkness and himself into an endless sleep. <br /><br />The nine axes of Zermelo the Magnificent embody the nine axioms of ZFC. If applied to the problem, they will allow set theory to be reformulated in a way that makes the paradox impossible, lifting the curse and waking the Good King.<br /><br />Upon figuring out the mystery, the party strike the enchanted match and summon Heraclitus, who uses fire magic to reforge the Axes of Choice, Separation, and Extension. Thus armed, the party leave the Vampire Lord's castle and enter the system of caverns leading into the Golden Mountain.<br /><strong><br />CHAPTER SEVEN: THE KING UNDER THE MOUNTAIN</strong><br /><br />The party's travels through the cavern are quickly blocked by a chasm too deep to cross. Nomophilos saves the day by realizing that the enchanted copy of Godel Escher Bach creates the possibility of infinite recursion; he uses each copy of GEB to create another copy, and eventually fills the entire chasm with books, allowing the party to walk through to the other side.<br /><br />There they meet Ruth, Clancy, and Guy one last time; the three are standing in front of a Logic Gate, and to open it the five philosophers must solve <a href=\"http://en.wikipedia.org/wiki/The_Hardest_Logic_Puzzle_Ever\">the Hardest Logic Puzzle Ever</a>. In an epic feat that the bards will no doubt sing for years to come, Macx comes up with a solution to the puzzle, identifies each of the three successfully, and opens the Logic Gate.<br /><br />Inside the gate is the Good King, still asleep after two centuries. His resting place is guarded by the monster he unleashed, a fallen archangel who has become a Queen Paradox Beast. The Queen summons a small army of Paradox Beast servants with Principle of Explosion, and the battle begins in earnest. Cerune stands in a corner, trying to manifest her nine magic axes, but Nomophilos uses his Conservative spell \"Morning in America\" to summon a Raygun capable of piercing the Queen Paradox Beast's armored exoskeleton. Macx summons a Universal Quantifier and attaches it to his Banish Paradox Beast spell to decimate the Queen's armies. Ephraim desperately tries to wake the Good King, while Phaidros simply prays.<br /><br />After an intense battle, Cerune manifests all nine axes and casts them at the Queen Paradox Beast, dissolving the paradox and destroying the beast's magical defenses. The four others redouble their efforts, and finally manage to banish the Queen. When the Queen Paradox Beast is destroyed, Good King Bertrand awakens.<br /><br />Bertrand is temporarily discombobulated, but eventually regains his bearings and listens to the entire adventure. Then he tells his story. The attack that triggered the curse upon him, he says, was no coincidence, but rather a plot by a sinister organization against whom he had been waging a shadow war: the Bayesian Conspiracy. He first encountered the conspiracy when their espionage arm, the Bayes Network, tried to steal a magic emerald of unknown origin from his treasury. Since then, he worked tirelessly to unravel the conspiracy, and had reached the verge of success - learning that their aim was in some way linked to a plan to gain the shattered power of the Mad God Aleithos for themselves - when the Conspiracy took advantage of his oath and managed to put him out of action permanently.<br /><br />He is horrified to hear that two centuries have passed, and worries that the Bayesians' mysterious plan may be close to fruition. He begs the party to help him re-establish contact with the Conspiracy and continue figuring out their plans, which may be a dire peril to the entire world. But he expresses doubt that such a thing is even possible at this stage.<br /><br />In a burst of flame, Heraclitus appears, announcing that all is struggle and that he has come to join in theirs. He admits that the situation is grim, but declares it is not as hopeless as it seems, because they do not fight alone. He invokes the entire Western canon as the inspiration they follow and the giants upon whose shoulders they stand (<strong><a href=\"http://www.youtube.com/watch?v=6hCnakAtUpA\">\"Grand Finale\"</a></strong>).<br /><br />Heraclitus, Good King Bertrand, and the five scholars end the adventure by agreeing to seek out the Bayesian Conspiracy and discover whether Russell's old adversaries are still active. There are nebulous plans to continue the campaign (subject to logistical issues) in a second adventure, <em>Fermat's Last Stand</em>.</p>\n<p><span style=\"text-decoration: underline;\"><strong><a name=\"music\"></a>MUSIC</strong></span><br /><br /><strong>LYRICS ONLY</strong><br />Hobbes' Song: <a href=\"http://www.raikoth.net/Stuff/ddis/dsong_hobbes.html\">Let's Hear It For Leviathan</a><br />Kant's Song: <a href=\"http://www.raikoth.net/Stuff/ddis/dsong_kant.html \">I'm Evil Immanuel Kant<br /></a>Thales' Song: <a href=\"http://www.raikoth.net/Stuff/ddis/dsong_thales.html\">All Is Water</a><br />Putnam's Song: <a href=\"http://www.raikoth.net/Stuff/ddis/dsong_putnam.html\">All Is Water, Reprise<br /><br /></a><strong>GOOD ARTISTS BORROW, GREAT ARTISTS STEAL</strong><br />Dawkins' Song: <a href=\"http://www.youtube.com/watch?v=eaGgpGLxLQw#t=1m04s\">Beware The Believers</a> (credit: Michael Edmondson)<br />Chalmers' Song: <a href=\"http://www.emeraldrain.com/mp3/ERP_YZIL_04_Flee.mp3\">Flee: A History of Zombieism In Western Thought</a> (credit: <a href=\"http://www.emeraldrain.com\">Emerald Rain</a>)<br /><br /><strong>ORIGINAL ADAPTATIONS</strong><br />Pascal's Song: <a href=\"http://www.youtube.com/watch?v=8rNkTtJ35Xw\">The Devil and Blaise Pascal</a><br />Dennett's Song: <a href=\"http://www.youtube.com/watch?v=OFbE87P82\">The Zombies' Secret<br /></a>Vampire Nagel's Song: <a href=\"http://www.youtube.com/watch?v=nGDuXMu2MfE\">What Is It Like To Be A Bat?<br /></a>Heraclitus' Song: <a href=\"http://www.youtube.com/watch?v=6hCnakAtUpA\">Grand Finale</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "etDohXtBrXd8WqCtR": 1, "hNFdS3rRiYgqqD8aM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ep2Z42hYqj68QZz6w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 84, "extendedScore": null, "score": 0.00018, "legacy": true, "legacyId": "11111", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 84, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-25T22:57:47.129Z", "modifiedAt": null, "url": null, "title": "Communicating rationality to the public: Julia Galef's \"The Straw Vulcan\"", "slug": "communicating-rationality-to-the-public-julia-galef-s-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:24:12.595Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zuJmtSqt3TsnBTYyu/communicating-rationality-to-the-public-julia-galef-s-the", "pageUrlRelative": "/posts/zuJmtSqt3TsnBTYyu/communicating-rationality-to-the-public-julia-galef-s-the", "linkUrl": "https://www.lesswrong.com/posts/zuJmtSqt3TsnBTYyu/communicating-rationality-to-the-public-julia-galef-s-the", "postedAtFormatted": "Friday, November 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Communicating%20rationality%20to%20the%20public%3A%20Julia%20Galef's%20%22The%20Straw%20Vulcan%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACommunicating%20rationality%20to%20the%20public%3A%20Julia%20Galef's%20%22The%20Straw%20Vulcan%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuJmtSqt3TsnBTYyu%2Fcommunicating-rationality-to-the-public-julia-galef-s-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Communicating%20rationality%20to%20the%20public%3A%20Julia%20Galef's%20%22The%20Straw%20Vulcan%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuJmtSqt3TsnBTYyu%2Fcommunicating-rationality-to-the-public-julia-galef-s-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzuJmtSqt3TsnBTYyu%2Fcommunicating-rationality-to-the-public-julia-galef-s-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 8049, "htmlBody": "<p>Julia Galef's Skepticon IV&nbsp;talk, <a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\">The Straw Vulcan</a>, is the best intro-to-rationality talk for the general public I've ever seen. Share the link with everyone you know!</p>\n<p>&nbsp;</p>\n<p><strong>Update</strong>: Below is the transcript <a href=\"/user/daenerys/\">prepared</a> by daenerys:</p>\n<p>\n<p>Emcee: You may recognize our next speaker from somewhere-- She was here earlier on our panel, she&rsquo;s known for her work with the New York City Skeptics and their podcast &ldquo;Rationally Speaking&rdquo;, and she&rsquo;s also the co-author of the rationality blog &ldquo;Measure of Doubt&rdquo; with her brother Jesse: Julia Galef!</p>\n<p>[applause]</p>\n<p>Julia: Hey, it&rsquo;s really nice to be back. I&rsquo;m so excited to be giving a talk at Skepticon. Last year was my first year and I got to moderate a panel, so this is an exciting new step for me. My talk today has been sort of organically growing over the last couple of years as I&rsquo;ve become more and more involved in the skeptic and rationality movements, and I&rsquo;ve gotten more and more practice, and learned many lessons the hard way about communicating ideas about rationality and critical thinking and skepticism to people.</p>\n<p>The title of the talk is &ldquo;The Straw Vulcan: Hollywood&rsquo;s Illogical View of Logical Decision-Making&rdquo;. So if there&rsquo;s anyone in the audience who doesn&rsquo;t recognize this face, this is Mr. Spock&hellip;Someone&rsquo;s raising their hands, but it&rsquo;s a Vulcan salute, so I don&rsquo;t believe you don&rsquo;t know him&hellip;So this is Mr. Spock; He&rsquo;s one of the main characters on Star Trek and he&rsquo;s the First Officer and the Science Officer on the Starship Enterprise. And his mother is human, but his father is Vulcan.&nbsp;</p>\n<p>The Vulcans are this race of aliens that are known for trying to live in strict adherence of the rules of reason and logic, and also for eschewing emotion. This is something I wasn&rsquo;t clear on when I was remembering the show from my childhood, but it&rsquo;s not that the Vulcans don&rsquo;t have emotion, it&rsquo;s just that over time they&rsquo;ve developed very strict and successful ways of transcending and suppressing their emotions. So Spock being half-Vulcan has more lapses than a pure-blood Vulcan, but still, on the show Star Trek he is &ldquo;The Logical Character&rdquo; and that makes up a lot of the inter-character dynamics and the storylines on the show.&nbsp;</p>\n<p>[2:30]</p>\n<p>So, here&rsquo;s Spock. Here are the Vulcans. And I asked this question: &ldquo;Vulcans: Rational Aliens?&rdquo; with a question mark because the brand of rationality that&rsquo;s practiced by Spock and his fellow Vulcans isn&rsquo;t actually rationality. And that&rsquo;s what my talk is going to be about today.</p>\n<p>This term, &ldquo;Straw Vulcan&rdquo;, I wish I could personally take credit for it, but I borrowed it from a website called TvTropes [audience cheers], Yes! TvTropes! Some of the highest level of rationality that I can find on the internet, let alone on another pop culture or television blog. I highly recommend you check it out.</p>\n<p>So they coined the term &ldquo;Straw Vulcan&rdquo; to refer to the type of fictional character who is supposed to be &ldquo;The Logical One&rdquo; or &ldquo;The Rational One&rdquo;, but his brand of rationality is not real rationality. It&rsquo;s sort of this caricature; this weak, gimpy caricature of rationality that&hellip;Well, essentially, you would think that if someone were super-rational that they&rsquo;d be running circles around all the other characters, in the TV show or in the movie.</p>\n<p>&nbsp;But because it&rsquo;s this sort of &ldquo;fake&rdquo; rationality that&rsquo;s designed to demonstrate that the real success, the real route to glory and happiness and fulfillment, is all of these things that people consider to make us essentially human, like our passion, and our emotion, and our intuition, and yes, our irrationality. And since that&rsquo;s the point of the character, his brand of rationality is sort of this woeful character, and that&rsquo;s why it&rsquo;s called &ldquo;A Straw Vulcan&rdquo;.&nbsp;</p>\n<p>Because if you&rsquo;re arguing against some viewpoint that you disagree with, and you caricature that viewpoint in as simplistic and exaggerated a way as possible, to make it easy for yourself to just knock it down and pretend that you&rsquo;ve knocked that entire viewpoint down, that&rsquo;s a &ldquo;Straw Man&rdquo;&hellip;So these are &ldquo;Straw Vulcans&rdquo;.</p>\n<p>As I was saying, Spock and his fellow Straw Vulcans play this role in their respective TV shows and movies, of seeming like the character that should be able to save the day, but in practice the day normally gets saved by someone like this. [Kirk slide][laughter]</p>\n<p>Yup! &ldquo;I&rsquo;m sorry I can&rsquo;t hear you over the sound of how awesome I am&rdquo;&nbsp;</p>\n<p>So my talk today is going to be about Straw Vulcan rationality and how it diverges from actual rationality. And I think this is an important subject because&hellip;It&rsquo;s possible that many of you in the audience have some misconceptions about rationality that have been shaped by these Straw Vulcan characters that are so prevalent. And even if you haven&rsquo;t it&rsquo;s really useful to understand the concepts that are in people&rsquo;s minds when you talk to them about rationality.</p>\n<p>Because as I&rsquo;ve learned the hard way again and again; Even if it&rsquo;s so clear in your mind that rationality can make your life better, and can make the world better, if people are thinking of Straw Vulcan rationality you&rsquo;re never going to have any impact on them. So it&rsquo;s really useful to understand the differences between what you&rsquo;re thinking of and what many other people are thinking of when they talk about rationality.&nbsp;</p>\n<p>First what I&rsquo;m going to do is define what I mean by &ldquo;rationality&rdquo;. This is actual rationality. I&rsquo;m just defining this here because I&rsquo;m going to refer back to it throughout my talk, and I want you to know what I&rsquo;m talking about.&nbsp;</p>\n<p>There are two concepts that we use rationality to refer to. &nbsp;One of them is sometimes called &ldquo;epistemic rationality&rdquo;, and it&rsquo;s the method of obtaining an accurate view of reality, essentially. So the method of reasoning, and collecting evidence about the world, and updating your beliefs so as to make them as true as possible, hewing as closely to what&rsquo;s actually out there as possible.</p>\n<p>The other sense of the word rationality that we use is &ldquo;instrumental rationality&rdquo;. This is the method of achieving your goals, whatever they are. They could be selfish goals; They could be altruistic goals. Whatever you care about and want to achieve, instrumental rationality is defined as the method most likely to help you achieve them.</p>\n<p>And obviously they&rsquo;re related. It helps to have an accurate view of reality if you want to achieve your goals, with very few exceptions&hellip; But I&rsquo;m not going to talk about that right now, I just want to define the concepts for you.</p>\n<p>This is the first principle of Straw Vulcan Rationality: Being rational means expecting other people to be rational too. This is the sort of thing that tends to trip up a Straw Vulcan, and I&rsquo;m going to give you an example:</p>\n<p>This scene that&rsquo;s about to take place, the Starship&rsquo;s shuttle has just crash-landed on this potentially hostile alien planet. Mr. Spock is in charge and he&rsquo;s come up with this very rational plan, in his mind, that is going to help them escape the wrath of the potentially aggressive aliens: They&rsquo;re going to display their superior force, and the aliens are going to see that, and they&rsquo;re going to think rationally: &ldquo;Oh, they have more force than we do, so it would be against our best interests to fight back and therefore we won&rsquo;t.&rdquo; And this is what Spock does and it goes awry because the aliens are angered by the display of aggression and they strike back.&nbsp;</p>\n<p>This scene is taking place between Spock and McCoy, who&rsquo;s like Spock&rsquo;s foil. He&rsquo;s the very emotional, passion and intuition-driven doctor on the ship.</p>\n<p>[7:45]</p>\n<p>[video playing]</p>\n<p>McCoy: Well, Mr. Spock, they didn&rsquo;t stay frightened very long, did they?&nbsp;</p>\n<p>Spock: Most illogical reaction. When we demonstrated our superior weapons, they should have fled.</p>\n<p>McCoy: You mean they should have respected us?</p>\n<p>Spock: Of course!</p>\n<p>McCoy: Mr. Spock, respect is a rational process. Did it ever occur to you that they might react emotionally, with anger?</p>\n<p>Spock: Doctor, I&rsquo;m not responsible for their unpredictability.</p>\n<p>McCoy: They were perfectly predictable. To anyone with feeling. You might as well admit it, Mr. Spock. Your precious logic brought them down on us!</p>\n<p>[end video]</p>\n<p>[8:45}</p>\n<p>Julia: So you see what happens when you try to be logical&hellip;People die!</p>\n<p>Except of course, exactly the opposite. This is irrationality, not rationality. Rationality is about having as accurate a view of the world as possible and also about achieving your goals. And clearly Spock has persistent evidence accumulated again and again over time that other people are not actually perfectly rational, and he&rsquo;s just willfully neglecting the evidence; The exact opposite of epistemic rationality. Of course it also leads to the opposite of instrumental rationality too, because if people behave constantly the opposite of what you expect them to, you can&rsquo;t possibly make decisions that are going to be achieving your goals.</p>\n<p>So this concept of rationality, or this particular tenet of Straw Vulcan Rationality can be found outside of Star Trek as well. I was sort of surprised by the prevalence of that, but I&rsquo;ll give you an example: This was an article earlier this year in InfoWorld and basically the article is making the argument that one of the big problems with Google, and Microsoft, and Facebook, is that the engineers there don&rsquo;t really understand that their customers don&rsquo;t have the same worldview and values and preferences that they do.&nbsp;</p>\n<p>For example, if you remember the debacle that was Google Buzz; It was a huge privacy disaster because it signed you up automatically and then as soon as you were signed up all of your close personal contacts, like the friends that you emailed the most, suddenly got broadcast publicly to all your other friends. So the author of this article was arguing that, &ldquo;Well, people at Google don&rsquo;t care about this privacy, and so it didn&rsquo;t occur to them that other people in the world would actually care about privacy.&rdquo;</p>\n<p>And there&rsquo;s nothing wrong with that argument. That&rsquo;s a fine point to make. Except, he titled the article: &ldquo;Google&rsquo;s biggest problem is that it&rsquo;s too rational&rdquo;, which is exactly the same problem as the last example. That is they&rsquo;re too Straw Vulcan Rational, which is irrational.</p>\n<p>This is another example from a friend of mine who&rsquo;s also a skeptic writer and author of several really good books. This is Dan Gardner; he wrote Future Babel, and he&rsquo;s spoken at North-East Conference of Science and Skepticism where I&rsquo;ve also moderated and organized last year, and he&rsquo;s great! He&rsquo;s really smart. But on his blog I found this article that he wrote about how&hellip; he was criticizing an economist who was making the argument that the best way to fight crime would be to make harsher penalties because that would be a deterrent and that would reduce crime, because people respond to incentives.</p>\n<p>And Dan said, &ldquo;well that would make sense except for that the empirical argument shows that crime rates don&rsquo;t nearly respond quite as much to deterrent incentives as we think they do and so this economist is failing to update his model on how he thinks people should behave based on how the evidence suggests they actually do behave.&rdquo; Which again is fine, except that his conclusion was; &ldquo;Don&rsquo;t Be Too Rational About Crime Policy.&rdquo; So it&rsquo;s exactly the same kind of thinking.</p>\n<p>It&rsquo;s sort of a semantic point, in that he&rsquo;s defining rationality in this weird way, although I&rsquo;m not disagreeing with his actual argument. But it&rsquo;s this kind of thinking about rationality that can be detrimental in the long run.</p>\n<p>&nbsp;</p>\n<p>This is the second principle of Straw Vulcan rationality: Being rational means you should never make a decision until you have all the information.&nbsp;</p>\n<p>I&rsquo;ll give you an example. So I couldn&rsquo;t find a clip of this, unfortunately, but this scene takes place in an episode called &ldquo;The Immunity Syndrome&rdquo; in season 2, and basically people on the Starship Enterprise are falling ill mysteriously in droves, and there&rsquo;s this weird high-pitched sound that they&rsquo;re experiencing that&rsquo;s making them nauseated and Kirk and Spock see this big black blob on their screen and they don&rsquo;t know what it is&hellip; It turns out it&rsquo;s a giant space amoeba&hellip;Of course!</p>\n<p>But at this point early in the episode they don&rsquo;t really know much about it and so Kirk turns to Spock for input, for advice, for his opinion on what he thinks this thing is and what they should do. And Spock&rsquo;s response is: &ldquo;I have no analysis due to insufficient information&hellip;The computers contain nothing on this phenomenon. It is beyond our experience, and the new information is not yet significant.&rdquo;</p>\n<p>It&rsquo;s great to be loathe, to be hesitant to make a decision based on small amounts of evidence that isn&rsquo;t yet significant if you have a reasonable amount of time. But there are snap judgments that need to be made all the time, and you have to decide between paying the cost of all of the additional information that you want (and that cost could be in time or in money or in risk, if waiting is forcing you to incur more risk.) or just acting based on what you have at the moment.&nbsp;</p>\n<p>The rational approach, what a rationalist wants to do, is to maximize his&hellip;essentially to make sure he has the best possible expected outcome. The way to do that is not to always wait until you have all the information, but to weigh the cost of the information against how much do you think you&rsquo;re going to get from getting that information.&nbsp;</p>\n<p>We all know this intuitively in other areas of life; Like you don&rsquo;t want the best sandwich you can get, you want the best sandwich relative to how much you have to pay for it. So you&rsquo;d be willing to spend an extra dollar in order to make your sandwich a lot better, but if you had to spend $300 to make your sandwich slightly better, that wouldn&rsquo;t be worth it. You wouldn&rsquo;t actually be optimizing if you paid those $300 to make your sandwich slightly better.</p>\n<p>And again, this phenomenon, this interpretation of rationality, I found outside of Star Trek as well. Gerd Gigerenzer is a very well respected psychologist, but this is him describing how a rational actor would find a wife:</p>\n<p>&ldquo;He would have to look at the probabilities of various consequences of marrying each of them&mdash;whether the woman would still talk to him after they&rsquo;re married, whether she&rsquo;d take care of their children, whatever is important to him&mdash;and the utilites of each of these&hellip;After many years of research he&rsquo;d probably find out that his final choice had already married another person who didn&rsquo;t do these computations, and actually just fell in love with her.&rdquo;</p>\n<p>So Gerd Gigerenzer is a big critic of the idea of rational decision making, but as far as I can tell, one of the reasons he&rsquo;s a critic is because this is how he defines rational decision making. Clearly this isn&rsquo;t actual optimal decision making. Clearly someone who&rsquo;s actually interested in maximizing their eventual outcome would take into account the fact that doing years of research would limit the amount of women who would still be available and actually interested in dating you after all of that research was said and done.</p>\n<p>&nbsp;</p>\n<p>This is Straw Vulcan Rationality Principle number 3: Being rational means never relying on intuition.</p>\n<p>Here&rsquo;s an example. This is Captain Kirk, this is in the original series, and he and Spock are playing a game of three-dimensional chess.</p>\n<p>[16:00]</p>\n<p>[video starts, but there&rsquo;s no sound]</p>\n<p>Julia as Kirk: Checkmate! (He said)</p>\n<p>Julia as Spock: Your illogical approach to chess does have its advantages on occasion, Captain.</p>\n<p>[end video]</p>\n<p>[laughter and applause]</p>\n<p>Julia: Um, let me just check my sound&mdash;Maximize my long-term expected outcome in this presentation by incurring a short-term cost. Well, we&rsquo;ll hope that doesn&rsquo;t happen again.</p>\n<p>Anyway, so clearly an approach that causes you to win at chess cannot by any sensible definition be called an illogical way of playing chess. But from the perspective of Straw Vulcan Rationality it can, because anything intuition-based is illogical in Straw Vulcan rationality.&nbsp;</p>\n<p>Essentially there are two systems that people use to make decisions. They&rsquo;re rather boringly called System 1 and System 2, but they&rsquo;re more colloquially known as the intuitive system of reasoning and the deliberative system of reasoning.&nbsp;</p>\n<p>The intuitive system of reasoning is an older system; it allows us to make automatic judgments, to make judgments using shortcuts which are sometimes known as heuristics. They&rsquo;re sort of useful rules of thumb for what&rsquo;s going to work, that don&rsquo;t always work. But they&rsquo;re good enough most of the time. They don&rsquo;t require a lot of cognitive processing ability or memory or time or attention.</p>\n<p>And then System 2, the deliberative system of reasoning, is much more recently evolved. It takes a lot more cognitive resources, a lot more attention, but it allows us to do more abstract critical thinking. It allows us to construct models of what might happen when it&rsquo;s something that hasn&rsquo;t happened before, whereas say a System 1 approach would decide what to do based on how things happened in the past.</p>\n<p>System 2 is much more useful when you can&rsquo;t actually safely rely on precedence and you actually think. &ldquo;What are the possible future scenarios and what would likely happen if I behaved in a certain way in each of those scenarios?&rdquo; That&rsquo;s System 2.</p>\n<p>&nbsp;System 1 is more prone to bias. Eliezer Yudkowsky gave a great talk earlier this morning about some of the biases that we can fall prey to, especially when we&rsquo;re engaging in System 1 reasoning. But that doesn&rsquo;t mean that it&rsquo;s always the wrong system to use.&nbsp;</p>\n<p>I&rsquo;ll give you a couple examples of System 1 reasoning before I go any farther. So there&rsquo;s a problem that logic teachers sometimes give to their students. It&rsquo;s a very simple problem; They say a bat and a ball together add up to $1.10. The bat costs a dollar more than the ball. How much does the ball cost?</p>\n<p>The intuitive System 1 answer to that question in 10 cents, because you look at $1.10, and you look at $1 , and you take away the dollar and you get 10 cents. But if the ball was actually 10 cents and the bat was actually a dollar, then the bat would not cost a dollar more than the ball. So essentially that&rsquo;s the kind of answer you get when you&rsquo;re not really thinking about the problem, you&rsquo;re just feeling around for&hellip;&ldquo;Well, what do problems like this generally involve? Well, you generally take one thing away from another thing, so I dunno, do that.&rdquo;</p>\n<p>In fact, when this problem was given to a class at Princeton, 50% of them got the wrong answer. It just shows how quickly we reach for our System 1 answer and how rarely we feel the need to actually go back and check in, in a deliberative fashion.</p>\n<p>Another example of System 1 reasoning that I really like is&hellip;you may have heard of this classic social psychology experiment in which researchers sent someone to wait in line at a copy machine, and they asked the person ahead of them; &ldquo;Excuse me, do you mind if I cut in line?&rdquo; And maybe about 50% or 40% of them agreed to let the experimenters&rsquo;&rsquo; plant cut ahead of them.&nbsp;</p>\n<p>But when the experimenters redid the study, and this time instead of saying &ldquo;Can I cut in front of you?&rdquo;, they said &ldquo;Can I cut in front of you because I need to make copies?&rdquo; Then the agreement rate went up to like 99%. Something really high.&nbsp;</p>\n<p>And there&rsquo;s literally&hellip;.Like of course they need to make copies! That&rsquo;s the only reason they would have to cut in line to a copy machine. Except, because the request was phrased in terms of giving a reason, our system 1 reasoning kicks in and we go &ldquo;Oh, they have a reason! So, sure! You have a reason.&rdquo;</p>\n<p>[21:00]</p>\n<p>System 1 and System 2 have their pros and cons in different contexts. System 1 is especially good when you have a short time span, and a limited amount of resources and attention to devote to a problem. It&rsquo;s also good when you know that you have experience and memory that&rsquo;s relevant to the question, but it&rsquo;s not that easily accessible; like you&rsquo;ve had a lot of experiences of things like this problem, but our memories aren&rsquo;t stored in this easy list where we can sort according to key words and find the mean of the number of items in our memory base. So you have information in there, and really the only way to access it sometimes is to rely on your intuition. It&rsquo;s also helpful when there are important factors that go into a decision that are hard to quantify.</p>\n<p>&nbsp;There are a number of recent studies which have been exploring when System 1 reasoning is successful, and it tends to be successful when people are making purchasing decisions or other decisions about their personal life. And there are a lot of factors involved; there&rsquo;s dozens of factors relevant to what car you buy that you could consider, but a lot of what makes you happy with your purchase or your choice is just your personal liking of the car. And that&rsquo;s not the sort of thing that&rsquo;s easy to quantify.&nbsp;</p>\n<p>When people try to think about using their System 2 reasoning they don&rsquo;t really know how to quantify their liking of the car, and so when they rely on System 2 they often tend to just look at the mileage, and the cost, and all these other things, which are important but they don&rsquo;t really get at that emotional preference about the car. So that kind of information can be helpfully drawn out by System 1 reasoning.&nbsp;</p>\n<p>Also if you&rsquo;re an expert in a field, say chess for example, you can easily win over someone who&rsquo;s using careful deliberative reasoning just based on all of your experience; You&rsquo;ve built up this incredible pattern recognition ability with chess. So a chess master can just walk past a chess board, glance at it, and say &ldquo;Oh, white&rsquo;s going to checkmate black in three moves.&rdquo; Or chess masters can play many different chess games at once and win them all. And obviously they don&rsquo;t have the cognitive resources to devote to each game fully, but their automatic pattern recognition system that they&rsquo;ve built up over thousands and thousands of chess games works just well enough.</p>\n<p>Intuition is less reliable in cases where the kinds of heuristics or the kinds of biases that Eliezer spoke about earlier are relevant, or when you have a good reason to believe that your intuition is based on something that isn&rsquo;t relevant to the task at hand. So if you&rsquo;re using &nbsp;your intuition to say how likely work in artificial intelligence is going to be to lead to some sort of global disaster, you might rely on your intuition. But you also have to think about the fact that your intuition in this case is probably shaped by fiction. There are a lot more stories about robot apocalypses and AI explosions that took over the world than there are stories about AI going in a nice, boring, pleasant way. So being able to recognize where your intuition comes from can help you decide when it&rsquo;s a good guide in a particular context.</p>\n<p>System 2 is better when you have more resources and more time. It&rsquo;s also good, as I mentioned, in new and unprecedented situations, new and unprecedented decision making contexts, where you can&rsquo;t just rely on patterns of what&rsquo;s worked in the past. So a problem like global warming, or a problem like other existential risks that face our world, our species, potential of a nuclear war&hellip;We don&rsquo;t really have precedence to draw on, so it&rsquo;s hard to think that we can rely on our intuition to tell us what&rsquo;s going to happen or what we should do. And System 2 tends to be worse when there are many, many factors to consider and we don&rsquo;t have the cognitive ability to consider them all fairly.</p>\n<p>But the main takeaway of the System 1/System 2 comparison is that both systems have their strengths and weaknesses. And rationality is about trying to find the truest path to an accurate picture of reality, and it&rsquo;s about trying to find what actually maximizes your own happiness or whatever goal you have. So what you do is you don&rsquo;t rely on one or the other blindly. You decide: Based on this context, which method is going to be the most likely one to get me to what I want? The truth, or whatever other goals I have.&nbsp;</p>\n<p>And I think that a lot of the times when you hear people say that it&rsquo;s possible to be too rational, what they&rsquo;re really talking about is that it&rsquo;s possible to use System 2 deliberative reasoning in contexts where it&rsquo;s inappropriate, or to use it poorly.</p>\n<p>Here&rsquo;s a real life example: This is a headline article that came out earlier this year. So if you can&rsquo;t read it, it says &ldquo;Is the Teen Brain too Rational?&rdquo; And the argument of the article (It was actually a study) and it found that teenagers, when they were deciding to take some risks like to do drugs or to drive above the speed limit, they often do what is technically System 2 reasoning, so they&rsquo;ll think about the pros and cons, and think about what the risks are likely to be.</p>\n<p>&nbsp;But the reason they do it anyways is because they&rsquo;re really bad at this System 2 reasoning. They poorly weigh the risks and the benefits and that&rsquo;s why they end up doing stupid things. So the conclusion I would draw from that is: Teens are bad at system 2 reasoning. The conclusion the author drew from that is that teens are too rational.&nbsp;</p>\n<p>Another example: I found this quote when I was Googling around for examples to use in this talk, and I found what I thought was a perfect quote illustrating this principle that I&rsquo;m trying to describe to you:</p>\n<p>&ldquo;It is therefore equally unbalanced to be mostly &ldquo;intuitive&rdquo; (i.e. ignoring that one&rsquo;s first impression can be wrong), or too rational (i.e. ignoring one&rsquo;s hunches as surely misguided)&rdquo;</p>\n<p>Here I would say if you ignore your hunches blindly and assume they&rsquo;re misguided then you&rsquo;re not being rational, you&rsquo;re being irrational. And so I was happily copying down the quote, before having looked at the author. Then I check to see who the author of the post was and it&rsquo;s the co-host of my podcast, &ldquo;Rationallly Speaking&rdquo;, Massimo Pigliucci, who I am very fond of, and probably going to get in trouble with now. But I couldn&rsquo;t pass up this perfect example, and this is just how committed I am to teaching you guys about true rationality that I will brave the wrath of that Italian man there.</p>\n<p>&nbsp;</p>\n<p>So Straw Vulcan Rationality Principle number four: Being rational means not having emotions.&nbsp;</p>\n<p>And this is something I want to focus on a lot, because I think the portrayal of rationality and emotions by Spock&rsquo;s version, by the Straw Vulcan version of rationality, is definitely confused, is definitely wrong. But I think the truth is nuanced and complicated, so I want to draw this one out a little bit more.</p>\n<p>But first, a clip</p>\n<p>[video]</p>\n<p>Julia: Oh! Spock thinks the captain&rsquo;s dead.</p>\n<p>Spock: Doctor, I shall be resigning my commission immediately of course, so I would appreciate your making the final arrangements.</p>\n<p>McCoy: Spock, I&hellip;</p>\n<p>Spock: Doctor, please. Let me finish. There can be no excuse for the crime of which I am guilty. I intend to offer no defense. Furthermore, I will order Mr. Scot to take immediate command of this vessel.</p>\n<p>Kirk: (walking up from behind) Don&rsquo;t you think you better check with me, first?</p>\n<p>Spock: Captain! Jim! (Big smile, then regains control)&hellip;.I&rsquo;m&hellip;pleased&hellip;to see you again, Captain. You seem&hellip;uninjured.</p>\n<p>[end video]</p>\n<p>[29:15]</p>\n<p>Julia: So he almost slipped up there, but he caught himself just in time. Hopefully none of the other Vulcans found out about it.&nbsp;</p>\n<p>This is essentially the Spock model of how emotions and rationality relate to each other: You have a goal, and use rationality, unencumbered by emotion, to figure out what action to take to achieve that goal. Then emotion can get in the way and screw up this process if you&rsquo;re not really careful. This is the Spock model. And it&rsquo;s not wrong per se. Emotions can clearly, and frequently do, screw up attempts at rational decision making.&nbsp;</p>\n<p>I&rsquo;m sure you all have anecdotal examples just like I do, but to throw some out there; if you&rsquo;re really angry it can be hard to recognize the clear truth that lashing out at the person you&rsquo;re angry at is probably not going to be a good idea for you in the long run. Or if you&rsquo;re in love it can be hard to recognize the ways in which you are completely incompatible with this other person and that you&rsquo;re going to be really unhappy with this person in the long run if you stay with them. Or if you&rsquo;re disgusted and irritated by hippies, it can be hard to objectively evaluate arguments that you associate with hippies, like say criticisms of capitalism.&nbsp;</p>\n<p>These are just a few examples. These are just anecdotal examples, but there&rsquo;s plenty of experimental research out there that demonstrates that people&rsquo;s rational decision making abilities suffer when they&rsquo;re in states of heightened emotion. For example when people are anxious they over-estimate risks by a lot. When people are depressed, they under-estimate how much they are going to enjoy some future activity that&rsquo;s proposed to them.&nbsp;</p>\n<p>And then there&rsquo;s a series of really interesting studies by a couple of psychologists named Woodsman and Galinski that demonstrate that when people are feeling threatened or vulnerable, or like they don&rsquo;t have control, they tend to be much more superstitious; they perceive patterns where there are no patterns; they&rsquo;re likely to believe conspiracy theories; they&rsquo;re more likely to see patterns in companies and financial data that aren&rsquo;t actually there; and they&rsquo;re more likely to invest, to put their own money down, based on these non-existent patterns that they thought they saw.</p>\n<p>So Spock is not actually wrong. The problem with this model is that it is just incomplete. And the reason it&rsquo;s incomplete is that &ldquo;Goal&rdquo; box. Where does that &ldquo;Goal&rdquo; box come from? It&rsquo;s not handed down to us from on high. It&rsquo;s not sort of written into the fabric of the universe. The only real reason reason that you have goals is because you have emotions-- because you care about some outcomes of the world&rsquo;s more than others; because you feel positively about some potential outcomes and negatively about other potential outcomes.&nbsp;</p>\n<p>If you really didn't care about any potential state of the world more or less than other potential state of the world, it wouldn't matter how skilled your reasoning abilities were, you'd never have reason to do anything. Essentially you&rsquo;d just look like this... &ldquo;Meh!&rdquo; I mean even rationality for its own sake isn&rsquo;t really coherent without some emotion, because if you want to do rationality, if you want to be rational, it&rsquo;s because you care more about having the truth than you do about being ignorant.</p>\n<p>&nbsp;Emotions are clearly necessary for forming the goals, rationality is simply lame without them. But there&rsquo;s also some interesting evidence that emotions are important for making the decisions themselves.</p>\n<p>&nbsp;There&rsquo;s a psychologist named Antonio Demasio who studies patients with brain damage to a certain part of their brain&hellip;Ventral parietal frontal cortex&hellip;I can&rsquo;t remember the name, but essentially it&rsquo;s part of the brain that&rsquo;s crucial for reacting emotionally to one&rsquo;s thoughts.&nbsp;</p>\n<p>The patients who suffered from this injury were perfectly undamaged in other ways. They could perform just as well on tasks on visual perception, and language processing, and probabilistic reasoning, and all these other forms of deliberative reasoning and other senses. But their lives very quickly fell apart after this injury, because when they were making decisions they couldn&rsquo;t actually simulate viscerally what the value was to them of the different options. So their jobs fell apart, their interpersonal relations fell apart, and also a lot of them became incredibly indecisive.</p>\n<p>Demasio tells the story of one patient of his, who, when he left the doctor&rsquo;s office Demasio gave him the choice of a pen or a wallet... Some cheap little wallet, whatever you want&hellip; And the patient sat there for about twenty minutes trying to decide. Finally he picked the wallet, but when he went home he left a message on the doctor&rsquo;s voicemail saying &ldquo;I changed my mind. Can I come back tomorrow and take the pen instead of the wallet?&rdquo;</p>\n<p>And the problem is that the way we make decisions is we sort of query our brains to see how we feel about the different options, and if you can&rsquo;t feel, then you just don&rsquo;t know what to do. So it seems like there&rsquo;s a strong case that emotions are essential for this ideal decision-making process, not just in forming your goals, but in actually weighing your different options in the context for a specific decision.</p>\n<p>This is the first revision I would make to the model of Straw Vulcan decision-making. And this is sort of the standard model for ideal decision-making as say economics formulates it. You have your values. (Economics doesn&rsquo;t particularly care what they are.) But the way economics formulates a rational actor is someone who acts in such a way as to maximize their chances of getting what they value, whatever that is.</p>\n<p>And again, that&rsquo;s a pretty good model. It&rsquo;s not a bad simplification of what&rsquo;s going on. But the thing about this model is that it takes your emotional desire as a given. It just says: &ldquo;Given what you desire, what&rsquo;s the best way to get it?&rdquo; And we don&rsquo;t have to take our desires as a given. In fact, I think this is where rationality comes back into the equation. We can actually use rationality to think about our instinctual emotional desires, and as a consequence of them, the things that we value: our goals. And think about what makes sense rationally.&nbsp;</p>\n<p>It&rsquo;s a little bit of a controversial statement. Some psychologists and philosophers would say that emotions and desires can&rsquo;t be rational or irrational; you just want what you want. And certainly they can&rsquo;t be rational or irrational in the same way that the leaps can&rsquo;t be rational or irrational. Some philosophers might argue about this, but I would say that you can't be wrong about what you want.</p>\n<p>But I think there&rsquo;s still a strong case to be made for some emotions being irrational, and if you think back to the two definitions of rationality that I gave you earlier; There was epistemic rationality which was about making your beliefs about the world as true as possible, and there was instrumental rationality, which was about maximizing your chances of getting what you want, whatever that is. So I think it makes sense to talk about emotions as being epistemically irrational, if they&rsquo;re implicitly, at their core, based on a false model of the world.&nbsp;</p>\n<p>And this happens all the time. For example, you might be angry at your husband for not asking how this presentation at work went. It was a really important presentation, and you can&rsquo;t believe he didn&rsquo;t ask you. And that anger is predicated on the assumption, whether conscious or not, that he should have known that was important. That he should have known that this was an important presentation to you. But if you actually take a step back and think about it, it could be that no, you never actually ever gave him any indication that this was important, and that you were worried about it. So then that would make that emotion irrational, because it&rsquo;s based on a false model of reality.</p>\n<p>Or for example you might feel guilty about something. Even though when you consciously think about it, you would have to acknowledge that you know you did nothing to cause it and that there was nothing you could have done to prevent it. So I would be inclined to call that guilt also epistemically irrational.&nbsp;</p>\n<p>Or for example, people might feel depressed because that&rsquo;s predicated on the assumption that there&rsquo;s nothing they could do to better their situation, and sometimes that might be true, but a lot of the times it&rsquo;s not. &nbsp;I would call that also an irrational emotion, because you may have some false beliefs about your capabilities of improving whatever the problem is. That&rsquo;s epistemic irrationality.&nbsp;</p>\n<p>Emotions can clearly be instrumentally irrational if they&rsquo;re making you worse off. Something like jealousy, or spite, or rage, or envy is unpleasant to you and it&rsquo;s not actually inspiring you to make any positive changes in your life. And it&rsquo;s perhaps causing rifts with people you care about, and making you less happy that way, then I&rsquo;d say that&rsquo;s pretty clearly preventing you from achieving your goals.</p>\n<p>Emotions can be instrumentally and epistemically irrational. Using rationality is what helps us recognize that and shape our goals based not on what our automatic emotional desires are, but on what our rationality-filtered emotional desires are. I put several emotional desires here, because another role that rationality plays in this ideal decision process is recognizing when you have conflicting desires, and weighing them against each other; Deciding which is more important to you and one of them can be changed, etc, etc.&nbsp;</p>\n<p>For example you might value being the kind of person who tells the truth, but you also might value being the kind of person that&rsquo;s liked by people. So you have to have some kind of way of weighing those two desires against each other before you decide what your goal in a particular situation actually is.&nbsp;</p>\n<p>This would be my next update to the Straw Vulcan model of emotion and rationality. But you can actually improve it a little bit more too. You can change your emotions using rationality. This is not that easy and it can often take some time. But it&rsquo;s definitely something that we know how to do, at least in limited ways.&nbsp;</p>\n<p>For example there&rsquo;s a field of psychotherapy called cognitive therapy, sometimes combined with behavioral techniques and called cognitive behavioral therapy. Their motto, if you can call it that, is &ldquo;Changing the way you think can change the way you feel.&rdquo; They have all of these techniques and exercises you can do to get over depression and anger, or anxiety, and other instrumentally and often epistemically irrational emotions.</p>\n<p>Here&rsquo;s an example: This is a cognitive therapy worksheet. A &ldquo;Thought Record&rdquo; is one of the most common exercises that cognitive therapy has their patients do, and it&rsquo;s common sense, essentially. It&rsquo;s about writing down and noting your thoughts when your emotions start to run away with you, or run away with themselves. And then stopping, asking &ldquo;What is the evidence that supports this thought that I have?&rdquo;</p>\n<p>I&rsquo;m sorry, to back up&hellip;Noting the thoughts that are underlying the emotions that you&rsquo;re feeling. So I was talking about these implicit assumptions about the world that your emotions are based on. It gets you to make those explicit, and then question whether you actually have good evidence for believing them.&nbsp;</p>\n<p>This sort of process, plus lots of other exercises that psychotherapists do with their patients and that people can even do at home by themselves from a book, is by far the most empirically validated and well-tested form of psychotherapy. In fact, some would say it&rsquo;s the only one that&rsquo;s really supported by evidence so far.</p>\n<p>Even if you&rsquo;re not doing an official campaign of cognitive therapy to change your emotions, and by way of your emotions, your desires and goals, there&rsquo;s still plenty of informal ways to rationally change your emotions and make yourself better off.&nbsp;</p>\n<p>For example, in the short term you could recognize when you feel that first spark of anger, and decide whether or not you want to fuel that anger by dwelling on what that person has done in the past that angered you, and imagining what they were thinking about you at that moment. Or you could decide to try to dampen the flames of your burgeoning anger by instead thinking about times that you&rsquo;ve screwed up in the past, or thinking about things that that person has done for you that were actually kind. So you do have actually a lot of conscious control, if you choose to take it, over which direction your emotions push you in, than I think a lot of people are used to realizing.</p>\n<p>[41:00]</p>\n<p>In the longer term, you can even change what you value. It&rsquo;s hard, and it does tend to take a while, but let&rsquo;s say that you wish you were a more compassionate person. And you have conflicting desires. One of your desires is to lie on the couch every night, and another one of your desires is to be the kind of person that other people will look up to. So you want to bring those conflicting desires into harmony with each other.</p>\n<p>You can actually, to some extent, make yourself more compassionate over time if that&rsquo;s what you want to do. You can choose to expose yourself to material, to images, and to descriptions of suffering refuges, and you can consciously decide to imagine that it&rsquo;s you in that situation. Or that it&rsquo;s your friends and family and you can do the thought experiment of asking yourself what the difference is between these people suffering, and you or your friends and family suffering, and you can bring the emotions about by thinking of the situations rationally.</p>\n<p>This is essentially my rough final model of the relationships between emotions and rationality in ideal decision-making, as distinguished from the Straw Vulcan model of the relationships between emotions and rationality.</p>\n<p>Here&rsquo;s Straw Vulcan Rationality Principle number 5: Being rational means valuing only quantifiable things&mdash;like money, efficiency, or productivity.</p>\n<p>[video]</p>\n<p>[43:45]</p>\n<p>McCoy: There&rsquo;s just one thing, Mr. Spock. You can&rsquo;t tell me when you first saw Jim alive, you weren&rsquo;t on the verge of giving us an emotional scene that brought the house down.</p>\n<p>Spock: Merely my quite logical relief that Starfleet had not lost a highly proficient captain.</p>\n<p>Kirk: Yes. I understand.</p>\n<p>Spock: Thank you, Captain.</p>\n<p>McCoy: Of course, Mr. Spock. You&rsquo;re reaction was quite logical.</p>\n<p>Spock: Thank you, Doctor. (starts walking away)</p>\n<p>McCoy: In a pig&rsquo;s eye.</p>\n<p>Kirk: Come on, Spock. Let&rsquo;s go mind the store.</p>\n<p>[end video]</p>\n<p>Julia: So it&rsquo;s not acceptable, in Straw Vulcan Rationality world, to feel happiness because your best friend and Captain is actually alive instead of dead. But it is acceptable to feel relief, I suppose Spock did say, because a proficient worker in your Starfleet is alive, and can therefore do more proficient work. That kind of thing is rationally justifiable from the Straw Vulcan model of how rationality works.</p>\n<p>Here&rsquo;s another example: This is from an episode called &ldquo;This Side of Paradise&rdquo;, and in this episode they&rsquo;re visiting a planet where there are these flowers that release spores, where if you inhale them, you suddenly get really emotional. This woman, who has a crush on Spock, makes sure to position him in front of one of the flowers when it opens and releases its spores. So all of a sudden, he&rsquo;s actually romantic and emotional. This is Kirk and the crew trying to get in touch with him while he&rsquo;s out frolicking in the meadow with his lady love.&nbsp;</p>\n<p>[45:30]</p>\n<p>[video]</p>\n<p>Kirk: (on communication device) Spock?</p>\n<p>Spock: That one looks like a dragon. See the tail, and dorsal spines.</p>\n<p>Lady: I&rsquo;ve never seen a dragon.</p>\n<p>Spock: I have. On Berengarias 7. But I&rsquo;ve never stopped to look at clouds before, or rainbows. Do you know I can tell you exactly why one appears in the sky. But considering its beauty has always been out of the question.</p>\n<p>[end video]</p>\n<p>Julia: So this model of rationality, in which the only things to value are quantifiable things that don&rsquo;t have to do with love or joy or beauty&hellip; I&rsquo;ve been trying to figure out where this came from. One of my theories is that is comes from the way economists talk about rationality, where a rational actor maximizes his expected monetary gain.&nbsp;</p>\n<p>This is a convenient proxy, because in a lot of ways money can proxy for happiness, because whatever it is that you want that you think is going to make you happy you can often buy with money, or things that are making you unhappy you can often get rid of with money. It&rsquo;s obviously not a perfect model, but it&rsquo;s good enough in a way that economists use money as a proxy for utility or happiness sometimes, when they&rsquo;re modeling how a rational actor should make decisions.&nbsp;</p>\n<p>But no economists in their right mind would tell you that money is inherently valuable or useful. You can&rsquo;t do anything with it. It can only be useful, valuable, worth caring about for what it can do for you.&nbsp;</p>\n<p>All of these things in the Straw Vulcan method of rationality, which they consider acceptable things to value, make no sense as values in and of themselves. It makes no sense to value productivity in and of itself if you are not allowed to get happy over someone you care about being alive instead of dead. It doesn&rsquo;t make sense at all to care about productivity or efficiency. The only way that could possibly be useful to you is in getting you more outcomes like the one where your best friend is alive instead of dead. So if you don&rsquo;t value that, then don&rsquo;t bother.</p>\n<p>This is one more example from real life, if you can consider an internet message board &ldquo;real life&rdquo;. I found a discussion where people were arguing whether or not it was possible to be too rational, and one of them said, &ldquo;Well, sure it is!&rdquo;, and one of them said, &ldquo;Well, give me an example,&rdquo; and he said &ldquo;Well fine, I will.&rdquo;&nbsp;</p>\n<p>His example was two guys driving in a car, and one of them says &ldquo;Oh, well we need to get from here to there, so let&rsquo;s take this road.&rdquo; And the second guys says, &ldquo;No, but that road has all this beautiful scenery, and it has this historical site which is really exciting, and it might have a UFO on it.&rdquo; And the first guy says, &ldquo;No we have to take this first road because it is .2 miles shorter, and we will save .015 liters of gas.&rdquo;&nbsp;</p>\n<p>And that was this message board commenter&rsquo;s model of how a rational actor would think about things. So I don&rsquo;t actually know if that kind of thinking is what created Straw Vulcans in TV and the movies, or whether Straw Vulcans is what created people&rsquo;s thinking about what rationality is, or it&rsquo;s probably some combination of the two. But it&rsquo;s definitely a widespread conception of what rationality consists of.</p>\n<p>&nbsp;I myself had a conversation with a friend of mine a couple years back when I was first starting to get excited about rationality, and read about it, and study it. She said, &ldquo;Oh it&rsquo;s interesting that you&rsquo;re interested in this because I&rsquo;m trying to be less rational.&rdquo;&nbsp;</p>\n<p>It took me a while to get to the bottom of what she actually meant by that. But it turns out what she meant was that she was trying to enjoy life more. And she thought that rationality was about valuing money, and getting a good job, and being productive and efficient. And she just wanted to relax and enjoy sunsets, and take it easy. To express that she said that she wanted to be less rational.</p>\n<p>Here&rsquo;s one more clip of Spock and Kirk after they left that planet. Basically, sorry for the spoiler, guys, but Kirk finds a way to cure Spock of his newfound emotions, and bring him back on board as the emotionless Vulcan he always was.</p>\n<p>[50:00]</p>\n<p>[video]</p>\n<p>Kirk: We haven&rsquo;t heard much from you about Omnicron Seti 3, Mr. Spock.</p>\n<p>Spock: I have little to say about it, Captain. Except that, for the first time in my life, I was happy.</p>\n<p>[end video]</p>\n<p>Julia: I know, awww. So I want to end on this because I think the main takeaway from all of this that I want to leave you guys with is:</p>\n<p>If you think you&rsquo;re acting rationally, but you consistently keep getting the wrong answer, and you consistently keep ending up worse off than you could be&hellip;.Then the conclusion that you should draw from that is not that rationality is bad. It&rsquo;s that you&rsquo;re being bad at rationality. In other words, you&rsquo;re doing it wrong! Thank you!</p>\n<p>[applause]</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BhfefamXXee6c2CH8": 1, "Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zuJmtSqt3TsnBTYyu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 33, "extendedScore": null, "score": 7.3e-05, "legacy": true, "legacyId": "11112", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T00:13:10.616Z", "modifiedAt": null, "url": null, "title": "Fix my Friends Head", "slug": "fix-my-friends-head", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.672Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MatthewBaker", "createdAt": "2011-06-03T22:19:50.449Z", "isAdmin": false, "displayName": "MatthewBaker"}, "userId": "xEPvhkraqrPSryfFr", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2mDD6aGbuB3A9NgT2/fix-my-friends-head", "pageUrlRelative": "/posts/2mDD6aGbuB3A9NgT2/fix-my-friends-head", "linkUrl": "https://www.lesswrong.com/posts/2mDD6aGbuB3A9NgT2/fix-my-friends-head", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Fix%20my%20Friends%20Head&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFix%20my%20Friends%20Head%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mDD6aGbuB3A9NgT2%2Ffix-my-friends-head%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Fix%20my%20Friends%20Head%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mDD6aGbuB3A9NgT2%2Ffix-my-friends-head", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2mDD6aGbuB3A9NgT2%2Ffix-my-friends-head", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 197, "htmlBody": "<p>I may not have the community Karma of our dear Alicorn to make a <a href=\"/lw/7no/fix_my_head/\">top level post</a> asking about a dear friend of mines illness that so far has remain undiagnosed but I would appreciate any discussion level support on these Symptoms.</p>\n<div id=\"msg_681057094_1322261571111:465177121\" class=\"fbChatMessage fsm direction_ltr\">She has high white blood cell and neutrophil count and low lymphocytes, akaline phosphatase and sodium.</div>\n<div id=\"msg_681057094_1322261613308:3294421699\" class=\"fbChatMessage fsm direction_ltr\">One doctor says this is iron deficient anemia</div>\n<div class=\"mhs mbs pts fbChatConvItem fbChatMessageGroup clearfix small\">\n<div class=\"mhs mbs pts fbChatConvItem fbChatMessageGroup clearfix small\">\n<div id=\"msg_681057094_1322261678439:1779709925\" class=\"fbChatMessage fsm direction_ltr\">She is on antidepressants.<br /></div>\n<div id=\"msg_681057094_1322261717924:2859645066\" class=\"fbChatMessage fsm direction_ltr\">She gets digestion problems and muscle and joint issues and she fees she has terrible memory and impaired learning</div>\n<div id=\"msg_681057094_1322261743778:1530705842\" class=\"fbChatMessage fsm direction_ltr\">She's very sensitive to temperature as well and her bones start to hurt and joints as well as her chest.</div>\n<a class=\"profileLink\" href=\"http://www.facebook.com/Michi.Rawr\"></a>\n<div class=\"messages\">\n<div id=\"msg_681057094_1322262187774:1092771697\" class=\"fbChatMessage fsm direction_ltr\">She feels like she's always had eating problems but lately its been getting worse and she's started throwing up.<br /></div>\n</div>\n<div class=\"messages\">She always wakes up with body aches and/or cramps and air bubbles in her chest (pressure).</div>\n</div>\n<div class=\"messages\"><br /></div>\n<div class=\"messages\">I submit to your collective donated wisdom and google-fu for mine has failed me.</div>\n<div class=\"messages\"><br /></div>\n<div class=\"messages\">EDIT:</div>\n<div class=\"messages\">I appreciate all your kind rebuttals mixed with your useful theories. She is currently being treated for iron deficient anemia and she will continue seeking help from medical professionals as well as random friends who have people to ask :) <br /></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2mDD6aGbuB3A9NgT2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -17, "extendedScore": null, "score": 8.052630297641935e-07, "legacy": true, "legacyId": "11113", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GefH4KTTczgTT3dGX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T01:58:38.167Z", "modifiedAt": null, "url": null, "title": "Recent updates to gwern.net (2011)", "slug": "recent-updates-to-gwern-net-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:19.276Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Lk9NHCNPp3h44uhxA/recent-updates-to-gwern-net-2011", "pageUrlRelative": "/posts/Lk9NHCNPp3h44uhxA/recent-updates-to-gwern-net-2011", "linkUrl": "https://www.lesswrong.com/posts/Lk9NHCNPp3h44uhxA/recent-updates-to-gwern-net-2011", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Recent%20updates%20to%20gwern.net%20(2011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARecent%20updates%20to%20gwern.net%20(2011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLk9NHCNPp3h44uhxA%2Frecent-updates-to-gwern-net-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Recent%20updates%20to%20gwern.net%20(2011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLk9NHCNPp3h44uhxA%2Frecent-updates-to-gwern-net-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLk9NHCNPp3h44uhxA%2Frecent-updates-to-gwern-net-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 25, "htmlBody": "<blockquote>\n<p>A list of things I have written or researched in 2011 which I put on my personal site.</p>\n</blockquote>\n<p>This has been split out to <a href=\"http://www.gwern.net/Changelog#section-3\">http://www.gwern.net/Changelog</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8byoqYZfdwHffYLZ6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Lk9NHCNPp3h44uhxA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 45, "extendedScore": null, "score": 8.053007948897318e-07, "legacy": true, "legacyId": "11119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T05:05:41.138Z", "modifiedAt": null, "url": null, "title": "(Subjective Bayesianism vs. Frequentism) VS. Formalism ", "slug": "subjective-bayesianism-vs-frequentism-vs-formalism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "potato", "createdAt": "2011-06-15T09:18:51.735Z", "isAdmin": false, "displayName": "Ronny"}, "userId": "kY5hs2WkacnSZd937", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pbsH5ysDG3zKXDLCk/subjective-bayesianism-vs-frequentism-vs-formalism", "pageUrlRelative": "/posts/pbsH5ysDG3zKXDLCk/subjective-bayesianism-vs-frequentism-vs-formalism", "linkUrl": "https://www.lesswrong.com/posts/pbsH5ysDG3zKXDLCk/subjective-bayesianism-vs-frequentism-vs-formalism", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20(Subjective%20Bayesianism%20vs.%20Frequentism)%20VS.%20Formalism%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A(Subjective%20Bayesianism%20vs.%20Frequentism)%20VS.%20Formalism%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbsH5ysDG3zKXDLCk%2Fsubjective-bayesianism-vs-frequentism-vs-formalism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=(Subjective%20Bayesianism%20vs.%20Frequentism)%20VS.%20Formalism%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbsH5ysDG3zKXDLCk%2Fsubjective-bayesianism-vs-frequentism-vs-formalism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpbsH5ysDG3zKXDLCk%2Fsubjective-bayesianism-vs-frequentism-vs-formalism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1453, "htmlBody": "<p>One of the core aims of the philosophy of probability is to explain the relationship between frequency and probability. The frequentist proposes identity as the relationship. This use of identity is highly dubious. We know how to check for identity between numbers, or even how to check for the weaker copula relation between particular objects; but how would we test the identity of frequency and probability? It is not immediately obvious that there is some simple value out there which is modeled by probability, like position and mass are values that are modeled by Newton's Principia. You can actually check if density * volume = mass, by taking separate measurements of mass, density and volume, but what would you measure to check a frequency against a probability?</p>\n<p>There are certain appeals to frequentest philosophy: we would like to say that if a bag has 100 balls in it, only 1 of which is white, then the probability of drawing the white ball is 1/100, and that if we take a non-white ball out, the probability of drawing the white ball is now 1/99. Frequentism would make the philosophical justification of that inference trivial. But of course, anything a frequentist can do, a Bayesian can do (better). I mean that literally: <a href=\"/lw/21c/frequentist_magic_vs_bayesian_magic/\">it's the stronger magic</a>.</p>\n<p>A Subjective Bayesian, more or less, says that the reason frequencies are related to probabilities is because when you learn a frequency you thereby learn a fact about the world, and one must update one's degrees of belief on every available fact. The subjective Bayesian actually uses the copula in another strange way:</p>\n<blockquote>\n<p>Probability is subjective degree of belief.</p>\n</blockquote>\n<p>and subjective Bayesians also claim:</p>\n<blockquote>\n<p>Probabilities are not in the world, they are in your mind.</p>\n</blockquote>\n<p>These two statements are brilliantly championed in <a href=\"/lw/s6/probability_is_subjectively_objective/\">Probability is Subjectively Objective</a>. But ultimately, the formalism which I would like to suggest denies both of these statements. Formalists do not ontologically commit themselves to probabilities, just as they do not say that numbers exist; hence we don't allocate probabilities in the mind or anywhere else; we only commit ourselves to number theory, and probability theory. Mathematical theories are simply repeatable processes which construct certain sequences of squiggles called \"theorems\", by changing the squiggles of other theorems, according to certain rules called \"inferences\". Inferences always take as input certain sequences of squiggles called premises, and output a sequence of squiggles called the conclusion. The only thing an inference ever does is add squiggles to a theorem, take away squiggles from a theorem, or both. It turns out that these squiggle sequences mixed with inferences can talk about almost anything, certainly any computable thing. The formalist does not need to ontologically commit to numbers to assert that \"There is a prime greater than 10000.\", even though \"There is x such that\" is a flat assertion of existence; because for the formalist \"There is a prime greater than 10000.\" simply means that number theory contains a theorem which is interpreted as \"there is a prime greater than 10000.\" When you say a mathematical fact in English, you are interpreting a theorem from a formal theory. If under your suggested interpretation, all of the theorems of the theory are true, then whatever system/mechanism your interpretation of the theory talks about, is said to be modeled by the theory.</p>\n<p>So, what is the relation between frequency and probability proposed by formalism? Theorems of probability, may be interpreted as true statements about frequencies, when you assign certain squiggles certain words and claim the resulting natural language sentence. Or for short we can say: \"Probability theory models frequency.\" It is trivial to show that Komolgorov models frequency, since it also models fractions; it is an algebra after all. More interestingly, probability theory models rational distributions of subjective degree of believe, and the optimal updating of degree of believe given new information. This is somewhat harder to show; <a href=\"/lw/3cp/dutch_books_and_decision_theory_an_introduction/\">dutch-book arguments</a> do nicely to at least provide some intuitive understanding of the relation between degree of belief, betting, and probability, but there is still work to be done here. If Bayesian probability theory really does model rational belief, which many believe it does, then that is likely the most interesting thing we are ever going to be able to model with probability. But probability theory also models spatial measurement? Why not add the position that probability <strong>is</strong> volume to the debating lines of the philosophy of probability?</p>\n<p>Why are frequentism's and subjective Bayesianism's misuses of the copula not as obvious as <em>volumeism's</em>? This is because what the Bayesian and frequentest are really arguing about is statistical methodology, they've just disguised the argument as an argument about <em>what probability is.</em> Your interpretation of probability theory will determine how you model uncertainty, and hence determine your statistical methodology. Volumeism cannot handle uncertainty in any obvious way; however, the Bayesian and frequentest interpretations of probability theory, imply two radically different ways of handling uncertainty.</p>\n<p>The easiest way to understand the philosophical dispute between the frequentist and the subjective Bayesian is to look at the classic biased coin:</p>\n<p style=\"padding-left: 30px;\">A subjective Bayesian and a frequentist are at a bar, and the bartender (being rather bored) tells the two that he has a biased coin, and asks them \"what is the probability that the coin will come up heads on the first flip?\" The frequentist says that for the coin to be biased means for it not have a 50% chance of coming up heads, so all we know is that it has a probability that is not equal 50%. The Bayesain says that that any evidence I have for it coming up heads, is also evidence for it coming up tails, since I know nothing about one outcome, that doesn't hold for its negation, and the only value which represents that symmetry is 50%.</p>\n<p>I ask you. What is the difference between these two, and the poor souls engaged in endless debate over realism about sound in the beginning of <a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Making Beliefs Pay Rent</a>?</p>\n<blockquote>\n<p><em>If a tree falls in a forest and no one hears it, does it make a sound? One says, \"Yes it does, for it makes vibrations in the air.\" Another says, \"No it does not, for there is no auditory processing in any brain.\"</em></p>\n</blockquote>\n<p>One is being asked: \"Are there pressure waves in the air if we aren't around?\" the other is being asked: \"Are there auditory experiences if we are not around?\" The problem is that \"sound\" is being used to stand for both \"auditory experience\" and \"pressure waves through air\". They are both giving the right answers to these respective questions. But they are failing to <a href=\"/lw/nv/replace_the_symbol_with_the_substance/\">Replace the Symbol with the Substance</a> and <a href=\"/lw/oc/variable_question_fallacies/\">they're using one word with two different meanings in different places</a>. In the exact same way, \"probability\" is being used to stand for both \"frequency of occurrence\" and \"rational degree of belief\" in the dispute between the Bayesian and the frequentist. The correct answer to the question: \"If the coin is flipped an infinite amount of times, how frequently would we expect to see a coin that landed on heads?\" is \"All we know, is that it wouldn't be 50%.\" because that is what it means for the coin to be biased. The correct answer to the question: \"What is the optimal degree of belief that we should assign to the first trial being heads?\" is \"Precisely 50%.\", because of the symmetrical evidential support the results get from our background information. How we should actually model the situation as statisticians depends on our goal. But remember that Bayesianism is the stronger magic, and the only contender for perfection in the competition.</p>\n<p>For us formalists, probabilities are not anywhere. We do not even believe in probability technically, we only believe in probability theory. The only coherent uses of \"probability\" in natural language are purely syncategorematic. We should be very careful when we colloquially use \"probability\" as a noun or verb, and be very careful and clear about what we mean by this word play. Probability theory models many things, including degree of belief, and frequency. Whatever we may learn about rationality, frequency, measure, or any of the other mechanisms that probability models, through the interpretation of probability theorems, we learn because probability theory is <em>isomorphic</em> to those mechanisms. When you use the copula like the frequentist or the subjective Bayesian, it makes it hard to notice that probability theory modeling both frequency and degree of belief, is not a contradiction. If we use \"is\" instead of \"model\", it is clear that frequency is not degree of belief, so if probability is belief, then it is not frequency.&nbsp; Though frequency is not degree of belief, frequency does model degree of belief, so if probability models frequency, it must also model degree of belief.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pbsH5ysDG3zKXDLCk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 32, "extendedScore": null, "score": 8.053677855026296e-07, "legacy": true, "legacyId": "11097", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 109, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LkdL2BuGdEAZYysXp", "XhaKvQyHzeXdNnFKy", "kjmN3fdwTgN8ejGTd", "a7n8GdKiAZRX86T5A", "GKfPL6LQFgB49FEnv", "shoMpaoZypfkXv84Y"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T06:02:39.355Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Guardians of the Gene Pool", "slug": "seq-rerun-guardians-of-the-gene-pool", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y2hKo33T5T8YbptGr/seq-rerun-guardians-of-the-gene-pool", "pageUrlRelative": "/posts/y2hKo33T5T8YbptGr/seq-rerun-guardians-of-the-gene-pool", "linkUrl": "https://www.lesswrong.com/posts/y2hKo33T5T8YbptGr/seq-rerun-guardians-of-the-gene-pool", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Gene%20Pool&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Gene%20Pool%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2hKo33T5T8YbptGr%2Fseq-rerun-guardians-of-the-gene-pool%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Guardians%20of%20the%20Gene%20Pool%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2hKo33T5T8YbptGr%2Fseq-rerun-guardians-of-the-gene-pool", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2hKo33T5T8YbptGr%2Fseq-rerun-guardians-of-the-gene-pool", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p>Today's post, <a href=\"/lw/m0/guardians_of_the_gene_pool/\">Guardians of the Gene Pool</a> was originally published on 16 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries#Guardians_of_the_Gene_Pool\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is a common misconception that the Nazis wanted their eugenics program to create a new breed of supermen. In fact, they wanted to breed back to the archetypal Nordic man. They located their ideals in the past, which is a counterintuitive idea for many of us.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8kh/seq_rerun_guardians_of_the_truth\">Guardians of the Truth</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y2hKo33T5T8YbptGr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.0538819100227e-07, "legacy": true, "legacyId": "11123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["aFtWRL3QihoF5uQd5", "hrqfAJn56bY8djRvY", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T06:09:41.382Z", "modifiedAt": null, "url": null, "title": "'Next post' and 'Previous post' links for posts in a sequence", "slug": "next-post-and-previous-post-links-for-posts-in-a-sequence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.285Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fc4gkeC9YxivW264z/next-post-and-previous-post-links-for-posts-in-a-sequence", "pageUrlRelative": "/posts/fc4gkeC9YxivW264z/next-post-and-previous-post-links-for-posts-in-a-sequence", "linkUrl": "https://www.lesswrong.com/posts/fc4gkeC9YxivW264z/next-post-and-previous-post-links-for-posts-in-a-sequence", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20'Next%20post'%20and%20'Previous%20post'%20links%20for%20posts%20in%20a%20sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A'Next%20post'%20and%20'Previous%20post'%20links%20for%20posts%20in%20a%20sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffc4gkeC9YxivW264z%2Fnext-post-and-previous-post-links-for-posts-in-a-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text='Next%20post'%20and%20'Previous%20post'%20links%20for%20posts%20in%20a%20sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffc4gkeC9YxivW264z%2Fnext-post-and-previous-post-links-for-posts-in-a-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffc4gkeC9YxivW264z%2Fnext-post-and-previous-post-links-for-posts-in-a-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 245, "htmlBody": "<p>Less Wrong would be <a href=\"http://www.phoenixrealm.com/how-to-increase-your-website-stickiness/\">stickier</a> if there were links at the bottom of each post in <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a> to the next and previous posts in that sequence.</p>\n<p>I just added those links for each post in my own sequences: <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">The Science of Winning at Life</a>, <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a>, and <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">No-Nonsense Metaethics</a>.</p>\n<p>I can't do that for sequences written by somebody else. Perhaps one or more of the <a href=\"/lw/5kt/who_are_the_lw_editors/\">LW editors</a> would be willing to start hacking away on that project?</p>\n<p>Here's the algorithm I executed:</p>\n<ol>\n<li>Open all the posts from one sequence, in order, in browser tabs.</li>\n<li>Go to first post in the sequence.</li>\n<li>Click 'Edit'.</li>\n<li>Click 'HTML' and uncheck 'Word Wrap.'</li>\n<li>Scroll to the bottom of the post (not counting notes and references) and paste in the following:<br />\n<p>&lt;p&gt;&amp;nbsp;&lt;/p&gt;<br />&lt;p align=\"right\"&gt;Next post: &lt;a href=\"\"&gt;&lt;/a&gt;&lt;/p&gt;<br />&lt;p align=\"right\"&gt;Previous post: &lt;a href=\"\"&gt;&lt;/a&gt;&lt;/p&gt;<br />&lt;p&gt;&amp;nbsp;&lt;/p&gt;<br />&lt;p&gt;&amp;nbsp;&lt;/p&gt;</p>\n</li>\n<li>If post is first in sequence, remove 'Previous post' line.</li>\n<li>If post is last in sequence, remove 'Next post' line.</li>\n<li>Paste in URL and post title for remaining 'Next post' or 'Previous post' lines of HTML.</li>\n<li>Click 'Update', then click 'Submit'.</li>\n<li>If this is the last post in the sequence, return 0. Else, move to next post in sequence and go to step #3.</li>\n</ol>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fc4gkeC9YxivW264z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 28, "extendedScore": null, "score": 8.053907104028554e-07, "legacy": true, "legacyId": "11124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iXxSyYLybSvjP7sMX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T06:30:25.217Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft: From digital intelligence to intelligence explosion", "slug": "intelligence-explosion-analysis-draft-from-digital", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.817Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6kRj95eAFJkGvMoEe/intelligence-explosion-analysis-draft-from-digital", "pageUrlRelative": "/posts/6kRj95eAFJkGvMoEe/intelligence-explosion-analysis-draft-from-digital", "linkUrl": "https://www.lesswrong.com/posts/6kRj95eAFJkGvMoEe/intelligence-explosion-analysis-draft-from-digital", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%3A%20From%20digital%20intelligence%20to%20intelligence%20explosion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%3A%20From%20digital%20intelligence%20to%20intelligence%20explosion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kRj95eAFJkGvMoEe%2Fintelligence-explosion-analysis-draft-from-digital%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%3A%20From%20digital%20intelligence%20to%20intelligence%20explosion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kRj95eAFJkGvMoEe%2Fintelligence-explosion-analysis-draft-from-digital", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kRj95eAFJkGvMoEe%2Fintelligence-explosion-analysis-draft-from-digital", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1852, "htmlBody": "<p>\n<p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on. This section is less complete than the others; missing text is indicated with brackets: [].</p>\n<p>_____</p>\n</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2>From digital intelligence to intelligence explosion</h2>\n<p>\n<p>Humans are the first terrestrial intelligences sophisticated enough to produce a technological civilization. It seems unlikely that we are near the ceiling of possible intelligences, rather than simply being the first such intelligence that happened to evolve. Computers far outperform humans in many narrow niches, and there is reason to believe that similar large improvements over human performance are possible for general reasoning, technology design, and other tasks of interest.</p>\n<p>Below, we discuss the advantages of digital intelligence that will likely produce an intelligence explosion, along with the nature and speed of this &ldquo;takeoff.&rdquo;</p>\n<p>&nbsp;</p>\n<h3>Advantages from mere digitality</h3>\n<p>One might think the first human-level digital intelligence would not affect the world much. After all, we have seven billion human-level intelligences right now, and few or none of them have prompted massive, sudden AI breakthroughs. A single added intelligence might seem to be a tiny portion of the total drivers of technological innovation.</p>\n<p>But while human-level <em>humans</em> do not suddenly lead to smarter AI, there are reasons to expect human-level digital intelligence to enable faster change. Digital intelligences can be ported across hardware platforms, and this has at least three significant consequences: speed, copyability, and goal coordination.</p>\n<p>&nbsp;</p>\n<h4>Speed</h4>\n<p>Axons carry spike signals at 75 meters per second or less (Kandel et al. 2000). That speed is a fixed consequence of the type of physiology we humans run on. In contrast, software minds could be ported to any available hardware, and could therefore think more rapidly as faster hardware became available. This is analogous to the way in which older video game systems can be emulated on (much faster) modern computers.&nbsp;</p>\n<p>Thus, if digital intelligence is invented that can think as fast as a human can, faster hardware would enable that same digital intelligence to think faster than a human. The speed of human thought would not be a stable resting place; it would be just one speed among many at which the digital intelligence could be run.</p>\n<p>&nbsp;</p>\n<h4>Copyability</h4>\n<p>Our colleague Steve Rayhawk likes to call digital intelligence &ldquo;instant intelligence; just add hardware!&rdquo; What Steve means is that while it will require extensive research to design the first digital intelligence, creating additional digital intelligences is just a matter of copying software. The population of digital minds can thus expand to fill the available hardware base, either through purchase (until the economic product of a new AI is less than the cost of the necessary computation) or through other means, for example hacking.</p>\n<p>Depending on the hardware demands and the intelligence of initial digital intelligences, we might move fairly rapidly from the first human-level digital intelligence to a population of digital minds that outnumbers biological humans.</p>\n<p>Copying also allows potentially rapid shifts in which digital intelligences, with which skills, fill the population. Since a digital intelligence&rsquo;s skills are stored digitally, its exact current state can be copied, including memories and acquired skills. Thus if one digital intelligence becomes, say, 10% better at earning money per dollar of rentable hardware than other digital intelligences, then it could replace the others across the hardware base, for about a 10% gain in the economic productivity of those resources.</p>\n<p>Digitality also opens up more parameters for variation. We can put humans through job-training programs, but we can&rsquo;t do precise, replicable brain surgeries on them. Digital workers would probably be more editable than human workers are. In the case of whole brain emulation, we know that transcranial magnetic stimulation (TMS) applied to an area of the prefrontal cortex can improve working memory performance (Fregni et al. 2005). Since TMS works by temporarily decreasing or increasing the excitability of populations of neurons, it seems plausible that decreasing or increasing the &ldquo;excitability&rdquo; parameter of a certain population of (virtual) neurons in a digital mind could improve performance. We could also experimentally modify scores of other whole brain emulation parameters, e.g. simulated glucose levels, (virtual) fetal brain cells grafted onto particular brain modules, and rapid connections across different parts of the brain.<sup>1</sup> A modular, transparent AI could be even more directly editable than a whole brain emulation &mdash; for example via its source code.</p>\n<p>Copyability thus dramatically changes the payoff from job training or other forms of innovation. If a human spends his summer at a job training program, his own future productivity is slightly boosted. But now, consider a &ldquo;copy clan&rdquo; &mdash; a set of copies of a single digital intelligence. If a copy clan of a million identical workers allocates one copy to such training, the learned skills can be copied to the rest of the copy clan, for a return on investment roughly a million times larger.</p>\n<p>&nbsp;</p>\n<h4>Goal coordination</h4>\n<p>Depending on the construction of the AIs, each instance within a copy clan could either: (a) have separate goals, heedless of the (indexically distinct) goals of its &ldquo;copy siblings&rdquo;; or (b) have a shared external goal that all instances of the AI care about, independently of what happens to their own particular copy. From the point of view of the AIs' creators, option (b) has obvious advantages in that the copy clan would not face the politics, principal agent problems, and other goal coordination problems that limit human effectiveness (Friedman 1993). A human who suddenly makes 500 times a subsistence income cannot use this to acquire 500 times as many productive hours per day. An AI of this sort, if its tasks are parallelizable, can.</p>\n<p>Any gains made by such a copy clan, or by a human or human organization controlling that clan, could potentially be invested in further AI development, allowing initial advantages to compound. The ease of copying skills and goals across digital media thus seems to lead to a world in which agents' intelligence, productivity levels, and goals are unstable and prone to monoculture.</p>\n<p>&nbsp;</p>\n<h3>Further advantages to digital intelligence</h3>\n<p>How much room is there for design improvements to the human brain? Likely, quite a bit. As noted above, AI designers could seek improved hardware or more efficient algorithms to enable increased speed. They could also search for &ldquo;qualitative&rdquo; improvements &mdash; analogs of the difference between humans and chimpanzees or mice that enable humans to do tasks that are likely impossible for any number of chimpanzees or mice in any amount of time, such as engineering our way to the moon. AI designers can make use of several resources that were less accessible to evolution:</p>\n<p><em>Increased serial depth</em>. Due to neurons&rsquo; slow firing speed, the human brain relies on massive parallelization and is incapable of rapidly performing any computation that requires more than about 100 sequential operations (Feldman and Ballard 1982). Perhaps there are cognitive tasks that could be performed more efficiently and precisely if the organic brain&rsquo;s ability to support parallelizable pattern-matching algorithms were supplemented by and integrated with support for fast sequential processes?</p>\n<p><em>Increased real-time introspective access, high-bandwidth communication, or consciously editable algorithms</em>. We humans access and revise our cognitive processes largely by fixed and limited pathways. Perhaps digitally recording states, consciously revising a larger portion of one&rsquo;s thought patterns, or sharing high-bandwidth states with other agents would increase cognitive capacity?</p>\n<p><em>Increased computational resources</em>. &nbsp;The human brain is small relative to the systems that could be built. The brain&rsquo;s approximately 85&ndash;100 billion neurons is a size limit imposed not only by constraints on head size and metabolism, but also by the difficulty of maintaining integration between distant parts of the brain given the slow speed at which impulses can be transmitted along neurons (Fox 2011). While algorithms would need to be changed in order to be usefully scaled up, one can perhaps get a rough feel for the potential impact here by noting that humans have about [number] times the brain mass of chimps [citation], and that brain mass and cognitive ability correlate positively, with a correlation coefficient of about 0.4, in both humans and rats. [cite humans study, cite rats study]</p>\n<p><em>Improved rationality</em>. Some economists model humans as <em>Homo economicus</em>: self-interested rational agents who do what they believe will maximize the fulfillment of their goals (Friedman 1953). Behavioral studies, in contrast, suggest that we are more like Homer Simpson (Schneider 2010): we are irrational and non-maximizing beings that lack consistent, stable goals (Stanovich 2010; Cartwright 2011). But imagine if you were an instance of <em>Homo economicus</em>. You could stay on that diet, spend all your time learning which practices will make you wealthy, and then engage in precisely those, no matter how tedious or irritating they are. Some types of digital intelligence, especially transparent AI, could be written to be vastly more rational than humans, and accrue the benefits of rational thought and action. Indeed, the rational agent model (using Bayesian probability theory and expected utility theory) is a mature paradigm in current AI design (Russel and Norvig 2010, ch. 2).</p>\n<p>&nbsp;</p>\n<h3>Recursive &ldquo;self&rdquo;-improvement</h3>\n<p>Once a digital intelligence becomes better at AI design work than the team of programmers that brought it to that point, a positive feedback loop may ensue. Now when the digital intelligence improves itself, it improves the intelligence that does the improving. Thus, if mere human efforts suffice to produce digital intelligence this century, a large population of sped-up digital intelligences may be able to create a rapid cascade of self-improvement cycles, enabling a rapid transition. Chalmers (this volume) discusses this process in some detail, so here we will make only a few additional points.</p>\n<p>The term &ldquo;self,&rdquo; in phrases like &ldquo;recursive self-improvement&rdquo; or &ldquo;when the digital intelligence improves itself,&rdquo; is something of a misnomer. The digital intelligence could conceivably edit its own code while it is running, but it could also create a new intelligence that runs independently. These &ldquo;other&rdquo; digital minds could perhaps be designed to have the same goals as the original, or to otherwise further its goals. In any case, the distinction between &ldquo;self&rdquo;-improvement and other AI improvement does not matter from the perspective of creating an intelligence explosion. The significant part is only that: (a) within a certain range, many digital intelligences probably can design intelligences smarter than themselves; and (b) given the possibility of shared goals, many digital intelligences will probably find greater intelligence useful (as discussed in more detail in section 4.1).</p>\n<p>Depending on the abilities of the first digital intelligences, recursive self-improvement could either occur as soon as digital intelligence arrives, or it could occur after human design efforts, augmented by advantages from digitality, create a level of AI design competence that exceeds the summed research power of non-digital human AI designers. In any case, at least once self-improvement kicks in, AI development need not proceed on the timescale we are used to in human technological innovation. In fact, as discussed in the next section, the range of scenarios in which takeoff <em>isn&rsquo;t</em> fairly rapid appears to be small, although non-negligible.</p>\n</p>\n<p>&nbsp;</p>\n<p>[the next section under 'from digital intelligence to explosion' is very large, and not included here]</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> This third possibility is particularly interesting, in that many suspect that the slowness of cross-brain connections has been a major factor limiting the usefulness of large brains (Fox 2011).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6kRj95eAFJkGvMoEe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 1, "extendedScore": null, "score": 8.053981359056527e-07, "legacy": true, "legacyId": "11125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>\n</p><p>Again, I invite your feedback on this snippet from an <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis</a> Anna Salamon and myself have been working on. This section is less complete than the others; missing text is indicated with brackets: [].</p>\n<p>_____</p>\n<p></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"From_digital_intelligence_to_intelligence_explosion\">From digital intelligence to intelligence explosion</h2>\n<p>\n</p><p>Humans are the first terrestrial intelligences sophisticated enough to produce a technological civilization. It seems unlikely that we are near the ceiling of possible intelligences, rather than simply being the first such intelligence that happened to evolve. Computers far outperform humans in many narrow niches, and there is reason to believe that similar large improvements over human performance are possible for general reasoning, technology design, and other tasks of interest.</p>\n<p>Below, we discuss the advantages of digital intelligence that will likely produce an intelligence explosion, along with the nature and speed of this \u201ctakeoff.\u201d</p>\n<p>&nbsp;</p>\n<h3 id=\"Advantages_from_mere_digitality\">Advantages from mere digitality</h3>\n<p>One might think the first human-level digital intelligence would not affect the world much. After all, we have seven billion human-level intelligences right now, and few or none of them have prompted massive, sudden AI breakthroughs. A single added intelligence might seem to be a tiny portion of the total drivers of technological innovation.</p>\n<p>But while human-level <em>humans</em> do not suddenly lead to smarter AI, there are reasons to expect human-level digital intelligence to enable faster change. Digital intelligences can be ported across hardware platforms, and this has at least three significant consequences: speed, copyability, and goal coordination.</p>\n<p>&nbsp;</p>\n<h4 id=\"Speed\">Speed</h4>\n<p>Axons carry spike signals at 75 meters per second or less (Kandel et al. 2000). That speed is a fixed consequence of the type of physiology we humans run on. In contrast, software minds could be ported to any available hardware, and could therefore think more rapidly as faster hardware became available. This is analogous to the way in which older video game systems can be emulated on (much faster) modern computers.&nbsp;</p>\n<p>Thus, if digital intelligence is invented that can think as fast as a human can, faster hardware would enable that same digital intelligence to think faster than a human. The speed of human thought would not be a stable resting place; it would be just one speed among many at which the digital intelligence could be run.</p>\n<p>&nbsp;</p>\n<h4 id=\"Copyability\">Copyability</h4>\n<p>Our colleague Steve Rayhawk likes to call digital intelligence \u201cinstant intelligence; just add hardware!\u201d What Steve means is that while it will require extensive research to design the first digital intelligence, creating additional digital intelligences is just a matter of copying software. The population of digital minds can thus expand to fill the available hardware base, either through purchase (until the economic product of a new AI is less than the cost of the necessary computation) or through other means, for example hacking.</p>\n<p>Depending on the hardware demands and the intelligence of initial digital intelligences, we might move fairly rapidly from the first human-level digital intelligence to a population of digital minds that outnumbers biological humans.</p>\n<p>Copying also allows potentially rapid shifts in which digital intelligences, with which skills, fill the population. Since a digital intelligence\u2019s skills are stored digitally, its exact current state can be copied, including memories and acquired skills. Thus if one digital intelligence becomes, say, 10% better at earning money per dollar of rentable hardware than other digital intelligences, then it could replace the others across the hardware base, for about a 10% gain in the economic productivity of those resources.</p>\n<p>Digitality also opens up more parameters for variation. We can put humans through job-training programs, but we can\u2019t do precise, replicable brain surgeries on them. Digital workers would probably be more editable than human workers are. In the case of whole brain emulation, we know that transcranial magnetic stimulation (TMS) applied to an area of the prefrontal cortex can improve working memory performance (Fregni et al. 2005). Since TMS works by temporarily decreasing or increasing the excitability of populations of neurons, it seems plausible that decreasing or increasing the \u201cexcitability\u201d parameter of a certain population of (virtual) neurons in a digital mind could improve performance. We could also experimentally modify scores of other whole brain emulation parameters, e.g. simulated glucose levels, (virtual) fetal brain cells grafted onto particular brain modules, and rapid connections across different parts of the brain.<sup>1</sup> A modular, transparent AI could be even more directly editable than a whole brain emulation \u2014 for example via its source code.</p>\n<p>Copyability thus dramatically changes the payoff from job training or other forms of innovation. If a human spends his summer at a job training program, his own future productivity is slightly boosted. But now, consider a \u201ccopy clan\u201d \u2014 a set of copies of a single digital intelligence. If a copy clan of a million identical workers allocates one copy to such training, the learned skills can be copied to the rest of the copy clan, for a return on investment roughly a million times larger.</p>\n<p>&nbsp;</p>\n<h4 id=\"Goal_coordination\">Goal coordination</h4>\n<p>Depending on the construction of the AIs, each instance within a copy clan could either: (a) have separate goals, heedless of the (indexically distinct) goals of its \u201ccopy siblings\u201d; or (b) have a shared external goal that all instances of the AI care about, independently of what happens to their own particular copy. From the point of view of the AIs' creators, option (b) has obvious advantages in that the copy clan would not face the politics, principal agent problems, and other goal coordination problems that limit human effectiveness (Friedman 1993). A human who suddenly makes 500 times a subsistence income cannot use this to acquire 500 times as many productive hours per day. An AI of this sort, if its tasks are parallelizable, can.</p>\n<p>Any gains made by such a copy clan, or by a human or human organization controlling that clan, could potentially be invested in further AI development, allowing initial advantages to compound. The ease of copying skills and goals across digital media thus seems to lead to a world in which agents' intelligence, productivity levels, and goals are unstable and prone to monoculture.</p>\n<p>&nbsp;</p>\n<h3 id=\"Further_advantages_to_digital_intelligence\">Further advantages to digital intelligence</h3>\n<p>How much room is there for design improvements to the human brain? Likely, quite a bit. As noted above, AI designers could seek improved hardware or more efficient algorithms to enable increased speed. They could also search for \u201cqualitative\u201d improvements \u2014 analogs of the difference between humans and chimpanzees or mice that enable humans to do tasks that are likely impossible for any number of chimpanzees or mice in any amount of time, such as engineering our way to the moon. AI designers can make use of several resources that were less accessible to evolution:</p>\n<p><em>Increased serial depth</em>. Due to neurons\u2019 slow firing speed, the human brain relies on massive parallelization and is incapable of rapidly performing any computation that requires more than about 100 sequential operations (Feldman and Ballard 1982). Perhaps there are cognitive tasks that could be performed more efficiently and precisely if the organic brain\u2019s ability to support parallelizable pattern-matching algorithms were supplemented by and integrated with support for fast sequential processes?</p>\n<p><em>Increased real-time introspective access, high-bandwidth communication, or consciously editable algorithms</em>. We humans access and revise our cognitive processes largely by fixed and limited pathways. Perhaps digitally recording states, consciously revising a larger portion of one\u2019s thought patterns, or sharing high-bandwidth states with other agents would increase cognitive capacity?</p>\n<p><em>Increased computational resources</em>. &nbsp;The human brain is small relative to the systems that could be built. The brain\u2019s approximately 85\u2013100 billion neurons is a size limit imposed not only by constraints on head size and metabolism, but also by the difficulty of maintaining integration between distant parts of the brain given the slow speed at which impulses can be transmitted along neurons (Fox 2011). While algorithms would need to be changed in order to be usefully scaled up, one can perhaps get a rough feel for the potential impact here by noting that humans have about [number] times the brain mass of chimps [citation], and that brain mass and cognitive ability correlate positively, with a correlation coefficient of about 0.4, in both humans and rats. [cite humans study, cite rats study]</p>\n<p><em>Improved rationality</em>. Some economists model humans as <em>Homo economicus</em>: self-interested rational agents who do what they believe will maximize the fulfillment of their goals (Friedman 1953). Behavioral studies, in contrast, suggest that we are more like Homer Simpson (Schneider 2010): we are irrational and non-maximizing beings that lack consistent, stable goals (Stanovich 2010; Cartwright 2011). But imagine if you were an instance of <em>Homo economicus</em>. You could stay on that diet, spend all your time learning which practices will make you wealthy, and then engage in precisely those, no matter how tedious or irritating they are. Some types of digital intelligence, especially transparent AI, could be written to be vastly more rational than humans, and accrue the benefits of rational thought and action. Indeed, the rational agent model (using Bayesian probability theory and expected utility theory) is a mature paradigm in current AI design (Russel and Norvig 2010, ch. 2).</p>\n<p>&nbsp;</p>\n<h3 id=\"Recursive__self__improvement\">Recursive \u201cself\u201d-improvement</h3>\n<p>Once a digital intelligence becomes better at AI design work than the team of programmers that brought it to that point, a positive feedback loop may ensue. Now when the digital intelligence improves itself, it improves the intelligence that does the improving. Thus, if mere human efforts suffice to produce digital intelligence this century, a large population of sped-up digital intelligences may be able to create a rapid cascade of self-improvement cycles, enabling a rapid transition. Chalmers (this volume) discusses this process in some detail, so here we will make only a few additional points.</p>\n<p>The term \u201cself,\u201d in phrases like \u201crecursive self-improvement\u201d or \u201cwhen the digital intelligence improves itself,\u201d is something of a misnomer. The digital intelligence could conceivably edit its own code while it is running, but it could also create a new intelligence that runs independently. These \u201cother\u201d digital minds could perhaps be designed to have the same goals as the original, or to otherwise further its goals. In any case, the distinction between \u201cself\u201d-improvement and other AI improvement does not matter from the perspective of creating an intelligence explosion. The significant part is only that: (a) within a certain range, many digital intelligences probably can design intelligences smarter than themselves; and (b) given the possibility of shared goals, many digital intelligences will probably find greater intelligence useful (as discussed in more detail in section 4.1).</p>\n<p>Depending on the abilities of the first digital intelligences, recursive self-improvement could either occur as soon as digital intelligence arrives, or it could occur after human design efforts, augmented by advantages from digitality, create a level of AI design competence that exceeds the summed research power of non-digital human AI designers. In any case, at least once self-improvement kicks in, AI development need not proceed on the timescale we are used to in human technological innovation. In fact, as discussed in the next section, the range of scenarios in which takeoff <em>isn\u2019t</em> fairly rapid appears to be small, although non-negligible.</p>\n<p></p>\n<p>&nbsp;</p>\n<p>[the next section under 'from digital intelligence to explosion' is very large, and not included here]</p>\n<p>&nbsp;</p>\n<p><sup>1</sup> This third possibility is particularly interesting, in that many suspect that the slowness of cross-brain connections has been a major factor limiting the usefulness of large brains (Fox 2011).</p>", "sections": [{"title": "From digital intelligence to intelligence explosion", "anchor": "From_digital_intelligence_to_intelligence_explosion", "level": 1}, {"title": "Advantages from mere digitality", "anchor": "Advantages_from_mere_digitality", "level": 2}, {"title": "Speed", "anchor": "Speed", "level": 3}, {"title": "Copyability", "anchor": "Copyability", "level": 3}, {"title": "Goal coordination", "anchor": "Goal_coordination", "level": 3}, {"title": "Further advantages to digital intelligence", "anchor": "Further_advantages_to_digital_intelligence", "level": 2}, {"title": "Recursive \u201cself\u201d-improvement", "anchor": "Recursive__self__improvement", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T07:13:39.993Z", "modifiedAt": null, "url": null, "title": "Let's Make an Open Problems Page", "slug": "let-s-make-an-open-problems-page", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:34.767Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ron_Fern", "createdAt": "2011-11-26T03:10:37.987Z", "isAdmin": false, "displayName": "Ron_Fern"}, "userId": "mYhdqu5e7JzR5P6n8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tKYqR8zpfFWsBrC8f/let-s-make-an-open-problems-page", "pageUrlRelative": "/posts/tKYqR8zpfFWsBrC8f/let-s-make-an-open-problems-page", "linkUrl": "https://www.lesswrong.com/posts/tKYqR8zpfFWsBrC8f/let-s-make-an-open-problems-page", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Let's%20Make%20an%20Open%20Problems%20Page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALet's%20Make%20an%20Open%20Problems%20Page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKYqR8zpfFWsBrC8f%2Flet-s-make-an-open-problems-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Let's%20Make%20an%20Open%20Problems%20Page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKYqR8zpfFWsBrC8f%2Flet-s-make-an-open-problems-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtKYqR8zpfFWsBrC8f%2Flet-s-make-an-open-problems-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 339, "htmlBody": "<p>It's simple enough of a proposal. Make a page where we post open problems, work on solutions to said problems, and where we can discuss these problems prior to attempting to solve them with other LWers. I would suggest two main parts:</p>\n<ol>\n<li>Lesswrong specific/original problems, i.e., problems posed by LWers, or which are particularly relevant to LW style rationality.</li>\n<li>General problems which we believe LWers should have the tools to solve, but which have not been solved.</li>\n</ol>\n<p>&nbsp;</p>\n<p>I would propose using the same karma system we use now. Each problem could have a long list of proposed solutions, cautionary comments, and newly found lemmas relevant to the problem, and each solution, warning, or lemma, could have its own comment tree section. But it is important that we don't think of solutions as articles; they should be scientific writing. No sentence or phrase should be in a solution without adding to the literal meaning of the solution. We should judge solutions based on how well they solve the problem, not on how well written they are. We might even want to make chat rooms devoted to each problem, so that we can actively engage in conversation in stead of waiting for comment responses.&nbsp;</p>\n<p>This would have at least three major benefits:</p>\n<ol>\n<li>We would have a place for LW students to actually practice their skills on fresh problems without putting them through the less effective torture of already solved, homework-like tasks. </li>\n<li>We would have a large record of our successes and failures as rationalists, and an easy way to get a sense of how far we've come since 2011.</li>\n<li>This would provide us with a way to test the results of LW style rationality training/study by looking at who has been giving the best solutions. If we find that after a few years of being on LW, 50% of LWers give a really good solution to one of the open problems, then we can conclude that reading/using LW has had positive effects on the rationality of its readers/users. </li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tKYqR8zpfFWsBrC8f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 8.054136267203913e-07, "legacy": true, "legacyId": "11127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T07:23:57.551Z", "modifiedAt": null, "url": null, "title": "Video: Skepticon talks", "slug": "video-skepticon-talks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:48.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "komponisto", "createdAt": "2009-03-01T21:10:23.585Z", "isAdmin": false, "displayName": "komponisto"}, "userId": "h48TMtPzfimsEobTm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YQDHpozPcxWKTW7o6/video-skepticon-talks", "pageUrlRelative": "/posts/YQDHpozPcxWKTW7o6/video-skepticon-talks", "linkUrl": "https://www.lesswrong.com/posts/YQDHpozPcxWKTW7o6/video-skepticon-talks", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Video%3A%20Skepticon%20talks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVideo%3A%20Skepticon%20talks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQDHpozPcxWKTW7o6%2Fvideo-skepticon-talks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Video%3A%20Skepticon%20talks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQDHpozPcxWKTW7o6%2Fvideo-skepticon-talks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYQDHpozPcxWKTW7o6%2Fvideo-skepticon-talks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>The <a href=\"http://skepticon.org/schedule.php\">talks</a> from <a href=\"/lw/8fn/skepticon_iv_meetup_planning/\">Skepticon</a> <a href=\"/lw/8ho/meetup_skepticon_iv_meetup_saturday_night/\">IV</a> are being <a href=\"http://www.youtube.com/user/HamboneProductions#p/u\">posted to YouTube</a>.&nbsp;</p>\n<p>So far we have:</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=HHIz-gR4xHo\">Richard Carrier on Bayes</a> (my favorite)</li>\n<li><a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\">Julia Galef on the Straw Vulcan</a>&nbsp;</li>\n<li><a href=\"http://www.youtube.com/watch?v=GUI_ML1qkQE&amp;feature=related\">Greta Christina on angry atheists</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=bx2Agfhjv20&amp;feature=related\">Hermant Mehta on math education</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=DTi8dq4KAeE&amp;feature=related\">David Fitzgerald on Mormonism</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=UI-YvrHZVvk&amp;feature=related\">J.T. Eberhard on mental illness</a> (a dramatic end to the conference)</li>\n<li><a href=\"http://www.youtube.com/watch?v=bRpzwVAgOpI&amp;feature=related\">an \"atheist revival\" by Sam Singleton</a> (on the lighter side)</li>\n</ul>\n<p>ADDED:</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=Cd0yF2ucFbE\">\"Death Panel\"</a> featuring Julia Galef, Eliezer Yudkowsky, Greta Christina, and James Croft</li>\n<li><a href=\"http://www.youtube.com/watch?v=83P6epN1e9w&amp;feature=related\">Darrel Ray on secularism and sex</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=TwqYB1uzcU4\">Eliezer Yudkowsky on heuristics and biases</a>&nbsp;(really more like a crash course in the core LW sequences)</li>\n<li><a href=\"http://www.youtube.com/watch?v=RAAjcUrhido\">Joe Nickell on paranormal investigations</a>&nbsp;(I missed this at the conference; and even more regrettably, missed the chance to ask Joe Nickell <a href=\"/lw/1kh/the_correct_contrarian_cluster/1crn\">what he thinks of many-worlds</a>.)</li>\n<li><a href=\"http://www.youtube.com/watch?v=mf5ZcpgZop4\">Jen McCreight on \"skeptical genetics</a>\"&nbsp;(the other talk I missed)</li>\n<li><a href=\"http://www.youtube.com/watch?v=f6VXNTZYsyk\">Rebecca Watson on the religious right</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=wW_oNxax5RQ\">Spencer Greenberg on self-skepticism</a></li>\n<li><a href=\"http://www.youtube.com/watch?v=dup6xkvj1S0\">Dan Barker on atheist clergy</a></li>\n</ul>\n<p>More to come soon, hopefully...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YQDHpozPcxWKTW7o6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 20, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "11128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZJovXWGuKJnBkDjAZ", "t3Sqox4HczYwoN5YW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T08:05:33.067Z", "modifiedAt": null, "url": null, "title": "Facing the Intelligence Explosion discussion page", "slug": "facing-the-intelligence-explosion-discussion-page", "viewCount": null, "lastCommentedAt": "2021-11-20T10:18:46.836Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LEESyXYFuW7R3Q9G5/facing-the-intelligence-explosion-discussion-page", "pageUrlRelative": "/posts/LEESyXYFuW7R3Q9G5/facing-the-intelligence-explosion-discussion-page", "linkUrl": "https://www.lesswrong.com/posts/LEESyXYFuW7R3Q9G5/facing-the-intelligence-explosion-discussion-page", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Facing%20the%20Intelligence%20Explosion%20discussion%20page&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFacing%20the%20Intelligence%20Explosion%20discussion%20page%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEESyXYFuW7R3Q9G5%2Ffacing-the-intelligence-explosion-discussion-page%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Facing%20the%20Intelligence%20Explosion%20discussion%20page%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEESyXYFuW7R3Q9G5%2Ffacing-the-intelligence-explosion-discussion-page", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLEESyXYFuW7R3Q9G5%2Ffacing-the-intelligence-explosion-discussion-page", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>I've created a new website for my ebook <em><a href=\"http://intelligenceexplosion.com/\">Facing the Intelligence Explosion</a></em>:</p>\n<blockquote>\n<p>&nbsp;</p>\n<p>Sometime this century, machines will surpass human levels of intelligence and ability, and the human era will be over. This will be the most important event in Earth&rsquo;s history, and navigating it wisely may be the most important thing we can ever do.</p>\n<p>Luminaries from <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Turing-Intelligent-Machinery-a-heretical-theory.pdf\">Alan Turing</a> and <a href=\"http://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion\">Jack Good</a> to <a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Bill Joy</a> and <a href=\"http://www.zdnet.com/news/stephen-hawking-humans-will-fall-behind-ai/116616\">Stephen Hawking</a> have warned us about this. Why do I think they&rsquo;re right, and what can we do about it?</p>\n<p><em>Facing the Intelligence Explosion</em> is my attempt to answer those questions.</p>\n<p>&nbsp;</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>This page is the dedicated discussion page for <em>Facing the Intelligence Explosion</em>.</p>\n<p>If you'd like to comment on a particular chapter, please give the chapter name at top of your comment so that others can more easily understand your comment. For example:</p>\n<blockquote>\n<p>Re: <a href=\"http://intelligenceexplosion.com/2011/from-skepticism-to-technical-rationality/\">From Skepticism to Technical Rationality</a></p>\n<p>Here, Luke neglects to mention that...</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1be": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LEESyXYFuW7R3Q9G5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 22, "extendedScore": null, "score": 8.054320399414212e-07, "legacy": true, "legacyId": "11129", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 142, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T15:47:26.872Z", "modifiedAt": null, "url": null, "title": "Welcome to LessWrong (For highschoolers)", "slug": "welcome-to-lesswrong-for-highschoolers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:01.437Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/N9NKjw4bCXEixqmPa/welcome-to-lesswrong-for-highschoolers", "pageUrlRelative": "/posts/N9NKjw4bCXEixqmPa/welcome-to-lesswrong-for-highschoolers", "linkUrl": "https://www.lesswrong.com/posts/N9NKjw4bCXEixqmPa/welcome-to-lesswrong-for-highschoolers", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20LessWrong%20(For%20highschoolers)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20LessWrong%20(For%20highschoolers)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9NKjw4bCXEixqmPa%2Fwelcome-to-lesswrong-for-highschoolers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20LessWrong%20(For%20highschoolers)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9NKjw4bCXEixqmPa%2Fwelcome-to-lesswrong-for-highschoolers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FN9NKjw4bCXEixqmPa%2Fwelcome-to-lesswrong-for-highschoolers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 281, "htmlBody": "<p>As a continuation of the original <a href=\"/lw/2ku/welcome_to_less_wrong_2010/\">Welcome thread</a> (if you haven't gone there, go there fist) I think we need a separate introduction thread for highschoolers.&nbsp;</p>\n<p><strong>Who:</strong> As a demographic, I think that we can probably be characterized by:</p>\n<p>1. Our newness to LW.<br /> 2. Our uncertainty about which college or career to choose.<br /> 3. (if we are in a public school) Looking for ways to game the system (because we're not learning much in it).<br />4. Our potential to make a huge impact (the best advantage is an early start).<br /> 5. An lack of face to face interaction with intellectual people.&nbsp;<br /><br /><strong>Why:</strong> I can think of several things this could help highschoolers with.&nbsp;<br /><br />1. See where you stack up compared to others your age (We're probably all big fish in small ponds. At least I am. Let's get an idea of what the big pond is like).&nbsp;<br />2. Make friends with people like you.&nbsp;<br />3. Consider college and career ideas you hadn't considered before.&nbsp;<br />4. Perhaps find people to apply with for the <a href=\"http://thielfoundation.org\">Thiel Fellowship</a>.<br />5. Find a <a href=\"/lw/6j1/find_yourself_a_worthy_opponent_a_chavruta/\">chavruta</a> to go through the sequences with you.</p>\n<p><strong>What:</strong> Tell us the following:</p>\n<p>1. How old/what year are you?<br /> 2. How have you tried to enhance your education beyond what's normally offered at schools?<br /> 3. How many rationalist/philosophical people are at your school/family?&nbsp;<br />4. What careers/schools are you considering?<br />5. Are you going to apply for a Thiel Fellowship?<br />6. EDIT: link to your old \"introduce yourself\" post.&nbsp;<br /><br />If you're not in highschool, tell us what you would have told your old highschool self.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "N9NKjw4bCXEixqmPa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 33, "extendedScore": null, "score": 8.055977036578562e-07, "legacy": true, "legacyId": "11131", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 83, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hoh3ysTRDXJmcWjEH", "AZJhBqBvdMCH4oLSP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T16:45:51.965Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-4", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QhmkEfJvzjMniYH2B/meetup-austin-tx-4", "pageUrlRelative": "/posts/QhmkEfJvzjMniYH2B/meetup-austin-tx-4", "linkUrl": "https://www.lesswrong.com/posts/QhmkEfJvzjMniYH2B/meetup-austin-tx-4", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhmkEfJvzjMniYH2B%2Fmeetup-austin-tx-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhmkEfJvzjMniYH2B%2Fmeetup-austin-tx-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQhmkEfJvzjMniYH2B%2Fmeetup-austin-tx-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/58'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 November 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin meetup is back to its Caffe Medici location. We sit on the second floor to the left, near (or often on) the stage. (Sorry about how late this is; I got derailed by Thanksgiving.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/58'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QhmkEfJvzjMniYH2B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.056186383635042e-07, "legacy": true, "legacyId": "11133", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/58\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 November 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The Austin meetup is back to its Caffe Medici location. We sit on the second floor to the left, near (or often on) the stage. (Sorry about how late this is; I got derailed by Thanksgiving.)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/58\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-26T20:00:49.011Z", "modifiedAt": null, "url": null, "title": "[LINK] Scientists create mammalian H5N1", "slug": "link-scientists-create-mammalian-h5n1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oligopsony", "createdAt": "2010-08-03T16:27:12.586Z", "isAdmin": false, "displayName": "Oligopsony"}, "userId": "pyLy9zaTeZRW42iin", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fHRgeoEWcWccJ6YNP/link-scientists-create-mammalian-h5n1", "pageUrlRelative": "/posts/fHRgeoEWcWccJ6YNP/link-scientists-create-mammalian-h5n1", "linkUrl": "https://www.lesswrong.com/posts/fHRgeoEWcWccJ6YNP/link-scientists-create-mammalian-h5n1", "postedAtFormatted": "Saturday, November 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Scientists%20create%20mammalian%20H5N1&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Scientists%20create%20mammalian%20H5N1%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHRgeoEWcWccJ6YNP%2Flink-scientists-create-mammalian-h5n1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Scientists%20create%20mammalian%20H5N1%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHRgeoEWcWccJ6YNP%2Flink-scientists-create-mammalian-h5n1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfHRgeoEWcWccJ6YNP%2Flink-scientists-create-mammalian-h5n1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 423, "htmlBody": "<p>I'm woefully underinformed on this topic, but <a href=\"http://news.sciencemag.org/scienceinsider/2011/11/scientists-brace-for-media-storm.html\">this</a> doesn't seem good at all:</p>\n<blockquote>\n<p><strong>ROTTERDAM, THE NETHERLANDS</strong>&mdash;Locked up in the bowels of the medical faculty building here and accessible to only a handful of scientists lies a man-made flu virus that could change world history if it were ever set free.</p>\n<p>The virus is an H5N1 avian influenza strain that has been genetically altered and is now easily transmissible between ferrets, the animals that most closely mimic the human response to flu. Scientists believe it's likely that the pathogen, if it emerged in nature or were released, would trigger an influenza pandemic, quite possibly with many millions of deaths.</p>\n<p>In a 17th floor office in the same building, virologist Ron Fouchier of Erasmus Medical Center calmly explains why his team created what he says is \"probably one of the most dangerous viruses you can make\"&mdash;and why he wants to publish a paper describing how they did it. Fouchier is also bracing for a media storm. After he talked to<em>Science</em>Insider yesterday, he had an appointment with an institutional press officer to chart a communication strategy.</p>\n<p>Fouchier's paper is one of two studies that have triggered an intense debate about the limits of scientific freedom and that could portend changes in the way U.S. researchers handle so-called dual-use research: studies that have a potential public health benefit but could also be useful for nefarious purposes like biowarfare or bioterrorism.</p>\n<p>The other study&mdash;also on H5N1, and with comparable results&mdash;was done by a team led by virologist Yoshihiro Kawaoka at the University of Wisconsin, Madison, and the University of Tokyo, several scientists told<em>Science</em>Insider. (Kawaoka did not respond to interview requests.) Both studies have been submitted for publication, and both are currently under review by the U.S. National Science Advisory Board for Biosecurity (NSABB), which on a few previous occasions has been asked by scientists or journals to review papers that caused worries.</p>\n<p>NSABB chair Paul Keim, a microbial geneticist, says he cannot discuss specific studies but confirms that the board has \"worked very hard and very intensely for several weeks on studies about H5N1 transmissibility in mammals.\" The group plans to issue a public statement soon, says Keim, and is likely to issue additional recommendations about this type of research. \"We'll have a lot to say,\" he says</p>\n</blockquote>\n<p>I feel as though I ought provide more commentary instead of just an article dump, but I feel more strongly than that that what I have to say would be obvious or stupid or both, so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fHRgeoEWcWccJ6YNP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 17, "extendedScore": null, "score": 8.056885077849679e-07, "legacy": true, "legacyId": "11134", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T05:12:27.899Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Guardians of Ayn Rand", "slug": "seq-rerun-guardians-of-ayn-rand", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:36.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4vu7sWD6bRp3NY34v/seq-rerun-guardians-of-ayn-rand", "pageUrlRelative": "/posts/4vu7sWD6bRp3NY34v/seq-rerun-guardians-of-ayn-rand", "linkUrl": "https://www.lesswrong.com/posts/4vu7sWD6bRp3NY34v/seq-rerun-guardians-of-ayn-rand", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Guardians%20of%20Ayn%20Rand&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Guardians%20of%20Ayn%20Rand%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vu7sWD6bRp3NY34v%2Fseq-rerun-guardians-of-ayn-rand%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Guardians%20of%20Ayn%20Rand%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vu7sWD6bRp3NY34v%2Fseq-rerun-guardians-of-ayn-rand", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4vu7sWD6bRp3NY34v%2Fseq-rerun-guardians-of-ayn-rand", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/m1/guardians_of_ayn_rand/\">Guardians of Ayn Rand</a> was originally published on 18 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Ayn Rand, the leader of the Objectivists, praised reason and rationality. The group she created became a cult. Praising rationality does not provide immunity to the human trend towards cultishness.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8kz/seq_rerun_guardians_of_the_gene_pool/\">Guardians of the Gene Pool</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4vu7sWD6bRp3NY34v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.058862739233608e-07, "legacy": true, "legacyId": "11138", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["96TBXaHwLbFyeAxrg", "y2hKo33T5T8YbptGr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T06:29:42.211Z", "modifiedAt": null, "url": null, "title": "AI reflection problem ", "slug": "ai-reflection-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "rgSakPrcDAcn9X8ae", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ibstymPprmtrLbZJ4/ai-reflection-problem", "pageUrlRelative": "/posts/ibstymPprmtrLbZJ4/ai-reflection-problem", "linkUrl": "https://www.lesswrong.com/posts/ibstymPprmtrLbZJ4/ai-reflection-problem", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20reflection%20problem%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20reflection%20problem%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibstymPprmtrLbZJ4%2Fai-reflection-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20reflection%20problem%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibstymPprmtrLbZJ4%2Fai-reflection-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FibstymPprmtrLbZJ4%2Fai-reflection-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<p>I tried to write down my idea a few times, but it was badly wrong each time. Now, Instead of solving the problem, I'm just going to give a more conservative summary of what the problem is.</p>\n<p>-</p>\n<p>Eliezer's talk at the 2011 Singularity Summit focused largely on the AI reflection problem (how to build AI that can prove things about its own proofs, and execute self modifications on the basis of those proofs, without thereby reducing its self modification mojo). To that end, it would be nice to have a \"reflection principle\" by which an AI (or its theorem prover) can know in a self-referential way that its theorem proving activities are working as they should.</p>\n<p>The naive way to do this is to use the standard provability predicate, \u25fb, which can be thought of as asking whether a proof of a given formula exists. Using this we can try to formalize our intuition that a fully reflective AI, one that can reason about itself in order to improve itself, should understand that its proof deriving behavior does in fact produce sentences derivable from its axioms:</p>\n<p><span style=\"white-space:pre\"> </span>AI \u22a2 \u25fbP &rarr;&nbsp;P,</p>\n<p>which is intended to be read as \"The formal system \"AI\" understand in general that \"If a sentence is provable then it is true\" \", though literally it means something a bit more like \"It is derivable from the formal system \"AI\" that \"if there exists a proof of sentence P, then P\", in general for P\".</p>\n<p>Surprisingly, attempting to add this to a formal system, like Peano Arithmetic, doesn't work so well. In particular, it was <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem\">shown</a> by <span class=\"st\">L&ouml;b that adding this reflection principle in general lets us derive any statement, including contradictions.<br /></span></p>\n<p>So our nice reflection principle is broken. We don't understand reflective reasoning as well as we'd like. At this point, we can brainstorm some new reflection principles: Maybe our reflection principles should be derivable within our formal system, instead of being tacked on. Also, we can try to figure out in a deeper way why <span style=\"white-space: pre;\"> </span>\"AI \u22a2 \u25fbP &rarr; P\" doesn't work: If we can derive all sentences, does that mean that proofs of contradictions actually do exist? If so, why weren't those proofs breaking our theorem provers before we added the reflection principle? Or do the proofs for all sentences only exist for the augmented theorem prover, not for the initial formal system? That would suggest our reflection principle is allowing the AI to trust itself too much, allowing it to derive things because deriving them allows them to be derived. Though it doesn't really look like that if you just stare at the old reflection principle. We are confused. Let's unconfuse ourselves.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ibstymPprmtrLbZJ4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "11139", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T14:42:54.376Z", "modifiedAt": null, "url": null, "title": "Against WBE (Whole Brain Emulation)", "slug": "against-wbe-whole-brain-emulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:20.120Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rujDuQumnsK4HWMEH/against-wbe-whole-brain-emulation", "pageUrlRelative": "/posts/rujDuQumnsK4HWMEH/against-wbe-whole-brain-emulation", "linkUrl": "https://www.lesswrong.com/posts/rujDuQumnsK4HWMEH/against-wbe-whole-brain-emulation", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Against%20WBE%20(Whole%20Brain%20Emulation)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAgainst%20WBE%20(Whole%20Brain%20Emulation)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrujDuQumnsK4HWMEH%2Fagainst-wbe-whole-brain-emulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Against%20WBE%20(Whole%20Brain%20Emulation)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrujDuQumnsK4HWMEH%2Fagainst-wbe-whole-brain-emulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrujDuQumnsK4HWMEH%2Fagainst-wbe-whole-brain-emulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 529, "htmlBody": "<p><strong>problem: </strong>I've read arguments for WBE, but I can't find any against.&nbsp;<br /><br />Most people agree that WBE is the first step to FAI (EDIT: I mean to say that if we were going to try to build AGI in the safest way possible, WBE would be the first step. I did not mean to imply that I thought WBE would come before AGI). I've read a significant portion of Bostrom's WBE roadmap. My question is, are there any good arguments against the feasibility of WBE? A quick google search did not turn up anything other than&nbsp;</p>\n<p>&nbsp;<a href=\"http://www.youtube.com/watch?v=caOyiy4E5Xc\">This video</a>. Given that many people consider the scenario in which WBE comes before AGI, to be safer than the converse, shouldn't we be talking about this more? What probability do you guys assign to the&nbsp;likelihood&nbsp;that WBE comes before AGI?</p>\n<p><a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">Bostrom's WBE roadmap</a> details what technological advancement is needed to get towards WBE:</p>\n<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal.dotm</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>281</o:Words> <o:Characters>1604</o:Characters> <o:Company>none</o:Company> <o:Lines>13</o:Lines> <o:Paragraphs>3</o:Paragraphs> <o:CharactersWithSpaces>1969</o:CharactersWithSpaces> <o:Version>12.256</o:Version> </o:DocumentProperties> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:TrackMoves>false</w:TrackMoves> <w:TrackFormatting /> <w:PunctuationKerning /> <w:DrawingGridHorizontalSpacing>18 pt</w:DrawingGridHorizontalSpacing> <w:DrawingGridVerticalSpacing>18 pt</w:DrawingGridVerticalSpacing> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:DontGrowAutofit /> <w:DontAutofitConstrainedTables /> <w:DontVertAlignInTxbx /> </w:Compatibility> </w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"276\"> </w:LatentStyles> </xml><![endif]--> <!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; mso-pagination:widow-orphan; font-size:12.0pt; font-family:\"Times New Roman\"; mso-ascii-font-family:Cambria; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Cambria; mso-hansi-theme-font:minor-latin;} --> <!--[endif] --> <!--StartFragment--></p>\n<blockquote>\n<p class=\"MsoNormal\">Different required technologies have different support and drivers for development. Computers are developed independently of any emulation goal, driven by mass market forces and the need for special high performance hardware. Moore&rsquo;s law and related exponential trends appear likely to continue some distance into the future, and the feedback loops powering them are unlikely to rapidly disappear (see further discussion in Appendix B: Computer Performance Development). There is independent (and often sizeable) investment into computer games, virtual reality, physics simulation and medical simulations. Like computers, these fields produce their own revenue streams and do not require WBE\u2010specific or scientific encouragement.</p>\n<p class=\"MsoNormal\">A large number of the other technologies, such as microscopy, image processing, and computational neuroscience are driven by research and niche applications. This means less funding, more variability of the funding, and dependence on smaller groups developing them. Scanning technologies are tied to how much money there is in research (including brain emulation research) unless medical or other applications can be found. Validation techniques are not widely used in neuroscience yet, but could (and should) become standard as systems biology becomes more common and widely applied.&nbsp;&nbsp;</p>\n<p class=\"MsoNormal\">&nbsp;</p>\n<p class=\"MsoNormal\">Finally there are a few areas relatively specific to WBE: large\u2010scale neuroscience, physical handling of large amounts of tissue blocks, achieving high scanning volumes, measuring functional information from the images, automated identification of cell types, synapses, connectivity and parameters. These areas are the ones that need most support in order to enable WBE. &nbsp;The latter group is also the hardest to forecast, since it has weak drivers and a small number of researchers. The first group is easier to extrapolate by using current trends, with the assumption that they remain unbroken sufficiently far into the future.&nbsp;</p>\n</blockquote>\n<!--EndFragment-->\n<p>&nbsp;</p>\n<p><strong>Implications for those trying to accelerate the future:</strong></p>\n<p>Because much of the technological requirements are going to be driven by business-as-usual funding and standard application, anybody who wants to help bring about WBE faster (and hence FAI) should focus on either donating towards the niche applications that won't receive a lot of funding otherwise, or try to become a researcher in those areas (but what good would becoming a researcher be if there's no funding?). Also, how probable is it that once the business-as-usual technologies become more advanced, more government/corporate funding will go towards the niche applications?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rujDuQumnsK4HWMEH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": -3, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "11137", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T15:57:13.142Z", "modifiedAt": null, "url": null, "title": "[LINK] SMBC on Confirmation Bias", "slug": "link-smbc-on-confirmation-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:36.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Lapsed_Lurker", "createdAt": "2011-08-03T20:17:55.936Z", "isAdmin": false, "displayName": "Lapsed_Lurker"}, "userId": "Sm6xC9oobDyuoAMsK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pH3C2B6SAKzytBz7Z/link-smbc-on-confirmation-bias", "pageUrlRelative": "/posts/pH3C2B6SAKzytBz7Z/link-smbc-on-confirmation-bias", "linkUrl": "https://www.lesswrong.com/posts/pH3C2B6SAKzytBz7Z/link-smbc-on-confirmation-bias", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20SMBC%20on%20Confirmation%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20SMBC%20on%20Confirmation%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpH3C2B6SAKzytBz7Z%2Flink-smbc-on-confirmation-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20SMBC%20on%20Confirmation%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpH3C2B6SAKzytBz7Z%2Flink-smbc-on-confirmation-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpH3C2B6SAKzytBz7Z%2Flink-smbc-on-confirmation-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<p>The <a title=\"SMBC 2444\" href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2444#comic\" target=\"_blank\">latest SMBC</a> made me laugh a bit, so I thought I'd bring extra LessWrong attention to it.</p>\n<p>I don't know if pointing out the subject of the comic in advance will make it more or less funny. Knowing that might be more data regarding that recent <a title=\"Spoiler study news release\" href=\"http://ucsdnews.ucsd.edu/newsrel/soc/2011_08spoilers.asp\" target=\"_blank\">study</a> claiming that spoilers don't actually spoil stuff...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pH3C2B6SAKzytBz7Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 4, "extendedScore": null, "score": 8.061175274564518e-07, "legacy": true, "legacyId": "11140", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T16:50:44.087Z", "modifiedAt": null, "url": null, "title": "Created smartgiving subreddit", "slug": "created-smartgiving-subreddit", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/amqBZ6DGXqAJdytGW/created-smartgiving-subreddit", "pageUrlRelative": "/posts/amqBZ6DGXqAJdytGW/created-smartgiving-subreddit", "linkUrl": "https://www.lesswrong.com/posts/amqBZ6DGXqAJdytGW/created-smartgiving-subreddit", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Created%20smartgiving%20subreddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACreated%20smartgiving%20subreddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamqBZ6DGXqAJdytGW%2Fcreated-smartgiving-subreddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Created%20smartgiving%20subreddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamqBZ6DGXqAJdytGW%2Fcreated-smartgiving-subreddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FamqBZ6DGXqAJdytGW%2Fcreated-smartgiving-subreddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p>I just created a new subreddit:<a href=\"http://www.reddit.com/r/smartgiving/\">r/smartgiving</a></p>\n<p>The goal is to have a place to talk about optimal philanthropy, altruism, and earnings maximization.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "amqBZ6DGXqAJdytGW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 8.061367271909672e-07, "legacy": true, "legacyId": "11141", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-27T19:57:37.899Z", "modifiedAt": null, "url": null, "title": "Article idea: Good argumentation", "slug": "article-idea-good-argumentation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:37.971Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ezekiel", "createdAt": "2011-11-26T00:28:29.106Z", "isAdmin": false, "displayName": "Ezekiel"}, "userId": "YCj6TWDLvQGSQzteA", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2SvH7qTgPbPkNKRYJ/article-idea-good-argumentation", "pageUrlRelative": "/posts/2SvH7qTgPbPkNKRYJ/article-idea-good-argumentation", "linkUrl": "https://www.lesswrong.com/posts/2SvH7qTgPbPkNKRYJ/article-idea-good-argumentation", "postedAtFormatted": "Sunday, November 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Article%20idea%3A%20Good%20argumentation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AArticle%20idea%3A%20Good%20argumentation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2SvH7qTgPbPkNKRYJ%2Farticle-idea-good-argumentation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Article%20idea%3A%20Good%20argumentation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2SvH7qTgPbPkNKRYJ%2Farticle-idea-good-argumentation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2SvH7qTgPbPkNKRYJ%2Farticle-idea-good-argumentation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 528, "htmlBody": "<p>I'm <a href=\"/lw/2ku/welcome_to_less_wrong_20102011/5bkj\">newly subscribed</a>&nbsp;but looking to contribute,&nbsp;and I was wondering if anyone would be interested in an article (or series thereof) teaching, in a formal and well-defined manner, how to argue. It would cover things like:</p>\n<ul>\n<li>The general outline for a compelling argument;</li>\n<li>How to systematically construct a sound deduction from premise to conclusion;</li>\n<li>How to properly substantiate (that is, bring examples and illustrations for) a point;</li>\n<li>How to notice an unsound argument;</li>\n<li>How to define and point out the flaws once you do;</li>\n<li>Et cetera.</li>\n</ul>\n<p>There are two reasons why I think such an article might be helpful for this community: Firstly because having learned those skills you could apply them in the privacy of your own head, both to your own arguments and those of others, which in my experience gives a <em>huge</em> boost to critical thinking ability. Secondly because I think most of this community would like to actually <a href=\"/lw/cz/the_craft_and_the_community/\">make the world a saner place</a>, and those skills are really handy when trying to explain to some intelligent-but-uninitiated schlub <em>why</em>&nbsp;a given silly belief or deduction is in fact silly, or conversely why any of the tools of rationality we use here are in fact rational. It's not magic, but it helps.</p>\n<p>It might also improve the level of discourse on the site, of course, but frankly the level of actual argument here is really high in comparison to that of most real-world forums (let alone most Internet forums), and I'd guess with about .75 certainty that nothing I could teach would make it noticeably \"better\" (a higher growth rate for the function describing the probability of reaching truth as dependent on resources spent arguing). But at the moment I'm inclined to attribute that to the kind of people doing the arguing, rather than the training they have.</p>\n<p>And yes, I've noticed articles here that touch on the subject, like Yudkowsky's on&nbsp;<a href=\"/lw/kg/expecting_short_inferential_distances\">inferential distances</a>&nbsp;or&nbsp;<a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">language</a>, but I haven't seen anyone having the hubris to try and cover the entire subject in broad strokes. Hubris is a personal&nbsp;speciality&nbsp;of mine.</p>\n<p>If this would be helpful, and particularly if there's anything you feel should fall under this heading that I missed in the above list, please say so. If you don't think such an article would be helpful, also please say so. Two reasons I've considered for why that might be the case:</p>\n<ul>\n<li>It's irrelevant - most LWers know it already, or it wouldn't actually improve their critical thinking and they're not interested in playing the missionary;</li>\n<li>It would do more harm than good, in much the same way as <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">knowing about biases</a> can. This is an especially large risk since what I'm teaching would come from my years as a tournament debater, a game which while great fun spits in the face of the very concept of truth. I'm mostly relying on the objectivity and lightness I've witnessed here to mitigate that risk, but if I'm wrong by all means point it out.</li>\n</ul>\n<p>PS: No, of course it's not a good idea to finish an argument with reasons that it's likely to be wrong! Who told you to do that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2SvH7qTgPbPkNKRYJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 8.06203786153739e-07, "legacy": true, "legacyId": "11142", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YdcF6WbBmJhaaDqoD", "HLqWn5LASfhhArZ7w", "FaJaCgqBKphrDzDSj", "AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T00:17:39.940Z", "modifiedAt": null, "url": null, "title": "Meta-reading recommendations", "slug": "meta-reading-recommendations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:37.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "pwZ6qMgzoKr3JPqx4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SMfD9CHKSqhWv7WwD/meta-reading-recommendations", "pageUrlRelative": "/posts/SMfD9CHKSqhWv7WwD/meta-reading-recommendations", "linkUrl": "https://www.lesswrong.com/posts/SMfD9CHKSqhWv7WwD/meta-reading-recommendations", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta-reading%20recommendations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta-reading%20recommendations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMfD9CHKSqhWv7WwD%2Fmeta-reading-recommendations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta-reading%20recommendations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMfD9CHKSqhWv7WwD%2Fmeta-reading-recommendations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSMfD9CHKSqhWv7WwD%2Fmeta-reading-recommendations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>Despite a glut of reading recommendation engines, I still find that I rely on personal recommendations for 90% of the books that I read. Given that, I thought it might be useful to try to compile a list of prolific recommenders - people who provide a large number of reliable book recommendations.</p>\n<p>I'll start off with the two obvious ones, <a href=\"http://marginalrevolution.com/marginalrevolution/category/books\">Tyler Cowen</a> and <a href=\"http://cscs.umich.edu/~crshalizi/notabene/\">Cosma Shalizi</a>. The LW/OB group of Eliezer, Robin Hanson, and Michael Vassar also provide useful recommendations (though much less frequently).</p>\n<p>Who else can reliably recommend a good book?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SMfD9CHKSqhWv7WwD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.062971032796462e-07, "legacy": true, "legacyId": "11145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T01:11:48.673Z", "modifiedAt": null, "url": null, "title": "Log-odds (or logits)", "slug": "log-odds-or-logits", "viewCount": null, "lastCommentedAt": "2021-03-30T09:53:44.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brilee", "createdAt": "2009-11-24T14:36:56.816Z", "isAdmin": false, "displayName": "brilee"}, "userId": "bbMiGjzXWpEqRMwe6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6Ltniokkr3qt7bzWw/log-odds-or-logits", "pageUrlRelative": "/posts/6Ltniokkr3qt7bzWw/log-odds-or-logits", "linkUrl": "https://www.lesswrong.com/posts/6Ltniokkr3qt7bzWw/log-odds-or-logits", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Log-odds%20(or%20logits)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALog-odds%20(or%20logits)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ltniokkr3qt7bzWw%2Flog-odds-or-logits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Log-odds%20(or%20logits)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ltniokkr3qt7bzWw%2Flog-odds-or-logits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6Ltniokkr3qt7bzWw%2Flog-odds-or-logits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p>(I wrote this post for my own blog, and given the warm reception, I figured it would also be suitable for the LW audience. It contains some nicely formatted equations/tables in LaTeX, hence I've left it as a dropbox download.)</p>\n<p>Logarithmic probabilities have appeared previously on LW <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">here</a>, <a href=\"/lw/jn/how_much_evidence_does_it_take/\">here</a>, and sporadically in the comments. The first is a link to a Eliezer post which covers essentially the same material. I believe this is a better introduction/description/guide to logarithmic probabilities than anything else that's appeared on LW thus far.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Introduction:</p>\n<p>Our conventional way of expressing probabilities has always frustrated me. For example, it is very easy to say nonsensical statements like, &ldquo;110% chance of working&rdquo;. Or, it is not obvious that the difference between 50% and 50.01% is trivial compared to the difference between 99.98% and 99.99%. It also fails to accommodate the math correctly when we want to say things like, &ldquo;five times more likely&rdquo;, because 50% * 5 overflows 100%.<br />Jacob and I have (re)discovered a mapping from probabilities to log- odds which addresses all of these issues. To boot, it accommodates Bayes&rsquo; theorem beautifully. For something so simple and fundamental, it certainly took a great deal of google searching/wikipedia surfing to discover that they are actually called &ldquo;log-odds&rdquo;, and that they were &ldquo;discovered&rdquo; in 1944, instead of the 1600s. Also, nobody seems to use log-odds, even though they are conceptually powerful. Thus, this primer serves to explain why we need log-odds, what they are, how to use them, and when to use them.</p>\n<p>&nbsp;</p>\n<p><a href=\"http://dl.dropbox.com/u/34547557/log-probability.pdf\">Article is here (Updated 11/30 to use base 10)<br /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6Ltniokkr3qt7bzWw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 31, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "11151", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGkYCwyC7wTDyt3yT", "nj8JKFoLSMEmD3RGp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T05:31:28.528Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Litany Against Gurus", "slug": "seq-rerun-the-litany-against-gurus", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ys7Ho2HCNQCgtZq3W/seq-rerun-the-litany-against-gurus", "pageUrlRelative": "/posts/Ys7Ho2HCNQCgtZq3W/seq-rerun-the-litany-against-gurus", "linkUrl": "https://www.lesswrong.com/posts/Ys7Ho2HCNQCgtZq3W/seq-rerun-the-litany-against-gurus", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Litany%20Against%20Gurus&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Litany%20Against%20Gurus%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs7Ho2HCNQCgtZq3W%2Fseq-rerun-the-litany-against-gurus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Litany%20Against%20Gurus%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs7Ho2HCNQCgtZq3W%2Fseq-rerun-the-litany-against-gurus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYs7Ho2HCNQCgtZq3W%2Fseq-rerun-the-litany-against-gurus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/m2/the_litany_against_gurus/\">The Litany Against Gurus</a> was originally published on 18 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A piece of poetry written to describe the proper attitude to take towards a mentor, or a hero.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8le/seq_rerun_guardians_of_ayn_rand/\">Guardians of Ayn Rand</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ys7Ho2HCNQCgtZq3W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.064097440014102e-07, "legacy": true, "legacyId": "11157", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["t6Fe2PsEwb3HhcBEr", "4vu7sWD6bRp3NY34v", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T05:48:15.225Z", "modifiedAt": null, "url": null, "title": "Kickstarter fundraising for largest Tesla Coils in history", "slug": "kickstarter-fundraising-for-largest-tesla-coils-in-history", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:35.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c2NgoZcS5ZGj6Cd3w/kickstarter-fundraising-for-largest-tesla-coils-in-history", "pageUrlRelative": "/posts/c2NgoZcS5ZGj6Cd3w/kickstarter-fundraising-for-largest-tesla-coils-in-history", "linkUrl": "https://www.lesswrong.com/posts/c2NgoZcS5ZGj6Cd3w/kickstarter-fundraising-for-largest-tesla-coils-in-history", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Kickstarter%20fundraising%20for%20largest%20Tesla%20Coils%20in%20history&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AKickstarter%20fundraising%20for%20largest%20Tesla%20Coils%20in%20history%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2NgoZcS5ZGj6Cd3w%2Fkickstarter-fundraising-for-largest-tesla-coils-in-history%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Kickstarter%20fundraising%20for%20largest%20Tesla%20Coils%20in%20history%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2NgoZcS5ZGj6Cd3w%2Fkickstarter-fundraising-for-largest-tesla-coils-in-history", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc2NgoZcS5ZGj6Cd3w%2Fkickstarter-fundraising-for-largest-tesla-coils-in-history", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 82, "htmlBody": "<p><a href=\"http://www.kickstarter.com/projects/648673855/the-lightning-foundry\">\"If the government is not willing to fund the building of two 10-story tall Tesla Coils, then why the hell do I even pay taxes?\"</a></p>\n<p>This seems like by far the best investment of $300,000 out there, if your metric is revolutionary new physics discovered per dollar. I pointed the founder at Thiel's Breakout Labs, which is probably more suited to this kind of thing than Kickstarter. But there is still a very non-negligible chance that the Kickstarter Grant will come to fruition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c2NgoZcS5ZGj6Cd3w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 3, "extendedScore": null, "score": 8.064157672811846e-07, "legacy": true, "legacyId": "11158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T15:01:41.359Z", "modifiedAt": null, "url": null, "title": "Living Metaphorically", "slug": "living-metaphorically", "viewCount": null, "lastCommentedAt": "2021-02-08T21:24:50.347Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nEA5vYzYS5zqrtQTm/living-metaphorically", "pageUrlRelative": "/posts/nEA5vYzYS5zqrtQTm/living-metaphorically", "linkUrl": "https://www.lesswrong.com/posts/nEA5vYzYS5zqrtQTm/living-metaphorically", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Living%20Metaphorically&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALiving%20Metaphorically%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEA5vYzYS5zqrtQTm%2Fliving-metaphorically%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Living%20Metaphorically%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEA5vYzYS5zqrtQTm%2Fliving-metaphorically", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnEA5vYzYS5zqrtQTm%2Fliving-metaphorically", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2156, "htmlBody": "<p><a href=\"http://www.amazon.com/Metaphors-We-Live-George-Lakoff/dp/0226468011/\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/metaphors-we-live-by.jpg\" alt=\"\" /></a><small>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a></small></p>\n<p>In my <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">last post</a>, I showed that the brain does not encode concepts in terms of necessary and sufficient conditions. So, any philosophical practice which assumes this &mdash; as much of 20th century conceptual analysis seems to do &mdash; is misguided.</p>\n<p>Next, I want to show that human abstract thought is pervaded by metaphor, and that this has implications for how we think about the nature of philosophical questions and philosophical answers. As <a href=\"http://www.amazon.com/Philosophy-Flesh-Embodied-Challenge-Western/dp/0465056741/\">Lakoff &amp; Johnson (1999)</a> write:</p>\n<blockquote>\n<p>If we are going to ask philosophical questions, we have to remember that we are human... The fact that abstract thought is mostly metaphorical means that answers to philosophical questions have always been, and always will be, mostly metaphorical. In itself, that is neither good nor bad. It is simply a fact about the capacities of the human mind. But it has major consequences for every aspect of philosophy. Metaphorical thought is the principal tool that makes philosophical insight possible, and that constrains the forms that philosophy can take.</p>\n</blockquote>\n<p>To understand how fundamental metaphor is to our thinking, we must remember that human cognition is <em>embodied</em>:</p>\n<blockquote>\n<p>We have inherited from the Western philosophical tradition a theory of faculty psychology, in which we have a \"faculty\" of reason that is separate from and independent of what we do with our bodies. In particular, reason is seen as independent of perception and bodily movement...</p>\n<p>The evidence from cognitive science shows that classical faculty psychology is wrong. There is no such fully autonomous faculty of reason separate from and independent of bodily capacities such as perception and movement. The evidence supports, instead, an evolutionary view, in which reason uses and grows out of such bodily capacities.</p>\n</blockquote>\n<p>Consider, for example, the fact that as <em>neural</em> beings we <em>must categorize things</em>:</p>\n<blockquote>\n<p>We are neural beings. Our brains each have 100 billion neurons and 100 trillion synaptic connections. It is common in the brain for information to be passed from one dense ensemble of neurons to another via a relatively sparse set of connections. Whenever this happens, the pattern of activation distributed over the first set of neurons is too great to be represented in a one-to-one manner in the sparse set of connections. Therefore, the sparse set of connections necessarily groups together certain input patterns in mapping them across to the output ensemble. Whenever a neural ensemble provides the same output with different inputs, there is neural categorization.</p>\n<p>To take a concrete example, each human eye has 100 million light-sensing cells, but only about 1 million fibers leading to the brain. Each incoming image must therefore be reduced in complexity by a factor of 100. That is, information in each fiber constitutes a \"categorization\" of the information from about 100 cells.</p>\n</blockquote>\n<p>Moreover, almost all our categorizations are determined by the unconscious associative mind &mdash; outside our control and even our awareness &mdash; as we interact with the world. As Lakoff &amp; Johnson note, \"Even when we think we are deliberately forming new categories, our unconscious categories enter into our choice of possible conscious categories.\"</p>\n<p><a id=\"more\"></a></p>\n<p>And because our categories are shaped not by a transcendent, universal faculty of reason but by the components of our sensorimotor system that process our interaction with the world, our concepts and categories end up being largely sensorimotor concepts and categories.</p>\n<p>Here are some examples of metaphorical thought shaped by the sensorimotor system:</p>\n<p><strong>Important Is Big</strong> <br /> Example: \"Tomorrow is a <em>big</em> day.\" <br /> Mapping: From importance to size. <br /> Experience: As a child, finding that big things (e.g. parents) are important and can exert major forces on you and dominate your visual experience.</p>\n<p><strong>Intimacy Is Closeness</strong> <br /> Example: \"We've been <em>close</em> for years, but we're beginning to <em>drift apart</em>.\" <br /> Mapping: From intimacy to physical proximity. <br /> Experience: Being physically close to people you are intimate with.</p>\n<p><strong>Difficulties Are Burdens</strong> <br /> Example: \"She's <em>weighed down</em> by her responsibilities.\" <br /> Mapping: From difficulty to muscular exertion. <br /> Experience: The discomfort or disabling effect of lifting or carrying heavy objects.</p>\n<p><strong>More Is Up</strong> <br /> Example: \"Prices are high.\" <br /> Mapping: From quantity to vertical orientation. <br /> Experience: Observing the rise and fall of levels of piles and fluids as more is added or subtracted.</p>\n<p><strong>Categories Are Containers</strong> <br /> Example: \"Are tomatoes <em>in</em> the fruit or vegetable category?\" <br /> Mapping: From kinds to spatial location. <br /> Experience: Observing that things that go together tend to be in the same bounded region.</p>\n<p><strong>Linear Scales Are Paths</strong> <br /> Example: \"John's intelligence <em>goes way beyond</em> Bill's.\" <br /> Mapping: From degree to motion in space. <br /> Experience: Observing the amount of progress made by an object.</p>\n<p><strong>Organization Is Physical Structure</strong> <br /> Example: \"How do the pieces of this theory fit together?\" <br /> Mapping: From abstract relationships to experience with physical objects. <br /> Experience: Interacting with complex objects and attending to their structure.</p>\n<p><strong>States Are Locations</strong> <br /> Example: \"I'm <em>close</em> to being <em>in</em> a depression and the next thing that goes wrong will <em>send me over the edge</em>. <br /> Mapping: From a subjective state to being in a bounded region of space. <br /> Experience: Experiencing a certain state as correlated with a certain location (e.g. being cool under a tree, feeling secure in a bed).</p>\n<p><strong>Purposes Are Destinations</strong> <br /> Example: \"He'll ultimately be successful, but he isn't <em>there</em> yet.\" <br /> Mapping: From achieving a purpose to reaching a destination in space. <br /> Experience: Reaching destinations throughout everyday life and thereby achieving purposes (e.g. if you want food, you have to go to the fridge).</p>\n<p><strong>Actions Are Motions</strong> <br /> Example: \"I'm <em>moving</em> right along on the project.\" <br /> Mapping: From action to moving your body through space. <br /> Experience: The common action of moving yourself through space, especially in the early years of life when that is to some degree the <em>only</em> kind of action you can take.</p>\n<p><strong>Understanding Is Grasping</strong> <br /> Example: \"I've never been able to grasp transfinite numbers.\" <br /> Mapping: From comprehension to object manipulation. <br /> Experience: Getting information about an object by grasping and manipulating it.</p>\n<p>As a neural being interacting with the world, you can't help but build up such \"primary\" metaphors:</p>\n<blockquote>\n<p>If you are a normal human being, you inevitably acquire an enormous range of primary metaphors just by going about the world constantly moving and perceiving. Whenever a domain of subjective experience or judgment is coactivated regularly with a sensorimotor domain, permanent neural connections are established via synaptic weight changes. Those connections, which you have unconsciously formed by the thousands, provide inferential structure and qualitative experience activated in the sensorimotor system to the subjective domains they are associated with. Our enormous metaphoric conceptual system is thus built up by a process of neural selection. Certain neural connections between the activated source- and target-domain networks are randomly established at first and then have their synaptic weights increased through their recurrent firing. The more times those connections are activated, the more the weights are increased, until permanent connections are forged.</p>\n</blockquote>\n<p>Primary metaphors are combined to build complex metaphors. For example, Actions Are Motions and Purposes Are Destinations are often combined to form a new metaphor:</p>\n<p><strong>A Purposeful Life is a Journey</strong> <br /> Example: \"She seems lost, without direction. She's fallen off track. She needs to find her purpose and get moving again.\"</p>\n<p>Can we think <em>without</em> metaphor, then? Yes. Our concepts of so-called \"<a href=\"http://www.wisegeek.com/what-is-a-basic-level-category.htm\">basic level</a>\" objects (that we interact with in everyday experience) are often literal, as are sensorimotor concepts. Our concepts of \"tree\" (the thing that grows in dirt), \"grasp\" (holding an object), and \"in\" (in the spatial sense) are all literal. But when it comes to abstract reasoning or subjective judgment, we tend to think in metaphor. We can't help it.</p>\n<h4>Implications for philosophical method</h4>\n<p>What happens when we fail to realize that our thinking is metaphorical? Let's consider a famous example: Zeno's paradox of the arrow.</p>\n<p>Zeno described time as a sequence of points along a timeline. Now, consider an arrow in flight. At any point on the timeline, the arrow is at some particular fixed location. At a later point on the timeline, the arrow is at a different location. But since the arrow is located at a single fixed place at every point in time, then where is the motion?</p>\n<blockquote>\n<p>Suppose, Zeno argues, that time really is a sequence of points constituting a time line. Consider the flight of an arrow. At any point in time, the arrow is at some fixed location. At a later point, it is at another fixed location. The flight of the arrow would be like the sequence of still frames that make up a movie. Since the arrow is located at a single fixed place at every time, where, asks Zeno, is the motion?</p>\n</blockquote>\n<p>The puzzle arises when you take the metaphor of time as discrete <em>points</em> along the <em>space</em> of a time<em>line</em> as being literal:</p>\n<blockquote>\n<p>Zeno's brilliance was to concoct an example that forced a contradiction upon us: [a contradiction between] literal motion and motion metaphorically conceptualized as a sequence of fixed locations at fixed points in time.</p>\n</blockquote>\n<h4>Moral concepts as metaphors</h4>\n<p>For a more detailed illustration of the philosophical implications of metaphorical thought, let's examine the metaphors that ground our moral concepts:</p>\n<blockquote>\n<p>Morality is fundamentally seen as the enhancing of well-being, especially of others. For this reason, ...basic folk theories of what constitutes fundamental well-being form the grounding for systems of moral metaphors around the world. For example, since most people find it better to have enough wealth to live comfortably than to be impoverished, we are not surprised to find that well-being is conceptualized as wealth...</p>\n<p>We all conceptualize well-being as wealth. We understand an increase in well-being as a <em>gain</em> and a decrease of well-being as a <em>loss</em> or a <em>cost</em>. We speak of <em>profiting</em> from an experience, of having a <em>rich</em> life, of <em>investing</em> in happiness, and of <em>wasting</em> our lives... If you do something good for me, then I <em>owe</em> you something, I am in your <em>debt</em>. If I do something equally good for you, then I have <em>repaid</em> you and we are <em>even</em>. The books are balanced.</p>\n</blockquote>\n<p>Well-Being Is Wealth is not the only metaphor behind our moral thinking. Here are a few others:</p>\n<p><strong>Being Moral Is Being Upright; Being Immoral Is Being Low; Evil Is a Force</strong> <br /> Example: \"He's an <em>upstanding</em> citizen. She's on the <em>up and up</em>. She's as <em>upright</em> as they come. That was a <em>low</em> thing to do. He's <em>under</em>handed. I would never <em>stoop</em> to such a thing. She <em>fell</em> from grace. She succumbed to the <em>floods</em> of emotion and the <em>fires</em> of passion. She didn't have enough moral <em>backbone</em> to <em>stand up to</em> evil.\"</p>\n<p>How does the metaphorical nature of our moral concepts constrain moral philosophy? Let us contrast a traditional view of moral concepts with the view of moral concepts emerging from cognitive science:</p>\n<blockquote>\n<p>The traditional view of moral concepts and reasoning says the following: Human reasoning is compartmentalized, depending on what aspects of experience it is directed to. There are scientific judgments, technical judgments, prudential judgments, aesthetic judgments, and ethical judgments. For each type of judgment, there is a corresponding distinct type of literal concept. Therefore, there exists a unique set of concepts that pertain only to ethical issues. These ethical concepts are literal and must be understood only \"in themselves\" or by virtue of their relations to other purely ethical concepts. Moral rules and principles are made up from purely ethical concepts like these, concepts such as <em>good</em>, <em>right</em>, <em>duty</em>, <em>justice</em>, and <em>freedom</em>. We use our reason to apply these ethical concepts and rules to concrete, actual situations in order to decide how we ought to act in a given case.</p>\n<p>&hellip; [But] there is no set of pure moral concepts that could be understood \"in themselves\" or \"on their own terms.\" Instead, we understand morality via mappings of structures from other aspects and domains of our experience: wealth, balance, order, boundaries, light/dark, beauty, strength, and so on. If our moral concepts are metaphorical, then their structure and logic come primarily from the source domains that ground the metaphors. We are thus understanding morality by means of structures drawn from a broad range of dimensions of human experience, including domains that are never considered by the traditional view to be \"ethical\" domains. In other words, the constraints on our moral reasoning are mostly imported from other conceptual domains and aspects of experience...</p>\n</blockquote>\n<p>An explosion of productivity in moral psychology since Lakoff &amp; Johnson's book was published has confirmed these claims. The convergence of evidence suggests that <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Cushman-et-al-Multi-system-moral-psychology.pdf\">multiple competing systems</a> contribute to our moral reasoning, and they engage many processes <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Cushman-Young-patterns-of-moral-judgment-derive-from-nonmoral-psychological-representations.pdf\">not unique to moral reasoning</a>.</p>\n<p>Once again, knowledge of cognitive science constrains philosophy:</p>\n<blockquote>\n<p>This view of moral concepts as metaphoric profoundly calls into question the idea of a \"pure\" moral reason... [Moreover,] we do not have a monolithic, homogeneous, consistent set of moral concepts. For example, we have different, inconsistent, metaphorical structurings of our notion of well-being, and these are employed in moral reasoning.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">Intuitions Aren't Shared That Way</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">Concepts Don't Work That Way</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FtT2T9bRbECCGYxrL": 1, "GLykb6NukBeBQtDvQ": 1, "zQw5d37qwzdpgQs5P": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nEA5vYzYS5zqrtQTm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 33, "extendedScore": null, "score": 6.5e-05, "legacy": true, "legacyId": "11159", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "yFvZa9wkv5JoqhM8F", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "intuitions-aren-t-shared-that-way", "canonicalPrevPostSlug": "concepts-don-t-work-that-way", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://www.amazon.com/Metaphors-We-Live-George-Lakoff/dp/0226468011/\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/metaphors-we-live-by.jpg\" alt=\"\"></a><small>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a></small></p>\n<p>In my <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">last post</a>, I showed that the brain does not encode concepts in terms of necessary and sufficient conditions. So, any philosophical practice which assumes this \u2014 as much of 20th century conceptual analysis seems to do \u2014 is misguided.</p>\n<p>Next, I want to show that human abstract thought is pervaded by metaphor, and that this has implications for how we think about the nature of philosophical questions and philosophical answers. As <a href=\"http://www.amazon.com/Philosophy-Flesh-Embodied-Challenge-Western/dp/0465056741/\">Lakoff &amp; Johnson (1999)</a> write:</p>\n<blockquote>\n<p>If we are going to ask philosophical questions, we have to remember that we are human... The fact that abstract thought is mostly metaphorical means that answers to philosophical questions have always been, and always will be, mostly metaphorical. In itself, that is neither good nor bad. It is simply a fact about the capacities of the human mind. But it has major consequences for every aspect of philosophy. Metaphorical thought is the principal tool that makes philosophical insight possible, and that constrains the forms that philosophy can take.</p>\n</blockquote>\n<p>To understand how fundamental metaphor is to our thinking, we must remember that human cognition is <em>embodied</em>:</p>\n<blockquote>\n<p>We have inherited from the Western philosophical tradition a theory of faculty psychology, in which we have a \"faculty\" of reason that is separate from and independent of what we do with our bodies. In particular, reason is seen as independent of perception and bodily movement...</p>\n<p>The evidence from cognitive science shows that classical faculty psychology is wrong. There is no such fully autonomous faculty of reason separate from and independent of bodily capacities such as perception and movement. The evidence supports, instead, an evolutionary view, in which reason uses and grows out of such bodily capacities.</p>\n</blockquote>\n<p>Consider, for example, the fact that as <em>neural</em> beings we <em>must categorize things</em>:</p>\n<blockquote>\n<p>We are neural beings. Our brains each have 100 billion neurons and 100 trillion synaptic connections. It is common in the brain for information to be passed from one dense ensemble of neurons to another via a relatively sparse set of connections. Whenever this happens, the pattern of activation distributed over the first set of neurons is too great to be represented in a one-to-one manner in the sparse set of connections. Therefore, the sparse set of connections necessarily groups together certain input patterns in mapping them across to the output ensemble. Whenever a neural ensemble provides the same output with different inputs, there is neural categorization.</p>\n<p>To take a concrete example, each human eye has 100 million light-sensing cells, but only about 1 million fibers leading to the brain. Each incoming image must therefore be reduced in complexity by a factor of 100. That is, information in each fiber constitutes a \"categorization\" of the information from about 100 cells.</p>\n</blockquote>\n<p>Moreover, almost all our categorizations are determined by the unconscious associative mind \u2014 outside our control and even our awareness \u2014 as we interact with the world. As Lakoff &amp; Johnson note, \"Even when we think we are deliberately forming new categories, our unconscious categories enter into our choice of possible conscious categories.\"</p>\n<p><a id=\"more\"></a></p>\n<p>And because our categories are shaped not by a transcendent, universal faculty of reason but by the components of our sensorimotor system that process our interaction with the world, our concepts and categories end up being largely sensorimotor concepts and categories.</p>\n<p>Here are some examples of metaphorical thought shaped by the sensorimotor system:</p>\n<p><strong>Important Is Big</strong> <br> Example: \"Tomorrow is a <em>big</em> day.\" <br> Mapping: From importance to size. <br> Experience: As a child, finding that big things (e.g. parents) are important and can exert major forces on you and dominate your visual experience.</p>\n<p><strong>Intimacy Is Closeness</strong> <br> Example: \"We've been <em>close</em> for years, but we're beginning to <em>drift apart</em>.\" <br> Mapping: From intimacy to physical proximity. <br> Experience: Being physically close to people you are intimate with.</p>\n<p><strong>Difficulties Are Burdens</strong> <br> Example: \"She's <em>weighed down</em> by her responsibilities.\" <br> Mapping: From difficulty to muscular exertion. <br> Experience: The discomfort or disabling effect of lifting or carrying heavy objects.</p>\n<p><strong>More Is Up</strong> <br> Example: \"Prices are high.\" <br> Mapping: From quantity to vertical orientation. <br> Experience: Observing the rise and fall of levels of piles and fluids as more is added or subtracted.</p>\n<p><strong>Categories Are Containers</strong> <br> Example: \"Are tomatoes <em>in</em> the fruit or vegetable category?\" <br> Mapping: From kinds to spatial location. <br> Experience: Observing that things that go together tend to be in the same bounded region.</p>\n<p><strong>Linear Scales Are Paths</strong> <br> Example: \"John's intelligence <em>goes way beyond</em> Bill's.\" <br> Mapping: From degree to motion in space. <br> Experience: Observing the amount of progress made by an object.</p>\n<p><strong>Organization Is Physical Structure</strong> <br> Example: \"How do the pieces of this theory fit together?\" <br> Mapping: From abstract relationships to experience with physical objects. <br> Experience: Interacting with complex objects and attending to their structure.</p>\n<p><strong>States Are Locations</strong> <br> Example: \"I'm <em>close</em> to being <em>in</em> a depression and the next thing that goes wrong will <em>send me over the edge</em>. <br> Mapping: From a subjective state to being in a bounded region of space. <br> Experience: Experiencing a certain state as correlated with a certain location (e.g. being cool under a tree, feeling secure in a bed).</p>\n<p><strong>Purposes Are Destinations</strong> <br> Example: \"He'll ultimately be successful, but he isn't <em>there</em> yet.\" <br> Mapping: From achieving a purpose to reaching a destination in space. <br> Experience: Reaching destinations throughout everyday life and thereby achieving purposes (e.g. if you want food, you have to go to the fridge).</p>\n<p><strong>Actions Are Motions</strong> <br> Example: \"I'm <em>moving</em> right along on the project.\" <br> Mapping: From action to moving your body through space. <br> Experience: The common action of moving yourself through space, especially in the early years of life when that is to some degree the <em>only</em> kind of action you can take.</p>\n<p><strong>Understanding Is Grasping</strong> <br> Example: \"I've never been able to grasp transfinite numbers.\" <br> Mapping: From comprehension to object manipulation. <br> Experience: Getting information about an object by grasping and manipulating it.</p>\n<p>As a neural being interacting with the world, you can't help but build up such \"primary\" metaphors:</p>\n<blockquote>\n<p>If you are a normal human being, you inevitably acquire an enormous range of primary metaphors just by going about the world constantly moving and perceiving. Whenever a domain of subjective experience or judgment is coactivated regularly with a sensorimotor domain, permanent neural connections are established via synaptic weight changes. Those connections, which you have unconsciously formed by the thousands, provide inferential structure and qualitative experience activated in the sensorimotor system to the subjective domains they are associated with. Our enormous metaphoric conceptual system is thus built up by a process of neural selection. Certain neural connections between the activated source- and target-domain networks are randomly established at first and then have their synaptic weights increased through their recurrent firing. The more times those connections are activated, the more the weights are increased, until permanent connections are forged.</p>\n</blockquote>\n<p>Primary metaphors are combined to build complex metaphors. For example, Actions Are Motions and Purposes Are Destinations are often combined to form a new metaphor:</p>\n<p><strong>A Purposeful Life is a Journey</strong> <br> Example: \"She seems lost, without direction. She's fallen off track. She needs to find her purpose and get moving again.\"</p>\n<p>Can we think <em>without</em> metaphor, then? Yes. Our concepts of so-called \"<a href=\"http://www.wisegeek.com/what-is-a-basic-level-category.htm\">basic level</a>\" objects (that we interact with in everyday experience) are often literal, as are sensorimotor concepts. Our concepts of \"tree\" (the thing that grows in dirt), \"grasp\" (holding an object), and \"in\" (in the spatial sense) are all literal. But when it comes to abstract reasoning or subjective judgment, we tend to think in metaphor. We can't help it.</p>\n<h4 id=\"Implications_for_philosophical_method\">Implications for philosophical method</h4>\n<p>What happens when we fail to realize that our thinking is metaphorical? Let's consider a famous example: Zeno's paradox of the arrow.</p>\n<p>Zeno described time as a sequence of points along a timeline. Now, consider an arrow in flight. At any point on the timeline, the arrow is at some particular fixed location. At a later point on the timeline, the arrow is at a different location. But since the arrow is located at a single fixed place at every point in time, then where is the motion?</p>\n<blockquote>\n<p>Suppose, Zeno argues, that time really is a sequence of points constituting a time line. Consider the flight of an arrow. At any point in time, the arrow is at some fixed location. At a later point, it is at another fixed location. The flight of the arrow would be like the sequence of still frames that make up a movie. Since the arrow is located at a single fixed place at every time, where, asks Zeno, is the motion?</p>\n</blockquote>\n<p>The puzzle arises when you take the metaphor of time as discrete <em>points</em> along the <em>space</em> of a time<em>line</em> as being literal:</p>\n<blockquote>\n<p>Zeno's brilliance was to concoct an example that forced a contradiction upon us: [a contradiction between] literal motion and motion metaphorically conceptualized as a sequence of fixed locations at fixed points in time.</p>\n</blockquote>\n<h4 id=\"Moral_concepts_as_metaphors\">Moral concepts as metaphors</h4>\n<p>For a more detailed illustration of the philosophical implications of metaphorical thought, let's examine the metaphors that ground our moral concepts:</p>\n<blockquote>\n<p>Morality is fundamentally seen as the enhancing of well-being, especially of others. For this reason, ...basic folk theories of what constitutes fundamental well-being form the grounding for systems of moral metaphors around the world. For example, since most people find it better to have enough wealth to live comfortably than to be impoverished, we are not surprised to find that well-being is conceptualized as wealth...</p>\n<p>We all conceptualize well-being as wealth. We understand an increase in well-being as a <em>gain</em> and a decrease of well-being as a <em>loss</em> or a <em>cost</em>. We speak of <em>profiting</em> from an experience, of having a <em>rich</em> life, of <em>investing</em> in happiness, and of <em>wasting</em> our lives... If you do something good for me, then I <em>owe</em> you something, I am in your <em>debt</em>. If I do something equally good for you, then I have <em>repaid</em> you and we are <em>even</em>. The books are balanced.</p>\n</blockquote>\n<p>Well-Being Is Wealth is not the only metaphor behind our moral thinking. Here are a few others:</p>\n<p><strong>Being Moral Is Being Upright; Being Immoral Is Being Low; Evil Is a Force</strong> <br> Example: \"He's an <em>upstanding</em> citizen. She's on the <em>up and up</em>. She's as <em>upright</em> as they come. That was a <em>low</em> thing to do. He's <em>under</em>handed. I would never <em>stoop</em> to such a thing. She <em>fell</em> from grace. She succumbed to the <em>floods</em> of emotion and the <em>fires</em> of passion. She didn't have enough moral <em>backbone</em> to <em>stand up to</em> evil.\"</p>\n<p>How does the metaphorical nature of our moral concepts constrain moral philosophy? Let us contrast a traditional view of moral concepts with the view of moral concepts emerging from cognitive science:</p>\n<blockquote>\n<p>The traditional view of moral concepts and reasoning says the following: Human reasoning is compartmentalized, depending on what aspects of experience it is directed to. There are scientific judgments, technical judgments, prudential judgments, aesthetic judgments, and ethical judgments. For each type of judgment, there is a corresponding distinct type of literal concept. Therefore, there exists a unique set of concepts that pertain only to ethical issues. These ethical concepts are literal and must be understood only \"in themselves\" or by virtue of their relations to other purely ethical concepts. Moral rules and principles are made up from purely ethical concepts like these, concepts such as <em>good</em>, <em>right</em>, <em>duty</em>, <em>justice</em>, and <em>freedom</em>. We use our reason to apply these ethical concepts and rules to concrete, actual situations in order to decide how we ought to act in a given case.</p>\n<p>\u2026 [But] there is no set of pure moral concepts that could be understood \"in themselves\" or \"on their own terms.\" Instead, we understand morality via mappings of structures from other aspects and domains of our experience: wealth, balance, order, boundaries, light/dark, beauty, strength, and so on. If our moral concepts are metaphorical, then their structure and logic come primarily from the source domains that ground the metaphors. We are thus understanding morality by means of structures drawn from a broad range of dimensions of human experience, including domains that are never considered by the traditional view to be \"ethical\" domains. In other words, the constraints on our moral reasoning are mostly imported from other conceptual domains and aspects of experience...</p>\n</blockquote>\n<p>An explosion of productivity in moral psychology since Lakoff &amp; Johnson's book was published has confirmed these claims. The convergence of evidence suggests that <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Cushman-et-al-Multi-system-moral-psychology.pdf\">multiple competing systems</a> contribute to our moral reasoning, and they engage many processes <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Cushman-Young-patterns-of-moral-judgment-derive-from-nonmoral-psychological-representations.pdf\">not unique to moral reasoning</a>.</p>\n<p>Once again, knowledge of cognitive science constrains philosophy:</p>\n<blockquote>\n<p>This view of moral concepts as metaphoric profoundly calls into question the idea of a \"pure\" moral reason... [Moreover,] we do not have a monolithic, homogeneous, consistent set of moral concepts. For example, we have different, inconsistent, metaphorical structurings of our notion of well-being, and these are employed in moral reasoning.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">Intuitions Aren't Shared That Way</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">Concepts Don't Work That Way</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Implications for philosophical method", "anchor": "Implications_for_philosophical_method", "level": 1}, {"title": "Moral concepts as metaphors", "anchor": "Moral_concepts_as_metaphors", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "78 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wHjpCxeDeuFadG3jF", "jaN4EKrRnZdynTJjH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T15:40:15.138Z", "modifiedAt": null, "url": null, "title": "LW Philosophers versus Analytics", "slug": "lw-philosophers-versus-analytics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:07.273Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "potato", "createdAt": "2011-06-15T09:18:51.735Z", "isAdmin": false, "displayName": "Ronny"}, "userId": "kY5hs2WkacnSZd937", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CXLSsWxekNewc4Pb9/lw-philosophers-versus-analytics", "pageUrlRelative": "/posts/CXLSsWxekNewc4Pb9/lw-philosophers-versus-analytics", "linkUrl": "https://www.lesswrong.com/posts/CXLSsWxekNewc4Pb9/lw-philosophers-versus-analytics", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Philosophers%20versus%20Analytics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Philosophers%20versus%20Analytics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXLSsWxekNewc4Pb9%2Flw-philosophers-versus-analytics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Philosophers%20versus%20Analytics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXLSsWxekNewc4Pb9%2Flw-philosophers-versus-analytics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCXLSsWxekNewc4Pb9%2Flw-philosophers-versus-analytics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1378, "htmlBody": "<p>By and large, I would bet money that the devoted, experienced, and properly <em>sequenced </em>LWer, is a better philosopher than the average current philosophy majors concentrating in the analytic tradition. I say this because I have regular philosophical conversations with both populations, and notice many philosophical desiderata lacking in my conversations with my classmates, from my school and others, that I find abundantly on this website. Those desiderata, which are roughly the <a href=\"http://yudkowsky.net/rational/virtues\">twelve virtues</a>. I find that though my classmates have healthy doses of curiosity, empiricism and even scholarship, they lack in, evenness, lightness, relinquishment, precision, perfectionism and true humility.</p>\n<p>How could that be? LW has built a huge <em>positivized</em> reductionist metaphysics, and a Bayesian epistemology which can almost be read as a self improvement manual. These are unprecedented, and in some circles, outrageous truths. This is not to mention the original work that has been done in LW posts and comment trees on, meta-ethics, ethics, biases, mathematics, rationality, quantum physics, economics, self-hack, etc.&nbsp; We have here a self-updating reliably transmittable well oiled machine, the likes of which philosophy has only so rarely seen.</p>\n<p>What is even more impressive to me about LW as a philosophical movement, is that it seems to be nearly self contained when it comes to philosophy. I mean most <em>experienced</em> LWers probably really haven't read very much Kant, maybe some Wittgenstein or Quine; but LWers can still somehow solve the problems philosophers spend their lives solving by building disconnected and competing philosophical systems specifically designed for each task, by the use of roughly one rather generally successful epistemology and metaphysics, which can be called together LWism.</p>\n<p>So if you agree that LW does better philosophy than analytic philosophers, let's put our money where our mouths are, as our own philosophy suggests we should. I will post a series of discussion posts each concentrating only on one currentish question from academic philosophy. In each post, I will cover the essentials of the problem, as well as provide external resources on the problem. Each post will also include a list of posts from the sequences which are recommended before participation. Each question will be solved with a consensus of less than 2 to 1 odds amongst professional philosophers, i.e., if more than 2/3s of professional philosophers agree, we won't bother. So as to not waste our time with small fish.</p>\n<p>You guys, will then in turn cooperate in comment trees to find solutions and decide amongst them, then I'll compare the LW solutions to the solutions given by a random sampling of vaguely successful analytic philosophers, (I will use a university search for my sampling). I will compare the ratio of types of solutions of the two populations, and look for solutions that happen in the one population that don't occur in the other, then I'll post the results, hopefully the next week. (edit): This process of comparison will be the hardest part of this project for me, and if anyone with training or experience in statistics might want to help me with this, please let me know, and we can work on the comparison and the report thereof together. My prediction is that we will be able to quickly reach a high consensus on many issues that analytics have not internally resolved.</p>\n<p>The series will be called: the \"Enthusiastic Youngsters Formally Tackle Analytic Problems Test\" or \"the Eyftapt series\" [pronounced: <em>afe-taped</em>]. Alternatively Eyftapt could stand for the \"Eliezer Yudkowsky and Friends Train Amazing Philosophers Test.\" Besides shedding moderate light on our philosophical-competence/toolbox juxtaposed to analytic philosophical-competence/toolbox, I'd also like to learn what LW training offers that analytics are currently missing. So that we can focus in on that kind of training for our own benefit, and so that we can offer some advice to the analytics. That is, assuming my prediction that we'll do better is correct. This will not be as easy as comparing solutions, and I may need much more data than what I'll get out of this series, but it couldn't hurt to have a bunch of LWers doing difficult philosophy added to the available data.</p>\n<p>What do you guys and gals think, might you be interested in something like this? Mind you it would be in discussion posts, since the main point is to discuss an issue.</p>\n<p>(I know some of you cats don't like \"philosophy\", just call it \"arguing about systems and elucidating messy language and thought in order to answer questions\" instead. That is what I think we do better.)</p>\n<p>BTW, if you have some problem you think we should work on, or or if you think we would be really good at solving some problem or really bad at it compared to non-LW philosophy, message me or comment below, and I'll give you credit for the suggestion. These are the topics I am already decided on: universals/nominalism, correspondence/deflation/coherency, grue/induction, science realism/constructivism, what is math?, scientific underdetermination, a priori knowledge?, radical translation, analytic synthetic division, proper name/description, deduction induction division, modality and possible worlds, what does it mean for a grammatical sentence to be <em>meaningless</em> and how do you tell?, meta-philosophy, i.e., questions about philosophy, and finally, personal identity, roughly to be posted in that order.</p>\n<p>&nbsp;</p>\n<hr />\n<p>(edited after first posting, I just realized it may be worth mention that):</p>\n<p>I was not happy about coming to this view. I have always thought of myself as an aspiring analytic philosopher, and even got attached to the ascetics of analytic philosophy. I thought of analytic philosophy as the new science of philosophy that finally got it right. It bothered me to no end that I had been lead to have more faith in the philosophical maturity/competence of a bunch of amateurs on a blog, than in the experts and students of the field that I planned to spend the rest of my life on. I have committed myself to the methods of academic-analytic philosophy publicly in speeches and to my closest friends, colleagues, and family; to turn around in under a year and say that that was all naive enthusiasm, and that there's this blog of college kids that do it better, made me look very stupid in more than one eye, I cared and care about. More than once, I have dissolved a question in my philosophy and cog-sci classes into an obvious cognitive error, explained why we are built to make this error, and left the class with little to do. Professors have praised me for this, and had even started approaching me outside of class to ask me about where I got my analysis from; their faces often came to a sincere awe when I tell them: \"I made it up myself, but all the methods I used are neatly organized, generalized, and exemplified in this text called the 'sequences' on this blog of youngsters called 'Less Wrong'. It's only a few hundred pages, kinda reads like G.E.B.\"</p>\n<p>One day, a few months back, one of my professors who I am on a particularly friendly basis with asked me: \"Every time we are in class and there is a question, you use this blog of yours, and it seems it gives you an answer for everything, so why are you still studying the analytics, instead of just studying your blog?\" I think he meant to ask this question sardonically, but that is not how I took it. I took it as a serious question about how to optimize my time if my goal is to do good philosophy. Not having a good answer to this question, and craving one, probably more than anything, is what prompted me to think of doing this series.</p>\n<p>I may be wrong, and it may be that LW has just as hard of a time forming consensus on the issues that analytics have a hard time with, though I doubt it. But I am much more confident, that for some reason, even though I have had very good training, have a very high GPA, have read every classic philosophy text I could get my hands on, and had been reading several modern philosophy journals, all before I even knew about LW, LW has done more for my philosophical maturity, competence, and persuasiveness, than the entirety of the rest of my training, and I wouldn't doubt that many others have had similar thoughts.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CXLSsWxekNewc4Pb9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 50, "extendedScore": null, "score": 0.0005743363696095945, "legacy": true, "legacyId": "11160", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 85, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T16:03:17.389Z", "modifiedAt": null, "url": null, "title": "Statisticsish Question", "slug": "statisticsish-question", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:00.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "damang", "createdAt": "2011-07-31T01:21:03.995Z", "isAdmin": false, "displayName": "damang"}, "userId": "uyJ7fMWvMR8kkNtSL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZhaGKPRH2d3BygToS/statisticsish-question", "pageUrlRelative": "/posts/ZhaGKPRH2d3BygToS/statisticsish-question", "linkUrl": "https://www.lesswrong.com/posts/ZhaGKPRH2d3BygToS/statisticsish-question", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Statisticsish%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStatisticsish%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhaGKPRH2d3BygToS%2Fstatisticsish-question%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Statisticsish%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhaGKPRH2d3BygToS%2Fstatisticsish-question", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZhaGKPRH2d3BygToS%2Fstatisticsish-question", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>This is a question really, not a post, I just can't find the answer formally. Does laplace's rule of succession work when you are taking from a finite population without replacement? If I know that some papers in a hat have \"yes\" on them, and I know that the rest don't, and that there is a finite amount of papers, and every time I take a paper out I burn it, but I have no clue how many papers are in the hat, should I still use laplace's rule to figure out how much to expect the next paper to have a \"yes\" on it? or is there some adjustment you make, since every time I see a yes paper the odds of yes papers:~yes papers in the hat goes down.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZhaGKPRH2d3BygToS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.066366158004099e-07, "legacy": true, "legacyId": "11161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T20:32:19.231Z", "modifiedAt": null, "url": null, "title": "Large scale problems, cognitive biases, and existential risk", "slug": "large-scale-problems-cognitive-biases-and-existential-risk", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.000Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JoshuaZ", "createdAt": "2010-04-05T04:07:01.214Z", "isAdmin": false, "displayName": "JoshuaZ"}, "userId": "fmTiLqp6mmXeLjwfN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TpvvxdTzM2kH54cC9/large-scale-problems-cognitive-biases-and-existential-risk", "pageUrlRelative": "/posts/TpvvxdTzM2kH54cC9/large-scale-problems-cognitive-biases-and-existential-risk", "linkUrl": "https://www.lesswrong.com/posts/TpvvxdTzM2kH54cC9/large-scale-problems-cognitive-biases-and-existential-risk", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Large%20scale%20problems%2C%20cognitive%20biases%2C%20and%20existential%20risk&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALarge%20scale%20problems%2C%20cognitive%20biases%2C%20and%20existential%20risk%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpvvxdTzM2kH54cC9%2Flarge-scale-problems-cognitive-biases-and-existential-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Large%20scale%20problems%2C%20cognitive%20biases%2C%20and%20existential%20risk%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpvvxdTzM2kH54cC9%2Flarge-scale-problems-cognitive-biases-and-existential-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTpvvxdTzM2kH54cC9%2Flarge-scale-problems-cognitive-biases-and-existential-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p>There's <a href=\"http://www.apa.org/pubs/journals/releases/psp-ofp-shepherd.pdf\"> a recent paper</a>(PDF) which finds that people who don't know much about a problem are more inclined to not find out more about that problem. Moreover, the larger scale and more complex a problem looked like, the more likely people were to try to avoid learning more about it, and the more likely they were to trust that pre-existing institutions such as the government could handle the problem. This looks like a potentially interesting form of cognitive bias. It may also explain why people are so unwilling to look at existential risk. There's essentially no issue that occurs on a larger scale than existential risk. This suggests that in trying to get people to understand existential risk, it may make sense to first address the easier to understand existential risks like large asteroids.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TpvvxdTzM2kH54cC9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "11162", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T21:33:36.850Z", "modifiedAt": null, "url": null, "title": "Probability puzzle", "slug": "probability-puzzle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.294Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malthrin", "createdAt": "2011-03-22T15:23:59.536Z", "isAdmin": false, "displayName": "malthrin"}, "userId": "5b5DcLkcYGD9YGRfF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3yEF9jBd5sb4FAz8W/probability-puzzle", "pageUrlRelative": "/posts/3yEF9jBd5sb4FAz8W/probability-puzzle", "linkUrl": "https://www.lesswrong.com/posts/3yEF9jBd5sb4FAz8W/probability-puzzle", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20puzzle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20puzzle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3yEF9jBd5sb4FAz8W%2Fprobability-puzzle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20puzzle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3yEF9jBd5sb4FAz8W%2Fprobability-puzzle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3yEF9jBd5sb4FAz8W%2Fprobability-puzzle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p>I came up with this puzzle after reading Vaniver's excellent post on the <a href=\"/lw/85x/value_of_information_four_examples/\">Value of Information</a>. I enjoyed working it out over Thanksgiving and thought I'd share it with the rest of you.</p>\n<p>&nbsp;</p>\n<p style=\"padding-left: 30px;\">Your friend holds up a curiously warped coin. \"Let's play a game,\" he says. \"I've tampered with this quarter. It could come up all heads, all tails, or any value in between. I want you to predict a coin flip; if you get it right, I'll pay you $1, and if you're wrong, you pay me $3.\"</p>\n<p style=\"padding-left: 30px;\">\"Absolutely, on one condition,\" you reply. \"We repeat this bet until I decide to stop or we finish N games.\"</p>\n<p style=\"padding-left: 30px;\">What is the minimum value of N that lets you come out ahead on average?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p>Each game, you may choose heads or tails, or to end the sequence of bets with that coin. Assume that all heads:tails ratios are equally likely for the coin.</p>\n<p>&nbsp;</p>\n<p>edit: since a couple people have gotten it, I'll link my solution:&nbsp;<a href=\"http://pastebin.com/XsEizNFL\">http://pastebin.com/XsEizNFL</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3yEF9jBd5sb4FAz8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 8.067552735818921e-07, "legacy": true, "legacyId": "11163", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-28T23:29:16.303Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-13", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oapiHxoCjc4XXLLch/meetup-austin-tx-13", "pageUrlRelative": "/posts/oapiHxoCjc4XXLLch/meetup-austin-tx-13", "linkUrl": "https://www.lesswrong.com/posts/oapiHxoCjc4XXLLch/meetup-austin-tx-13", "postedAtFormatted": "Monday, November 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoapiHxoCjc4XXLLch%2Fmeetup-austin-tx-13%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoapiHxoCjc4XXLLch%2Fmeetup-austin-tx-13", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoapiHxoCjc4XXLLch%2Fmeetup-austin-tx-13", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/59'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 December 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, the Austin LW meetup is meeting at Caffe Medici. We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>I, unfortunately, will be in Maryland for the weekend, but I hope you have a good time without me!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/59'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oapiHxoCjc4XXLLch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.067968269226897e-07, "legacy": true, "legacyId": "11164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/59\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 December 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, the Austin LW meetup is meeting at Caffe Medici. We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>I, unfortunately, will be in Maryland for the weekend, but I hope you have a good time without me!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/59\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T01:22:07.828Z", "modifiedAt": null, "url": null, "title": "[POLL] LessWrong group on YourMorals.org", "slug": "poll-lesswrong-group-on-yourmorals-org", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:35.511Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/voZGmjLfzMMQqH8k8/poll-lesswrong-group-on-yourmorals-org", "pageUrlRelative": "/posts/voZGmjLfzMMQqH8k8/poll-lesswrong-group-on-yourmorals-org", "linkUrl": "https://www.lesswrong.com/posts/voZGmjLfzMMQqH8k8/poll-lesswrong-group-on-yourmorals-org", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20LessWrong%20group%20on%20YourMorals.org&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20LessWrong%20group%20on%20YourMorals.org%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvoZGmjLfzMMQqH8k8%2Fpoll-lesswrong-group-on-yourmorals-org%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20LessWrong%20group%20on%20YourMorals.org%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvoZGmjLfzMMQqH8k8%2Fpoll-lesswrong-group-on-yourmorals-org", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvoZGmjLfzMMQqH8k8%2Fpoll-lesswrong-group-on-yourmorals-org", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 31, "htmlBody": "<p>Here's the news article on this: <a href=\"http://www.yourmorals.org/blog/2011/11/how-to-use-groups-at-yourmorals-org/\">http://www.yourmorals.org/blog/2011/11/how-to-use-groups-at-yourmorals-org/</a></p>\n<p>And here's the group that the LW community just created:&nbsp;<a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; background-color: #f7f7f8;\" rel=\"nofollow\" href=\"http://www.yourmorals.org/setgraphgroup.php?grp=623d5410f705f6a1f92c83565a3cfffc\">http://www.yourmorals.org/setgraphgroup.php?grp=623d5410f705f6a1f92c83565a3cfffc</a></p>\n<p>I think it will be very interesting to see what we can all get on this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "voZGmjLfzMMQqH8k8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 44, "extendedScore": null, "score": 8.068373782676133e-07, "legacy": true, "legacyId": "11144", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T01:23:26.980Z", "modifiedAt": null, "url": null, "title": "How rationality can make your life more awesome", "slug": "how-rationality-can-make-your-life-more-awesome", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:45.818Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Julia_Galef", "createdAt": "2009-12-20T01:44:38.850Z", "isAdmin": false, "displayName": "Julia_Galef"}, "userId": "qkDSxJnyKhPCJyKdD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G6zs6Gnouji6itsxb/how-rationality-can-make-your-life-more-awesome", "pageUrlRelative": "/posts/G6zs6Gnouji6itsxb/how-rationality-can-make-your-life-more-awesome", "linkUrl": "https://www.lesswrong.com/posts/G6zs6Gnouji6itsxb/how-rationality-can-make-your-life-more-awesome", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20rationality%20can%20make%20your%20life%20more%20awesome&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20rationality%20can%20make%20your%20life%20more%20awesome%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6zs6Gnouji6itsxb%2Fhow-rationality-can-make-your-life-more-awesome%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20rationality%20can%20make%20your%20life%20more%20awesome%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6zs6Gnouji6itsxb%2Fhow-rationality-can-make-your-life-more-awesome", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6zs6Gnouji6itsxb%2Fhow-rationality-can-make-your-life-more-awesome", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1428, "htmlBody": "<p><em>I'm currently working with Lukeprog on a crash course in rationality. It's essentially a streamlined version of the Sequences, but one area we want to beef up is the answer to the question, \"Why learn about rationality?\" </em></p>\n<p><em>I've gone through all of the previous threads I can find on this topic -- <a href=\"/lw/50p/reflections_on_rationality_a_year_out/\">Reflections on rationality a year out</a>, <a href=\"/lw/5ny/personal_benefits_from_rationality/\">Personal benefits from rationality</a>, <a href=\"/lw/52n/q_what_has_rationality_done_for_you/\">What has rationality done for you?,</a> and <a href=\"/lw/6t/the_benefits_of_rationality/\">The benefits of rationality</a> -- but most of the examples people give of rationality helping them are a little too general. People cite things like \"I hold off on proposing solutions,\" or \"I ask myself if there's a better way to be doing this.\" </em></p>\n<p><em>To someone who's not already sold on this whole rationality thing, general statements like that won't mean very much. What I think we really need is a list of concrete examples of how the tools of epistemic rationality, as they're taught in the Sequences, can improve your health, your career, your love life, the causes you care about, your psychological well-being, and so on.</em></p>\n<p><em>Below, my first attempt at doing just that. (I explain what rationality is, and how to practice it, elsewhere in the guide -- this section is just about benefits.) I'd appreciate feedback: Is it clear? Can you think of any other good examples in this vein? Would it be convincing to someone who isn't intrinsically interested in epistemic rationality for its own sake? <br /></em></p>\n<p>&nbsp;</p>\n<p><em>... <br /></em></p>\n<p>For some people, rationality is an end in itself &ndash; they value having true beliefs. But rationality&rsquo;s also a powerful tool for achieving pretty much anything else you care about. Below, a survey of some of the ways that rationality can make your life more awesome:<br /><br /><strong>Rationality alerts you when you have a false belief that&rsquo;s making you worse off.</strong></p>\n<p>You&rsquo;ve undoubtedly got beliefs about yourself &ndash; about what kind of job would be fulfilling for you, for example, or about what kind of person would be a good match for you. You&rsquo;ve also got beliefs about the world &ndash; say, about what it&rsquo;s like to be rich, or about &ldquo;what men want&rdquo; or &ldquo;what women want.&rdquo; And you&rsquo;ve probably internalized some fundamental maxims, such as: <em>When it&rsquo;s true love, you&rsquo;ll know. You should always follow your dreams. Natural things are better. Promiscuity reduces your worth as a person.</em></p>\n<p>Those beliefs shape your decisions about your career, what to do when you&rsquo;re sick, what kind of people you decide to pursue romantically and how you pursue them, how much effort you should be putting into making yourself richer, or more attractive, or more skilled (and skilled in what?), more accommodating, more aggressive, and so on.</p>\n<p>But where did these beliefs come from? The startling truth is that many of our beliefs became lodged in our psyches rather haphazardly. We&rsquo;ve read them, or heard them, or picked them up from books or TV or movies, or perhaps we generalized from one or two real-life examples.</p>\n<p>Rationality trains you to notice your beliefs, many of which you may not even be consciously aware of, and ask yourself: where did those beliefs come from, and do I have good reason to believe they&rsquo;re accurate? How would I know if&nbsp; they&rsquo;re false? Have I considered any other, alternative hypotheses?</p>\n<p><strong>Rationality helps you get the information you need.</strong><br /><br />Sometimes you need to figure out the answer to a question in order to make an important decision about, say, your health, or your career, or the causes that matter to you. Studying rationality reveals that some ways of investigating those questions are much more likely to yield the truth than others. Just a few examples:</p>\n<p><em>&ldquo;How should I run my business?&rdquo;</em> If you&rsquo;re looking to launch or manage a company, you&rsquo;ll have a huge leg up over your competition if you&rsquo;re able to rationally determine how well your product works, or whether it meets a need, or what marketing strategies are effective.</p>\n<p><em>&ldquo;What career should I go into?&rdquo; </em>Before committing yourself to a career path, you&rsquo;ll probably want to learn about the experiences of people working in that field. But a rationalist also knows to ask herself, &ldquo;Is my sample biased?&rdquo; If you&rsquo;re focused on a few famous success stories from the field, that doesn&rsquo;t tell you very much about what a typical job is like, or what your odds are of making it in that field.</p>\n<p>It&rsquo;s also an unfortunate truth that not every field uses reliable methods, and so not every field produces true or useful work. If that matters to you, you&rsquo;ll need the tools of rationality to evaluate the fields you&rsquo;re considering working in. Fields whose methods are controversial include psychotherapy, nutrition science, economics, sociology, management consulting, string theory, and alternative medicine.</p>\n<p><em>&ldquo;How can I help the world?&rdquo;</em> Many people invest huge amounts of money, time, and effort in causes they care about. But if you want to ensure that your investment makes a difference, you need to be able to evaluate the relevant evidence. How serious of a problem is, say, climate change, or animal welfare, or globalization? How effective is lobbying, or marching, or boycotting? How far do your contributions go at charity X versus charity Y?</p>\n<p><strong>Rationality teaches you how to evaluate advice.</strong></p>\n<p>Learning about rationality, and how widespread irrationality is, sparks an important realization: You can&rsquo;t assume other people have good reasons for the things they believe. And that means you need to know how to evaluate other people&rsquo;s opinions, not just based on how plausible their opinions seem, but based on the reliability of the methods they used to form those opinions.</p>\n<p>So when you get business advice, you need to ask yourself: What evidence does she have for that advice, and are her circumstances relevant enough to mine? The same is true when a friend swears by some particular remedy for acne, or migraines, or cancer. Is he repeating a recommendation made by multiple doctors? Or did he try it once and get better? What kind of evidence is reliable?</p>\n<p>In many cases, people can&rsquo;t articulate exactly how they&rsquo;ve arrived at a particular belief; it&rsquo;s just the product of various experiences they&rsquo;ve had and things they&rsquo;ve heard or read. But once you&rsquo;ve studied rationality, you&rsquo;ll recognize the signs of people who are more likely to have accurate beliefs: People who adjust their level of confidence to the evidence for a claim; people who actually change their minds when presented with new evidence; people who seem interested in getting the right answer rather than in defending their own egos.</p>\n<p><strong>Rationality saves you from bad decisions.&nbsp; </strong></p>\n<p>Knowing about the heuristics your brain uses and how they can go wrong means you can escape some very common, and often very serious, decision-making traps.</p>\n<p>For example, people often stick with their original career path or business plan for years after the evidence has made clear that it was a mistake, because they don&rsquo;t want their previous investment to be wasted. That&rsquo;s thanks to the <em>sunk cost fallacy</em>. Relatedly, people often allow <em>cognitive dissonance</em> to convince them that things aren&rsquo;t so bad, because the prospect of changing course is too upsetting.</p>\n<p>And in many major life decisions, such as choosing a career, people envision one way things could play out (&ldquo;I&rsquo;m going to run my own lab, and live in a big city&hellip;&rdquo;) &ndash; but they don&rsquo;t spend much time thinking about how probable that outcome is, or what the other probable outcomes are. The <em>narrative fallacy</em> is that situations imagined in high detail seem more plausible, regardless of how probable they actually are.&nbsp;&nbsp;&nbsp;</p>\n<p><strong>Rationality trains you to step back from your emotions so that they don&rsquo;t cloud your judgment.</strong><br /><br />Depression, anxiety, rage, envy, and other unpleasant and self-destructive emotions tend to be fueled by what cognitive therapy calls &ldquo;cognitive distortions,&rdquo; irrationalities in your thinking such as jumping to conclusions based on limited evidence; focusing selectively on negatives; all-or-nothing thinking; and blaming yourself, or someone else, without reason.</p>\n<p>Rationality breaks your habit of automatically trusting your instinctive, emotional judgments, encouraging you instead to notice the beliefs underlying your emotions and ask yourself whether those beliefs are justified.</p>\n<p>It also trains you to notice when your beliefs about the world are being colored by what you want, or don&rsquo;t want, to be true. Beliefs about your own abilities, about the motives of other people, about the likely consequences of your behavior, about what happens after you die, can be emotionally fraught. But a solid background in rationality keeps you from flinching away from the truth &ndash; about your situation, or yourself -- when learning the truth can help you change it.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G6zs6Gnouji6itsxb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 37, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "11170", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>I'm currently working with Lukeprog on a crash course in rationality. It's essentially a streamlined version of the Sequences, but one area we want to beef up is the answer to the question, \"Why learn about rationality?\" </em></p>\n<p><em>I've gone through all of the previous threads I can find on this topic -- <a href=\"/lw/50p/reflections_on_rationality_a_year_out/\">Reflections on rationality a year out</a>, <a href=\"/lw/5ny/personal_benefits_from_rationality/\">Personal benefits from rationality</a>, <a href=\"/lw/52n/q_what_has_rationality_done_for_you/\">What has rationality done for you?,</a> and <a href=\"/lw/6t/the_benefits_of_rationality/\">The benefits of rationality</a> -- but most of the examples people give of rationality helping them are a little too general. People cite things like \"I hold off on proposing solutions,\" or \"I ask myself if there's a better way to be doing this.\" </em></p>\n<p><em>To someone who's not already sold on this whole rationality thing, general statements like that won't mean very much. What I think we really need is a list of concrete examples of how the tools of epistemic rationality, as they're taught in the Sequences, can improve your health, your career, your love life, the causes you care about, your psychological well-being, and so on.</em></p>\n<p><em>Below, my first attempt at doing just that. (I explain what rationality is, and how to practice it, elsewhere in the guide -- this section is just about benefits.) I'd appreciate feedback: Is it clear? Can you think of any other good examples in this vein? Would it be convincing to someone who isn't intrinsically interested in epistemic rationality for its own sake? <br></em></p>\n<p>&nbsp;</p>\n<p><em>... <br></em></p>\n<p>For some people, rationality is an end in itself \u2013 they value having true beliefs. But rationality\u2019s also a powerful tool for achieving pretty much anything else you care about. Below, a survey of some of the ways that rationality can make your life more awesome:<br><br><strong>Rationality alerts you when you have a false belief that\u2019s making you worse off.</strong></p>\n<p>You\u2019ve undoubtedly got beliefs about yourself \u2013 about what kind of job would be fulfilling for you, for example, or about what kind of person would be a good match for you. You\u2019ve also got beliefs about the world \u2013 say, about what it\u2019s like to be rich, or about \u201cwhat men want\u201d or \u201cwhat women want.\u201d And you\u2019ve probably internalized some fundamental maxims, such as: <em>When it\u2019s true love, you\u2019ll know. You should always follow your dreams. Natural things are better. Promiscuity reduces your worth as a person.</em></p>\n<p>Those beliefs shape your decisions about your career, what to do when you\u2019re sick, what kind of people you decide to pursue romantically and how you pursue them, how much effort you should be putting into making yourself richer, or more attractive, or more skilled (and skilled in what?), more accommodating, more aggressive, and so on.</p>\n<p>But where did these beliefs come from? The startling truth is that many of our beliefs became lodged in our psyches rather haphazardly. We\u2019ve read them, or heard them, or picked them up from books or TV or movies, or perhaps we generalized from one or two real-life examples.</p>\n<p>Rationality trains you to notice your beliefs, many of which you may not even be consciously aware of, and ask yourself: where did those beliefs come from, and do I have good reason to believe they\u2019re accurate? How would I know if&nbsp; they\u2019re false? Have I considered any other, alternative hypotheses?</p>\n<p><strong>Rationality helps you get the information you need.</strong><br><br>Sometimes you need to figure out the answer to a question in order to make an important decision about, say, your health, or your career, or the causes that matter to you. Studying rationality reveals that some ways of investigating those questions are much more likely to yield the truth than others. Just a few examples:</p>\n<p><em>\u201cHow should I run my business?\u201d</em> If you\u2019re looking to launch or manage a company, you\u2019ll have a huge leg up over your competition if you\u2019re able to rationally determine how well your product works, or whether it meets a need, or what marketing strategies are effective.</p>\n<p><em>\u201cWhat career should I go into?\u201d </em>Before committing yourself to a career path, you\u2019ll probably want to learn about the experiences of people working in that field. But a rationalist also knows to ask herself, \u201cIs my sample biased?\u201d If you\u2019re focused on a few famous success stories from the field, that doesn\u2019t tell you very much about what a typical job is like, or what your odds are of making it in that field.</p>\n<p>It\u2019s also an unfortunate truth that not every field uses reliable methods, and so not every field produces true or useful work. If that matters to you, you\u2019ll need the tools of rationality to evaluate the fields you\u2019re considering working in. Fields whose methods are controversial include psychotherapy, nutrition science, economics, sociology, management consulting, string theory, and alternative medicine.</p>\n<p><em>\u201cHow can I help the world?\u201d</em> Many people invest huge amounts of money, time, and effort in causes they care about. But if you want to ensure that your investment makes a difference, you need to be able to evaluate the relevant evidence. How serious of a problem is, say, climate change, or animal welfare, or globalization? How effective is lobbying, or marching, or boycotting? How far do your contributions go at charity X versus charity Y?</p>\n<p><strong id=\"Rationality_teaches_you_how_to_evaluate_advice_\">Rationality teaches you how to evaluate advice.</strong></p>\n<p>Learning about rationality, and how widespread irrationality is, sparks an important realization: You can\u2019t assume other people have good reasons for the things they believe. And that means you need to know how to evaluate other people\u2019s opinions, not just based on how plausible their opinions seem, but based on the reliability of the methods they used to form those opinions.</p>\n<p>So when you get business advice, you need to ask yourself: What evidence does she have for that advice, and are her circumstances relevant enough to mine? The same is true when a friend swears by some particular remedy for acne, or migraines, or cancer. Is he repeating a recommendation made by multiple doctors? Or did he try it once and get better? What kind of evidence is reliable?</p>\n<p>In many cases, people can\u2019t articulate exactly how they\u2019ve arrived at a particular belief; it\u2019s just the product of various experiences they\u2019ve had and things they\u2019ve heard or read. But once you\u2019ve studied rationality, you\u2019ll recognize the signs of people who are more likely to have accurate beliefs: People who adjust their level of confidence to the evidence for a claim; people who actually change their minds when presented with new evidence; people who seem interested in getting the right answer rather than in defending their own egos.</p>\n<p><strong id=\"Rationality_saves_you_from_bad_decisions___\">Rationality saves you from bad decisions.&nbsp; </strong></p>\n<p>Knowing about the heuristics your brain uses and how they can go wrong means you can escape some very common, and often very serious, decision-making traps.</p>\n<p>For example, people often stick with their original career path or business plan for years after the evidence has made clear that it was a mistake, because they don\u2019t want their previous investment to be wasted. That\u2019s thanks to the <em>sunk cost fallacy</em>. Relatedly, people often allow <em>cognitive dissonance</em> to convince them that things aren\u2019t so bad, because the prospect of changing course is too upsetting.</p>\n<p>And in many major life decisions, such as choosing a career, people envision one way things could play out (\u201cI\u2019m going to run my own lab, and live in a big city\u2026\u201d) \u2013 but they don\u2019t spend much time thinking about how probable that outcome is, or what the other probable outcomes are. The <em>narrative fallacy</em> is that situations imagined in high detail seem more plausible, regardless of how probable they actually are.&nbsp;&nbsp;&nbsp;</p>\n<p><strong>Rationality trains you to step back from your emotions so that they don\u2019t cloud your judgment.</strong><br><br>Depression, anxiety, rage, envy, and other unpleasant and self-destructive emotions tend to be fueled by what cognitive therapy calls \u201ccognitive distortions,\u201d irrationalities in your thinking such as jumping to conclusions based on limited evidence; focusing selectively on negatives; all-or-nothing thinking; and blaming yourself, or someone else, without reason.</p>\n<p>Rationality breaks your habit of automatically trusting your instinctive, emotional judgments, encouraging you instead to notice the beliefs underlying your emotions and ask yourself whether those beliefs are justified.</p>\n<p>It also trains you to notice when your beliefs about the world are being colored by what you want, or don\u2019t want, to be true. Beliefs about your own abilities, about the motives of other people, about the likely consequences of your behavior, about what happens after you die, can be emotionally fraught. But a solid background in rationality keeps you from flinching away from the truth \u2013 about your situation, or yourself -- when learning the truth can help you change it.</p>\n<p>&nbsp;</p>", "sections": [{"title": "Rationality teaches you how to evaluate advice.", "anchor": "Rationality_teaches_you_how_to_evaluate_advice_", "level": 1}, {"title": "Rationality saves you from bad decisions.\u00a0 ", "anchor": "Rationality_saves_you_from_bad_decisions___", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "41 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SFG9Cm7mf5eP4juKs", "gJxiBnGuFLytyvhbe", "4Yb52EoyqQ7TthFe5", "DXBbQBHACYwAdKKyx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T01:59:20.969Z", "modifiedAt": null, "url": null, "title": "Several Topics that May or May Not deserve their own Post", "slug": "several-topics-that-may-or-may-not-deserve-their-own-post", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.093Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EphemeralNight", "createdAt": "2011-09-07T19:48:02.531Z", "isAdmin": false, "displayName": "EphemeralNight"}, "userId": "uWtJm9TRd8jFyRmwb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xSuZEugynJKSMmogZ/several-topics-that-may-or-may-not-deserve-their-own-post", "pageUrlRelative": "/posts/xSuZEugynJKSMmogZ/several-topics-that-may-or-may-not-deserve-their-own-post", "linkUrl": "https://www.lesswrong.com/posts/xSuZEugynJKSMmogZ/several-topics-that-may-or-may-not-deserve-their-own-post", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Several%20Topics%20that%20May%20or%20May%20Not%20deserve%20their%20own%20Post&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeveral%20Topics%20that%20May%20or%20May%20Not%20deserve%20their%20own%20Post%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxSuZEugynJKSMmogZ%2Fseveral-topics-that-may-or-may-not-deserve-their-own-post%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Several%20Topics%20that%20May%20or%20May%20Not%20deserve%20their%20own%20Post%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxSuZEugynJKSMmogZ%2Fseveral-topics-that-may-or-may-not-deserve-their-own-post", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxSuZEugynJKSMmogZ%2Fseveral-topics-that-may-or-may-not-deserve-their-own-post", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 739, "htmlBody": "<p><strong>Topic the First - Asking \"Why?\"</strong></p>\n<p>There is a certain cliche of a young child asking \"why?\", getting an answer, asking \"why?\" to that, and so on until the adult finally dismisses them out of frustration. And we all smile and laugh at how ignorant the child is and pat ourselves on the back for being so grown up.</p>\n<p>But I don't think this story is very funny. This story, told in countless variations, has the rather repugnant moral that it is rude and childish to ask that most important of questions. \"Why?\"</p>\n<p>So <em>why</em> do parents near-universally admonish their children when they persist with the questions? What is motivating parents all over the world to teach their children not to ask \"why?\"? Do parents simply not want to admit to their ignorance? I thought so at first, but I suspect it is deeper than that.</p>\n<p>It seems more likely to me, that this practice is a defense against acknowledging that one's answers are mysterious. It is easier for a parent to attribute a young child's lack of understanding to a lack of intelligence, than to comprehend that their own answer is a curiosity stopper and not an answer at all.</p>\n<p>In&nbsp;essence, children are being trained to accept curiosity-stoppers without hesitation, by being reprimanded for continuing to ask \"why?\" I find this more than a little alarming; it would seem that for parents in particular, it is especially dangerous not to notice when they're confused.</p>\n<p>\n<hr />\n</p>\n<p><strong>Topic the Second - The Behavior of Hope</strong></p>\n<p>Is tenuous hope more emotionally taxing than certain doom?</p>\n<p>I wouldn't think so, but whenever the subject of death comes up (among those who don't believe in an afterlife) I've noticed a very curious pattern.</p>\n<p>I have only a guess, but it seems possible that when doom is certain,&nbsp;when there's no escape for you or anyone, it is easier to numb the emotions. Accepting the possibility of escape makes the doom not-certain, which forces fear of the doom to the surface.</p>\n<p>\n<hr />\n</p>\n<p><strong>Topic the Third - Abuse of the word \"Love\"</strong></p>\n<p>On another site I happened to be perusing, someone posted a bit of a rant about teenagers not knowing the difference between love and lust, to which I gave this response:</p>\n<blockquote>\n<p><span style=\"color: #2c3635; font-family: Verdana, sans-serif; font-size: 12px;\">The word \"love\" is abused so much because we live in a society that looks down on pursuing relationships based on lust. A society that goes out of its way to make us feel bad about ourselves if we want to be intimate with someone we don't love. So of course the emotionally vulnerable try to convince themselves that love is involved even when it isn't, because they don't want to feel that misplaced guilt.<br /><br />It's kind of sick, when you think about it. Real love</span><span style=\"color: #2c3635; font-size: 12px;\"><span style=\"font-family: mceinline;\">*</span></span><span style=\"color: #2c3635; font-family: Verdana, sans-serif; font-size: 12px;\"> is quite rare, so believing that it is only proper to form a sexual relationship with someone you love, causes all kinds of problems. It is simply not healthy for the human animal to form sexual relationships that rarely, so due to this erroneous belief, you get people thinking they're SUPPOSED to be in love, or ASSUMING they're in love, or just TELLING themselves they're in love, because they've had it drilled into their heads that it's wrong to feel otherwise, and that what they're doing doesn't actually feel wrong so it MUST be love.<br /><br />You want kids to stop abusing the word \"love\"? Stop teaching them that they need love as a justification for acting on their lust.</span></p>\n</blockquote>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\">* I define \"real love\" as the state of valuing another's quality of life <em>more</em> than your own quality of life.</span></span></span></p>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\">\n<hr />\n</span></span></span></p>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\"><strong>Topic the Fourth - A \"Good\" Parent</strong></span></span></span></p>\n<p>Let's take a moment to think about how modern parents are generally expected to treat the subject of their offspring's sexuality. This is one of those things that I firmly believe any good future for humanity will look back on in horror.</p>\n<p>With alarming commonality, adults with maturing offspring go out of their way to stunt their children's sociosexual development, due primarily, I think, to a desire to conform to the current societal archetype of Good Parent. Despite ambiguous-at-best psychological evidence, parents fight to keep kids ignorant, unequipped, and chaste due to the social consensus that having sexually active children makes one a Bad Parent.</p>\n<p>I would even go so far as to call such deliberate impediment of sociosexual development a form of abuse, despite its extreme&nbsp;prevalence and acceptableness in today's world.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xSuZEugynJKSMmogZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 9, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "11172", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Topic_the_First___Asking__Why__\">Topic the First - Asking \"Why?\"</strong></p>\n<p>There is a certain cliche of a young child asking \"why?\", getting an answer, asking \"why?\" to that, and so on until the adult finally dismisses them out of frustration. And we all smile and laugh at how ignorant the child is and pat ourselves on the back for being so grown up.</p>\n<p>But I don't think this story is very funny. This story, told in countless variations, has the rather repugnant moral that it is rude and childish to ask that most important of questions. \"Why?\"</p>\n<p>So <em>why</em> do parents near-universally admonish their children when they persist with the questions? What is motivating parents all over the world to teach their children not to ask \"why?\"? Do parents simply not want to admit to their ignorance? I thought so at first, but I suspect it is deeper than that.</p>\n<p>It seems more likely to me, that this practice is a defense against acknowledging that one's answers are mysterious. It is easier for a parent to attribute a young child's lack of understanding to a lack of intelligence, than to comprehend that their own answer is a curiosity stopper and not an answer at all.</p>\n<p>In&nbsp;essence, children are being trained to accept curiosity-stoppers without hesitation, by being reprimanded for continuing to ask \"why?\" I find this more than a little alarming; it would seem that for parents in particular, it is especially dangerous not to notice when they're confused.</p>\n<p>\n</p><hr>\n<p></p>\n<p><strong id=\"Topic_the_Second___The_Behavior_of_Hope\">Topic the Second - The Behavior of Hope</strong></p>\n<p>Is tenuous hope more emotionally taxing than certain doom?</p>\n<p>I wouldn't think so, but whenever the subject of death comes up (among those who don't believe in an afterlife) I've noticed a very curious pattern.</p>\n<p>I have only a guess, but it seems possible that when doom is certain,&nbsp;when there's no escape for you or anyone, it is easier to numb the emotions. Accepting the possibility of escape makes the doom not-certain, which forces fear of the doom to the surface.</p>\n<p>\n</p><hr>\n<p></p>\n<p><strong id=\"Topic_the_Third___Abuse_of_the_word__Love_\">Topic the Third - Abuse of the word \"Love\"</strong></p>\n<p>On another site I happened to be perusing, someone posted a bit of a rant about teenagers not knowing the difference between love and lust, to which I gave this response:</p>\n<blockquote>\n<p><span style=\"color: #2c3635; font-family: Verdana, sans-serif; font-size: 12px;\">The word \"love\" is abused so much because we live in a society that looks down on pursuing relationships based on lust. A society that goes out of its way to make us feel bad about ourselves if we want to be intimate with someone we don't love. So of course the emotionally vulnerable try to convince themselves that love is involved even when it isn't, because they don't want to feel that misplaced guilt.<br><br>It's kind of sick, when you think about it. Real love</span><span style=\"color: #2c3635; font-size: 12px;\"><span style=\"font-family: mceinline;\">*</span></span><span style=\"color: #2c3635; font-family: Verdana, sans-serif; font-size: 12px;\"> is quite rare, so believing that it is only proper to form a sexual relationship with someone you love, causes all kinds of problems. It is simply not healthy for the human animal to form sexual relationships that rarely, so due to this erroneous belief, you get people thinking they're SUPPOSED to be in love, or ASSUMING they're in love, or just TELLING themselves they're in love, because they've had it drilled into their heads that it's wrong to feel otherwise, and that what they're doing doesn't actually feel wrong so it MUST be love.<br><br>You want kids to stop abusing the word \"love\"? Stop teaching them that they need love as a justification for acting on their lust.</span></p>\n</blockquote>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\">* I define \"real love\" as the state of valuing another's quality of life <em>more</em> than your own quality of life.</span></span></span></p>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\">\n</span></span></span></p><hr>\n<p></p>\n<p><span style=\"font-family: Verdana, sans-serif; color: #2c3635;\"><span style=\"font-size: 12px;\"><span style=\"font-family: Verdana, Arial, Helvetica, sans-serif; font-size: small; color: #000000;\"><strong>Topic the Fourth - A \"Good\" Parent</strong></span></span></span></p>\n<p>Let's take a moment to think about how modern parents are generally expected to treat the subject of their offspring's sexuality. This is one of those things that I firmly believe any good future for humanity will look back on in horror.</p>\n<p>With alarming commonality, adults with maturing offspring go out of their way to stunt their children's sociosexual development, due primarily, I think, to a desire to conform to the current societal archetype of Good Parent. Despite ambiguous-at-best psychological evidence, parents fight to keep kids ignorant, unequipped, and chaste due to the social consensus that having sexually active children makes one a Bad Parent.</p>\n<p>I would even go so far as to call such deliberate impediment of sociosexual development a form of abuse, despite its extreme&nbsp;prevalence and acceptableness in today's world.</p>", "sections": [{"title": "Topic the First - Asking \"Why?\"", "anchor": "Topic_the_First___Asking__Why__", "level": 1}, {"title": "Topic the Second - The Behavior of Hope", "anchor": "Topic_the_Second___The_Behavior_of_Hope", "level": 1}, {"title": "Topic the Third - Abuse of the word \"Love\"", "anchor": "Topic_the_Third___Abuse_of_the_word__Love_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "70 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 70, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T03:11:34.662Z", "modifiedAt": null, "url": null, "title": "[Link]: GiveWell is aiming to have a new #1 charity by December ", "slug": "link-givewell-is-aiming-to-have-a-new-1-charity-by-december", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:36.905Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Normal_Anomaly", "createdAt": "2010-11-14T03:31:54.691Z", "isAdmin": false, "displayName": "Normal_Anomaly"}, "userId": "WgGYj5bqcZKsFNG6F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XXjHMjzp33TkcSqxw/link-givewell-is-aiming-to-have-a-new-1-charity-by-december", "pageUrlRelative": "/posts/XXjHMjzp33TkcSqxw/link-givewell-is-aiming-to-have-a-new-1-charity-by-december", "linkUrl": "https://www.lesswrong.com/posts/XXjHMjzp33TkcSqxw/link-givewell-is-aiming-to-have-a-new-1-charity-by-december", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%3A%20GiveWell%20is%20aiming%20to%20have%20a%20new%20%231%20charity%20by%20December%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%3A%20GiveWell%20is%20aiming%20to%20have%20a%20new%20%231%20charity%20by%20December%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXjHMjzp33TkcSqxw%2Flink-givewell-is-aiming-to-have-a-new-1-charity-by-december%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%3A%20GiveWell%20is%20aiming%20to%20have%20a%20new%20%231%20charity%20by%20December%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXjHMjzp33TkcSqxw%2Flink-givewell-is-aiming-to-have-a-new-1-charity-by-december", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXXjHMjzp33TkcSqxw%2Flink-givewell-is-aiming-to-have-a-new-1-charity-by-december", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>GiveWell, LessWrong's most cited organization for optimal philanthropy, is currently re-evaluating its charity rankings with the goal of naming a new #1 charity by December 2011. Essentially, VillageReach (the current top charity) has met all of its short-term funding needs, to the point where it no longer has the greatest marginal return.</p>\n<blockquote>\n<p>Our current top-rated charity is <a href=\"http://www.givewell.org/international/top-charities/villagereach\">VillageReach.</a> In 2010, we <a href=\"http://www.givewell.org/about/impact\">directed</a> over $1.1 million to it, which met its short-term funding needs (i.e., its needs for the next year or so).</p>\n</blockquote>\n<blockquote>\n<p>VillageReach still has longer-term needs, and in the absence of other giving opportunities that we consider comparable, we&rsquo;ve continued to feature it as #1 on our website. However, we&rsquo;ve also <a href=\"http://blog.givewell.org/2011/02/14/givewells-plan-for-2011-top-level-priorities/\">been focusing most of our effort this year</a> on identifying and investigating other potential top-rated charities, with the hope that we can refocus attention on an organization with shorter-term needs this December. (In general, <a href=\"http://www.givewell.org/about/impact\">the vast bulk of our impact on donations comes in December</a>.) We believe that we will be able to do so. <strong>We don&rsquo;t believe we&rsquo;ll be able to recommend a giving opportunity as good as giving to VillageReach was last year, but given VillageReach&rsquo;s lack of short-term (1-year) room for more funding, we do expect to have a different top recommendation by this December</strong>.</p>\n</blockquote>\n<p>EDIT: The <a href=\"http://blog.givewell.org/2011/11/29/top-charities-for-holiday-season-2011-against-malaria-foundation-and-schistosomiasis-control-initiative/\">new charities</a> are up! They are the Against Malaria Foundation and the Schistosomiasis Control Initiative.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xEZwTHPd5AWpgQx9w": 1, "EeSkeTcT4wtW2fWsL": 1, "qAvbtzdG2A2RBn7in": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XXjHMjzp33TkcSqxw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 8.068767072836358e-07, "legacy": true, "legacyId": "11175", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T03:47:27.155Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Politics and Awful Art", "slug": "seq-rerun-politics-and-awful-art", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.453Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cz4na5rhP5GjYh3KB/seq-rerun-politics-and-awful-art", "pageUrlRelative": "/posts/Cz4na5rhP5GjYh3KB/seq-rerun-politics-and-awful-art", "linkUrl": "https://www.lesswrong.com/posts/Cz4na5rhP5GjYh3KB/seq-rerun-politics-and-awful-art", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Politics%20and%20Awful%20Art&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Politics%20and%20Awful%20Art%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCz4na5rhP5GjYh3KB%2Fseq-rerun-politics-and-awful-art%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Politics%20and%20Awful%20Art%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCz4na5rhP5GjYh3KB%2Fseq-rerun-politics-and-awful-art", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCz4na5rhP5GjYh3KB%2Fseq-rerun-politics-and-awful-art", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/m3/politics_and_awful_art/\">Politics and Awful Art</a> was originally published on 20 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When producing art that has some sort of political purpose behind it (like persuading people, or conveying a message), don't forget to actually make it art. It can't just be politics.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8lx/seq_rerun_the_litany_against_gurus/\">The Litany Against Gurus</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cz4na5rhP5GjYh3KB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 8.068895993729186e-07, "legacy": true, "legacyId": "11178", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5xT2RJy2fWxCA3eH", "Ys7Ho2HCNQCgtZq3W", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T05:22:35.104Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 11-30-2011", "slug": "meetup-west-la-meetup-11-30-2011", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qg9ghzWjGjJ9w7s7g/meetup-west-la-meetup-11-30-2011", "pageUrlRelative": "/posts/Qg9ghzWjGjJ9w7s7g/meetup-west-la-meetup-11-30-2011", "linkUrl": "https://www.lesswrong.com/posts/Qg9ghzWjGjJ9w7s7g/meetup-west-la-meetup-11-30-2011", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2011-30-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2011-30-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQg9ghzWjGjJ9w7s7g%2Fmeetup-west-la-meetup-11-30-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2011-30-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQg9ghzWjGjJ9w7s7g%2Fmeetup-west-la-meetup-11-30-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQg9ghzWjGjJ9w7s7g%2Fmeetup-west-la-meetup-11-30-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 142, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5a'>West LA Meetup 11-30-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 November 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, November 2nd.</p>\n\n<p><strong>Where:</strong> <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">The Westside Tavern</a> <em>in the upstairs Wine Bar</em>, located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt People</a></li>\n<li><a href=\"http://lesswrong.com/lw/2p/the_skeptics_trilemma\">The Skeptics Trilema</a></li>\n<li><a href=\"http://lesswrong.com/lw/ii/conservation_of_expected_evidence/\">Conservation Of Expected Evidence</a></li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5a'>West LA Meetup 11-30-2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qg9ghzWjGjJ9w7s7g", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.069237882659532e-07, "legacy": true, "legacyId": "11179", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_11_30_2011\">Discussion article for the meetup : <a href=\"/meetups/5a\">West LA Meetup 11-30-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 November 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, November 2nd.</p>\n\n<p><strong>Where:</strong> <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">The Westside Tavern</a> <em>in the upstairs Wine Bar</em>, located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong id=\"Recommended_Reading_\">Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt People</a></li>\n<li><a href=\"http://lesswrong.com/lw/2p/the_skeptics_trilemma\">The Skeptics Trilema</a></li>\n<li><a href=\"http://lesswrong.com/lw/ii/conservation_of_expected_evidence/\">Conservation Of Expected Evidence</a></li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_11_30_20111\">Discussion article for the meetup : <a href=\"/meetups/5a\">West LA Meetup 11-30-2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 11-30-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_11_30_2011", "level": 1}, {"title": "Recommended Reading:", "anchor": "Recommended_Reading_", "level": 2}, {"title": "Discussion article for the meetup : West LA Meetup 11-30-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_11_30_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AdYdLP2sRqPMoe8fb", "M7rwT264CSYY6EdR3", "jiBFC7DcCrZjGmZnJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T16:41:39.265Z", "modifiedAt": null, "url": null, "title": "All free Stanford classes, continually updated", "slug": "all-free-stanford-classes-continually-updated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:50.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XFrequentist", "createdAt": "2009-03-22T17:06:22.991Z", "isAdmin": false, "displayName": "XFrequentist"}, "userId": "zfW5w3TbDWjRW3YaD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XreWyjnnW8vuhhFbR/all-free-stanford-classes-continually-updated", "pageUrlRelative": "/posts/XreWyjnnW8vuhhFbR/all-free-stanford-classes-continually-updated", "linkUrl": "https://www.lesswrong.com/posts/XreWyjnnW8vuhhFbR/all-free-stanford-classes-continually-updated", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20All%20free%20Stanford%20classes%2C%20continually%20updated&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAll%20free%20Stanford%20classes%2C%20continually%20updated%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXreWyjnnW8vuhhFbR%2Fall-free-stanford-classes-continually-updated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=All%20free%20Stanford%20classes%2C%20continually%20updated%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXreWyjnnW8vuhhFbR%2Fall-free-stanford-classes-continually-updated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXreWyjnnW8vuhhFbR%2Fall-free-stanford-classes-continually-updated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 84, "htmlBody": "<p>[The Stanford-class.org scheme seems to be adding new courses every few days. Instead of a new post each time a new class is added, I commit to keeping this post stocked with links to all current course offerings until at least January 2012.]</p>\n<p>&nbsp;</p>\n<p><em>Complex Systems</em></p>\n<p><a href=\"http://www.modelthinker-class.org/\">Model Thinking</a></p>\n<p>&nbsp;</p>\n<p><em>Electrical Engineering</em></p>\n<p><a href=\"http://www.infotheory-class.org/\">Information Theory</a></p>\n<p>&nbsp;</p>\n<p><em>Civil Engineering</em></p>\n<p><a href=\"http://www.greenbuilding-class.org/\">Making Green Buildings</a></p>\n<p>&nbsp;</p>\n<p><em>Medicine</em></p>\n<p><a href=\"http://www.anatomy-class.org/\">Anatomy</a></p>\n<p>&nbsp;</p>\n<p>[Cancelled]<em> <del>Entrepreneurship</del></em></p>\n<p><del>The Lean Launchpad</del></p>\n<p><del>Technology Entrepreneurship</del></p>\n<p>&nbsp;</p>\n<p><em>Computer Science</em></p>\n<p><a rel=\"nofollow\" href=\"http://www.cs101-class.org/\">Computer Science 101</a></p>\n<p><a rel=\"nofollow\" href=\"http://www.saas-class.org/\">Software as a Service</a></p>\n<p><a rel=\"nofollow\" href=\"http://www.hci-class.org/\">Human-Computer Interfaces</a></p>\n<p><a rel=\"nofollow\" href=\"http://www.nlp-class.org/\">Natural Language Processing</a></p>\n<p><a rel=\"nofollow\" href=\"http://www.game-theory-class.org/\">Game Theory</a></p>\n<p><a rel=\"nofollow\" href=\"http://www.pgm-class.org/\">Probabilistic Graphical Models</a></p>\n<p><a href=\"http://jan2012.ml-class.org/\">Machine Learning</a></p>\n<p><a href=\"http://www.crypto-class.org/\">Cryptography</a></p>\n<p><a href=\"http://www.algo-class.org/\">Design and Analysis of Algorithms</a></p>\n<p><a href=\"http://www.security-class.org/\">Computer Security</a></p>\n<p>[Ran Sept-Dec] <a href=\"http://www.ml-class.org\">Machine Learning</a></p>\n<p>[Ran Sept-Dec] <a rel=\"nofollow\" href=\"https://www.ai-class.com/\">Introduction to Artificial Intelligence</a></p>\n<p>[Ran Sept-Dec] <a rel=\"nofollow\" href=\"http://www.db-class.org/\">Introduction to Databases</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GQyPQcdEQF4zXhJBq": 1, "fF9GEdWXKJ3z73TmB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XreWyjnnW8vuhhFbR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 39, "extendedScore": null, "score": 8.071679080739041e-07, "legacy": true, "legacyId": "11182", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T17:10:14.946Z", "modifiedAt": null, "url": null, "title": "Meetup : Waterloo Meetup: Nomic", "slug": "meetup-waterloo-meetup-nomic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:28.061Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paamayim", "createdAt": "2009-04-17T21:20:46.668Z", "isAdmin": false, "displayName": "Paamayim"}, "userId": "4W4hmXdXqHnfMQHHv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iSX3ytwc58uGKgT9a/meetup-waterloo-meetup-nomic", "pageUrlRelative": "/posts/iSX3ytwc58uGKgT9a/meetup-waterloo-meetup-nomic", "linkUrl": "https://www.lesswrong.com/posts/iSX3ytwc58uGKgT9a/meetup-waterloo-meetup-nomic", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Waterloo%20Meetup%3A%20Nomic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Waterloo%20Meetup%3A%20Nomic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiSX3ytwc58uGKgT9a%2Fmeetup-waterloo-meetup-nomic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Waterloo%20Meetup%3A%20Nomic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiSX3ytwc58uGKgT9a%2Fmeetup-waterloo-meetup-nomic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiSX3ytwc58uGKgT9a%2Fmeetup-waterloo-meetup-nomic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5b'>Waterloo Meetup: Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 December 2011 09:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2-4 King Street North Waterloo, Ontario, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second meeting of our newly founded LW Waterloo chapter. Suggested activity is a game of nomic, with a discussion topic to be decided.</p>\n\n<p>Join our mailing list (<a href=\"http://groups.google.com/group/lesswrongwaterloo\" rel=\"nofollow\">http://groups.google.com/group/lesswrongwaterloo</a>) for more announcements and a better venue for discussion.</p>\n\n<p>Hope to see everyone there! Slight edit: This week's meetup is at Symposium (<a href=\"http://www.symposiumcafe.com/waterloorestaurants.html\" rel=\"nofollow\">http://www.symposiumcafe.com/waterloorestaurants.html</a>).</p>\n\n<p>EDIT: Tonight's meetup is at 9pm, not 8pm. Sorry!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5b'>Waterloo Meetup: Nomic</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iSX3ytwc58uGKgT9a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.071781904754089e-07, "legacy": true, "legacyId": "11183", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Waterloo_Meetup__Nomic\">Discussion article for the meetup : <a href=\"/meetups/5b\">Waterloo Meetup: Nomic</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 December 2011 09:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2-4 King Street North Waterloo, Ontario, Canada</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second meeting of our newly founded LW Waterloo chapter. Suggested activity is a game of nomic, with a discussion topic to be decided.</p>\n\n<p>Join our mailing list (<a href=\"http://groups.google.com/group/lesswrongwaterloo\" rel=\"nofollow\">http://groups.google.com/group/lesswrongwaterloo</a>) for more announcements and a better venue for discussion.</p>\n\n<p>Hope to see everyone there! Slight edit: This week's meetup is at Symposium (<a href=\"http://www.symposiumcafe.com/waterloorestaurants.html\" rel=\"nofollow\">http://www.symposiumcafe.com/waterloorestaurants.html</a>).</p>\n\n<p>EDIT: Tonight's meetup is at 9pm, not 8pm. Sorry!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Waterloo_Meetup__Nomic1\">Discussion article for the meetup : <a href=\"/meetups/5b\">Waterloo Meetup: Nomic</a></h2>", "sections": [{"title": "Discussion article for the meetup : Waterloo Meetup: Nomic", "anchor": "Discussion_article_for_the_meetup___Waterloo_Meetup__Nomic", "level": 1}, {"title": "Discussion article for the meetup : Waterloo Meetup: Nomic", "anchor": "Discussion_article_for_the_meetup___Waterloo_Meetup__Nomic1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T18:06:36.193Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wedneday-7pm-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KSW56zAsLAENGGvSA/meetup-fort-collins-colorado-meetup-wedneday-7pm-1", "pageUrlRelative": "/posts/KSW56zAsLAENGGvSA/meetup-fort-collins-colorado-meetup-wedneday-7pm-1", "linkUrl": "https://www.lesswrong.com/posts/KSW56zAsLAENGGvSA/meetup-fort-collins-colorado-meetup-wedneday-7pm-1", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSW56zAsLAENGGvSA%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wedneday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSW56zAsLAENGGvSA%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKSW56zAsLAENGGvSA%2Fmeetup-fort-collins-colorado-meetup-wedneday-7pm-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5c'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">30 November 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet some very cool people.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5c'>Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KSW56zAsLAENGGvSA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.071984556256009e-07, "legacy": true, "legacyId": "11184", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm\">Discussion article for the meetup : <a href=\"/meetups/5c\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">30 November 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Meet some very cool people.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/5c\">Fort Collins, Colorado Meetup Wedneday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wedneday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wedneday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T18:40:35.859Z", "modifiedAt": null, "url": null, "title": "[LINKS] New GiveWell Top Charities", "slug": "links-new-givewell-top-charities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.924Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ShHhMmGjbCyFxHd6F/links-new-givewell-top-charities", "pageUrlRelative": "/posts/ShHhMmGjbCyFxHd6F/links-new-givewell-top-charities", "linkUrl": "https://www.lesswrong.com/posts/ShHhMmGjbCyFxHd6F/links-new-givewell-top-charities", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINKS%5D%20New%20GiveWell%20Top%20Charities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINKS%5D%20New%20GiveWell%20Top%20Charities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShHhMmGjbCyFxHd6F%2Flinks-new-givewell-top-charities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINKS%5D%20New%20GiveWell%20Top%20Charities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShHhMmGjbCyFxHd6F%2Flinks-new-givewell-top-charities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FShHhMmGjbCyFxHd6F%2Flinks-new-givewell-top-charities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>As of <a href=\"http://blog.givewell.org/2011/11/29/top-charities-for-holiday-season-2011-against-malaria-foundation-and-schistosomiasis-control-initiative/\">today</a>, GiveWell has two new <a href=\"http://www.givewell.org/charities/top-charities\">top charities</a>:</p>\n<p>1. <a href=\"http://givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a></p>\n<p>2. <a href=\"http://www.givewell.org/international/top-charities/schistosomiasis-control-initiative\">Schistosomiastis Control Initiative</a></p>\n<p>Their previous top charity, <a href=\"http://www.givewell.org/international/top-charities/villagereach\">Village Reach</a>, no longer has much <a href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a>.&nbsp; Which is a good thing: partly through givewell's influence they now have as much money as they can put to good use.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ShHhMmGjbCyFxHd6F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 34, "extendedScore": null, "score": 8.07210680584546e-07, "legacy": true, "legacyId": "11185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T20:02:55.119Z", "modifiedAt": null, "url": null, "title": "[LINK]: Interview with Daniel Kahneman", "slug": "link-interview-with-daniel-kahneman", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.667Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "bradm", "createdAt": "2011-11-09T17:56:00.702Z", "isAdmin": false, "displayName": "bradm"}, "userId": "Lgua9P4XifSEuXkud", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zszz7GLJPwepKooA9/link-interview-with-daniel-kahneman", "pageUrlRelative": "/posts/zszz7GLJPwepKooA9/link-interview-with-daniel-kahneman", "linkUrl": "https://www.lesswrong.com/posts/zszz7GLJPwepKooA9/link-interview-with-daniel-kahneman", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%3A%20Interview%20with%20Daniel%20Kahneman&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%3A%20Interview%20with%20Daniel%20Kahneman%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzszz7GLJPwepKooA9%2Flink-interview-with-daniel-kahneman%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%3A%20Interview%20with%20Daniel%20Kahneman%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzszz7GLJPwepKooA9%2Flink-interview-with-daniel-kahneman", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzszz7GLJPwepKooA9%2Flink-interview-with-daniel-kahneman", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<p><a href=\"http://www.freakonomics.com/2011/11/28/daniel-kahneman-answers-your-questions/\">Here is a Q &amp; A</a> with Daniel Kahneman. &nbsp;He gives a brief answer to a question about heuristics and AI:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>Q.With the launch of Siri and a stated aim to be using the data collected to improve the performance of its AI, should we expect these types of quasi-intelligences to develop the same behavioral foibles that we exhibit, or should we expect something completely different? And if something different, would that something be more likely to reflect the old &ldquo;rational&rdquo; assumptions of behavior, or some totally other emergent set of biases and quirks based on its own underlying architecture? My money&rsquo;s on emergent weirdness, but then, I don&rsquo;t have a Nobel Prize.-Peter Bennett</p>\n<p>A.Emergent weirdness is a good bet. Only deduction is certain. Whenever an inductive short-cut is applied, you can search for cases in which it will fail. It is always useful to ask &ldquo;What relevant factors are not considered?&rdquo; and &ldquo;What irrelevant factors affect the conclusions?&rdquo; By their very nature, heuristic shortcuts will produce biases, and that is true for both humans and artificial intelligence, but the heuristics of AI are not necessarily the human ones.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zszz7GLJPwepKooA9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 7, "extendedScore": null, "score": 8.072402859421395e-07, "legacy": true, "legacyId": "11186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T23:12:11.341Z", "modifiedAt": "2020-01-01T20:02:50.230Z", "url": null, "title": "Uncertainty", "slug": "uncertainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:55.502Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Vaniver", "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AnbKBK236LtCdXSzj/uncertainty", "pageUrlRelative": "/posts/AnbKBK236LtCdXSzj/uncertainty", "linkUrl": "https://www.lesswrong.com/posts/AnbKBK236LtCdXSzj/uncertainty", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Uncertainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUncertainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnbKBK236LtCdXSzj%2Funcertainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Uncertainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnbKBK236LtCdXSzj%2Funcertainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAnbKBK236LtCdXSzj%2Funcertainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2008, "htmlBody": "<html><head></head><body><p>This is part of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a>.</p><p>Decision-making under certainty is pretty boring. You know exactly what each choice will do, and so you order the outcomes based on your preferences, and pick the action that leads to the best outcome.</p><p>Human decision-making, though, is made in the presence of uncertainty. Decision analysis - careful decision making - is all about coping with the existence of uncertainty.</p><p>Some terminology: a <i>distinction</i> is something uncertain; an <i>event</i> is each of the possible outcomes of that distinction; a <i>prospect</i> is an event that you have a personal stake in, and a <i>deal</i> is a distinction over prospects. This post will focus on distinctions and events. If you're comfortable with probability just jump to the four bolded questions and make sure you get the answers right. Deals are the interesting part, but require this background.</p><p>I should say from the very start that I am quantifying uncertainty as \"probability.\" There is only one 800th digit of Pi (in base 10), other people already know it, and it's not going to change. I don't know what it is, though, and so when I talk about the probability that the 800th digit of Pi is a particular number what I'm describing is <a href=\"/lw/oj/probability_is_in_the_mind\">what's going on in my head</a>. Right now, my map is mostly blank (I assign .1 probability to 0 to 9); once I look it up, the map will change but the territory will not. I'll use uncertainty and probability interchangeably throughout this post.</p><p>The 800th digit of Pi (in base 10) is a distinction with 10 possible events, 0 through 9. To be sensible, distinctions should be clear and unambiguous. A distinction like \"the temperature tomorrow\" is unclear- the temperature where, and at what time tomorrow? A distinction like \"the maximum temperature recorded by the National Weather Service at the <a href=\"http://weather.noaa.gov/weather/current/KAUS.html\">Austin-Bergstrom International Airport</a> in the 24 hours before midnight (EST) on 11/30/2011\" is unambiguous. Think of it like <a href=\"http://predictionbook.com/\">PredictionBook</a>- you want to be able to create this distinction such that anyone could come across it and know what you're referring to.</p><p>Possibilities can be discrete or continuous. There are only a finite number of possible digits for the 800th digit of Pi, but the temperature is continuous and unbounded.1 A biased coin has a continuous parameter <i>p</i> that refers to how likely it is to land on heads in certain conditions; while that's bounded by 0 and 1, there are an infinite number of possibilities in between.</p><p>For now, let's focus on distinctions with discrete possibilities. Suppose we have four cards- two blue and two red. We shuffle the cards and draw two of them. <strong>What is the probability that both drawn cards will be red? </strong>(answer below the picture)</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_640 640w\"></figure><p>This is a simple problem, but one that many people get wrong, so let's step through it as carefully as possible. There are two distinctions here- the color of the first drawn card, and the color of the second drawn card. For each distinction, the possible events are blue (B) and red (R). The probability that the first card is red we'll express as P(R|&amp;). That should be read as \"probability of drawing a red card given background knowledge.\" The \"&amp;\" refers to all the knowledge the problem has given us; sometimes it's left off and we just talk about P(R). There are four possible cards, two of which are red, and so P(R|&amp;)=2/4=1/2.</p><p>Now we need to figure out the probability that the second card is red. We'll express that as P(R|R&amp;), which means \"the probability of drawing a red card given background knowledge and a drawn red card.\" There are three cards left, one of which is red, and so the probability is now 1/3.</p><p>But what we're really interested in is P(RR|&amp;), \"the probability of drawing two red cards given background knowledge.\" We can divide this single distinction into two distinctions: P(RR|&amp;)=P(R|R&amp;)*P(R|&amp;)=1/2*1/3=<strong>1/6</strong>. Probabilities are conjoined by multiplication.</p><p>Notice that, for the first two cards drawn, there are four events: RR, RB, BR, and BB. Those events have different probabilities: 1/6, 1/3, 1/3, and 1/6. Those represent the <i>joint probability distribution</i> of the first two cards, and the joint probability distribution contains all the information we need. If you're interested in the chance that the second card is blue with no information about the first (P(*B|&amp;)), you add up RB and BB to get 1/3+1/6=1/2 (which is what you should have expected it to be).</p><p>Bayes' Rule, by the way, is easy to see when discussing events. If I wanted to figure out P(RB|*B&amp;), what I want to do is take the event RB (probability 1/3) and make it more likely by dividing out the probability of my current state of knowledge (that the second card was blue, probability 1/2). Alternatively, I could consider the event RB as a fraction of the set of events that fit my knowledge, which is both RB and BB- (1/3)/(1/3+1/6)=2/3.</p><h2>Relevance<br>&nbsp;</h2><p>Most people who get the question about cards wrong get it wrong because they square 1/2 to get 1/4, forgetting that the second card <i>depends</i> on the first. Since there's a limited supply of cards, as soon as you draw one you can be more certain that the next card isn't that color.</p><p>Dependence is distinct from causality. If I hear the weatherman claim that it will rain with 50% probability, that will adjust my certainty that it will rain, even though the weatherman can't directly influence whether or not it will rain. Some people use the word <i>relevance</i> instead, as it's natural to think that the weatherman's prediction is relevant to the likelihood of rain but may not be natural to think that the chance of rain depends on the weatherman's prediction.</p><p>Relevance goes both ways. If the weatherman's prediction gives me knowledge about whether or not it will rain, then knowing whether or not it rained gives me knowledge about what the weatherman's prediction was. Bayes' Rule is critical for maneuvering through relevant distinctions. Suppose the weatherman could give only two predictions: Sunny or Rainy. If he predicts Sunny, it will rain with 10% probability. If he predicts Rainy, it will rain with 50% probability. <strong>If it rains 20% of the time, how often does he predict Rainy?</strong> (<a href=\"http://www.wolframalpha.com/input/?i=.5x%2B.1%281-x%29%3D.2\">answer</a>)</p><p><strong>Suppose it rains. What's the chance that the weatherman predicted Rainy? </strong>(answer below the picture)</p><figure><img src=\"http://nextstopwonderland.files.wordpress.com/2008/06/weatherman.jpg\"></figure><p>This is a simple application of Bayes' Rule: P(Rainy|Rain)=<a href=\"http://www.wolframalpha.com/input/?i=.5*.25%2F.2\">P(Rain|Rainy)P(Rainy)/P(Rain)</a>.</p><p>Alternatively, we can figure out the probabilities of the four elementary events: P(Rainy,Rain)=.125, P(Rainy,Sun)=.125, P(Sunny,Rain)=.075, P(Sunny,Sun)=.675. If we know it rained and want to know if he predicted Rainy, we care about <a href=\"http://www.wolframalpha.com/input/?i=.125%2F%28.125%2B.075%29\">P(Rainy,Rain)/(P(Rainy,Rain)+P(Sunny,Rain))</a>.</p><p>This can get very complicated if there are a large number of events or relevant distinctions, but software exists to solve that problem.</p><h2>Continuous Distributions<br>&nbsp;</h2><p>Suppose, though, that you don't have just two events to assign probability to. Instead of being uncertain about whether or not it will rain, I might be uncertain about how much it will rain, conditioned on it raining.2 If I try to elicit a probability for every possible amount, that'll take me a long time (unless I bin the heights, making it discrete, which still might take far longer or be far harder than I can deal with, if there are lots of bins).</p><p>In that case, I would express my uncertainty as a <i>probability density function</i> (pdf) or <i>cumulative probability density function </i>(cdf). The first is the probability density at a particular value, whereas the second is the density integrated from the beginning of the domain to that value. To get a probability from a density, you have to integrate. A pdf can have any non-negative value and any shape over the domain, though it has to integrate to 1, while a cdf has a minimum of 0, a maximum of 1, and is non-decreasing.</p><p>Let's take the example of the biased coin. To make it more precise, since coin flips are messy and physical, suppose I have some random number generator that uniformly generates any real number between 0 and 1, and a device hooked up to it with an unknown threshold value <i>p</i> between 0 and 1.3 When I press a button, the generator generates a random number, hands it to the device, which then shows a picture of heads if the number is below or equal to the threshold and a picture of tails if the number is above the threshold. I don't get to see the number that was generated- just a head or tail every time I press the button.</p><p>I begin by being uncertain about the threshold value, except knowing its domain. I assign a uniform prior- I think it's equally likely that the threshold value is at every point between 0 and 1. Mathematically, that means my pdf is P(<i>p=</i>x)=1. I can integrate that from 0 to y to get a cdf of C(<i>p\u2264</i>y)=\u222b1dx=y. Like we needed, the pdf integrates to 1, the cdf has a minimum of 0 and maximum of 1, and is non-decreasing. From those, we can calculate my certainty that the threshold value is in a particular range (by integrating the pdf over that range) or any particular point (0, because it's an integral of 0 width).</p><h2>Updating<br>&nbsp;</h2><p>Now we press the button, see something, and need to update our uncertainty (probability distribution). How should we do that?</p><p>Well, by Bayes' rule of course! But I'll do it in a somewhat roundabout way, to give you some more intuition why the rule works. Suppose we saw heads. For each possible threshold value, we know how likely that was- <i>p</i>, the threshold value. We can now compute the probability density of (heads if <i>p</i>) and (<i>p</i>) by multiplying those together, and x times 1 = x. So my pdf is now P(<i>p=</i>x)=x and cdf is C(<i>p\u2264</i>y)=.5y2.</p><p>Well, not quite. My pdf doesn't integrate to 1, and my cdf, while it does have a min at 0, doesn't have a max of 1. I need to renormalize- that is, divide by the chance that I saw heads in the first place. That was 1/2, and so I get P(<i>p=</i>x)=2x and C(<i>p\u2264</i>y)=y2 and everything works out. If I saw tails, my likelihood is instead 1<i>-p</i>, and that propagates through to P(<i>p=</i>x)=2-2x and C(<i>p\u2264</i>y)=2y-y2.</p><p>Suppose my setup were even less helpful. Instead of showing heads or tails, it instead generates two numbers, computes heads or tails for each number separately, and then prints out either \"S\" if both results were the same or \"D\" if the results were different. <strong>If I start with a uniform prior, what will my pdf and cdf on the threshold value </strong><i><strong>p</strong></i><strong> be after I see S? If I saw D instead?</strong> (If you don't know calculus, don't worry- most of the rest of this sequence will deal with discrete events.)</p><p>I recommend giving it a try before checking, but the pdf is linked <a href=\"http://www.wolframalpha.com/input/?i=integrate+3%2F2*%281-2*x*%281-x%29%29+from+0+to+1\">here for S</a> and <a href=\"http://www.wolframalpha.com/input/?i=integrate+%286%29%28x%281-x%29%29+from+0+to+1\">here for D</a>. (cdfs: <a href=\"http://www.wolframalpha.com/input/?i=plot+integrate+3%2F2*%281-2*x*%281-x%29%29+from+0+to+y+from+y%3D0+to+y%3D1\">S</a> and <a href=\"http://www.wolframalpha.com/input/?i=plot+integrate+%286%29%28x%281-x%29%29+from+0+to+y+from+y%3D0+to+y%3D1\">D</a>)</p><h2>Conjugate Priors</h2><p>That's a lot of work to do every time you get information, though. If you pick what's called a <i>conjugate prior</i>, updating is simple, whereas it requires multiplication and integration for an arbitrary prior. The uniform prior is a conjugate prior for the simple biased coin problem, because uniform is a special case of the <a href=\"http://en.wikipedia.org/wiki/Beta_distribution\">beta distribution</a>. You can use Be(heads+1,tails+1) as your posterior probability for any number of heads and tails that you see, and the math is already done for you. Conjugate priors are a big part of doing continuous Bayesian analysis in practice, but won't be too relevant to the rest of this sequence.</p><p>&nbsp;</p><hr><p>1. The temperature as recorded by the National Weather Service is not continuous and is, in practice, bounded. (The NWS will only continue existing for some temperature range, and even if a technical error caused the NWS to record a bizarre temperature, they're limited by how their system stores numbers.)</p><p>2. I would probably narrow my prediction down to the height of the water in a graduated cylinder set in a representative location.</p><p>3. In case you're wondering, this sort of thing is fairly easy to create with a two-level quantum system and thus get \"genuine\" randomness.</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AnbKBK236LtCdXSzj", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 27, "extendedScore": null, "score": 8.073083611313418e-07, "legacy": true, "legacyId": "11135", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is part of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a>.</p><p>Decision-making under certainty is pretty boring. You know exactly what each choice will do, and so you order the outcomes based on your preferences, and pick the action that leads to the best outcome.</p><p>Human decision-making, though, is made in the presence of uncertainty. Decision analysis - careful decision making - is all about coping with the existence of uncertainty.</p><p>Some terminology: a <i>distinction</i> is something uncertain; an <i>event</i> is each of the possible outcomes of that distinction; a <i>prospect</i> is an event that you have a personal stake in, and a <i>deal</i> is a distinction over prospects. This post will focus on distinctions and events. If you're comfortable with probability just jump to the four bolded questions and make sure you get the answers right. Deals are the interesting part, but require this background.</p><p>I should say from the very start that I am quantifying uncertainty as \"probability.\" There is only one 800th digit of Pi (in base 10), other people already know it, and it's not going to change. I don't know what it is, though, and so when I talk about the probability that the 800th digit of Pi is a particular number what I'm describing is <a href=\"/lw/oj/probability_is_in_the_mind\">what's going on in my head</a>. Right now, my map is mostly blank (I assign .1 probability to 0 to 9); once I look it up, the map will change but the territory will not. I'll use uncertainty and probability interchangeably throughout this post.</p><p>The 800th digit of Pi (in base 10) is a distinction with 10 possible events, 0 through 9. To be sensible, distinctions should be clear and unambiguous. A distinction like \"the temperature tomorrow\" is unclear- the temperature where, and at what time tomorrow? A distinction like \"the maximum temperature recorded by the National Weather Service at the <a href=\"http://weather.noaa.gov/weather/current/KAUS.html\">Austin-Bergstrom International Airport</a> in the 24 hours before midnight (EST) on 11/30/2011\" is unambiguous. Think of it like <a href=\"http://predictionbook.com/\">PredictionBook</a>- you want to be able to create this distinction such that anyone could come across it and know what you're referring to.</p><p>Possibilities can be discrete or continuous. There are only a finite number of possible digits for the 800th digit of Pi, but the temperature is continuous and unbounded.1 A biased coin has a continuous parameter <i>p</i> that refers to how likely it is to land on heads in certain conditions; while that's bounded by 0 and 1, there are an infinite number of possibilities in between.</p><p>For now, let's focus on distinctions with discrete possibilities. Suppose we have four cards- two blue and two red. We shuffle the cards and draw two of them. <strong>What is the probability that both drawn cards will be red? </strong>(answer below the picture)</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a57966e063bcd7414d2168a68405267a409e45bc9c8a0f37.jpg/w_640 640w\"></figure><p>This is a simple problem, but one that many people get wrong, so let's step through it as carefully as possible. There are two distinctions here- the color of the first drawn card, and the color of the second drawn card. For each distinction, the possible events are blue (B) and red (R). The probability that the first card is red we'll express as P(R|&amp;). That should be read as \"probability of drawing a red card given background knowledge.\" The \"&amp;\" refers to all the knowledge the problem has given us; sometimes it's left off and we just talk about P(R). There are four possible cards, two of which are red, and so P(R|&amp;)=2/4=1/2.</p><p>Now we need to figure out the probability that the second card is red. We'll express that as P(R|R&amp;), which means \"the probability of drawing a red card given background knowledge and a drawn red card.\" There are three cards left, one of which is red, and so the probability is now 1/3.</p><p>But what we're really interested in is P(RR|&amp;), \"the probability of drawing two red cards given background knowledge.\" We can divide this single distinction into two distinctions: P(RR|&amp;)=P(R|R&amp;)*P(R|&amp;)=1/2*1/3=<strong>1/6</strong>. Probabilities are conjoined by multiplication.</p><p>Notice that, for the first two cards drawn, there are four events: RR, RB, BR, and BB. Those events have different probabilities: 1/6, 1/3, 1/3, and 1/6. Those represent the <i>joint probability distribution</i> of the first two cards, and the joint probability distribution contains all the information we need. If you're interested in the chance that the second card is blue with no information about the first (P(*B|&amp;)), you add up RB and BB to get 1/3+1/6=1/2 (which is what you should have expected it to be).</p><p>Bayes' Rule, by the way, is easy to see when discussing events. If I wanted to figure out P(RB|*B&amp;), what I want to do is take the event RB (probability 1/3) and make it more likely by dividing out the probability of my current state of knowledge (that the second card was blue, probability 1/2). Alternatively, I could consider the event RB as a fraction of the set of events that fit my knowledge, which is both RB and BB- (1/3)/(1/3+1/6)=2/3.</p><h2 id=\"Relevance_\">Relevance<br>&nbsp;</h2><p>Most people who get the question about cards wrong get it wrong because they square 1/2 to get 1/4, forgetting that the second card <i>depends</i> on the first. Since there's a limited supply of cards, as soon as you draw one you can be more certain that the next card isn't that color.</p><p>Dependence is distinct from causality. If I hear the weatherman claim that it will rain with 50% probability, that will adjust my certainty that it will rain, even though the weatherman can't directly influence whether or not it will rain. Some people use the word <i>relevance</i> instead, as it's natural to think that the weatherman's prediction is relevant to the likelihood of rain but may not be natural to think that the chance of rain depends on the weatherman's prediction.</p><p>Relevance goes both ways. If the weatherman's prediction gives me knowledge about whether or not it will rain, then knowing whether or not it rained gives me knowledge about what the weatherman's prediction was. Bayes' Rule is critical for maneuvering through relevant distinctions. Suppose the weatherman could give only two predictions: Sunny or Rainy. If he predicts Sunny, it will rain with 10% probability. If he predicts Rainy, it will rain with 50% probability. <strong>If it rains 20% of the time, how often does he predict Rainy?</strong> (<a href=\"http://www.wolframalpha.com/input/?i=.5x%2B.1%281-x%29%3D.2\">answer</a>)</p><p><strong>Suppose it rains. What's the chance that the weatherman predicted Rainy? </strong>(answer below the picture)</p><figure><img src=\"http://nextstopwonderland.files.wordpress.com/2008/06/weatherman.jpg\"></figure><p>This is a simple application of Bayes' Rule: P(Rainy|Rain)=<a href=\"http://www.wolframalpha.com/input/?i=.5*.25%2F.2\">P(Rain|Rainy)P(Rainy)/P(Rain)</a>.</p><p>Alternatively, we can figure out the probabilities of the four elementary events: P(Rainy,Rain)=.125, P(Rainy,Sun)=.125, P(Sunny,Rain)=.075, P(Sunny,Sun)=.675. If we know it rained and want to know if he predicted Rainy, we care about <a href=\"http://www.wolframalpha.com/input/?i=.125%2F%28.125%2B.075%29\">P(Rainy,Rain)/(P(Rainy,Rain)+P(Sunny,Rain))</a>.</p><p>This can get very complicated if there are a large number of events or relevant distinctions, but software exists to solve that problem.</p><h2 id=\"Continuous_Distributions_\">Continuous Distributions<br>&nbsp;</h2><p>Suppose, though, that you don't have just two events to assign probability to. Instead of being uncertain about whether or not it will rain, I might be uncertain about how much it will rain, conditioned on it raining.2 If I try to elicit a probability for every possible amount, that'll take me a long time (unless I bin the heights, making it discrete, which still might take far longer or be far harder than I can deal with, if there are lots of bins).</p><p>In that case, I would express my uncertainty as a <i>probability density function</i> (pdf) or <i>cumulative probability density function </i>(cdf). The first is the probability density at a particular value, whereas the second is the density integrated from the beginning of the domain to that value. To get a probability from a density, you have to integrate. A pdf can have any non-negative value and any shape over the domain, though it has to integrate to 1, while a cdf has a minimum of 0, a maximum of 1, and is non-decreasing.</p><p>Let's take the example of the biased coin. To make it more precise, since coin flips are messy and physical, suppose I have some random number generator that uniformly generates any real number between 0 and 1, and a device hooked up to it with an unknown threshold value <i>p</i> between 0 and 1.3 When I press a button, the generator generates a random number, hands it to the device, which then shows a picture of heads if the number is below or equal to the threshold and a picture of tails if the number is above the threshold. I don't get to see the number that was generated- just a head or tail every time I press the button.</p><p>I begin by being uncertain about the threshold value, except knowing its domain. I assign a uniform prior- I think it's equally likely that the threshold value is at every point between 0 and 1. Mathematically, that means my pdf is P(<i>p=</i>x)=1. I can integrate that from 0 to y to get a cdf of C(<i>p\u2264</i>y)=\u222b1dx=y. Like we needed, the pdf integrates to 1, the cdf has a minimum of 0 and maximum of 1, and is non-decreasing. From those, we can calculate my certainty that the threshold value is in a particular range (by integrating the pdf over that range) or any particular point (0, because it's an integral of 0 width).</p><h2 id=\"Updating_\">Updating<br>&nbsp;</h2><p>Now we press the button, see something, and need to update our uncertainty (probability distribution). How should we do that?</p><p>Well, by Bayes' rule of course! But I'll do it in a somewhat roundabout way, to give you some more intuition why the rule works. Suppose we saw heads. For each possible threshold value, we know how likely that was- <i>p</i>, the threshold value. We can now compute the probability density of (heads if <i>p</i>) and (<i>p</i>) by multiplying those together, and x times 1 = x. So my pdf is now P(<i>p=</i>x)=x and cdf is C(<i>p\u2264</i>y)=.5y2.</p><p>Well, not quite. My pdf doesn't integrate to 1, and my cdf, while it does have a min at 0, doesn't have a max of 1. I need to renormalize- that is, divide by the chance that I saw heads in the first place. That was 1/2, and so I get P(<i>p=</i>x)=2x and C(<i>p\u2264</i>y)=y2 and everything works out. If I saw tails, my likelihood is instead 1<i>-p</i>, and that propagates through to P(<i>p=</i>x)=2-2x and C(<i>p\u2264</i>y)=2y-y2.</p><p>Suppose my setup were even less helpful. Instead of showing heads or tails, it instead generates two numbers, computes heads or tails for each number separately, and then prints out either \"S\" if both results were the same or \"D\" if the results were different. <strong>If I start with a uniform prior, what will my pdf and cdf on the threshold value </strong><i><strong>p</strong></i><strong> be after I see S? If I saw D instead?</strong> (If you don't know calculus, don't worry- most of the rest of this sequence will deal with discrete events.)</p><p>I recommend giving it a try before checking, but the pdf is linked <a href=\"http://www.wolframalpha.com/input/?i=integrate+3%2F2*%281-2*x*%281-x%29%29+from+0+to+1\">here for S</a> and <a href=\"http://www.wolframalpha.com/input/?i=integrate+%286%29%28x%281-x%29%29+from+0+to+1\">here for D</a>. (cdfs: <a href=\"http://www.wolframalpha.com/input/?i=plot+integrate+3%2F2*%281-2*x*%281-x%29%29+from+0+to+y+from+y%3D0+to+y%3D1\">S</a> and <a href=\"http://www.wolframalpha.com/input/?i=plot+integrate+%286%29%28x%281-x%29%29+from+0+to+y+from+y%3D0+to+y%3D1\">D</a>)</p><h2 id=\"Conjugate_Priors\">Conjugate Priors</h2><p>That's a lot of work to do every time you get information, though. If you pick what's called a <i>conjugate prior</i>, updating is simple, whereas it requires multiplication and integration for an arbitrary prior. The uniform prior is a conjugate prior for the simple biased coin problem, because uniform is a special case of the <a href=\"http://en.wikipedia.org/wiki/Beta_distribution\">beta distribution</a>. You can use Be(heads+1,tails+1) as your posterior probability for any number of heads and tails that you see, and the math is already done for you. Conjugate priors are a big part of doing continuous Bayesian analysis in practice, but won't be too relevant to the rest of this sequence.</p><p>&nbsp;</p><hr><p>1. The temperature as recorded by the National Weather Service is not continuous and is, in practice, bounded. (The NWS will only continue existing for some temperature range, and even if a technical error caused the NWS to record a bizarre temperature, they're limited by how their system stores numbers.)</p><p>2. I would probably narrow my prediction down to the height of the water in a graduated cylinder set in a representative location.</p><p>3. In case you're wondering, this sort of thing is fairly easy to create with a two-level quantum system and thus get \"genuine\" randomness.</p>", "sections": [{"title": "Relevance\u00a0", "anchor": "Relevance_", "level": 1}, {"title": "Continuous Distributions\u00a0", "anchor": "Continuous_Distributions_", "level": 1}, {"title": "Updating\u00a0", "anchor": "Updating_", "level": 1}, {"title": "Conjugate Priors", "anchor": "Conjugate_Priors", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "15 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "f6ZLxEWaankRZ2Crv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T23:22:28.899Z", "modifiedAt": null, "url": null, "title": "Review of Machery, 'Doing Without Concepts'", "slug": "review-of-machery-doing-without-concepts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.442Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rKL4aecWLNBpvfW5Q/review-of-machery-doing-without-concepts", "pageUrlRelative": "/posts/rKL4aecWLNBpvfW5Q/review-of-machery-doing-without-concepts", "linkUrl": "https://www.lesswrong.com/posts/rKL4aecWLNBpvfW5Q/review-of-machery-doing-without-concepts", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Review%20of%20Machery%2C%20'Doing%20Without%20Concepts'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReview%20of%20Machery%2C%20'Doing%20Without%20Concepts'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKL4aecWLNBpvfW5Q%2Freview-of-machery-doing-without-concepts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Review%20of%20Machery%2C%20'Doing%20Without%20Concepts'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKL4aecWLNBpvfW5Q%2Freview-of-machery-doing-without-concepts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKL4aecWLNBpvfW5Q%2Freview-of-machery-doing-without-concepts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 695, "htmlBody": "<p>Edouard Machery's <em><a href=\"http://www.amazon.com/Doing-without-Concepts-Edouard-Machery/dp/0199837562/\">Doing Without Concepts</a></em>&nbsp;made a big splash in 2009, since it argues in all seriousness that concepts do not exist.</p>\n<p>But wait. In order to claim that concepts don't exist, doesn't Machery need the concepts of \"concept\" and \"exist\"?&nbsp;To clarify what Machery means, I will summarize his book.</p>\n<p>Machery argues for the Heterogeneity Hypothesis, which makes five basic claims:</p>\n<ol>\n<li>The best available evidence suggests that for each category (for&nbsp;each substance, event, and so on), an individual typically has&nbsp;several concepts.</li>\n<li>Coreferential concepts have very few properties in common.&nbsp;They belong to very heterogeneous kinds of concept.</li>\n<li>Evidence strongly suggests that prototypes, exemplars, and&nbsp;theories are among these heterogeneous kinds of concept.</li>\n<li>Prototypes, exemplars, and theories are typically used in distinct&nbsp;cognitive processes.</li>\n<li>The notion of concept ought to be eliminated from the&nbsp;theoretical vocabulary of psychology.</li>\n</ol>\n<p>&nbsp;</p>\n<h4>Concepts in psychology and philosophy</h4>\n<p>After reviewing the psychological literature on concepts, Machery proposes that by \"concept\" psychologists usually mean something like this:</p>\n<blockquote>\n<p>A concept of x is a body of knowledge about x that is stored in longterm&nbsp;memory and that is used by default in the processes underlying&nbsp;most, if not all, higher cognitive competences when these processes&nbsp;result in judgments about x.</p>\n</blockquote>\n<p>Philosophers, by contrast, usually means something like this:</p>\n<blockquote>\n<p>Having a concept of x is being able to have propositional attitudes&nbsp;about x as x.</p>\n</blockquote>\n<p>As such, psychologists and philosophers are engaging in different projects when they talk about concepts, and Machery reviews some cases in which this has caused confusion.</p>\n<p>&nbsp;</p>\n<h4>Prototypes, exemplars, and theories</h4>\n<p>Since the <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">death</a> of the classical view of concepts, three paradigms about concepts have emerged in psychology: the prototypes paradigm, the exemplars paradigm, and the theories paradigm.&nbsp;</p>\n<p>In fact, we have pretty good evidence for the existence of all three kinds of concepts. Moreover, we seem to possess distinct processes for learning these kinds of concepts, and also distinct processes for categorizing.</p>\n<p>&nbsp;</p>\n<h4>Scientific eliminativism</h4>\n<p>The first seven chapters provide the evidence for Machery's first four claims. The eight chapter makes his eliminativist argument:</p>\n<blockquote>\n<p>In this section, I introduce in some detail a new type of eliminativist&nbsp;argument. Since this argument does not bear on the elimination of folk&nbsp;notions, but exclusively on the elimination of scientific notions and on&nbsp;their replacement by other theoretical notions, I call this form of eliminativism&nbsp;&ldquo;scientific eliminativism.&rdquo; Applied to &ldquo;concept,&rdquo; scientific eliminativism&nbsp;goes in substance as follows. In contrast to old-fashioned&nbsp;eliminativist arguments, the scientific eliminativist does not dispute that&nbsp;&ldquo;concept&rdquo; picks out a class of entities: there are bodies of knowledge&nbsp;stored in long-term memory and used by default in the processes underlying&nbsp;the higher cognitive competences. Instead of arguing that &ldquo;concept&rdquo;&nbsp;does not refer, the scientific eliminativist makes a case that the class of&nbsp;concepts does not possess the properties that characterize the classes that&nbsp;matter for the empirical sciences. Or, to use a slogan, that this class is not a&nbsp;natural kind. If &ldquo;concept&rdquo; does not pick out a natural kind, then it is&nbsp;unlikely to be a useful notion in psychology. It is even likely to stand in the&nbsp;way of progress in psychology, by preventing the development of a more&nbsp;adequate classificatory scheme that would identify the relevant natural&nbsp;kinds. If this is the case, the term &ldquo;concept&rdquo; ought to be eliminated&nbsp;from the theoretical vocabulary of psychology and replaced with more&nbsp;adequate theoretical terms.</p>\n<p>...If psychologists were to say that&nbsp;categorization involves prototypes, exemplars, and theories, rather than&nbsp;saying (as they now do) that it involves concepts, it would be clear that&nbsp;psychologists have to describe what prototypes, exemplars, and theories&nbsp;are, rather than describing what concepts are. It would also be clear that&nbsp;they have to explain how the categorization processes that use prototypes,&nbsp;exemplars, and theories are organized. Bringing these tasks to the fore is&nbsp;the main pragmatic reason that justifies the drastic conceptual change&nbsp;proposed in this book&mdash;doing psychology without the theoretical term&nbsp;&ldquo;concept.&rdquo;</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Whether or not you agree with Machery's scientific eliminativism, the main takeaway from his book is that \"concept\" is not a very good \"natural kind\" even if it may remain a useful class of natural kinds.</p>\n<p><strong>This has implications for philosophy. If we're trying to describe the \"concept\" of \"ought\" or of \"good,\" perhaps instead we ought to be discussing the <em>prototypes</em>, <em>exemplars</em>, or <em>theories</em>&nbsp;of \"ought\" or \"good.\"</strong></p>\n<p>For other discussions of Machery's book, see <a href=\"http://www.humanamente.eu/PDF/Issue11_BookReview_Fenici.pdf\">Fenici</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Chen-review-of-Doing-Without-Concepts.pdf\">Chen</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Glymour-review-of-Doing-Without-Concepts.pdf\">Glymour</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Woodfield-review-of-Doing-Without-Concepts.pdf\">Woodfield</a>, and especially <em><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Machery-Precis-of-Doing-without-Concepts.pdf\">BBS</a></em>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rKL4aecWLNBpvfW5Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 13, "extendedScore": null, "score": 8.073120633808057e-07, "legacy": true, "legacyId": "11188", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Edouard Machery's <em><a href=\"http://www.amazon.com/Doing-without-Concepts-Edouard-Machery/dp/0199837562/\">Doing Without Concepts</a></em>&nbsp;made a big splash in 2009, since it argues in all seriousness that concepts do not exist.</p>\n<p>But wait. In order to claim that concepts don't exist, doesn't Machery need the concepts of \"concept\" and \"exist\"?&nbsp;To clarify what Machery means, I will summarize his book.</p>\n<p>Machery argues for the Heterogeneity Hypothesis, which makes five basic claims:</p>\n<ol>\n<li>The best available evidence suggests that for each category (for&nbsp;each substance, event, and so on), an individual typically has&nbsp;several concepts.</li>\n<li>Coreferential concepts have very few properties in common.&nbsp;They belong to very heterogeneous kinds of concept.</li>\n<li>Evidence strongly suggests that prototypes, exemplars, and&nbsp;theories are among these heterogeneous kinds of concept.</li>\n<li>Prototypes, exemplars, and theories are typically used in distinct&nbsp;cognitive processes.</li>\n<li>The notion of concept ought to be eliminated from the&nbsp;theoretical vocabulary of psychology.</li>\n</ol>\n<p>&nbsp;</p>\n<h4 id=\"Concepts_in_psychology_and_philosophy\">Concepts in psychology and philosophy</h4>\n<p>After reviewing the psychological literature on concepts, Machery proposes that by \"concept\" psychologists usually mean something like this:</p>\n<blockquote>\n<p>A concept of x is a body of knowledge about x that is stored in longterm&nbsp;memory and that is used by default in the processes underlying&nbsp;most, if not all, higher cognitive competences when these processes&nbsp;result in judgments about x.</p>\n</blockquote>\n<p>Philosophers, by contrast, usually means something like this:</p>\n<blockquote>\n<p>Having a concept of x is being able to have propositional attitudes&nbsp;about x as x.</p>\n</blockquote>\n<p>As such, psychologists and philosophers are engaging in different projects when they talk about concepts, and Machery reviews some cases in which this has caused confusion.</p>\n<p>&nbsp;</p>\n<h4 id=\"Prototypes__exemplars__and_theories\">Prototypes, exemplars, and theories</h4>\n<p>Since the <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">death</a> of the classical view of concepts, three paradigms about concepts have emerged in psychology: the prototypes paradigm, the exemplars paradigm, and the theories paradigm.&nbsp;</p>\n<p>In fact, we have pretty good evidence for the existence of all three kinds of concepts. Moreover, we seem to possess distinct processes for learning these kinds of concepts, and also distinct processes for categorizing.</p>\n<p>&nbsp;</p>\n<h4 id=\"Scientific_eliminativism\">Scientific eliminativism</h4>\n<p>The first seven chapters provide the evidence for Machery's first four claims. The eight chapter makes his eliminativist argument:</p>\n<blockquote>\n<p>In this section, I introduce in some detail a new type of eliminativist&nbsp;argument. Since this argument does not bear on the elimination of folk&nbsp;notions, but exclusively on the elimination of scientific notions and on&nbsp;their replacement by other theoretical notions, I call this form of eliminativism&nbsp;\u201cscientific eliminativism.\u201d Applied to \u201cconcept,\u201d scientific eliminativism&nbsp;goes in substance as follows. In contrast to old-fashioned&nbsp;eliminativist arguments, the scientific eliminativist does not dispute that&nbsp;\u201cconcept\u201d picks out a class of entities: there are bodies of knowledge&nbsp;stored in long-term memory and used by default in the processes underlying&nbsp;the higher cognitive competences. Instead of arguing that \u201cconcept\u201d&nbsp;does not refer, the scientific eliminativist makes a case that the class of&nbsp;concepts does not possess the properties that characterize the classes that&nbsp;matter for the empirical sciences. Or, to use a slogan, that this class is not a&nbsp;natural kind. If \u201cconcept\u201d does not pick out a natural kind, then it is&nbsp;unlikely to be a useful notion in psychology. It is even likely to stand in the&nbsp;way of progress in psychology, by preventing the development of a more&nbsp;adequate classificatory scheme that would identify the relevant natural&nbsp;kinds. If this is the case, the term \u201cconcept\u201d ought to be eliminated&nbsp;from the theoretical vocabulary of psychology and replaced with more&nbsp;adequate theoretical terms.</p>\n<p>...If psychologists were to say that&nbsp;categorization involves prototypes, exemplars, and theories, rather than&nbsp;saying (as they now do) that it involves concepts, it would be clear that&nbsp;psychologists have to describe what prototypes, exemplars, and theories&nbsp;are, rather than describing what concepts are. It would also be clear that&nbsp;they have to explain how the categorization processes that use prototypes,&nbsp;exemplars, and theories are organized. Bringing these tasks to the fore is&nbsp;the main pragmatic reason that justifies the drastic conceptual change&nbsp;proposed in this book\u2014doing psychology without the theoretical term&nbsp;\u201cconcept.\u201d</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>Whether or not you agree with Machery's scientific eliminativism, the main takeaway from his book is that \"concept\" is not a very good \"natural kind\" even if it may remain a useful class of natural kinds.</p>\n<p><strong id=\"This_has_implications_for_philosophy__If_we_re_trying_to_describe_the__concept__of__ought__or_of__good___perhaps_instead_we_ought_to_be_discussing_the_prototypes__exemplars__or_theories_of__ought__or__good__\">This has implications for philosophy. If we're trying to describe the \"concept\" of \"ought\" or of \"good,\" perhaps instead we ought to be discussing the <em>prototypes</em>, <em>exemplars</em>, or <em>theories</em>&nbsp;of \"ought\" or \"good.\"</strong></p>\n<p>For other discussions of Machery's book, see <a href=\"http://www.humanamente.eu/PDF/Issue11_BookReview_Fenici.pdf\">Fenici</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Chen-review-of-Doing-Without-Concepts.pdf\">Chen</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Glymour-review-of-Doing-Without-Concepts.pdf\">Glymour</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Woodfield-review-of-Doing-Without-Concepts.pdf\">Woodfield</a>, and especially <em><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Machery-Precis-of-Doing-without-Concepts.pdf\">BBS</a></em>.</p>", "sections": [{"title": "Concepts in psychology and philosophy", "anchor": "Concepts_in_psychology_and_philosophy", "level": 1}, {"title": "Prototypes, exemplars, and theories", "anchor": "Prototypes__exemplars__and_theories", "level": 1}, {"title": "Scientific eliminativism", "anchor": "Scientific_eliminativism", "level": 1}, {"title": "This has implications for philosophy. If we're trying to describe the \"concept\" of \"ought\" or of \"good,\" perhaps instead we ought to be discussing the prototypes, exemplars, or theories\u00a0of \"ought\" or \"good.\"", "anchor": "This_has_implications_for_philosophy__If_we_re_trying_to_describe_the__concept__of__ought__or_of__good___perhaps_instead_we_ought_to_be_discussing_the_prototypes__exemplars__or_theories_of__ought__or__good__", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wHjpCxeDeuFadG3jF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-29T23:28:03.438Z", "modifiedAt": null, "url": null, "title": "What I think about you", "slug": "what-i-think-about-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.547Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/skqGDwaCkiMygSJGz/what-i-think-about-you", "pageUrlRelative": "/posts/skqGDwaCkiMygSJGz/what-i-think-about-you", "linkUrl": "https://www.lesswrong.com/posts/skqGDwaCkiMygSJGz/what-i-think-about-you", "postedAtFormatted": "Tuesday, November 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20I%20think%20about%20you&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20I%20think%20about%20you%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqGDwaCkiMygSJGz%2Fwhat-i-think-about-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20I%20think%20about%20you%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqGDwaCkiMygSJGz%2Fwhat-i-think-about-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FskqGDwaCkiMygSJGz%2Fwhat-i-think-about-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">So, having heard Mike Li compare Jaynes to a thousand-year-old vampire, one question immediately popped into my mind:</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\"Do you get the same sense off me?\" I asked.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">-- Eliezer Yudkowsky, <a href=\"/lw/ua/the_level_above_mine/\">The Level Above Mine</a></p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Most people need feedback in many areas. Most people can give feedback in many areas. But for some reason I don't see a lot of actual honest feedback happening, neither in my personal life, nor at work, nor here on LW. This looks like some sort of market failure, or perhaps a bug in society.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Would we benefit from a norm that encouraged asking for feedback or critique in any area, perhaps using open threads set up specially for that? I think we would. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "skqGDwaCkiMygSJGz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 24, "extendedScore": null, "score": 8.073140689433378e-07, "legacy": true, "legacyId": "11189", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kXSETKZ3X9oidMozA"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T01:00:02.187Z", "modifiedAt": null, "url": null, "title": "Anti-Akrasia Tactics Discussion", "slug": "anti-akrasia-tactics-discussion", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:52.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E6JMPphi3nypGBXEY/anti-akrasia-tactics-discussion", "pageUrlRelative": "/posts/E6JMPphi3nypGBXEY/anti-akrasia-tactics-discussion", "linkUrl": "https://www.lesswrong.com/posts/E6JMPphi3nypGBXEY/anti-akrasia-tactics-discussion", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anti-Akrasia%20Tactics%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnti-Akrasia%20Tactics%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6JMPphi3nypGBXEY%2Fanti-akrasia-tactics-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anti-Akrasia%20Tactics%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6JMPphi3nypGBXEY%2Fanti-akrasia-tactics-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE6JMPphi3nypGBXEY%2Fanti-akrasia-tactics-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 89, "htmlBody": "<p>You should probably read the <a href=\"/lw/1sm/akrasia_tactics_review\">Anti-Akrasia Tactics Review</a>, if you haven't already. There's lots of useful stuff there, and if it works for you but you haven't read it...</p>\n<p>You should totally go read it, implement it, and not come back to this thread until you've internalized your favorite tricks.</p>\n<p>&nbsp;</p>\n<p>Feel free to discuss outcomes here.</p>\n<p>Did anything work for you?</p>\n<p>Is there anything that should be added?</p>\n<p>&nbsp;</p>\n<p>Note: this is basically a \"bump\", but I suspect that it's a worthwhile reminder. Downvote me if I'm wrong.</p>\n<p>P.S. I'd actually really enjoy feedback on this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E6JMPphi3nypGBXEY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 23, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "11190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rRmisKb45dN7DK4BW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T01:47:10.475Z", "modifiedAt": null, "url": null, "title": "Life Extension versus Replacement", "slug": "life-extension-versus-replacement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:23.083Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Julia_Galef", "createdAt": "2009-12-20T01:44:38.850Z", "isAdmin": false, "displayName": "Julia_Galef"}, "userId": "qkDSxJnyKhPCJyKdD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YEGXe6GRt3AwZHfzR/life-extension-versus-replacement", "pageUrlRelative": "/posts/YEGXe6GRt3AwZHfzR/life-extension-versus-replacement", "linkUrl": "https://www.lesswrong.com/posts/YEGXe6GRt3AwZHfzR/life-extension-versus-replacement", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Life%20Extension%20versus%20Replacement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALife%20Extension%20versus%20Replacement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEGXe6GRt3AwZHfzR%2Flife-extension-versus-replacement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Life%20Extension%20versus%20Replacement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEGXe6GRt3AwZHfzR%2Flife-extension-versus-replacement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYEGXe6GRt3AwZHfzR%2Flife-extension-versus-replacement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 377, "htmlBody": "<p>Has anyone here ever addressed the question of why we should prefer</p>\n<p>(1) Life Extension: Extend the life of an existing person 100 years<br />to <br />(2) Replacement: Create a new person who will live for 100 years?</p>\n<p><br />I've seen some discussion of how the utility of potential people fits into a utilitarian calculus. Eliezer <a href=\"/lw/17h/the_lifespan_dilemma/\">has raised the Repugnant Conclusion</a>,&nbsp;in which 1,000,000 people who each have 1 util is preferable to 1,000 people who each have 100 utils. He rejected it, he said, because he's an average utilitarian.</p>\n<p>Fine. But in my thought experiment, average utility remains unchanged. So an average utilitarian should be indifferent between Life Extension and Replacement, right? Or is the harm done by depriving an existing person of life greater in magnitude than the benefit of creating a new life of equivalent utility? If so, why?</p>\n<p>Or is the transhumanist indifferent between Life Extension and Replacement, but feels that his efforts towards radical life extension have a much greater expected value than trying to increase the birth rate?</p>\n<p>&nbsp;</p>\n<p><em>(EDITED to make the thought experiment cleaner. Originally the options were:&nbsp;(1) Life Extension: Extend the life of an existing person for 800 years, and (2) Replacement: Create 10 new people who will each live for 80 years. But that version didn't maintain equal average utility.)</em></p>\n<p><br />*Optional addendum: Gustaf Arrhenius is a philosopher who has written a lot about this subject; I found him via <a href=\"/lw/17h/the_lifespan_dilemma/4jm6\">this comment</a> by utilitymonster. Here's his 2008 paper, \"<a href=\"http://people.su.se/~guarr/Texter/Life%20Extension%20versus%20Replacement%20in%20JAP.pdf\">Life Extension versus Replacement</a>,\" which explores an amendment to utilitarianism that would allow us to prefer Life Extension. Essentially, we begin by comparing potential outcomes according to overall utility, as usual, but we then penalize outcomes if they make any existing people worse off.</p>\n<p>So even though the overall utility of Life Extension is the same as Replacement, the latter is worse, because the existing person is worse off than he would have been in Life Extension. By contrast, the potential new person is not worse off in Life Extension, because in that scenario he doesn't exist, and non-existent people can't be harmed.&nbsp;Arrhenius goes through a whole list of problems with this moral theory, however, and by the end of the paper we aren't left with anything workable that would prioritize Life Extension over Replacement.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YEGXe6GRt3AwZHfzR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 20, "extendedScore": null, "score": 8.073641125119859e-07, "legacy": true, "legacyId": "11196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 99, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9RCoE7jmmvGd5Zsh2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T03:34:03.587Z", "modifiedAt": null, "url": null, "title": "A response to \"Torture vs. Dustspeck\": The Ones Who Walk Away From Omelas", "slug": "a-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:39.548Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Logos01", "createdAt": "2011-07-21T18:59:16.270Z", "isAdmin": false, "displayName": "Logos01"}, "userId": "WZxoXCWQviJp9dNc5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTKZ5AAM8GTaLs2wM/a-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "pageUrlRelative": "/posts/jTKZ5AAM8GTaLs2wM/a-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "linkUrl": "https://www.lesswrong.com/posts/jTKZ5AAM8GTaLs2wM/a-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20response%20to%20%22Torture%20vs.%20Dustspeck%22%3A%20The%20Ones%20Who%20Walk%20Away%20From%20Omelas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20response%20to%20%22Torture%20vs.%20Dustspeck%22%3A%20The%20Ones%20Who%20Walk%20Away%20From%20Omelas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTKZ5AAM8GTaLs2wM%2Fa-response-to-torture-vs-dustspeck-the-ones-who-walk-away%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20response%20to%20%22Torture%20vs.%20Dustspeck%22%3A%20The%20Ones%20Who%20Walk%20Away%20From%20Omelas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTKZ5AAM8GTaLs2wM%2Fa-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTKZ5AAM8GTaLs2wM%2Fa-response-to-torture-vs-dustspeck-the-ones-who-walk-away", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 472, "htmlBody": "<p>For those not familiar with the topic, <a href=\"/lw/kn/torture_vs_dust_specks/\">Torture vs. Dustspecks</a> asks the question: \"<em>Would you prefer that one person be horribly tortured for fifty years without hope or rest, or that 3^^^3 people get dust specks in their eyes?</em>\"</p>\n<p>&nbsp;</p>\n<p>Most of the discussion that I have noted on the topic takes one of two assumptions in deriving their answer to that question: I think of one as the 'linear additive' answer, which says that torture is the proper choice for the utilitarian consequentialist, because a single person can only suffer so much over a fifty year window, as compared to the incomprehensible number of individuals who suffer only minutely; the other I think of as the 'logarithmically additive' answer, which inverts the answer on the grounds that forms of suffering are not equal, and cannot be added as simple 'units'.</p>\n<p>What I have never yet seen is something akin to the notion expressed in Ursula K LeGuin's <a href=\"http://harelbarzilai.org/words/omelas.txt\">The Ones Who Walk Away From Omelas</a>.If you haven't read it, I won't spoil it for you.</p>\n<p>I believe that any metric of consequence which takes into account only suffering when making the choice of \"torture\" vs. \"dust specks\" misses the point. There are consequences to such a choice that extend beyond the suffering inflicted; moral responsibility, standards of behavior that either choice makes acceptable, and so on. Any solution to the question which ignores these elements in making its decision might be useful in revealing one's views about the nature of cumulative suffering, but beyond that are of no value in making practical decisions -- they cannot be, as 'consequence' extends beyond the mere instantiation of a given choice -- the exact pain inflicted by either scenario -- into the <em>kind of society</em> that such a choice would result in.</p>\n<p>While I myself tend towards the 'logarithmic' than the 'linear' additive view of suffering, even if I stipulate the linear additive view, I still cannot agree with the conclusion of torture over the dust speck, for the same reason why I do not condone torture even in the \"ticking time bomb\" scenario: I cannot accept the culture/society that would permit such a torture to exist. To arbitrarily select out one individual for maximal suffering in order to spare others a negligible amount would require a legal or moral framework that accepted such choices, and this violates the principle of individual self-determination -- a principle I have seen Less Wrong's community spend a great deal of time trying to consider how to incorporate into Friendliness solutions for AGI. We as a society already implement something similar to this, economically: we accept taxing everyone, even according to a graduated scheme. What we do <em>not</em> accept is enslaving 20% of the population to provide for the needs of the State.</p>\n<p>If there is a flaw in my reasoning here, <a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">please enlighten me</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTKZ5AAM8GTaLs2wM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": -8, "extendedScore": null, "score": 8.07402565244946e-07, "legacy": true, "legacyId": "11199", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3wYTFWY3LKQCnAptN", "DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T04:19:48.015Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Two Cult Koans", "slug": "seq-rerun-two-cult-koans", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:33.214Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DMsS4z4ArLjhTsocc/seq-rerun-two-cult-koans", "pageUrlRelative": "/posts/DMsS4z4ArLjhTsocc/seq-rerun-two-cult-koans", "linkUrl": "https://www.lesswrong.com/posts/DMsS4z4ArLjhTsocc/seq-rerun-two-cult-koans", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Two%20Cult%20Koans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Two%20Cult%20Koans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMsS4z4ArLjhTsocc%2Fseq-rerun-two-cult-koans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Two%20Cult%20Koans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMsS4z4ArLjhTsocc%2Fseq-rerun-two-cult-koans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDMsS4z4ArLjhTsocc%2Fseq-rerun-two-cult-koans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/m4/two_cult_koans/\">Two Cult Koans</a> was originally published on 21 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Two Koans about individuals concerned that they may have joined a cult.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8mi/seq_rerun_politics_and_awful_art/\">Politics and Awful Art</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DMsS4z4ArLjhTsocc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "11201", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Qr4MB9hFRzamuMRHJ", "Cz4na5rhP5GjYh3KB", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T13:41:49.565Z", "modifiedAt": null, "url": null, "title": "The Sword of Good: I need some help to translate it", "slug": "the-sword-of-good-i-need-some-help-to-translate-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:39.331Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "loup-vaillant", "createdAt": "2011-03-23T10:39:25.887Z", "isAdmin": false, "displayName": "loup-vaillant"}, "userId": "wdoZti3BcPbJXsZ66", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wduc5MSpvSKnsdcQi/the-sword-of-good-i-need-some-help-to-translate-it", "pageUrlRelative": "/posts/wduc5MSpvSKnsdcQi/the-sword-of-good-i-need-some-help-to-translate-it", "linkUrl": "https://www.lesswrong.com/posts/wduc5MSpvSKnsdcQi/the-sword-of-good-i-need-some-help-to-translate-it", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Sword%20of%20Good%3A%20I%20need%20some%20help%20to%20translate%20it&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Sword%20of%20Good%3A%20I%20need%20some%20help%20to%20translate%20it%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwduc5MSpvSKnsdcQi%2Fthe-sword-of-good-i-need-some-help-to-translate-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Sword%20of%20Good%3A%20I%20need%20some%20help%20to%20translate%20it%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwduc5MSpvSKnsdcQi%2Fthe-sword-of-good-i-need-some-help-to-translate-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwduc5MSpvSKnsdcQi%2Fthe-sword-of-good-i-need-some-help-to-translate-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>I am currently translating Eliezer's <a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">\"The Sword of Good\"</a> in French, and hit a rather thorny problem:</p>\n<p>How do I translate the words <em>Equilibrium</em> and <em>Balance</em>, given that both words are present in this fiction?</p>\n<p>Those two words are rather synonymous, and I can find but one French translation: <em>&eacute;quilibre</em>. I need a second one, which would convey about the same ideas and sound as solemn as \"equilibrium\". Or some trick&hellip;</p>\n<p>For the majority of you who don't speak French, other English words that could have replaced either \"equilibrium\" and \"balance\" may also give me valuable cues.</p>\n<p>(By the way, translation work is way harder that I anticipated. It strains my mastery of <em>both</em> English and French way beyond what I'm used to.)</p>\n<p>Second question, of less importance: which translation do you think suits <em>The Lord of Dark</em> best? \"Le Seigneur de la Noirceur\" ? Or \"Le Seigneur des T&eacute;n&egrave;bres\"? Or even something else?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wduc5MSpvSKnsdcQi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 8.076212754457731e-07, "legacy": true, "legacyId": "11206", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-11-30T16:56:31.972Z", "modifiedAt": null, "url": null, "title": "[LINK] Extra Credits: The Singularity ", "slug": "link-extra-credits-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.899Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "C3Sc9XJXd5AsAyksf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6M6iafdwxSMjsurDg/link-extra-credits-the-singularity", "pageUrlRelative": "/posts/6M6iafdwxSMjsurDg/link-extra-credits-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/6M6iafdwxSMjsurDg/link-extra-credits-the-singularity", "postedAtFormatted": "Wednesday, November 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Extra%20Credits%3A%20The%20Singularity%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Extra%20Credits%3A%20The%20Singularity%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6M6iafdwxSMjsurDg%2Flink-extra-credits-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Extra%20Credits%3A%20The%20Singularity%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6M6iafdwxSMjsurDg%2Flink-extra-credits-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6M6iafdwxSMjsurDg%2Flink-extra-credits-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 112, "htmlBody": "<p>The show <em>Extra Credits </em>released a video today about the singularity. The show's audience is video gamers. Some of these gamers may not know about the singularity, but may become interested upon hearing about it.</p>\n<p>The video itself is a very basic, non-technical introduction to the concept of intelligence explosion. It discusses it in the context of video games. At the end, they plug the SIAI as a place to go for more info.</p>\n<p>I thought the video and the plug were pretty awesome, and wanted to share. If you think they're awesome, too, then take a second to give the video a view. Let's positively reinforce this kind of behavior. Here's the link:</p>\n<p>http://penny-arcade.com/patv/episode/the-singularity</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6M6iafdwxSMjsurDg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 17, "extendedScore": null, "score": 8.076913646122402e-07, "legacy": true, "legacyId": "11207", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T02:28:11.684Z", "modifiedAt": null, "url": null, "title": "What you think about them", "slug": "what-you-think-about-them", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.912Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YRLCd6sikj26Wr2r4/what-you-think-about-them", "pageUrlRelative": "/posts/YRLCd6sikj26Wr2r4/what-you-think-about-them", "linkUrl": "https://www.lesswrong.com/posts/YRLCd6sikj26Wr2r4/what-you-think-about-them", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20you%20think%20about%20them&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20you%20think%20about%20them%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRLCd6sikj26Wr2r4%2Fwhat-you-think-about-them%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20you%20think%20about%20them%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRLCd6sikj26Wr2r4%2Fwhat-you-think-about-them", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYRLCd6sikj26Wr2r4%2Fwhat-you-think-about-them", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>LWers who asked for personal feedback/critique in the comments to <a href=\"/lw/8mt/what_i_think_about_you/\">What I think about you</a>: Dorikka, atucker, muflax, jsalvatier, Tyrell_McAllister, TheOtherDave.</p>\n<p>LWers who asked for personal feedback at lukeprog's <a href=\"/lw/8bt/tell_me_what_you_think_of_me/\">Tell me what you think about me</a>: lukeprog, Jolly, SilasBarta, gwern.</p>\n<p>If you have met any of these LWers face-to-face, or just have some constructive criticism to offer them, I urge you to do so by any discreet channel, e.g. private messages on LW or the various feedback forms that these LWers have set up. Most of them have declared <a href=\"http://www.sl4.org/crocker.html\">Crocker's rules</a>. Please don't hesitate: you have a chance to do a lot of good.</p>\n<p>There seems to be a large unmet demand for feedback. Any ideas how to better satisfy that demand are welcome!</p>\n<p>ETA: Antisuji, endoself, John_Maxwell_IV.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YRLCd6sikj26Wr2r4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 8.078972095288196e-07, "legacy": true, "legacyId": "11218", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["skqGDwaCkiMygSJGz", "zFj67rtrQ7HEaZ45F"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T04:47:21.436Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] False Laughter", "slug": "seq-rerun-false-laughter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:38.091Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/witNDfZPFFYTBNPpJ/seq-rerun-false-laughter", "pageUrlRelative": "/posts/witNDfZPFFYTBNPpJ/seq-rerun-false-laughter", "linkUrl": "https://www.lesswrong.com/posts/witNDfZPFFYTBNPpJ/seq-rerun-false-laughter", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20False%20Laughter&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20False%20Laughter%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwitNDfZPFFYTBNPpJ%2Fseq-rerun-false-laughter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20False%20Laughter%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwitNDfZPFFYTBNPpJ%2Fseq-rerun-false-laughter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwitNDfZPFFYTBNPpJ%2Fseq-rerun-false-laughter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/m5/false_laughter/\">False Laughter</a> was originally published on 22 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Finding a blow to the hated enemy to be funny is a dangerous feeling, especially if that is the only reason why the joke is funny. Jokes should be funny on their own merits before they become deserving of laughter.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8n5/seq_rerun_two_cult_koans/\">Two Cult Koans</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "witNDfZPFFYTBNPpJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.07947333435341e-07, "legacy": true, "legacyId": "11220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NbbK6YKTpQR7u7D6u", "DMsS4z4ArLjhTsocc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T06:22:31.684Z", "modifiedAt": null, "url": null, "title": "More \"Personal\" Introductions", "slug": "more-personal-introductions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:47.660Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Q74jqAQfhtQCzspL6/more-personal-introductions", "pageUrlRelative": "/posts/Q74jqAQfhtQCzspL6/more-personal-introductions", "linkUrl": "https://www.lesswrong.com/posts/Q74jqAQfhtQCzspL6/more-personal-introductions", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20More%20%22Personal%22%20Introductions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMore%20%22Personal%22%20Introductions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ74jqAQfhtQCzspL6%2Fmore-personal-introductions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=More%20%22Personal%22%20Introductions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ74jqAQfhtQCzspL6%2Fmore-personal-introductions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQ74jqAQfhtQCzspL6%2Fmore-personal-introductions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 521, "htmlBody": "<p>One of the things I loved about studying liberal arts is that you actually got to know your professors. They would discuss their personal experiences in a topic (\"Here's what I did during the feminist movement..\"), you might get slide shows from their vacation in the country of study, or even invited to their house for a group dinner.&nbsp;<br /><br />Going into engineering was rather jarring for me in that regard. The vast majority of professors would come to class, lecture on the topic, and that would be it. They might share what their specific field of study was, but they rarely shared any personal details. It actually made it harder for me to learn, because it was like \"Who <em>is</em> this person who is talking to me?\"</p>\n<p>(I think a large part of this for me personally was because I am motivated by a desire to please, and so if I liked my professors, then I wouldn't want to inconvenience them by handing things in late, or bore them by giving them another sub-par paper to read. But that's another discussion...)<br /><br />I've noticed that Less Wrong is similar in some ways. We may know about each other's views on particular topics, and general fields of study, but we know very little about each other as <em>people</em>, unless a personal topic happens to be related to a particular rationalist study. Even the intro thread set up here focuses mainly on non-personal information.</p>\n<p>For example, a Generic Intro post right now would be something like: \"I'm X years old. From place Y. The fields I study/want to study are Z. Here's what college/HS was/is like for me. I have akrasia.\" Pretty boring, right? INSTEAD, the things I would be interested in knowing about my fellow LWers include: \"On my time off I enjoy underwater basketweaving and climbing Mt Kilamanjaro. I have 6 young daughters and a dog named Grrr. I love pesto. etc\"<br /><br />From a rational perspective, an argument could be made that it's easier to have constructive arguments that remain civil when you humanize the people you are speaking with.&nbsp;</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>I was wondering how other LWers feel on the subject. Do you like that our discussions are un-hampered by personal data? Do you like the idea of providing personal intros? Do you not want to provide personalish information for safety reasons, or because you don't think it's anyone business?</p>\n<p>If you think you might need help writing a personal intro, I wrote [a general guide](<a href=\"http://lesswrong.com/lw/8nq/more_personal_introductions/5d4e\">http://lesswrong.com/lw/8nq/more_personal_introductions/5d4e</a>)&nbsp;on the topic in the comments below.&nbsp;</p>\n<p><strong>Note: I predict there will be two types of response to this post. People discussing how they feel about this (Meta-Comments), and people giving personal introductions (Intros). To make navigating the responses easier, I am trying an experiment where I set up a meta-comment thread and a personal introduction thread.&nbsp;</strong></p>\n<p><strong>PLEASE PLACE COMMENTS ABOUT THIS IDEA IN META-COMMENT THREAD, AND COMMENTS INTRODUCING YOURSELF IN INTRO THREAD.</strong></p>\n<p>&nbsp;</p>\n<p>Edited to make it more clear to focus on personality, hobbies, likes/dislikes, and NOT on what you study, or school.<br />ETA- Added link to \"How to Write Personal Intro\" comment</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Q74jqAQfhtQCzspL6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 11, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "11222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>One of the things I loved about studying liberal arts is that you actually got to know your professors. They would discuss their personal experiences in a topic (\"Here's what I did during the feminist movement..\"), you might get slide shows from their vacation in the country of study, or even invited to their house for a group dinner.&nbsp;<br><br>Going into engineering was rather jarring for me in that regard. The vast majority of professors would come to class, lecture on the topic, and that would be it. They might share what their specific field of study was, but they rarely shared any personal details. It actually made it harder for me to learn, because it was like \"Who <em>is</em> this person who is talking to me?\"</p>\n<p>(I think a large part of this for me personally was because I am motivated by a desire to please, and so if I liked my professors, then I wouldn't want to inconvenience them by handing things in late, or bore them by giving them another sub-par paper to read. But that's another discussion...)<br><br>I've noticed that Less Wrong is similar in some ways. We may know about each other's views on particular topics, and general fields of study, but we know very little about each other as <em>people</em>, unless a personal topic happens to be related to a particular rationalist study. Even the intro thread set up here focuses mainly on non-personal information.</p>\n<p>For example, a Generic Intro post right now would be something like: \"I'm X years old. From place Y. The fields I study/want to study are Z. Here's what college/HS was/is like for me. I have akrasia.\" Pretty boring, right? INSTEAD, the things I would be interested in knowing about my fellow LWers include: \"On my time off I enjoy underwater basketweaving and climbing Mt Kilamanjaro. I have 6 young daughters and a dog named Grrr. I love pesto. etc\"<br><br>From a rational perspective, an argument could be made that it's easier to have constructive arguments that remain civil when you humanize the people you are speaking with.&nbsp;</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>I was wondering how other LWers feel on the subject. Do you like that our discussions are un-hampered by personal data? Do you like the idea of providing personal intros? Do you not want to provide personalish information for safety reasons, or because you don't think it's anyone business?</p>\n<p>If you think you might need help writing a personal intro, I wrote [a general guide](<a href=\"http://lesswrong.com/lw/8nq/more_personal_introductions/5d4e\">http://lesswrong.com/lw/8nq/more_personal_introductions/5d4e</a>)&nbsp;on the topic in the comments below.&nbsp;</p>\n<p><strong id=\"Note__I_predict_there_will_be_two_types_of_response_to_this_post__People_discussing_how_they_feel_about_this__Meta_Comments___and_people_giving_personal_introductions__Intros___To_make_navigating_the_responses_easier__I_am_trying_an_experiment_where_I_set_up_a_meta_comment_thread_and_a_personal_introduction_thread__\">Note: I predict there will be two types of response to this post. People discussing how they feel about this (Meta-Comments), and people giving personal introductions (Intros). To make navigating the responses easier, I am trying an experiment where I set up a meta-comment thread and a personal introduction thread.&nbsp;</strong></p>\n<p><strong id=\"PLEASE_PLACE_COMMENTS_ABOUT_THIS_IDEA_IN_META_COMMENT_THREAD__AND_COMMENTS_INTRODUCING_YOURSELF_IN_INTRO_THREAD_\">PLEASE PLACE COMMENTS ABOUT THIS IDEA IN META-COMMENT THREAD, AND COMMENTS INTRODUCING YOURSELF IN INTRO THREAD.</strong></p>\n<p>&nbsp;</p>\n<p>Edited to make it more clear to focus on personality, hobbies, likes/dislikes, and NOT on what you study, or school.<br>ETA- Added link to \"How to Write Personal Intro\" comment</p>", "sections": [{"title": "Note: I predict there will be two types of response to this post. People discussing how they feel about this (Meta-Comments), and people giving personal introductions (Intros). To make navigating the responses easier, I am trying an experiment where I set up a meta-comment thread and a personal introduction thread.\u00a0", "anchor": "Note__I_predict_there_will_be_two_types_of_response_to_this_post__People_discussing_how_they_feel_about_this__Meta_Comments___and_people_giving_personal_introductions__Intros___To_make_navigating_the_responses_easier__I_am_trying_an_experiment_where_I_set_up_a_meta_comment_thread_and_a_personal_introduction_thread__", "level": 1}, {"title": "PLEASE PLACE COMMENTS ABOUT THIS IDEA IN META-COMMENT THREAD, AND COMMENTS INTRODUCING YOURSELF IN INTRO THREAD.", "anchor": "PLEASE_PLACE_COMMENTS_ABOUT_THIS_IDEA_IN_META_COMMENT_THREAD__AND_COMMENTS_INTRODUCING_YOURSELF_IN_INTRO_THREAD_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "128 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 128, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T06:56:51.233Z", "modifiedAt": null, "url": null, "title": "Intuitive Explanation of Solomonoff Induction", "slug": "intuitive-explanation-of-solomonoff-induction", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:27.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p3b5T8YbijEXDAfmR/intuitive-explanation-of-solomonoff-induction", "pageUrlRelative": "/posts/p3b5T8YbijEXDAfmR/intuitive-explanation-of-solomonoff-induction", "linkUrl": "https://www.lesswrong.com/posts/p3b5T8YbijEXDAfmR/intuitive-explanation-of-solomonoff-induction", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intuitive%20Explanation%20of%20Solomonoff%20Induction&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntuitive%20Explanation%20of%20Solomonoff%20Induction%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3b5T8YbijEXDAfmR%2Fintuitive-explanation-of-solomonoff-induction%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intuitive%20Explanation%20of%20Solomonoff%20Induction%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3b5T8YbijEXDAfmR%2Fintuitive-explanation-of-solomonoff-induction", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp3b5T8YbijEXDAfmR%2Fintuitive-explanation-of-solomonoff-induction", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3138, "htmlBody": "<p><strong>Update</strong>: Alex Altair has now <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">finished this tutorial</a>!</p>\n<p>&nbsp;</p>\n<p>A while ago I began writing a Solomonoff Induction tutorial, but now it's one of those <a href=\"/lw/85d/11_less_wrong_articles_i_probably_will_never_have/\">Less Wrong articles I probably will never have time to write</a>.</p>\n<p>But, maybe a few of you can take what I've started, team up, and finish the thing. I'm basically just following along with the Solomonoff induction tutorials from Shane Legg (<a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">2008</a>, <a href=\"http://www.vetta.org/documents/disSol.pdf\">2004</a>), but making them simpler and readable by a wider audience. I think this would be a valuable thing for Less Wrong to have, and one that would draw even more traffic to our community, but I don't have time to write it myself anymore!&nbsp;</p>\n<p>Who wants to be a hero and finish this thing? The original <a href=\"http://daringfireball.net/projects/markdown/\">markdown</a> source is available <a href=\"https://docs.google.com/document/pub?id=19myqT6YjQPGU25s2RYvRaEr8Bx4F6CeoJER9RGDyg7Y\">here</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>This is a tutorial for beginners. Those familiar with Solomonoff induction may prefer to read <a href=\"http://arxiv.org/pdf/1105.5721v1\">Rathmanner &amp; Hutter (2011)</a>.</small></p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/intuitive-explanation-of-solomonoff-induction.png\" alt=\"\" /></p>\n<p>People disagree about things. Some say that vaccines cause autism; others say they don't. Some say everything is physical; others believe in a god. Some say that complicated financial derivatives are essential to a modern competitive economy; others think a nation's economy will do better without them. It's hard to know what is true.</p>\n<p>And it's hard to know <em>how to figure out</em> what is true. Some think you should assume the things you are most certain about and then deduce all other beliefs from your original beliefs. Others think you should accept at face value the most intuitive explanations of your personal experience. Others think you should generally agree with the scientific consensus until it is disproven.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/detective.jpg\" alt=\"\" hspace=\"20\" />Wouldn't it be nice if finding out what's true was like baking a cake? What if there was a <em>recipe</em> for finding out what is true? All you'd have to do is <em>follow the written instruction exactly</em>, and after the last instruction you'd inevitably find yourself with some sweet, tasty <em>truth</em>!</p>\n<p>As it turns out, there <em>is</em> an exact recipe for finding truth. It was discovered in the 1960s.</p>\n<p>The problem is that you don't have <em>time</em> to follow the recipe. To find the truth to even a simple question using this recipe would require you to follow one step after another until long after the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">heat death</a> of the universe, and you can't do <em>that</em>.</p>\n<p>But we can find shortcuts. Suppose you know the <em>exact</em> recipe for baking a cake requires you to count out one molecule of H2O at a time until you have <em>exactly</em> 0.5 cups of water. If you did that, you might not finish before the heat death of the universe. But you could <em>approximate</em> that part of the recipe by measuring out something very close to 0.5 cups of water, and you'd probably still end up with a pretty good cake.</p>\n<p>Similarly, once we know the exact recipe for finding truth, we can try to <em>approximate</em> it in a way that allows us to finish all the steps sometime <em>before</em> the heat death of the universe.</p>\n<p>This tutorial explains the exact recipe for finding truth, <a href=\"http://www.wisegeek.com/what-is-solomonoff-induction.htm\">Solomonoff induction</a>, and also explains some attempts to approximate it in ways that allow us to figure out <em>now</em> what is (probably) true. Fear not; I shall not assume you know anything beyond grade-school math.</p>\n<p>Like my <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Crash Course in the Neuroscience of Human Motivation</a>, this tutorial is <em>long</em>. You may not have time for it; that's fine. But if you <em>do</em> read it, I recommend you read it in sections.</p>\n<p><strong>Contents</strong>:</p>\n<ol>\n<li><a href=\"#algorithms\">Algorithms</a></li>\n<li><a href=\"#induction\">Induction</a></li>\n<li><a href=\"#occam\">Occam&rsquo;s Razor</a></li>\n<li><a href=\"#updating\">Updating Beliefs</a></li>\n<li><a href=\"#priors\">The Problem of Priors</a></li>\n<li><a href=\"#binary\">Binary Sequence Prediction</a></li>\n<li><a href=\"#solomonoff\">Solomonoff and Kolmogorov</a></li>\n<li><a href=\"#lightsaber\">Solomonoff's Lightsaber</a></li>\n<li><a href=\"#approximations\">Approximations</a></li>\n<li><a href=\"#philosophy\">Philosophical Implications</a></li>\n</ol>\n<h3 id=\"algorithms\">Algorithms</h3>\n<p>At an early age you learned a set of precisely-defined steps &mdash; a 'recipe' or, more formally, an <em>algorithm</em> &mdash; that you could use to find the largest number in an unsorted list of numbers like this:</p>\n<blockquote>\n<p>21, 18, 4, 19, 55, 12, 30</p>\n</blockquote>\n<p>The algorithm you learned probably looked something like this:</p>\n<ol>\n<li>Note the leftmost item as the largest you've seen in this list so far. If this is the only item on the list, output it as the largest number on the list. Otherwise, proceed to step 2.</li>\n<li>Look at the next item. If it is larger than the largest item noted so far, note it as the largest you've seen in this list so far. Proceed to step 3.</li>\n<li>If you have not reached the end of the list, return to step 2. Otherwise, output the last noted item as the largest number in the list.</li>\n</ol>\n<p>Other algorithms could be used to solve the same problem. For example, you could work your way from right to left instead of from left to right. But the point is that if you follow this algorithm exactly, and you have enough time to complete the task, you can't <em>fail</em> to solve the problem. You can't get confused about what one of the steps means or what the next step is. Every instruction tells you exactly what to do next, all the way through to the answer.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/File:Euclid_flowchart_1.png\"><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Euclid.png\" alt=\"\" width=\"330\" height=\"742\" /></a>You probably learned other algorithms, too, like how to find the greatest common divisor of any two integers (see image on right).</p>\n<p>But not just <em>any</em> set of instructions is a precisely-defined <em>algorithm</em>. Sometimes, instructions are unclear or incomplete. Consider the following instructions based on <a href=\"http://science.howstuffworks.com/innovation/scientific-experiments/scientific-method6.htm\">an article</a> about the scientific method:</p>\n<ol>\n<li>Make an observation.</li>\n<li>Form a hypothesis that explains the observation.</li>\n<li>Conduct an experiment that will test the hypothesis.</li>\n<li>If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</li>\n</ol>\n<p>This is not an algorithm.</p>\n<p>First, many of the terms are not clearly defined. What counts as an observation? What counts as a hypothesis? What would a hypothesis need to be like in order to &lsquo;explain&rsquo; the observation? What counts as an experiment that will &lsquo;test&rsquo; the hypothesis? What does it mean for experimental results to &lsquo;confirm&rsquo; or &lsquo;disconfirm&rsquo; a hypothesis?</p>\n<p>Second, the instructions may be incomplete. What do we do if we reach step 4 and the experimental results neither &lsquo;confirm&rsquo; nor &lsquo;disconfirm&rsquo; the hypothesis under consideration, but instead are in some sense &lsquo;neutral&rsquo; toward the hypothesis? These instructions don&rsquo;t tell us what to do in that case.</p>\n<p>An algorithm is a well-defined procedure that takes some value or values as input and, after a finite series of steps, generates some value or values as output.</p>\n<p>For example, the &lsquo;find the largest number&rsquo; algorithm above could take the input {21, 18, 4, 19, 55, 12, 30} and would, after 13 steps, produce the following output: {55}. Or it could take the input {34} and, after 1 step, produce the output: {34}.</p>\n<p>What we&rsquo;re looking for is a precise algorithm that will produce truth as its output.</p>\n<h3 id=\"induction\">Induction</h3>\n<blockquote>\n<p>Whether we are a detective trying to catch a thief, a scientist trying to discover a new physical law, or a businessman attempting to understand a recent change in demand, we are all in the process of collecting information and trying to infer the underlying causes.<sup>a</sup></p>\n</blockquote>\n<p>The problem of inference is this: We have a collection of observations (data), and we have a collection of hypotheses about the underlying causes of those observations. We&rsquo;d like to know which hypothesis is correct, so we can use that knowledge to predict future events.</p>\n<p>Suppose your data concern a large set of stock market price changes and other events in the world. You&rsquo;d like to know the processes responsible for the stock market price changes, because then you can predict what the stock market will do in the future, and make some money.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/missing-cookies.jpg\" alt=\"\" hspace=\"20\" />Or, suppose you are a parent. You come home from work to find a chair propped against the refrigerator, with the cookie jar atop the fridge a bit emptier than before. One hypothesis that leaps to mind is that your young daughter used the chair to reach the cookies. However, many other hypothesis explain the data. Perhaps a very short thief broke into your home and stole some cookies. Perhaps your daughter put the chair in front of the fridge because the fridge door is broken and no longer stays shut, and you forgot that your friend ate a few cookies when he visited last night. Perhaps you moved the chair and ate the cookies yourself while sleepwalking the night before.</p>\n<p>All these hypotheses are possible, but intuitively it seems like some hypotheses are more likely than others. If you&rsquo;ve seen your daughter access the cookies this way before but have never been burgled, then the &lsquo;daughter hypothesis&rsquo; seems more plausible. If some expensive things from your bedroom and living room are missing and there is hateful graffiti on your door at the eye level of a very short person, then then &lsquo;short thief&rsquo; hypothesis seems more plausible than before. If you suddenly remember that your friend ate a few cookies and broke the fridge door last night, the &lsquo;broken fridge door&rsquo; hypothesis gains credibility. If you&rsquo;ve never been burgled and your daughter is out of town and you have a habit of moving and eating things while sleepwalking, the &lsquo;sleepwalking&rsquo; hypothesis is less bizarre.</p>\n<p>But the weight you give to each hypothesis depends greatly on your prior knowledge. What if you had just been hit on the head and lost all past memories, and for some reason the most urgent thing you wanted to do was to solve the mystery of the chair in front of the fridge door? <em>Then</em> how would weigh the likelihood of the available hypotheses?</p>\n<h3 id=\"occam\">Occam&rsquo;s Razor</h3>\n<p>Consider a different inductive problem. A computer program outputs the following sequence of numbers:</p>\n<blockquote>\n<p>1, 3, 5, 7</p>\n</blockquote>\n<p>Which number comes next? If you guess correctly, you&rsquo;ll win $500.</p>\n<p>In order to predict the next number in the sequence, you make a hypothesis about the process the computer is using to generate these numbers. One obvious hypothesis is that it is simply listing all the odd numbers in ascending order from 1. If that&rsquo;s true, you should guess 9 to win the $500.</p>\n<p>But perhaps the computer is using a different algorithm to generate the numbers. Suppose that <em>n</em> is the step in the sequence, so that n=1 when it generated &lsquo;1&rsquo;, n=2 when it generated &lsquo;3&rsquo;, and so on. Maybe the computer used this equation to calculate each number in the sequence:</p>\n<blockquote>\n<p>2n &minus; 1 + (n &minus; 1)(n &minus; 2)(n &minus; 3)(n &minus; 4)</p>\n</blockquote>\n<p>If so, the next number in the sequence will be 33. (Go ahead, <a href=\"http://www.wolframalpha.com/\">check</a> the calculations.) But doesn&rsquo;t the first hypothesis seem more likely?</p>\n<p>The principle behind this intuition, which goes back to <a href=\"http://en.wikipedia.org/wiki/William_of_Ockham\">William of Occam</a>, could be stated:</p>\n<blockquote>\n<p>Among all hypotheses consistent with the observations, the simplest is the most likely.</p>\n</blockquote>\n<p>The principle is called <a href=\"http://en.wikipedia.org/wiki/Occam%27s_razor\">Occam&rsquo;s razor</a> because it &lsquo;shaves away&rsquo; unnecessary assumptions.</p>\n<p>For example, think about the case of the missing cookies again. In most cases, the &lsquo;daughter&rsquo; hypothesis seems to make fewer unnecessary assumptions than the &lsquo;short thief&rsquo; hypothesis does. You already know you have a daughter that likes cookies and knows how to move chairs to reach cookies. But in order for the short thief hypothesis to be plausible, you have to assume that (1) a thief found a way to break into your home, that (2) the thief wanted cookies more than anything else from your home, that (3) the thief was, unusually, too short to reach the top of the fridge without the help of a chair, and many other unnecessary assumptions.</p>\n<p>Occam&rsquo;s razor <em>sounds</em> right, but can it be made more precise, and can it be justified? We will return to those questions later. For now, let us consider another important part of induction, that of updating our beliefs in response to new evidence.</p>\n<h3 id=\"updating\">Updating Beliefs</h3>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/soldier-in-trenches.png\" alt=\"\" hspace=\"20\" />You&rsquo;re a soldier in combat, crouching in a trench. You know for sure there is just one enemy soldier left on the battlefield, about 400 yards away. You also know that if the remaining enemy is a regular army troop, there&rsquo;s only a small chance he could hit you with one shot from that distance. But if the remaining enemy is a sniper, then there&rsquo;s a very good chance he can hit you with one shot from that distance. But snipers are rare, so it&rsquo;s probably just a regular army troop.</p>\n<p>You peek your head out of the trench, trying to get a better look.</p>\n<p><em>Bam!</em> A bullet glance off your helmet and you duck down again.</p>\n<p>&ldquo;Okay,&rdquo; you think. &ldquo;I know snipers are rare, but that guy just hit me with a bullet from 400 yards away. I suppose it <em>might</em> still be a regular army troop, but there&rsquo;s a seriously good chance it&rsquo;s a sniper, since he hit me from that far away.&rdquo;</p>\n<p>After another minute, you dare to take another look, and peek your head out of the trench again.</p>\n<p><em>Bam!</em> Another bullet glances off your helmet! You duck down again.</p>\n<p>&ldquo;Damn,&rdquo; you think. &ldquo;It&rsquo;s definitely a sniper. No matter how rare snipers are, there&rsquo;s no way that guy just hit me <em>twice in a row</em> from that distance if he&rsquo;s a regular army troop. He&rsquo;s gotta be a sniper. I&rsquo;d better call for support.&rdquo;</p>\n<p>This is an example of updating beliefs in response to evidence, and we do it all the time.</p>\n<p>You start with some <em>prior</em> beliefs, and all of them are uncertain. You are 99.99% certain the Earth revolves around the sun, 90% confident your best friend will attend your birthday party, and 40% sure that the song on the radio you&rsquo;re listening to was played by The Turtles.</p>\n<p>Then, you encounter new evidence &mdash; new observations &mdash; and update your beliefs in response.</p>\n<p>Suppose you start out 85% confident the one remaining enemy soldier is not a sniper, leaving only 15% credence to the hypothesis that he <em>is</em> a sniper. But then, a bullet glances off your helmet &mdash; an event far more likely if the enemy soldier is a sniper than if he is not. So now you&rsquo;re only 40% confident he&rsquo;s not a sniper, and 60% confident he <em>is</em>. Another bullet glances off your helmet, and you update again. Now you&rsquo;re only 2% confident he&rsquo;s not a sniper, and 98% confident he <em>is</em> a sniper.</p>\n<p>Now, I&rsquo;m about to show you some <em>math</em>, but don&rsquo;t run away. The math isn&rsquo;t <em>supposed</em> to make sense if you haven&rsquo;t studied it. Don&rsquo;t worry; I&rsquo;m going to explain it all.</p>\n<p>There&rsquo;s a theorem in probability theory that tells you how likely one observation is given some other observations. It&rsquo;s called Bayes&rsquo; Theorem.</p>\n<p>At this point you may want to take a break and read either <a href=\"http://yudkowsky.net/rational/bayes\">tutorial #1</a>, <a href=\"http://commonsenseatheism.com/?p=13156\">tutorial #2</a>, <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">tutorial #3</a>, or <a href=\"http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">tutorial #4</a> on Bayes&rsquo; Theorem. I&rsquo;ll only say a <em>little</em> bit more about Bayes&rsquo; Theorem in this tutorial.</p>\n<p>The short form of Bayes&rsquo; Theorem looks like this:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/eq_bayes_reduced.png\" alt=\"\" /></p>\n<p>Let&rsquo;s unpack this. The H refers to some hypothesis, and the E responds to some evidence. p(x) refers to the probability of x. The pipe symbol | means &lsquo;given&rsquo; or &lsquo;assuming&rsquo;. So, Bayes&rsquo; Theorem reads:</p>\n<blockquote>\n<p>[The probability of hypothesis H given evidence E] is equal to [the probability of evidence E given hypothesis H] times [the probability of hypothesis H] divided by [the probability of evidence E].</p>\n</blockquote>\n<p>You can see how this tells us what we&rsquo;d like to know. We want to know the probability of some hypothesis &mdash; some model of the world that, if correct, will allow us to successfully predict the future &mdash; given the evidence that we have. And that&rsquo;s what Bayes&rsquo; Theorem tells us. Now we just plug the numbers in, and solve for p(H|E).</p>\n<p>Of course, it&rsquo;s not easy to &ldquo;just plug the numbers in.&rdquo; You aren&rsquo;t an all-knowing god. You don&rsquo;t know <em>exactly</em> how likely it is that the enemy soldier would hit your helmet if he&rsquo;s a sniper, compared to how likely that is if he&rsquo;s not a sniper. But you can do your best. With enough evidence, it will become overwhelmingly clear which hypothesis is correct.</p>\n<p>At this point, I'll let the tutorials on Bayes' Theorem above do the heavy lifting on how to update beliefs. Let me get back to the part of induction those tutorials <em>don't</em> explain: the choice of priors.</p>\n<h3 id=\"priors\">The Problem of Priors</h3>\n<p>In the example above where you're a soldier in combat, I gave you your starting probabilities: 85% confidence that the enemy soldier was a sniper, and 15% confidence he was not. But what if you don't know your \"priors\"? What then?</p>\n<p>When using Bayes' Theorem to calculate your probabilities, your choice of prior can influence your results greatly. But how can we determine the probability of a hypothesis <em>before</em> seeing any data? What does that even <em>mean</em>?</p>\n<p>Legg (2008) explains:</p>\n<blockquote>\n<p>Bayesians respond to this in a number of ways. Firstly, they point out that the problem is generally small, in the sense that with a reasonable prior and quantity of data, the posterior distribution P(h|D) depends almost entirely on D rather than the chosen prior P(h). In fact on any sizable data set, not only does the choice of prior not especially matter, but Bayesian and classical statistical methods typically produce similar results, as one would expect. It is only with relatively small data sets or complex models that the choice of prior becomes an issue.</p>\n</blockquote>\n<blockquote>\n<p>If classical statistical methods could avoid the problem of prior bias when dealing with small data sets then this would be a significant argument in their favour. However Bayesians argue that all systems of inductive inference that obey some basic consistency principles define, either explicitly or implicitly, a prior distribution over hypotheses. Thus, methods from classical statistics make assumptions that are in effect equivalent to defining a prior. The difference is that in Bayesian statistics these assumptions take the form of an explicit prior distribution. In other words, it is not that prior bias in Bayesian statistics is necessarily any better or worse than in classical statistics, it is simply more transparent.</p>\n</blockquote>\n<p>But this doesn't <em>solve</em> our problem. It merely points out that other statistical techniques don't fare any better.</p>\n<p>What we'd like is to reduce the potential for abusing one's opportunity to select priors, and to use agreed-upon priors when possible. Thus, many standard \"prior distributions\" have been developed. Generally, they distribute probability equally across hypotheses.</p>\n<p>But to solve the problem of priors once and for all, we'd like to have an acceptable, <em>universal</em> prior distribution, so that there's no fuzziness about the process of induction. We need a <em>recipe</em>, and <em>algorithm</em>, for finding out the truth. And there can't be any fuzziness in an algorithm.</p>\n<h3 id=\"binary\">Binary Sequence Prediction</h3>\n<p>[intro to binary sequence prediction]</p>\n<h3 id=\"solomonoff\">Solomonoff and Kolmogorov</h3>\n<p>[summarize the contributions of Solomonoff and Kolmogorov]</p>\n<h3 id=\"lightsaber\">Solomonoff Lightsaber</h3>\n<p>[explain the result: Solomonoff Induction]</p>\n<h3 id=\"approximations\">Approximations</h3>\n<p>[survey a few of the approximations of Solomonoff Induction in use today]</p>\n<h3 id=\"philosophy\">Philosophical Implications</h3>\n<p>[survey of some philosophical implications of unviersal induction]</p>\n<h3>Notes</h3>\n<p><sup>a</sup> This quote and some of the examples to follow are from Legg (2008).</p>\n<h3>References</h3>\n<p>Legg (2008). <em><a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">Machine Superintelligence</a></em>. PhD thesis, Department of Informatics, University of Lugano.</p>\n<p>Rathmanner &amp; Hutter (2011). <a href=\"/arxiv.org/pdf/1105.5721v1\">A philosophical treatise of universal induction</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p3b5T8YbijEXDAfmR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 14, "extendedScore": null, "score": 8.079938075486939e-07, "legacy": true, "legacyId": "11223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Update</strong>: Alex Altair has now <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">finished this tutorial</a>!</p>\n<p>&nbsp;</p>\n<p>A while ago I began writing a Solomonoff Induction tutorial, but now it's one of those <a href=\"/lw/85d/11_less_wrong_articles_i_probably_will_never_have/\">Less Wrong articles I probably will never have time to write</a>.</p>\n<p>But, maybe a few of you can take what I've started, team up, and finish the thing. I'm basically just following along with the Solomonoff induction tutorials from Shane Legg (<a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">2008</a>, <a href=\"http://www.vetta.org/documents/disSol.pdf\">2004</a>), but making them simpler and readable by a wider audience. I think this would be a valuable thing for Less Wrong to have, and one that would draw even more traffic to our community, but I don't have time to write it myself anymore!&nbsp;</p>\n<p>Who wants to be a hero and finish this thing? The original <a href=\"http://daringfireball.net/projects/markdown/\">markdown</a> source is available <a href=\"https://docs.google.com/document/pub?id=19myqT6YjQPGU25s2RYvRaEr8Bx4F6CeoJER9RGDyg7Y\">here</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><small>This is a tutorial for beginners. Those familiar with Solomonoff induction may prefer to read <a href=\"http://arxiv.org/pdf/1105.5721v1\">Rathmanner &amp; Hutter (2011)</a>.</small></p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/intuitive-explanation-of-solomonoff-induction.png\" alt=\"\"></p>\n<p>People disagree about things. Some say that vaccines cause autism; others say they don't. Some say everything is physical; others believe in a god. Some say that complicated financial derivatives are essential to a modern competitive economy; others think a nation's economy will do better without them. It's hard to know what is true.</p>\n<p>And it's hard to know <em>how to figure out</em> what is true. Some think you should assume the things you are most certain about and then deduce all other beliefs from your original beliefs. Others think you should accept at face value the most intuitive explanations of your personal experience. Others think you should generally agree with the scientific consensus until it is disproven.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/detective.jpg\" alt=\"\" hspace=\"20\">Wouldn't it be nice if finding out what's true was like baking a cake? What if there was a <em>recipe</em> for finding out what is true? All you'd have to do is <em>follow the written instruction exactly</em>, and after the last instruction you'd inevitably find yourself with some sweet, tasty <em>truth</em>!</p>\n<p>As it turns out, there <em>is</em> an exact recipe for finding truth. It was discovered in the 1960s.</p>\n<p>The problem is that you don't have <em>time</em> to follow the recipe. To find the truth to even a simple question using this recipe would require you to follow one step after another until long after the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">heat death</a> of the universe, and you can't do <em>that</em>.</p>\n<p>But we can find shortcuts. Suppose you know the <em>exact</em> recipe for baking a cake requires you to count out one molecule of H2O at a time until you have <em>exactly</em> 0.5 cups of water. If you did that, you might not finish before the heat death of the universe. But you could <em>approximate</em> that part of the recipe by measuring out something very close to 0.5 cups of water, and you'd probably still end up with a pretty good cake.</p>\n<p>Similarly, once we know the exact recipe for finding truth, we can try to <em>approximate</em> it in a way that allows us to finish all the steps sometime <em>before</em> the heat death of the universe.</p>\n<p>This tutorial explains the exact recipe for finding truth, <a href=\"http://www.wisegeek.com/what-is-solomonoff-induction.htm\">Solomonoff induction</a>, and also explains some attempts to approximate it in ways that allow us to figure out <em>now</em> what is (probably) true. Fear not; I shall not assume you know anything beyond grade-school math.</p>\n<p>Like my <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">Crash Course in the Neuroscience of Human Motivation</a>, this tutorial is <em>long</em>. You may not have time for it; that's fine. But if you <em>do</em> read it, I recommend you read it in sections.</p>\n<p><strong>Contents</strong>:</p>\n<ol>\n<li><a href=\"#algorithms\">Algorithms</a></li>\n<li><a href=\"#induction\">Induction</a></li>\n<li><a href=\"#occam\">Occam\u2019s Razor</a></li>\n<li><a href=\"#updating\">Updating Beliefs</a></li>\n<li><a href=\"#priors\">The Problem of Priors</a></li>\n<li><a href=\"#binary\">Binary Sequence Prediction</a></li>\n<li><a href=\"#solomonoff\">Solomonoff and Kolmogorov</a></li>\n<li><a href=\"#lightsaber\">Solomonoff's Lightsaber</a></li>\n<li><a href=\"#approximations\">Approximations</a></li>\n<li><a href=\"#philosophy\">Philosophical Implications</a></li>\n</ol>\n<h3 id=\"Algorithms\">Algorithms</h3>\n<p>At an early age you learned a set of precisely-defined steps \u2014 a 'recipe' or, more formally, an <em>algorithm</em> \u2014 that you could use to find the largest number in an unsorted list of numbers like this:</p>\n<blockquote>\n<p>21, 18, 4, 19, 55, 12, 30</p>\n</blockquote>\n<p>The algorithm you learned probably looked something like this:</p>\n<ol>\n<li>Note the leftmost item as the largest you've seen in this list so far. If this is the only item on the list, output it as the largest number on the list. Otherwise, proceed to step 2.</li>\n<li>Look at the next item. If it is larger than the largest item noted so far, note it as the largest you've seen in this list so far. Proceed to step 3.</li>\n<li>If you have not reached the end of the list, return to step 2. Otherwise, output the last noted item as the largest number in the list.</li>\n</ol>\n<p>Other algorithms could be used to solve the same problem. For example, you could work your way from right to left instead of from left to right. But the point is that if you follow this algorithm exactly, and you have enough time to complete the task, you can't <em>fail</em> to solve the problem. You can't get confused about what one of the steps means or what the next step is. Every instruction tells you exactly what to do next, all the way through to the answer.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/File:Euclid_flowchart_1.png\"><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Euclid.png\" alt=\"\" width=\"330\" height=\"742\"></a>You probably learned other algorithms, too, like how to find the greatest common divisor of any two integers (see image on right).</p>\n<p>But not just <em>any</em> set of instructions is a precisely-defined <em>algorithm</em>. Sometimes, instructions are unclear or incomplete. Consider the following instructions based on <a href=\"http://science.howstuffworks.com/innovation/scientific-experiments/scientific-method6.htm\">an article</a> about the scientific method:</p>\n<ol>\n<li>Make an observation.</li>\n<li>Form a hypothesis that explains the observation.</li>\n<li>Conduct an experiment that will test the hypothesis.</li>\n<li>If the experimental results disconfirm the hypothesis, return to step #2 and form a hypothesis not yet used. If the experimental results confirm the hypothesis, provisionally accept the hypothesis.</li>\n</ol>\n<p>This is not an algorithm.</p>\n<p>First, many of the terms are not clearly defined. What counts as an observation? What counts as a hypothesis? What would a hypothesis need to be like in order to \u2018explain\u2019 the observation? What counts as an experiment that will \u2018test\u2019 the hypothesis? What does it mean for experimental results to \u2018confirm\u2019 or \u2018disconfirm\u2019 a hypothesis?</p>\n<p>Second, the instructions may be incomplete. What do we do if we reach step 4 and the experimental results neither \u2018confirm\u2019 nor \u2018disconfirm\u2019 the hypothesis under consideration, but instead are in some sense \u2018neutral\u2019 toward the hypothesis? These instructions don\u2019t tell us what to do in that case.</p>\n<p>An algorithm is a well-defined procedure that takes some value or values as input and, after a finite series of steps, generates some value or values as output.</p>\n<p>For example, the \u2018find the largest number\u2019 algorithm above could take the input {21, 18, 4, 19, 55, 12, 30} and would, after 13 steps, produce the following output: {55}. Or it could take the input {34} and, after 1 step, produce the output: {34}.</p>\n<p>What we\u2019re looking for is a precise algorithm that will produce truth as its output.</p>\n<h3 id=\"Induction\">Induction</h3>\n<blockquote>\n<p>Whether we are a detective trying to catch a thief, a scientist trying to discover a new physical law, or a businessman attempting to understand a recent change in demand, we are all in the process of collecting information and trying to infer the underlying causes.<sup>a</sup></p>\n</blockquote>\n<p>The problem of inference is this: We have a collection of observations (data), and we have a collection of hypotheses about the underlying causes of those observations. We\u2019d like to know which hypothesis is correct, so we can use that knowledge to predict future events.</p>\n<p>Suppose your data concern a large set of stock market price changes and other events in the world. You\u2019d like to know the processes responsible for the stock market price changes, because then you can predict what the stock market will do in the future, and make some money.</p>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/missing-cookies.jpg\" alt=\"\" hspace=\"20\">Or, suppose you are a parent. You come home from work to find a chair propped against the refrigerator, with the cookie jar atop the fridge a bit emptier than before. One hypothesis that leaps to mind is that your young daughter used the chair to reach the cookies. However, many other hypothesis explain the data. Perhaps a very short thief broke into your home and stole some cookies. Perhaps your daughter put the chair in front of the fridge because the fridge door is broken and no longer stays shut, and you forgot that your friend ate a few cookies when he visited last night. Perhaps you moved the chair and ate the cookies yourself while sleepwalking the night before.</p>\n<p>All these hypotheses are possible, but intuitively it seems like some hypotheses are more likely than others. If you\u2019ve seen your daughter access the cookies this way before but have never been burgled, then the \u2018daughter hypothesis\u2019 seems more plausible. If some expensive things from your bedroom and living room are missing and there is hateful graffiti on your door at the eye level of a very short person, then then \u2018short thief\u2019 hypothesis seems more plausible than before. If you suddenly remember that your friend ate a few cookies and broke the fridge door last night, the \u2018broken fridge door\u2019 hypothesis gains credibility. If you\u2019ve never been burgled and your daughter is out of town and you have a habit of moving and eating things while sleepwalking, the \u2018sleepwalking\u2019 hypothesis is less bizarre.</p>\n<p>But the weight you give to each hypothesis depends greatly on your prior knowledge. What if you had just been hit on the head and lost all past memories, and for some reason the most urgent thing you wanted to do was to solve the mystery of the chair in front of the fridge door? <em>Then</em> how would weigh the likelihood of the available hypotheses?</p>\n<h3 id=\"Occam_s_Razor\">Occam\u2019s Razor</h3>\n<p>Consider a different inductive problem. A computer program outputs the following sequence of numbers:</p>\n<blockquote>\n<p>1, 3, 5, 7</p>\n</blockquote>\n<p>Which number comes next? If you guess correctly, you\u2019ll win $500.</p>\n<p>In order to predict the next number in the sequence, you make a hypothesis about the process the computer is using to generate these numbers. One obvious hypothesis is that it is simply listing all the odd numbers in ascending order from 1. If that\u2019s true, you should guess 9 to win the $500.</p>\n<p>But perhaps the computer is using a different algorithm to generate the numbers. Suppose that <em>n</em> is the step in the sequence, so that n=1 when it generated \u20181\u2019, n=2 when it generated \u20183\u2019, and so on. Maybe the computer used this equation to calculate each number in the sequence:</p>\n<blockquote>\n<p>2n \u2212 1 + (n \u2212 1)(n \u2212 2)(n \u2212 3)(n \u2212 4)</p>\n</blockquote>\n<p>If so, the next number in the sequence will be 33. (Go ahead, <a href=\"http://www.wolframalpha.com/\">check</a> the calculations.) But doesn\u2019t the first hypothesis seem more likely?</p>\n<p>The principle behind this intuition, which goes back to <a href=\"http://en.wikipedia.org/wiki/William_of_Ockham\">William of Occam</a>, could be stated:</p>\n<blockquote>\n<p>Among all hypotheses consistent with the observations, the simplest is the most likely.</p>\n</blockquote>\n<p>The principle is called <a href=\"http://en.wikipedia.org/wiki/Occam%27s_razor\">Occam\u2019s razor</a> because it \u2018shaves away\u2019 unnecessary assumptions.</p>\n<p>For example, think about the case of the missing cookies again. In most cases, the \u2018daughter\u2019 hypothesis seems to make fewer unnecessary assumptions than the \u2018short thief\u2019 hypothesis does. You already know you have a daughter that likes cookies and knows how to move chairs to reach cookies. But in order for the short thief hypothesis to be plausible, you have to assume that (1) a thief found a way to break into your home, that (2) the thief wanted cookies more than anything else from your home, that (3) the thief was, unusually, too short to reach the top of the fridge without the help of a chair, and many other unnecessary assumptions.</p>\n<p>Occam\u2019s razor <em>sounds</em> right, but can it be made more precise, and can it be justified? We will return to those questions later. For now, let us consider another important part of induction, that of updating our beliefs in response to new evidence.</p>\n<h3 id=\"Updating_Beliefs\">Updating Beliefs</h3>\n<p><img style=\"float: right;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/soldier-in-trenches.png\" alt=\"\" hspace=\"20\">You\u2019re a soldier in combat, crouching in a trench. You know for sure there is just one enemy soldier left on the battlefield, about 400 yards away. You also know that if the remaining enemy is a regular army troop, there\u2019s only a small chance he could hit you with one shot from that distance. But if the remaining enemy is a sniper, then there\u2019s a very good chance he can hit you with one shot from that distance. But snipers are rare, so it\u2019s probably just a regular army troop.</p>\n<p>You peek your head out of the trench, trying to get a better look.</p>\n<p><em>Bam!</em> A bullet glance off your helmet and you duck down again.</p>\n<p>\u201cOkay,\u201d you think. \u201cI know snipers are rare, but that guy just hit me with a bullet from 400 yards away. I suppose it <em>might</em> still be a regular army troop, but there\u2019s a seriously good chance it\u2019s a sniper, since he hit me from that far away.\u201d</p>\n<p>After another minute, you dare to take another look, and peek your head out of the trench again.</p>\n<p><em>Bam!</em> Another bullet glances off your helmet! You duck down again.</p>\n<p>\u201cDamn,\u201d you think. \u201cIt\u2019s definitely a sniper. No matter how rare snipers are, there\u2019s no way that guy just hit me <em>twice in a row</em> from that distance if he\u2019s a regular army troop. He\u2019s gotta be a sniper. I\u2019d better call for support.\u201d</p>\n<p>This is an example of updating beliefs in response to evidence, and we do it all the time.</p>\n<p>You start with some <em>prior</em> beliefs, and all of them are uncertain. You are 99.99% certain the Earth revolves around the sun, 90% confident your best friend will attend your birthday party, and 40% sure that the song on the radio you\u2019re listening to was played by The Turtles.</p>\n<p>Then, you encounter new evidence \u2014 new observations \u2014 and update your beliefs in response.</p>\n<p>Suppose you start out 85% confident the one remaining enemy soldier is not a sniper, leaving only 15% credence to the hypothesis that he <em>is</em> a sniper. But then, a bullet glances off your helmet \u2014 an event far more likely if the enemy soldier is a sniper than if he is not. So now you\u2019re only 40% confident he\u2019s not a sniper, and 60% confident he <em>is</em>. Another bullet glances off your helmet, and you update again. Now you\u2019re only 2% confident he\u2019s not a sniper, and 98% confident he <em>is</em> a sniper.</p>\n<p>Now, I\u2019m about to show you some <em>math</em>, but don\u2019t run away. The math isn\u2019t <em>supposed</em> to make sense if you haven\u2019t studied it. Don\u2019t worry; I\u2019m going to explain it all.</p>\n<p>There\u2019s a theorem in probability theory that tells you how likely one observation is given some other observations. It\u2019s called Bayes\u2019 Theorem.</p>\n<p>At this point you may want to take a break and read either <a href=\"http://yudkowsky.net/rational/bayes\">tutorial #1</a>, <a href=\"http://commonsenseatheism.com/?p=13156\">tutorial #2</a>, <a href=\"/lw/2b0/bayes_theorem_illustrated_my_way/\">tutorial #3</a>, or <a href=\"http://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">tutorial #4</a> on Bayes\u2019 Theorem. I\u2019ll only say a <em>little</em> bit more about Bayes\u2019 Theorem in this tutorial.</p>\n<p>The short form of Bayes\u2019 Theorem looks like this:</p>\n<p><img src=\"http://commonsenseatheism.com/wp-content/uploads/2010/12/eq_bayes_reduced.png\" alt=\"\"></p>\n<p>Let\u2019s unpack this. The H refers to some hypothesis, and the E responds to some evidence. p(x) refers to the probability of x. The pipe symbol | means \u2018given\u2019 or \u2018assuming\u2019. So, Bayes\u2019 Theorem reads:</p>\n<blockquote>\n<p>[The probability of hypothesis H given evidence E] is equal to [the probability of evidence E given hypothesis H] times [the probability of hypothesis H] divided by [the probability of evidence E].</p>\n</blockquote>\n<p>You can see how this tells us what we\u2019d like to know. We want to know the probability of some hypothesis \u2014 some model of the world that, if correct, will allow us to successfully predict the future \u2014 given the evidence that we have. And that\u2019s what Bayes\u2019 Theorem tells us. Now we just plug the numbers in, and solve for p(H|E).</p>\n<p>Of course, it\u2019s not easy to \u201cjust plug the numbers in.\u201d You aren\u2019t an all-knowing god. You don\u2019t know <em>exactly</em> how likely it is that the enemy soldier would hit your helmet if he\u2019s a sniper, compared to how likely that is if he\u2019s not a sniper. But you can do your best. With enough evidence, it will become overwhelmingly clear which hypothesis is correct.</p>\n<p>At this point, I'll let the tutorials on Bayes' Theorem above do the heavy lifting on how to update beliefs. Let me get back to the part of induction those tutorials <em>don't</em> explain: the choice of priors.</p>\n<h3 id=\"The_Problem_of_Priors\">The Problem of Priors</h3>\n<p>In the example above where you're a soldier in combat, I gave you your starting probabilities: 85% confidence that the enemy soldier was a sniper, and 15% confidence he was not. But what if you don't know your \"priors\"? What then?</p>\n<p>When using Bayes' Theorem to calculate your probabilities, your choice of prior can influence your results greatly. But how can we determine the probability of a hypothesis <em>before</em> seeing any data? What does that even <em>mean</em>?</p>\n<p>Legg (2008) explains:</p>\n<blockquote>\n<p>Bayesians respond to this in a number of ways. Firstly, they point out that the problem is generally small, in the sense that with a reasonable prior and quantity of data, the posterior distribution P(h|D) depends almost entirely on D rather than the chosen prior P(h). In fact on any sizable data set, not only does the choice of prior not especially matter, but Bayesian and classical statistical methods typically produce similar results, as one would expect. It is only with relatively small data sets or complex models that the choice of prior becomes an issue.</p>\n</blockquote>\n<blockquote>\n<p>If classical statistical methods could avoid the problem of prior bias when dealing with small data sets then this would be a significant argument in their favour. However Bayesians argue that all systems of inductive inference that obey some basic consistency principles define, either explicitly or implicitly, a prior distribution over hypotheses. Thus, methods from classical statistics make assumptions that are in effect equivalent to defining a prior. The difference is that in Bayesian statistics these assumptions take the form of an explicit prior distribution. In other words, it is not that prior bias in Bayesian statistics is necessarily any better or worse than in classical statistics, it is simply more transparent.</p>\n</blockquote>\n<p>But this doesn't <em>solve</em> our problem. It merely points out that other statistical techniques don't fare any better.</p>\n<p>What we'd like is to reduce the potential for abusing one's opportunity to select priors, and to use agreed-upon priors when possible. Thus, many standard \"prior distributions\" have been developed. Generally, they distribute probability equally across hypotheses.</p>\n<p>But to solve the problem of priors once and for all, we'd like to have an acceptable, <em>universal</em> prior distribution, so that there's no fuzziness about the process of induction. We need a <em>recipe</em>, and <em>algorithm</em>, for finding out the truth. And there can't be any fuzziness in an algorithm.</p>\n<h3 id=\"Binary_Sequence_Prediction\">Binary Sequence Prediction</h3>\n<p>[intro to binary sequence prediction]</p>\n<h3 id=\"Solomonoff_and_Kolmogorov\">Solomonoff and Kolmogorov</h3>\n<p>[summarize the contributions of Solomonoff and Kolmogorov]</p>\n<h3 id=\"Solomonoff_Lightsaber\">Solomonoff Lightsaber</h3>\n<p>[explain the result: Solomonoff Induction]</p>\n<h3 id=\"Approximations\">Approximations</h3>\n<p>[survey a few of the approximations of Solomonoff Induction in use today]</p>\n<h3 id=\"Philosophical_Implications\">Philosophical Implications</h3>\n<p>[survey of some philosophical implications of unviersal induction]</p>\n<h3 id=\"Notes\">Notes</h3>\n<p><sup>a</sup> This quote and some of the examples to follow are from Legg (2008).</p>\n<h3 id=\"References\">References</h3>\n<p>Legg (2008). <em><a href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">Machine Superintelligence</a></em>. PhD thesis, Department of Informatics, University of Lugano.</p>\n<p>Rathmanner &amp; Hutter (2011). <a href=\"/arxiv.org/pdf/1105.5721v1\">A philosophical treatise of universal induction</a>.</p>", "sections": [{"title": "Algorithms", "anchor": "Algorithms", "level": 1}, {"title": "Induction", "anchor": "Induction", "level": 1}, {"title": "Occam\u2019s Razor", "anchor": "Occam_s_Razor", "level": 1}, {"title": "Updating Beliefs", "anchor": "Updating_Beliefs", "level": 1}, {"title": "The Problem of Priors", "anchor": "The_Problem_of_Priors", "level": 1}, {"title": "Binary Sequence Prediction", "anchor": "Binary_Sequence_Prediction", "level": 1}, {"title": "Solomonoff and Kolmogorov", "anchor": "Solomonoff_and_Kolmogorov", "level": 1}, {"title": "Solomonoff Lightsaber", "anchor": "Solomonoff_Lightsaber", "level": 1}, {"title": "Approximations", "anchor": "Approximations", "level": 1}, {"title": "Philosophical Implications", "anchor": "Philosophical_Implications", "level": 1}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "31 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kyc5dFDzBg4WccrbK", "Qs96DvwTCCdyRvM5E", "hN2aRnu798yas5b2k", "CMt3ijXYuCynhPWXa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T13:26:27.899Z", "modifiedAt": "2020-05-07T17:09:28.777Z", "url": null, "title": "Hack Away at the Edges", "slug": "hack-away-at-the-edges", "viewCount": null, "lastCommentedAt": "2013-07-31T19:29:12.688Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6bSHiD9TxsJwe2WqT/hack-away-at-the-edges", "pageUrlRelative": "/posts/6bSHiD9TxsJwe2WqT/hack-away-at-the-edges", "linkUrl": "https://www.lesswrong.com/posts/6bSHiD9TxsJwe2WqT/hack-away-at-the-edges", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Hack%20Away%20at%20the%20Edges&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHack%20Away%20at%20the%20Edges%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bSHiD9TxsJwe2WqT%2Fhack-away-at-the-edges%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Hack%20Away%20at%20the%20Edges%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bSHiD9TxsJwe2WqT%2Fhack-away-at-the-edges", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6bSHiD9TxsJwe2WqT%2Fhack-away-at-the-edges", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 917, "htmlBody": "<p><small>See also: <a href=\"http://wiki.lesswrong.com/wiki/Challenging_the_Difficult\">Challenging the Difficult</a> and&nbsp;<a href=\"/lw/1g4/tips_and_tricks_for_answering_hard_questions/\">Tips and Tricks for Answering Hard Questions</a>.</small></p>\n<p>From Michael Nielsen's <em><a href=\"http://www.amazon.com/Reinventing-Discovery-New-Networked-Science/dp/0691148902/\">Reinventing Discovery</a></em>:</p>\n<blockquote>\n<p>In January 2009, [mathematician Tim] Gowers decided to use his blog to run a very unusual social experiment. He picked out an important and difficult unsolved mathematical problem, a problem he said he&rsquo;d &ldquo;love to solve.&rdquo; But instead of attacking the problem on his own, or with a few close colleagues, he decided to attack the problem completely in the open, using his blog to post ideas and partial progress. What&rsquo;s more, he issued an open invitation asking other people to help out. Anyone could follow along and, if they had an idea, explain it in the comments section of the blog. Gowers hoped that many minds would be more powerful than one, that they would stimulate each other with different expertise and perspectives, and collectively make easy work of his hard mathematical problem. He dubbed the experiment the Polymath Project.</p>\n<p>The Polymath Project got off to a slow start. Seven hours after Gowers opened up his blog for mathematical discussion, not a single person had commented. Then a mathematician named Jozsef Solymosi from the University of British Columbia posted a comment suggesting a variation on Gowers&rsquo;s problem, a variation which was easier, but which Solymosi thought might throw light on the original problem. Fifteen minutes later, an Arizona high-school teacher named Jason Dyer chimed in with a thought of his own. And just three minutes after that, UCLA mathematician Terence Tao&mdash;like Gowers, a Fields medalist&mdash;added a comment. The comments erupted: over the next 37 days, 27 people wrote 800 mathematical comments, containing more than 170,000 words. Reading through the comments you see ideas proposed, refined, and discarded, all with incredible speed. You see top mathematicians making mistakes, going down wrong paths, getting their hands dirty following up the most mundane of details, relentlessly pursuing a solution. And through all the false starts and wrong turns, you see a gradual dawning of insight. Gowers described the Polymath process as being &ldquo;to normal research as driving is to pushing a car.&rdquo; Just 37 days after the project began Gowers announced that he was confident the polymaths had solved not just his original problem, but a harder problem that included the original as a special case.</p>\n</blockquote>\n<p>This episode is a microcosm of how intellectual progress happens.</p>\n<p>Humanity's intellectual history is not the story of a Few Great Men who had a burst of insight, cried \"Eureka!\" and jumped 10 paces ahead of everyone else. More often, an intellectual breakthrough is the story of dozens of people building on the ideas of others before them, making wrong turns, proposing and discarding ideas, combining insights from multiple subfields, slamming into brick walls and getting back up again. Very slowly, the space around the solution is crowded in by dozens of investigators until finally one of them hits the payload.</p>\n<p><a id=\"more\"></a></p>\n<p>The problem you're trying to solve may look impossible. It may look like a <a href=\"/lw/oh/righting_a_wrong_question/\">wrong question</a>, and you don't know what the <em>right</em>&nbsp;question to ask is. The problem may have stymied investigators for decades, or centuries.</p>\n<p>If so, take heart: we've been in your situation many times before. Almost every problem we've ever solved was once phrased as a wrong question, and looked impossible. <a href=\"http://commonsenseatheism.com/?p=4985\">Remember</a> the persistence required for science; what \"thousands of disinterested moral lives of men lie buried in its mere foundations; what patience and postponement... are wrought into its very stones and mortar.\"</p>\n<p>\"Genius is 1 percent inspiration, 99 percent perspiration,\" said Thomas Edison, and he should've <a href=\"http://www.amazon.com/Edisons-Electric-Light-Introductory-Technology/dp/0801894824/\">known</a>: It took him hundreds of tweaks to get his incandescent light bulb to work well, and he was already building on the work of 22 earlier inventors of incandescent lights.</p>\n<p>Pick any piece of progress you think of as a \"sudden breakthrough,\" read a history book about just that one breakthrough, and you will find that the breakthrough was the result of messy progress like the Polymath Project, but slower: multiple investigators, wrong turns, ideas proposed and combined and discarded, the space around the final breakthrough slowly encroached upon from many angles.</p>\n<p>Consider what I said <a href=\"/lw/7t2/hard_problem_hack_away_at_the_edges/\">earlier</a> about the problem of <a href=\"http://en.wikipedia.org/wiki/Friendly_artificial_intelligence\">Friendly AI</a>:</p>\n<blockquote>\n<p>I doubt the problem will be solved by getting smart people to sit in silence and <em>think real hard</em> about decision theory and metaethics. If the problem can be solved, it will be solved by dozens or hundreds of people hacking away at the tractable edges of Friendly AI subproblems, drawing novel connections, inching toward new insights, drawing from others' knowledge and intuitions, and doing lots of tedious, boring work.</p>\n<p>...This isn't the only way to solve hard problems, but when problems are sufficiently hard, then hacking away at their edges may be just about <em>all</em> you can do. And as you do, you start to see where the problem is more and less tractable. Your intuitions about how to solve the problem become more and more informed by regular encounters with it from all angles. You learn things from one domain that end up helping in a different domain. And, inch by inch, you make progress.</p>\n</blockquote>\n<p>So: Are you facing an impossible problem? <a href=\"/lw/up/shut_up_and_do_the_impossible/\">Don't let that stop you</a>, if the problem is important enough. Hack away at the edges. Look to similar problems in other fields for insight. Poke here and there and everywhere, and put extra pressure where the problem seems to give a little. Ask for help. Try different tools. Don't give up; keep hacking away at the edges.</p>\n<p>One day you may hit the payload.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"BzghQYM9GnkMHxZKb": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6bSHiD9TxsJwe2WqT", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 64, "baseScore": 86, "extendedScore": null, "score": 0.000185, "legacy": true, "legacyId": "11224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 86, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SEq8bvSXrzF4jcdS8", "rQEwySCcLtdKHkrHp", "guRASsKLfdQ5m7LLw", "nCvvhFBaayaXyuBiD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-12-01T13:26:27.899Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T15:49:42.543Z", "modifiedAt": null, "url": null, "title": "Tidbit: \u201cSemantic over-achievers\u201d", "slug": "tidbit-semantic-over-achievers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:49.265Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "kpreid", "createdAt": "2009-04-28T03:07:40.133Z", "isAdmin": false, "displayName": "kpreid"}, "userId": "rKiev4kfnTWRmCJit", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/89opBqNHKsbZvbGKJ/tidbit-semantic-over-achievers", "pageUrlRelative": "/posts/89opBqNHKsbZvbGKJ/tidbit-semantic-over-achievers", "linkUrl": "https://www.lesswrong.com/posts/89opBqNHKsbZvbGKJ/tidbit-semantic-over-achievers", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tidbit%3A%20%E2%80%9CSemantic%20over-achievers%E2%80%9D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATidbit%3A%20%E2%80%9CSemantic%20over-achievers%E2%80%9D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89opBqNHKsbZvbGKJ%2Ftidbit-semantic-over-achievers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tidbit%3A%20%E2%80%9CSemantic%20over-achievers%E2%80%9D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89opBqNHKsbZvbGKJ%2Ftidbit-semantic-over-achievers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89opBqNHKsbZvbGKJ%2Ftidbit-semantic-over-achievers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>[I'd put this in an open thread, but those don&rsquo;t seem to happen these days, and while this is a quote it isn't a Rationality Quote.]</p>\n<blockquote>You know, one of the really weird things about us human beings [&hellip;] is that we have somehow created for ourselves languages that are just a bit too flexible and expressive for our brains to handle. We have managed to build languages in which arbitrarily deep nesting of negation and quantification is possible, when we ourselves have major difficulties handling the semantics of anything beyond about depth 1 or 2. That is so weird. But that's how we are: semantic over-achievers, trying to use languages that are quite a bit beyond our intellectual powers.</blockquote>\n<p>&mdash; <a href=\"http://languagelog.ldc.upenn.edu/nll/?p=3591\">Geoffrey K. Pullum, Language Log, &ldquo;Never fails: semantic over-achievers&rdquo;, December 1, 2011</a></p>\n<p>This seems like it might lead to something interesting to say about the design of minds and the usefulness of generalization/abstraction, or perhaps just a good sound bite.<span style=\"white-space: pre;\"> </span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "89opBqNHKsbZvbGKJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 8.081859764924665e-07, "legacy": true, "legacyId": "11225", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T16:37:43.873Z", "modifiedAt": null, "url": null, "title": "Meetup : Seattle biweekly meetup: problem solving", "slug": "meetup-seattle-biweekly-meetup-problem-solving", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/k2oaCgnHFRi3zwGPQ/meetup-seattle-biweekly-meetup-problem-solving", "pageUrlRelative": "/posts/k2oaCgnHFRi3zwGPQ/meetup-seattle-biweekly-meetup-problem-solving", "linkUrl": "https://www.lesswrong.com/posts/k2oaCgnHFRi3zwGPQ/meetup-seattle-biweekly-meetup-problem-solving", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Seattle%20biweekly%20meetup%3A%20problem%20solving&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Seattle%20biweekly%20meetup%3A%20problem%20solving%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2oaCgnHFRi3zwGPQ%2Fmeetup-seattle-biweekly-meetup-problem-solving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Seattle%20biweekly%20meetup%3A%20problem%20solving%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2oaCgnHFRi3zwGPQ%2Fmeetup-seattle-biweekly-meetup-problem-solving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fk2oaCgnHFRi3zwGPQ%2Fmeetup-seattle-biweekly-meetup-problem-solving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5d'>Seattle biweekly meetup: problem solving</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 December 2011 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">5523 University Way NE, #501, seattle, wa</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'd like to try a problem solving meetup. Here's is roughly what I'm thinking:</p>\n\n<p>The idea is to have everyone come to the meetup with an area of their life they would like help improving. The problem can be general (\"I'm dissatisfied with my job\") or specific (\"I'm having trouble motivating myself to do my homework\"). We will split up into small groups and try to come up with productive strategies to eachother's issues then perhaps have a larger discussion. In one or two weeks we'll review how successful these strategies were.</p>\n\n<p>Afterwards we'll have dinner and chat.</p>\n\n<p>Ben has graciously agreed to host us this week (ring the buzzer to get let in). Thanks Ben :)</p>\n\n<p>See you guys there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5d'>Seattle biweekly meetup: problem solving</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "k2oaCgnHFRi3zwGPQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.082032835475782e-07, "legacy": true, "legacyId": "11226", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Seattle_biweekly_meetup__problem_solving\">Discussion article for the meetup : <a href=\"/meetups/5d\">Seattle biweekly meetup: problem solving</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 December 2011 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">5523 University Way NE, #501, seattle, wa</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I'd like to try a problem solving meetup. Here's is roughly what I'm thinking:</p>\n\n<p>The idea is to have everyone come to the meetup with an area of their life they would like help improving. The problem can be general (\"I'm dissatisfied with my job\") or specific (\"I'm having trouble motivating myself to do my homework\"). We will split up into small groups and try to come up with productive strategies to eachother's issues then perhaps have a larger discussion. In one or two weeks we'll review how successful these strategies were.</p>\n\n<p>Afterwards we'll have dinner and chat.</p>\n\n<p>Ben has graciously agreed to host us this week (ring the buzzer to get let in). Thanks Ben :)</p>\n\n<p>See you guys there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Seattle_biweekly_meetup__problem_solving1\">Discussion article for the meetup : <a href=\"/meetups/5d\">Seattle biweekly meetup: problem solving</a></h2>", "sections": [{"title": "Discussion article for the meetup : Seattle biweekly meetup: problem solving", "anchor": "Discussion_article_for_the_meetup___Seattle_biweekly_meetup__problem_solving", "level": 1}, {"title": "Discussion article for the meetup : Seattle biweekly meetup: problem solving", "anchor": "Discussion_article_for_the_meetup___Seattle_biweekly_meetup__problem_solving1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T18:59:02.357Z", "modifiedAt": null, "url": null, "title": "Open Thread: December 2011", "slug": "open-thread-december-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:14:31.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tripitaka", "createdAt": "2011-02-13T14:34:10.628Z", "isAdmin": false, "displayName": "Tripitaka"}, "userId": "fQzCyXjvZ9cxwra6P", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rhicHrc6CLSuws89u/open-thread-december-2011", "pageUrlRelative": "/posts/rhicHrc6CLSuws89u/open-thread-december-2011", "linkUrl": "https://www.lesswrong.com/posts/rhicHrc6CLSuws89u/open-thread-december-2011", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%3A%20December%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%3A%20December%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhicHrc6CLSuws89u%2Fopen-thread-december-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%3A%20December%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhicHrc6CLSuws89u%2Fopen-thread-december-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhicHrc6CLSuws89u%2Fopen-thread-december-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>If it's worth saying, but not worth its own post (even in Discussion), then it goes here.<br /><br />If continuing the discussion becomes impractical, that means you win at open threads; a celebratory top-level post on the topic is traditional.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rhicHrc6CLSuws89u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 2.6e-05, "legacy": true, "legacyId": "11227", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-01T22:22:50.504Z", "modifiedAt": "2020-12-11T18:09:29.196Z", "url": null, "title": "5 Axioms of Decision Making", "slug": "5-axioms-of-decision-making", "viewCount": null, "lastCommentedAt": "2020-12-11T18:08:58.828Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Vaniver", "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zFQQEkx4c6bxdshr4/5-axioms-of-decision-making", "pageUrlRelative": "/posts/zFQQEkx4c6bxdshr4/5-axioms-of-decision-making", "linkUrl": "https://www.lesswrong.com/posts/zFQQEkx4c6bxdshr4/5-axioms-of-decision-making", "postedAtFormatted": "Thursday, December 1st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%205%20Axioms%20of%20Decision%20Making&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A5%20Axioms%20of%20Decision%20Making%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFQQEkx4c6bxdshr4%2F5-axioms-of-decision-making%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=5%20Axioms%20of%20Decision%20Making%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFQQEkx4c6bxdshr4%2F5-axioms-of-decision-making", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzFQQEkx4c6bxdshr4%2F5-axioms-of-decision-making", "socialPreviewImageUrl": "https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg", "question": false, "authorIsUnreviewed": false, "wordCount": 1571, "htmlBody": "<html><head></head><body><p>This is part of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a>; the first post is a primer on <a href=\"/lw/8lb/uncertainty/\">Uncertainty</a>.</p><p>Decision analysis has two main parts: abstracting a real situation to math, and then cranking through the math to get an answer. We started by talking a bit about how probabilities work, and I'll finish up the inner math in this post. We're working from the inside out because it's easier to understand the shell once you understand the kernel. I'll provide an example of prospects and deals to demonstrate the math, but first we should talk about axioms. In order to be comfortable with using this method, there are five axioms1 you have to agree with, and if you agree with those axioms, then this method flows naturally. They are: <strong>Probability, Order, Equivalence, Substitution, </strong>and<strong> Choice.</strong></p><p>Probability</p><p>You must be willing to assign a probability to quantify any uncertainty important to your decision. You must have consistent probabilities.</p><p>Order</p><p>You must be willing to order outcomes without any cycles. This can be called transitivity of preferences: if you prefer A to B, and B to C, you must prefer A to C.</p><p>Equivalence</p><p>If you prefer A to B to C, then there must exist a <i>p</i> where you are indifferent between a deal where you receive B with certainty and a deal where you receive A with probability <i>p</i> and C otherwise.</p><p>Substitution</p><p>You must be willing to substitute an uncertain deal for a certain deal or vice versa if you are indifferent between them by the previous rule. Also called \"do you really mean it?\"</p><p>Choice</p><p>If you have a choice between two deals, both of which offer A or C, and you prefer A to C, then you must pick the deal with the higher probability of A.</p><p>&nbsp;</p><p>These five axioms correspond to five actions you'll take in solving a decision problem. You assign <strong>probabilities</strong>, then you <strong>order</strong> outcomes, then you determine <strong>equivalence</strong> so you can <strong>substitute</strong> complicated deals for simple deals, until you're finally left with one obvious <strong>choice</strong>.</p><p>You might be uncomfortable with some of these axioms. You might say that your preferences genuinely cycle, or you're not willing to assign numbers to uncertain events, or you want there to be an <a href=\"/lw/my/the_allais_paradox/\">additional value for certainty</a> beyond the prospects involved. I can only respond that these axioms are prescriptive, not descriptive: you will be better off if you behave this way, but you must choose to.</p><p>Let's look at an example:</p><h2>My Little Decision<br>&nbsp;</h2><p>Suppose I enter a lottery for MLP toys. I can choose from two kinds of tickets: an A ticket has a 1/3 chance of giving me a Twilight Sparkle, a 1/3 chance of giving me an Applejack, and a 1/3 chance of giving me a Pinkie Pie. A B ticket has a 1/3 chance of giving me a Rainbow Dash, a 1/3 chance of giving me a Rarity, and a 1/3 chance of giving me a Fluttershy. There are two deals for me to choose between- the A ticket and the B ticket- and six prospects, which I'll abbreviate to TS, AJ, PP, RD, R, and FS.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg/w_640 640w\"></figure><p>(Typically, decision nodes are represented as squares, and work just like uncertainty nodes, and so A would be above B with a decision node pointing to both. I've displayed them side by side because I suspect it looks better for small decisions.)</p><p>The first axiom- probability- is already taken care of for us, because our model of the world is already specified. We are rarely that lucky in the real world. The second axiom- order- is where we need to put in work. I need to come up with a preference ordering. I think about it and come up with the ordering TS &gt; RD &gt; R = AJ &gt; FS &gt; PP. Preferences are <i>personal</i>- beyond requiring internal consistency, we shouldn't require or expect that everyone will think Twilight Sparkle is the best pony. Preferences are also a source of uncertainty if prospects satisfy multiple different desires, as you may not be sure about your indifference tradeoffs between those desires. Even when prospects have only one <i>measure</i>, that is, they're all expressed in the same unit (say, dollars), you could be uncertain about your risk sensitivity, which shows up in preference probabilities but deserves a post of its own.</p><p>Now we move to axiom 3: I have an ordering, but that's not enough to solve this problem. I need a preference scoring to represent how much I prefer one prospect to another. I might prefer cake to chicken and chicken to death, but the second preference is far stronger than the first! To determine my scoring I need to imagine deals and assign indifference probabilities. There are a lot of ways to do this, but let's jump straight to the most sensible one: compare every prospect to a deal between the best and worst prospect.2</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1b35646eeedb60772e243dde673a4a606f7740e83f398283.jpg/w_640 640w\"></figure><p>I need to assign a <i>preference</i> probability <i>p</i> such that I'm indifferent between the two deals presented: either RD with certainty, or a chance at TS (and PP if I don't get it). I think about it and settle on .9: I like RD close to how much I like TS.3 This indifference needs to be two-way: I need to be indifferent about trading a ticket for that deal for a RD, and I need to be indifferent about trading a RD for that deal.4 I repeat this process with the rest, and decide .6 for R and AJ and .3 for FS. It's useful to check and make sure that all the relationships I elicited before hold- I prefer R and AJ the same, and the ordering is all correct. I don't need to do this process for TS or PP, as <i>p</i> is trivially 1 or 0 in that case.</p><p>Now that I have a preference scoring, I can move to axiom 4. I start by making things more complicated- I take all of the prospects that weren't TS or PP and turn them into deals of {<i>p </i>TS, 1-<i>p</i> PP}. (Pictured is just the expansion of the right tree; try expanding the tree for A. It's much easier.)</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/11847a8dd44d6119ad6015954e3ed7805c95e43650f18655.jpg/w_640 640w\"></figure><p>Then, using axiom 1 again, I rearrange this tree. The A tree (not shown) and B tree now have only two prospects, and I've expressed the probabilities of those prospects in a complicated way that I know how to simplify.</p><figure><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_80 80w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_160 160w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_240 240w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_320 320w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_480 480w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_560 560w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/09ec87770039ebbfab1d948534cb5bbca8401df6c6f42737.jpg/w_640 640w\"></figure><p>And we have one last axiom to apply: choice. Deal <i>B</i> has a higher chance of the better prospect, and so I pick it. Note that that's the case even though my actual chance of receiving TS with deal B is 0%- this is just how I'm representing my preferences, and this computation is telling me that my probability-weighted preference for deal B is higher than my probability-weighted preference for deal A. Not only do I know that I should choose deal B, but I know how much better deal B is for me than deal A.5</p><p>This was a toy example, but the beauty of this method is that all calculations are local. That means we can apply this method to a problem of arbitrary size without changes. Once we have probabilities and preferences for the possible outcomes, we can propagate those from the back of the tree through every node (decision or uncertainty) until we know what to do everywhere. Of course, whether the method will have a runtime shorter than the age of the universe depends on the size of your problem. You could use this to decide which chess moves to play against an opponent whose strategy you can guess from the board configuration, but I don't recommend it.6 Typical real-world problems you would use this for are too large to solve with intuition but small enough that a computer (or you working carefully) can solve it exactly if you give it the right input.</p><p>Next we start the meat of decision analysis: reducing the real world to math.</p><p>&nbsp;</p><hr><p>1. These axioms are <a href=\"http://en.wikipedia.org/wiki/Ronald_A._Howard\">Ronald Howard</a>'s 5 Rules of Actional Thought.</p><p>2. Another method you might consider is comparing a prospect to its neighbors; RD in terms of TS and R, R in terms of RD and FS, FS in terms of R and PP. You could then unpack those into the preference probability</p><p>3. Assigning these probabilities is tough, especially if you aren't comfortable with probabilities. Some people find it helpful to use a <a href=\"http://www.stanford.edu/~savage/software.htm\">probability wheel</a>, where they can <i>see</i> what 60% looks like, and adjust the wheel until it matches what they feel. See also <a href=\"/lw/7z9/1001_predictionbook_nights/\">1001 PredictionBook Nights</a> and <a href=\"http://rationalpoker.com/2011/04/21/this-is-what-5-feels-like/\">This is what 5% feels like</a>.</p><p>4. In actual practice, deals often come with friction and people tend to be attached to what they have beyond the amount that they want it. It's important to make sure that you're actually coming up with an indifference value, <i>not</i> the worst deal you would be willing to make, and flipping the deal around and making sure you feel the same way is a good way to check.</p><p>5. If you find yourself disagreeing with the results of your analysis, double check your math and make sure you agree with all of your elicited preferences. An unintuitive answer can be a sign of an error in your inputs or your calculations, but if you don't find either make sure you're not trying to <a href=\"/lw/js/the_bottom_line/\">start with the bottom line</a>.</p><p>6. There are supposedly <a href=\"http://www.nybooks.com/articles/archives/2010/feb/11/the-chess-master-and-the-computer/?pagination=false\">10120 possible games of chess</a>, and this method would evaluate all of them. Even with computation-saving implementation tricks, you're better off with another algorithm.</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 2, "dPPATLhRmhdJtJM2t": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zFQQEkx4c6bxdshr4", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 50, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "11165", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4bcdb0b06b529bac30c18bd6b5a025fd5bdbdb457600b18.jpg", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "AnbKBK236LtCdXSzj", "zJZvoiwydJ5zvzTHK", "yE4Fdx4kYmQchBCek", "34XxbRFe54FycoCDw"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-12-01T22:22:50.504Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T00:22:54.456Z", "modifiedAt": null, "url": null, "title": "Online Course in Evidence-Based Medicine", "slug": "online-course-in-evidence-based-medicine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:45.303Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Normal_Anomaly", "createdAt": "2010-11-14T03:31:54.691Z", "isAdmin": false, "displayName": "Normal_Anomaly"}, "userId": "WgGYj5bqcZKsFNG6F", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h9B9sNR2JnQL3B6je/online-course-in-evidence-based-medicine", "pageUrlRelative": "/posts/h9B9sNR2JnQL3B6je/online-course-in-evidence-based-medicine", "linkUrl": "https://www.lesswrong.com/posts/h9B9sNR2JnQL3B6je/online-course-in-evidence-based-medicine", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Online%20Course%20in%20Evidence-Based%20Medicine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnline%20Course%20in%20Evidence-Based%20Medicine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9B9sNR2JnQL3B6je%2Fonline-course-in-evidence-based-medicine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Online%20Course%20in%20Evidence-Based%20Medicine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9B9sNR2JnQL3B6je%2Fonline-course-in-evidence-based-medicine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh9B9sNR2JnQL3B6je%2Fonline-course-in-evidence-based-medicine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p>The Foundation for Blood Research has created an online course in Evidence-Based Medicine, aimed at \"advanced high school science students, college students, nursing students, and 1st or 2nd year medical students.\" It focuses on evaluating research papers and applying statistics to medical diagnosis. I have taken this course, and it was useful practice in Bayesian reasoning.</p>\n<p>The course involves working through a couple case studies of ER patients. Students will observe the patient, review research on relevant diagnostic tests, and calculate posterior probabilities given the available information. For instance: once case involves a woman who may have bacterial meningitis, but her spinal fluid test results are mixed. Students then read parts of a paper describing the success of different components of the spinal fluid test as predictors of meningitis.</p>\n<p>The course is self-paced and highly modular, alternating between videos, multiple choice or calculation questions, and short written submissions. There is no in-course interaction between students taking the same course, but it is divided into \"class sections\" for the convenience of teachers who want to observe their students. It works well with Firefox and Safari, and slightly less well (but still easily usable) with Internet Explorer.</p>\n<p>Anyone who is interested or wants more information, look at <a href=\"http://evidenceworksremote.com/\">their website</a> or ask me in the comments. Once a decent number of people have shown some interest, I will contact one of the site administrators and he'll set up an official class section for us.</p>\n<p>EDIT: I have contacted the site administrator, we should have a class section available soon. Section name and info on how to log in will be posted shortly.</p>\n<p>EDIT2: The course section is up: go to the http://evidenceworksremote.com/courses/ and then find the Less Wrong Community course. When you click on the course listing you will be asked to register. Once you receive the acknowledging email return to the course and enter the \"enrollment\" key: LW101 . I will be able to see your responses to the questions and possibly able to provide feedback. Once you have completed the course, Dr. Allan, who is one of the developers, would appreciate feedback by email.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h9B9sNR2JnQL3B6je", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.083709658927256e-07, "legacy": true, "legacyId": "11230", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T02:18:41.971Z", "modifiedAt": null, "url": null, "title": "Google AI challenge - status?", "slug": "google-ai-challenge-status", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:40.632Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e5xJfgPGmnj9bu6Zp/google-ai-challenge-status", "pageUrlRelative": "/posts/e5xJfgPGmnj9bu6Zp/google-ai-challenge-status", "linkUrl": "https://www.lesswrong.com/posts/e5xJfgPGmnj9bu6Zp/google-ai-challenge-status", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Google%20AI%20challenge%20-%20status%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGoogle%20AI%20challenge%20-%20status%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe5xJfgPGmnj9bu6Zp%2Fgoogle-ai-challenge-status%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Google%20AI%20challenge%20-%20status%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe5xJfgPGmnj9bu6Zp%2Fgoogle-ai-challenge-status", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe5xJfgPGmnj9bu6Zp%2Fgoogle-ai-challenge-status", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p>About one month ago there was some discussion&nbsp;regarding entering an LW team in the Google AI challenge.</p>\n<p>Has anyone actually entered? If so, what was the result? I scanned carefully the leaderboard, but could not find any&nbsp;familiar&nbsp;names.</p>\n<p>&nbsp;</p>\n<p>Links:</p>\n<p>(<a href=\"http://aichallenge.org/\">http://aichallenge.org/</a>)&nbsp;</p>\n<p>(<a href=\"/r/discussion/lw/8ay/ai_challenge_ants/\">http://lesswrong.com/r/discussion/lw/8ay/ai_challenge_ants/</a>)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e5xJfgPGmnj9bu6Zp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 8.084127149539765e-07, "legacy": true, "legacyId": "11237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rrX37rZkf2o4XEfuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T03:20:58.176Z", "modifiedAt": null, "url": null, "title": "The Gift We Give Tomorrow, Spoken Word [Finished?]", "slug": "the-gift-we-give-tomorrow-spoken-word-finished", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:28.016Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bkRpALFAwJQuntHiF/the-gift-we-give-tomorrow-spoken-word-finished", "pageUrlRelative": "/posts/bkRpALFAwJQuntHiF/the-gift-we-give-tomorrow-spoken-word-finished", "linkUrl": "https://www.lesswrong.com/posts/bkRpALFAwJQuntHiF/the-gift-we-give-tomorrow-spoken-word-finished", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Gift%20We%20Give%20Tomorrow%2C%20Spoken%20Word%20%5BFinished%3F%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Gift%20We%20Give%20Tomorrow%2C%20Spoken%20Word%20%5BFinished%3F%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkRpALFAwJQuntHiF%2Fthe-gift-we-give-tomorrow-spoken-word-finished%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Gift%20We%20Give%20Tomorrow%2C%20Spoken%20Word%20%5BFinished%3F%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkRpALFAwJQuntHiF%2Fthe-gift-we-give-tomorrow-spoken-word-finished", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbkRpALFAwJQuntHiF%2Fthe-gift-we-give-tomorrow-spoken-word-finished", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1848, "htmlBody": "<address><span style=\"font-style: normal;\">For reasons that shall remain temporarily mysterious, I wanted a version of the <a href=\"/lw/sa/the_gift_we_give_to_tomorrow/\">Gift We Give Tomorrow</a> that was designed to be spoken, rather than read. In particular, spoken in a relatively short period of time. It's one of my favorite sequence posts, but when I tried to read aloud, I found the words did not flow very well and it goes on for longer than I expect an audience to listen without getting bored. I also wanted certain phrasings to tie in with other sequence posts (hence a reference to <a href=\"/lw/kr/an_alien_god/\">Azathoth</a>, and <a href=\"/lw/uk/beyond_the_reach_of_god/\">Beyond the Reach of God</a>).</span></address><address><span style=\"font-style: normal;\"><br /></span></address><address><span style=\"font-style: normal;\">The following is the first draft of my efforts. It's about half as long as the original. It cuts out the section about the Shadowy Figure, which I'm slightly upset about, in particular because it would make the \"beyond the reach of God\" line stronger. But I felt like if I tried to include it at all, I had to include several paragraphs that took a little too long.</span></address><address><span style=\"font-style: normal;\"><br /></span></address><address><span style=\"font-style: normal;\">I attempted at first to convert to a \"true\" poem, (not rhyming, but going for a particular meter). I later decided that too much of it needed to have a conversational quality so it's more of a short play than a poem. Lines are broken up in a particular way to suggest timing and make it easier to read out loud.</span></address><address><span style=\"font-style: normal;\"><br /></span></address><address><span style=\"font-style: normal;\">I wanted a) to share the results with people on the chance that someone else might want to perform a little six minute dialog (my test run clocked in at 6:42), and b) get feedback on how I chose to abridge things. Do you think there were important sections that can be tied in without making it too long? Do you think some sections that I reworded could be reworded better, or that I missed some?</span></address><address><br /></address><address><span style=\"font-style: normal;\">Edit: I've addressed most of the concerns people had. I think I'm happy with it, at least for my purposes. If people are still concerned by the ending I'll revise it, but I think I've set it up better now.</span></address><address><span style=\"font-style: normal;\"><br /></span></address><address><span style=\"font-style: normal;\"><br /></span></address>\n<h2><strong>The Gift We Give Tomorrow</strong></h2>\n<address><span style=\"font-style: normal;\"><strong style=\"font-style: normal;\"><br /></strong> <address style=\"font-style: normal;\">How, oh how could the universe,<br />itself unloving, and mindless,<br />cough up creatures capable of love?</address><address style=\"font-style: normal;\"><br /><strong>No mystery in that.<br /></strong><strong>It's just a matter<br /></strong><strong>of natural selection.</strong></address><address style=\"font-style: normal;\"><strong></strong><br />But natural selection is cruel.&nbsp;Bloody.&nbsp;<br />And bloody stupid!</address><address style=\"font-style: normal;\"><br />Even when organisms <em>aren't</em>&nbsp;directly tearing at each other's throats&hellip;<br />&hellip;there's a deeper competition, going on between the genes.<br />A species could evolve to extinction,<br />if the winning genes were playing negative sum games</address><address style=\"font-style: normal;\"><br />How could a process,<br />Cruel as Azathoth,<br />Create minds that were capable of love?</address><address style=\"font-style: normal;\"><br /><strong>No mystery.</strong></address><address style=\"font-style: normal;\"><strong></strong><br /><strong>Mystery i</strong><strong>s a property of questions.<br /></strong><strong>Not answers.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>A mother's child shares her genes,<br /></strong><strong></strong></address><address style=\"font-style: normal;\"><strong>And so a mother loves her child.</strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">But mothers can adopt their children.<br />And still, come to love them.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Still no mystery.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Evolutionary psychology isn't about deliberately maximizing fitness.<br /></strong><strong>Through most of human history,&nbsp;<br /></strong><strong>we didn't know genes existed.<br /></strong><strong>Even subconsciously.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Well, fine. But still:<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Humans form friendships,<br />even with non-relatives.<br />How can <em>that</em> be?<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>No mystery.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Ancient hunter-gatherers would often play&nbsp;</strong><strong>the Iterated Prisoner's Dilemma.</strong></address><address style=\"font-style: normal;\"><strong>There could be profit in betrayal.<br /></strong><strong>But the best solution:<br /></strong><strong>was reciprocal altruism.</strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Sometimes,<br /></strong><strong>the most dangerous human is not the strongest,&nbsp;<br /></strong><strong>the prettiest,<br /></strong><strong>or even the smartest:<br /></strong><strong>But the one who has the most allies.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">But not all friends are fair-weather friends;&nbsp;<br />there are <em>true</em> friends -&nbsp;<br />those who would sacrifice their lives for another.<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Shouldn't that kind of devotion<br />remove itself from the gene pool?<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>You said it yourself:</strong><br /><strong>We have a concept of true friendship and fair-weather friendship.&nbsp;<br /></strong><strong>We wouldn't be true friends with someone who we didn't think was a true friend to us.</strong><br /><strong>And one with many true friends?<br /></strong><strong>They are far more formidable<br /></strong><strong>than one with mere fair-weather allies.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">And Mohandas Gandhi,&nbsp;<br />who really did turn the other cheek?&nbsp;<br />Those who try to serve all humanity,&nbsp;<br />whether or not all humanity serves them in turn?\\</address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>That&rsquo;s a more complex story.&nbsp;<br /></strong><strong>Humans aren&rsquo;t just social animals. </strong></address><address style=\"font-style: normal;\"><strong>We&rsquo;re political animals.<br />Sometimes the formidable human is not the strongest,&nbsp;<br /></strong><strong>but the one who skillfully argues that <em>their</em> preferred policies&nbsp;<br /></strong><strong>match the preferences of others.</strong></address><address style=\"font-style: normal;\"><strong></strong><br />Um... what?<br />How does that explain Gandhi?<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>The point is that we can argue about 'What should be done?'<br /></strong><strong>We can make those arguments and respond to them.<br /></strong><strong>Without that, politics couldn't take place.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Okay... but Gandhi?<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Believed certain complicated propositions about 'What should be done?'<br /></strong><strong>Then did them.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">That sounds suspiciously like it could explain <em>any</em> possible human behavior.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>If we traced back the chain of causality,<br /></strong><strong>through all the arguments...</strong><br /><strong>We'd find a moral architecture.<br /></strong><strong>The ability to argue abstract propositions.<br /></strong><strong>A preference for simple ideas.<br /></strong><strong>An appeal to hardwired intuitions about fairness.<br /></strong><strong>A concept of duty. Aversion to pain.<br /></strong><strong>Empathy.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Filtered by memetic selection,</strong></address><address style=\"font-style: normal;\"><strong>all of this resulted in a concept:<br /></strong><strong>\"You should not hurt people,\"<br /></strong><strong>In full generality.</strong></address><address style=\"font-style: normal;\"><strong></strong><br />And that gets you Gandhi.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>What else would you suggest?&nbsp;<br /></strong><strong>Some godlike figure?&nbsp;<br /></strong><strong>Reaching out from behind the scenes,<br /></strong><strong>directing evolution?</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Hell no. But -<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong style=\"font-style: normal;\">Because then I&rsquo;d would have to ask :<br /></strong><strong><span style=\"font-style: normal;\">How did that god </span>originally<span style=\"font-style: normal;\"> decide that love was even&nbsp;</span><em>desirable</em><span style=\"font-style: normal;\">.&nbsp;</span><br /></strong><strong style=\"font-style: normal;\">How it <em>got </em>preferences that included things like friendship, loyalty, and fairness.&nbsp;</strong><br /><span style=\"font-style: normal;\"><strong></strong></span></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Call it 'surprising' all you like.&nbsp;<br /></strong><strong>But through evolutionary psychology,&nbsp;<br /></strong><strong>You can see how parental love,&nbsp;</strong><strong>romance,&nbsp;</strong><strong>honor,</strong></address><address style=\"font-style: normal;\"><strong>even true altruism and moral arguments,&nbsp;<br /></strong><strong><em>all bear the specific design signature of natural selection</em>.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>If there were some benevolent god,&nbsp;</strong></address><address style=\"font-style: normal;\"><strong>reaching out to create a world of loving humans,<br /></strong><strong>it too must have evolved,<br /></strong><strong>defeating the point of postulating it at all.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">I'm not postulating a god!<br />I'm just asking how human beings ended up so <em>nice.</em><br /><strong><em></em></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong><em>Nice</em>?</strong></address><address style=\"font-style: normal;\"><strong>Have you <em>looked </em>at this planet lately?&nbsp;</strong></address><address style=\"font-style: normal;\"><strong>We bear all those other emotions that evolved as well.<br /></strong><strong>Which should make it very clear that we evolved, </strong></address><address style=\"font-style: normal;\"><strong>should you begin to doubt it.&nbsp;</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Humans aren't always nice.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">But, still, come on...&nbsp;<br />doesn't it seem a little...&nbsp;<br />amazing?<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">That nothing but millions of years of a cosmic death tournament&hellip;<br />could cough up mothers and fathers,&nbsp;<br />sisters and brothers,&nbsp;<br />husbands and wives,&nbsp;<br />steadfast friends,<br />honorable enemies,&nbsp;<br />true altruists and guardians of causes,&nbsp;<br />police officers and loyal defenders,&nbsp;<br />even artists, sacrificing themselves for their art?</address><address style=\"font-style: normal;\"><br />All practicing so many kinds of love?&nbsp;<br />For so many things other than genes?&nbsp;</address><address style=\"font-style: normal;\"><br />Doing their part to make their world less ugly,<br />something besides a sea of blood and violence and mindless replication?<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Are you honestly surprised by this?&nbsp;<br /></strong><strong>If so, question your underlying model.<br /></strong><strong>For it's led you to be surprised by the true state of affairs.&nbsp;</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Since the very beginning,&nbsp;<br /></strong><strong>not one unusual thing </strong></address><address style=\"font-style: normal;\"><strong>has ever happened.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">...<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">But how are you NOT amazed?<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Maybe there&rsquo;s no surprise from a causal viewpoint.&nbsp;</address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">But still, it seems to me,&nbsp;</address><address style=\"font-style: normal;\">in the creation of humans by evolution,&nbsp;<br />something happened that is precious and marvelous and wonderful.&nbsp;<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">If we can&rsquo;t call it a physical miracle, then call it a moral miracle.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Because it was only a miracle from the perspective of the morality that was produced?<br /></strong><strong>Explaining away all the apparent coincidence,<br /></strong><strong>from a causal and physical perspective?</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">Well... yeah. I suppose you could interpret it that way.<br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">I just meant that something was immensely surprising and wonderful on a moral level,<br />even if it's not really surprising,<br />on a physical level.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>I think that's what I said.</strong><br /></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\">It just seems to me that in your view, somehow you explain that wonder away.<br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>No.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>I explain it.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Of course there's a story behind love.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><strong>Behind all ordered events, one finds ordered stories.</strong></address><address style=\"font-style: normal;\"><strong>And that which has no story is nothing but random noise.<br /></strong><strong>Hardly any better.</strong></address><address style=\"font-style: normal;\"><strong><br /></strong></address><address style=\"font-style: normal;\"><strong>If you can't take joy in things with true stories behind them,<br /></strong><strong>your life will be empty.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Love has to begin somehow.<br /></strong><strong>It has to enter the universe somewhere.&nbsp;<br /></strong><strong>It&rsquo;s like asking how life itself begins.</strong></address><address style=\"font-style: normal;\"><strong>Though you were born of your father and mother,&nbsp;<br /></strong><strong>and though they arose from their living parents in turn,&nbsp;<br /></strong><strong>if you go far and far and far away back,&nbsp;<br /></strong><strong>you&rsquo;ll finally come to a replicator that arose by pure accident.</strong><br /><strong>The border between life and unlife.&nbsp;</strong><br /><strong>So too with love.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong style=\"font-style: normal;\">A complex pattern&nbsp;</strong><strong style=\"font-style: normal;\">must be explained by a cause&nbsp;<br /></strong><strong><span style=\"font-style: normal;\">that&rsquo;s </span>not<span style=\"font-style: normal;\"> already that complex pattern.&nbsp;</span></strong></address><address style=\"font-style: normal;\"><strong>For love to enter the universe,&nbsp;<br /></strong><strong>it has to arise from something that is not love.</strong></address><address style=\"font-style: normal;\"><strong>If that weren&rsquo;t possible, then love could not be.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Just as life itself required that first replicator,<br /></strong><strong>to come about by accident,&nbsp;<br /></strong><strong>parentless,<br /></strong><strong>but still caused:&nbsp;<br /></strong><strong>far, far back in the causal chain that led to you:&nbsp;<br /></strong><strong>3.8 billion years ago,&nbsp;<br /></strong><strong>in some little tidal pool.</strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>Perhaps your children's children will ask,</strong></address><address style=\"font-style: normal;\"><strong>how it is that they are capable of love.</strong><br /><strong>And their parents will say:<br /></strong><strong>Because we, who also love, created you to love.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>And your children's children may ask:&nbsp;</strong></address><address style=\"font-style: normal;\"><strong><span style=\"font-style: normal;\">But how is it that </span>you<span style=\"font-style: normal;\"> love?</span></strong><br /><span style=\"font-style: normal;\"><strong></strong></span></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>And their parents will reply:&nbsp;<br /></strong><strong>Because our own parents,&nbsp;<br /></strong><strong>who loved as well,&nbsp;<br /></strong><strong>created us to love in turn.</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>And then your children's children will ask:&nbsp;<br /></strong><strong>But where did it all begin?&nbsp;<br /></strong><strong>Where does the recursion end?</strong><br /><strong></strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong>And their parents will say:&nbsp;</strong></address><address style=\"font-style: normal;\"><br /></address><address style=\"font-style: normal;\"><strong style=\"font-style: italic;\">Once upon a time,&nbsp;</strong></address><address><address><strong>long ago and far away,<br /></strong><address><strong>there were intelligent beings who were not themselves intelligently designed.</strong></address><address><strong><br /></strong></address><address><address><strong>Once upon a time,&nbsp;</strong></address></address></address></address><address><strong>there were lovers,&nbsp;<br /></strong><strong>created by something that did not love.<br /></strong><strong></strong></address><address><br /></address><address><strong>Once upon a time,&nbsp;<br /></strong><strong>when all of civilization was a single galaxy,<br /></strong><strong>A single star.<br /></strong><strong>A single planet.<br /></strong><strong>A place called Earth.<br /></strong><strong></strong></address><address><br /></address><address><strong>Long ago,&nbsp;<br /></strong><strong>Far away,<br /></strong><strong>Ever So Long Ago.<br /></strong></address>\n<p>&nbsp;</p>\n<p><strong></strong></p>\n<p><strong></strong></p>\n<address style=\"font-style: normal;\"><strong></strong></address><address style=\"font-style: normal;\"><strong></strong></address><address style=\"font-style: normal;\"><strong></strong></address></span></address>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AXhEhCkTrHZbjXXu3": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bkRpALFAwJQuntHiF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 34, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "11238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 37, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pGvyqAQw6yqTjpKf4", "pLRogvJLPPg6Mrvg4", "sYgv4eYH82JEsTD34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T04:18:10.872Z", "modifiedAt": null, "url": null, "title": "[LINK] New experiment observes macroscopic quantum entaglement", "slug": "link-new-experiment-observes-macroscopic-quantum-entaglement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:50.047Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spurlock", "createdAt": "2010-03-24T17:13:19.572Z", "isAdmin": false, "displayName": "Spurlock"}, "userId": "mK7rKWbkuoDsm3aQb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7apbb8PjAtq3eFQHa/link-new-experiment-observes-macroscopic-quantum-entaglement", "pageUrlRelative": "/posts/7apbb8PjAtq3eFQHa/link-new-experiment-observes-macroscopic-quantum-entaglement", "linkUrl": "https://www.lesswrong.com/posts/7apbb8PjAtq3eFQHa/link-new-experiment-observes-macroscopic-quantum-entaglement", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20New%20experiment%20observes%20macroscopic%20quantum%20entaglement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20New%20experiment%20observes%20macroscopic%20quantum%20entaglement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7apbb8PjAtq3eFQHa%2Flink-new-experiment-observes-macroscopic-quantum-entaglement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20New%20experiment%20observes%20macroscopic%20quantum%20entaglement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7apbb8PjAtq3eFQHa%2Flink-new-experiment-observes-macroscopic-quantum-entaglement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7apbb8PjAtq3eFQHa%2Flink-new-experiment-observes-macroscopic-quantum-entaglement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 7, "htmlBody": "<p><span style=\"color: #ffffff; font-size: 11px;\"> </span></p>\n<p><a href=\"http://www.livescience.com/17264-quantum-entanglement-macroscopic-diamonds.html\">Two Diamonds Linked by Strange Quantum Entanglement</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7apbb8PjAtq3eFQHa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 8, "extendedScore": null, "score": 8.084557984437049e-07, "legacy": true, "legacyId": "11239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T05:58:02.986Z", "modifiedAt": null, "url": null, "title": "Probability puzzle: Coins in envelopes", "slug": "probability-puzzle-coins-in-envelopes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:41.334Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2RsJibdptru4F5v32/probability-puzzle-coins-in-envelopes", "pageUrlRelative": "/posts/2RsJibdptru4F5v32/probability-puzzle-coins-in-envelopes", "linkUrl": "https://www.lesswrong.com/posts/2RsJibdptru4F5v32/probability-puzzle-coins-in-envelopes", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Probability%20puzzle%3A%20Coins%20in%20envelopes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProbability%20puzzle%3A%20Coins%20in%20envelopes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RsJibdptru4F5v32%2Fprobability-puzzle-coins-in-envelopes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Probability%20puzzle%3A%20Coins%20in%20envelopes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RsJibdptru4F5v32%2Fprobability-puzzle-coins-in-envelopes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2RsJibdptru4F5v32%2Fprobability-puzzle-coins-in-envelopes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 263, "htmlBody": "<p>This went over well in the xkcd logic puzzle forum (<a href=\"http://www.xkcd.com/169/\">my hand was not removed</a>), so I thought I'd try it here. &nbsp;It came to me in a dream, so by solving it you may be helping to summon an elder god or something.</p>\n<p>&nbsp;</p>\n<p><span style=\"color: #323d4f; font-family: 'Lucida Grande', Verdana, Helvetica, Arial, sans-serif; font-size: 10px; \">\n<table class=\"tablebg\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; background-color: #a9b8c2; \" border=\"0\" cellspacing=\"1\" width=\"100%\">\n<tbody style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \">\n<tr class=\"row1\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 4px; padding-right: 4px; padding-bottom: 4px; padding-left: 4px; background-color: #ececec; \">\n<td style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 2px; padding-right: 2px; padding-bottom: 2px; padding-left: 2px; \" valign=\"top\">\n<table style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" border=\"0\" cellspacing=\"5\" width=\"100%\">\n<tbody style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \">\n<tr style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \">\n<td style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 2px; padding-right: 2px; padding-bottom: 2px; padding-left: 2px; \">\n<div class=\"postbody\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1.3em; line-height: 1.4em; font-family: 'Lucida Grande', 'Trebuchet MS', Helvetica, Arial, sans-serif; \">You're spying on Alice and Bob, both of whom are perfect mathematicians. Alice shows Bob a set of&nbsp;<span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-style: italic; \">m</span>&nbsp;envelopes. \"Each of these envelopes contains either a dollar coin or nothing,\" she says. \"I chose how many envelopes to fill randomly. What is the expected monetary value of the contents of a random envelope?\"&nbsp;<br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" /><br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" />Bob replies, \"That depends on what random function you used to choose how many envelopes to fill. If you, say, flipped&nbsp;<span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-style: italic; \">m</span>&nbsp;coins and put each one that came up heads in an envelope, the expected value is $.50.\"<br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" /><br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" />Alice explains what her random function was, and Bob calculates the expected value. For kicks, he pays her that amount, and she lets him pick a random envelope. It has a coin in it! Bob pockets the coin. Alice then takes the now-empty envelope back, and shuffles it into the others. \"Congratulations,\" she says. \"So, what's the expected value of playing the game again, now that there's one fewer coin?\"<br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" /><br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" />\"Same as before,\" Bob replies.<br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" /><br style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; \" /><span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-weight: bold; \">Problem 1:</span>&nbsp;Give a value for&nbsp;<span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-style: italic; \">m</span>&nbsp;and a random function for which this makes sense (there are many).</div>\n<div class=\"postbody\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-size: 1.3em; line-height: 1.4em; font-family: 'Lucida Grande', 'Trebuchet MS', Helvetica, Arial, sans-serif; \"><span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-weight: bold; \">Problem 2:</span>&nbsp;Suppose Alice chose the number of envelopes to fill by selecting an integer from 0 to&nbsp;<span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-style: italic; \">m</span>&nbsp;uniformly at random, and distributing that many coins among the envelopes. What is&nbsp;<span style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 0px; font-style: italic; \">m</span>?</div>\n</td>\n</tr>\n</tbody>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2RsJibdptru4F5v32", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 8.08491812797951e-07, "legacy": true, "legacyId": "11241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T05:58:49.605Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Effortless Technique", "slug": "seq-rerun-effortless-technique", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:39.204Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tJsZDZkT9EvoM6NyL/seq-rerun-effortless-technique", "pageUrlRelative": "/posts/tJsZDZkT9EvoM6NyL/seq-rerun-effortless-technique", "linkUrl": "https://www.lesswrong.com/posts/tJsZDZkT9EvoM6NyL/seq-rerun-effortless-technique", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Effortless%20Technique&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Effortless%20Technique%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJsZDZkT9EvoM6NyL%2Fseq-rerun-effortless-technique%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Effortless%20Technique%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJsZDZkT9EvoM6NyL%2Fseq-rerun-effortless-technique", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtJsZDZkT9EvoM6NyL%2Fseq-rerun-effortless-technique", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<p>Today's post, <a href=\"/lw/m6/effortless_technique/\">Effortless Technique</a> was originally published on 23 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Things like the amount of effort put into a project, or the number of lines in a computer program, are positive things to maximize. But this is silly. Surely it is better to accomplish the same task with fewer lines of code.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8no/seq_rerun_false_laughter/\">False Laughter</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tJsZDZkT9EvoM6NyL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.084920929911362e-07, "legacy": true, "legacyId": "11242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Eiw6fea93DhmGEBux", "witNDfZPFFYTBNPpJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T06:01:13.230Z", "modifiedAt": null, "url": null, "title": "Announcing the Quantified Health Prize", "slug": "announcing-the-quantified-health-prize", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:55.822Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Zvi", "createdAt": "2009-03-31T20:54:54.077Z", "isAdmin": false, "displayName": "Zvi"}, "userId": "N9zj5qpTfqmbn9dro", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y9iXchCMdYLsPNzZM/announcing-the-quantified-health-prize", "pageUrlRelative": "/posts/Y9iXchCMdYLsPNzZM/announcing-the-quantified-health-prize", "linkUrl": "https://www.lesswrong.com/posts/Y9iXchCMdYLsPNzZM/announcing-the-quantified-health-prize", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Announcing%20the%20Quantified%20Health%20Prize&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnnouncing%20the%20Quantified%20Health%20Prize%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY9iXchCMdYLsPNzZM%2Fannouncing-the-quantified-health-prize%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Announcing%20the%20Quantified%20Health%20Prize%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY9iXchCMdYLsPNzZM%2Fannouncing-the-quantified-health-prize", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY9iXchCMdYLsPNzZM%2Fannouncing-the-quantified-health-prize", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 435, "htmlBody": "<div id=\"magicdomid49\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">We are giving away a $5000 prize for well-researched, well-reasoned presentations that answer the </span><span class=\"author-g-xz122zpsidcpt2artm0e b\"><strong>following question</strong></span><span class=\"author-g-xz122zpsidcpt2artm0e\">:</span></div>\n<div id=\"magicdomid51\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e b\"><strong>What are the best recommendations for what quantities adults (ages 20-60) should take the important dietary minerals in, and what are the costs and benefits of various amounts</strong></span><span class=\"author-g-xz122zpsidcpt2artm0e\">?&nbsp;</span></div>\n<div id=\"magicdomid52\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">Part of the question is figuring out which ones are important. You may exclude any minerals for which an otherwise reasonable diet will always fall into the right range, or any minerals whose effects are relatively trivial.&nbsp;</span></div>\n<div id=\"magicdomid53\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">If you have an excellent entry, even if you don&rsquo;t win the grand prize, you can still win one of four additional cash prizes, you&rsquo;ll be under consideration for a job as a researcher with our company Personalized Medicine, and you&rsquo;ll get a leg up in the larger contest we plan to run after this one. You also get to help people get better nutrition and stay healthier.&nbsp;</span></div>\n<div id=\"magicdomid54\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">Proposal:</span></div>\n<div id=\"magicdomid55\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">Most of us spend a good portion of our time and money trying to figure out what would be best for our health, and then trying to implement those findings. We ask ourselves how to eat, how to exercise, what drugs and supplements to take and what treatments to seek, but everywhere we turn we find different opinions. Even if one reads the primary research, one finds studies riddled with problems. Most studies have an agenda to sell a product or prove a pet theory. They are then filtered by publication bias.&nbsp; When results are presented, many authors use framing to steer us to the conclusions they want us to draw.</span></div>\n<div id=\"magicdomid56\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">We can and must do better.</span></div>\n<div id=\"magicdomid795\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">We hereby challenge this community to do better.&nbsp; We're always saying how great and effective rationality is. This is our chance to prove it, and &nbsp;<a href=\"/lw/2s/3_levels_of_rationality_verification/\">put those skills to the test</a>. These problems badly need proper application of Less Wrong's favorite techniques, from Bayes' Theorem itself to the correction of a whole host of cogn</span><span class=\"author-g-8p8hs1g17758ybw9\">i</span><span class=\"author-g-xz122zpsidcpt2artm0e\">tive biases.</span></div>\n<div class=\"ace-line\">\n<div id=\"magicdomid59\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">This contest is also a pilot for a larger contest; before we go and put a lot more money on the line and ask more questions, we want a chance to work the kinks out.&nbsp;</span></div>\n<div id=\"magicdomid60\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">Entries are due by the end of day on January 15, 2012. This is a change of the original deadline, but it will not change again and it will be strictly enforced.</span></div>\n<div id=\"magicdomid61\" class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">Final judgment will be made by Personalized Medicine&rsquo;s Chief Science Officer, based on finalists chosen by our expert reviewers. If necessary, Peer Review will first be used to reduce the number of entries to a manageable size.&nbsp;</span></div>\n<div class=\"ace-line\"><span class=\"author-g-xz122zpsidcpt2artm0e\">The contest page can be found &nbsp;<a title=\"Contest Main Page\" href=\"http://www.medicineispersonal.com/contest/home\">here</a>, and the FAQ can be found &nbsp;<a title=\"Contest FAQ\" href=\"http://www.medicineispersonal.com/contest/faq\">here</a>.</span></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"khReijeucXJTnsyMT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y9iXchCMdYLsPNzZM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": 73, "extendedScore": null, "score": 0.00018, "legacy": true, "legacyId": "11229", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 79, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 144, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5K7CMa6dEL7TN7sae"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T06:01:27.343Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes December 2011", "slug": "rationality-quotes-december-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:34.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iQ4cpXc4tNW2oA9CK/rationality-quotes-december-2011", "pageUrlRelative": "/posts/iQ4cpXc4tNW2oA9CK/rationality-quotes-december-2011", "linkUrl": "https://www.lesswrong.com/posts/iQ4cpXc4tNW2oA9CK/rationality-quotes-december-2011", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20December%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20December%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQ4cpXc4tNW2oA9CK%2Frationality-quotes-december-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20December%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQ4cpXc4tNW2oA9CK%2Frationality-quotes-december-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiQ4cpXc4tNW2oA9CK%2Frationality-quotes-december-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p style=\"margin: 0px 0px 1em;\">Here's the new thread for posting quotes, with the usual rules:</p>\n<ul style=\"padding: 0px;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp;&nbsp;(If they are strongly related, reply to your own comments.&nbsp;&nbsp;If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iQ4cpXc4tNW2oA9CK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 8.084930410403602e-07, "legacy": true, "legacyId": "11205", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 586, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}